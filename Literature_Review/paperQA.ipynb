{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperQA - A Question Answering Dataset for Academic Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "!pip install paper-qa\n",
        "#!pip install openai==1.7.2\n",
        "!pip install openai==0.28\n",
        "!pip install langchain==0.1.1\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "!pip install sentence-transformers\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "# Configuration\n",
        "#output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "output_folder_json = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\" \n",
        "output_folder_pdf = \"/home/epas/Documents/MitoMAVEN/full_texts/\" \n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, embeddings=embeddings)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    process_json_files(output_folder_json)\n",
        "    process_research_papers(output_folder_pdf)\n",
        "    questions = read_questions(questions_file_path)\n",
        "    responses = [docs.query(question) for question in questions]\n",
        "    write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall openai -y\n",
        "!pip uninstall paperqa -y\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "def parse_qa_responses(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    responses = []\n",
        "    response = {}\n",
        "    references_lines = []\n",
        "    capturing_references = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('Question:'):\n",
        "            if response:  # Add the previous response with its references to the list\n",
        "                response['references'] = ''.join(references_lines).strip()\n",
        "                responses.append(response)\n",
        "                references_lines = []\n",
        "            # Handling split operation\n",
        "            split_line = line.split('    ')\n",
        "            if len(split_line) >= 2:\n",
        "                response = {'question': split_line[1].strip(), 'answer': '', 'references': ''}\n",
        "            else:\n",
        "                response = {'question': '', 'answer': '', 'references': ''}\n",
        "            capturing_references = False\n",
        "        elif 'I cannot answer' in line.strip() or 'The provided context does not contain' in line.strip():\n",
        "            response['answer'] = line.strip()\n",
        "        elif line.strip().startswith('References'):\n",
        "            capturing_references = True\n",
        "        elif capturing_references:\n",
        "            references_lines.append(line)\n",
        "    \n",
        "    if response:  # Add the last response with its references to the list\n",
        "        response['references'] = ''.join(references_lines).strip()\n",
        "        responses.append(response)\n",
        "\n",
        "    # Add timestamp and extract URLs from references\n",
        "    for response in responses:\n",
        "        response[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if response['references']:\n",
        "            response['references_urls'] = extract_urls(response['references'])\n",
        "    return responses\n",
        "\n",
        "\n",
        "# You would then continue with your original code for saving the JSON.\n",
        "\n",
        "\n",
        "def save_json_append(responses, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    combined_data = existing_data + responses\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "file_path =\"responses_file.txt\"\n",
        "output_json_file = 'structured_responses.json'\n",
        "\n",
        "responses = parse_qa_responses(file_path)\n",
        "save_json_append(responses, output_json_file)\n",
        "\n",
        "print(f\"Processed responses are saved in JSON format to {output_json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "laptop= \"/home/epas/Documents/docs.pickle\"\n",
        "desktop = \"/Users/tomriddle1/Documents/GitHub/docs.pickle\"\n",
        "# save\n",
        "with open(desktop, \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(desktop, \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Generate Search Queries\":\n",
        "        with open(\"generate_search_queries.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pydantic==2.0.3\n",
        "!pip install instructor\n",
        "#\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "!pip install openai==0.28\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import json\n",
        "#import instructor\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "import openai\n",
        "\n",
        "\n",
        "def generate_search_queries(question: str):\n",
        "    prompt = build_system_prompt(\"Generate Search Queries\")\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "\n",
        "    # Initialize the OpenAI client without explicitly passing the API key\n",
        "    #client = OpenAI(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        response =  openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": \"Search Queries:\"},\n",
        "            ]\n",
        "        )\n",
        "        return parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_unanswered_questions(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    unanswered_questions = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Checking for phrases that indicate an unanswered question\n",
        "        if \"cannot answer\" in entry[\"answer\"] or \"does not contain\" in entry[\"answer\"] or \"no answer\" in entry[\"answer\"] or \"no results\" in entry[\"answer\"] or \"no information\" in entry[\"answer\"]:\n",
        "            unanswered = True\n",
        "        else:\n",
        "            unanswered = False\n",
        "\n",
        "        # Building the result entry\n",
        "        result_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"answerable\": not unanswered,\n",
        "            \"timestamp\": entry.get(\"timestamp\", \"Unknown timestamp\")\n",
        "        }\n",
        "\n",
        "        if entry.get(\"references\"):\n",
        "            result_entry[\"references\"] = entry[\"references\"]\n",
        "\n",
        "        if entry.get(\"references_urls\"):  # Using .get to avoid KeyError\n",
        "            result_entry[\"references_urls\"] = entry[\"references_urls\"]\n",
        "\n",
        "        unanswered_questions.append(result_entry)\n",
        "\n",
        "    return unanswered_questions\n",
        "\n",
        "json_file = 'structured_responses.json'\n",
        "unanswered_questions = check_unanswered_questions(json_file)\n",
        "\n",
        "# Display the results\n",
        "for item in unanswered_questions:\n",
        "    print(f\"Question: {item['question']}\\nAnswerable: {item['answerable']}\\n\")\n",
        "    if item.get(\"references\") and item.get(\"references_urls\"):\n",
        "        print(f\"Url(s): {item['references_urls']}\\n\")\n",
        "        #print(f\"Reference(s): {item['references']}\\n\")\n",
        "    # Generate search queries for unanswered questions\n",
        "    if item['answerable'] == False:\n",
        "        if item.get(\"references\"):\n",
        "            search_queries = generate_search_queries(f\"{item['question']}\\n{item['references']}\")\n",
        "        else:\n",
        "            search_queries = generate_search_queries(item[\"question\"])\n",
        "        # save search queries to json file\n",
        "        with open(\"search_queries.json\", \"a\") as outfile:\n",
        "            json.dump(search_queries, outfile)\n",
        "        print(f\"Search Queries: {search_queries}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(api_key= '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')\n",
        "\n",
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": \"You are a kind helpfull assistant\"}\n",
        "]\n",
        "\n",
        "while True:\n",
        "    message = input(\"user:\")\n",
        "    if message:\n",
        "        messages.append(\n",
        "            {\"role\":\"user\", \"content\": message},\n",
        "        )\n",
        "        completion = client.chat.completions.create(\n",
        "            messages = messages,\n",
        "            model = \"gpt-3.5-turbo\"\n",
        "        )\n",
        "\n",
        "    reply = completion.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply}\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install paper-qa\n",
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('/home/epas/Programming/ResearchAgentSwarm/Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    #docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "    print(paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=1,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Research Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Function to clean entities based on new lines and remove leading hyphens\n",
        "def clean_and_separate_entities(entities_list):\n",
        "    entities_str = '\\n'.join(entities_list)\n",
        "    cleaned_entities = []\n",
        "    dirty_entities = []\n",
        "\n",
        "    for line in entities_str.split('\\n'):\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line.startswith('-'):\n",
        "            # Remove the leading hyphen and any extra space after it\n",
        "            cleaned_entities.append(stripped_line.lstrip('-').strip())\n",
        "        else:\n",
        "            dirty_entities.append(stripped_line)\n",
        "\n",
        "    return cleaned_entities, dirty_entities\n",
        "def test_clean_and_separate_entities():\n",
        "    \n",
        "    # Define the summary JSON file path\n",
        "    SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "    # Read the summaries.json file\n",
        "    with open(SUMMARY_JSON, \"r\") as file:\n",
        "        summaries_json = json.load(file)\n",
        "\n",
        "    # Extract the first entities entry\n",
        "    first_entities_list = summaries_json[0][\"entities\"][0]\n",
        "\n",
        "    # Clean the entities and separate the uncleaned ones\n",
        "    cleaned_entities, dirty_entities = clean_and_separate_entities(first_entities_list)\n",
        "\n",
        "    # Save the results to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp_file:\n",
        "        json.dump({\n",
        "            \"cleaned_entities\": cleaned_entities,\n",
        "            \"dirty_entities\": dirty_entities\n",
        "        }, temp_file, indent=4)\n",
        "\n",
        "    print(\"Results saved in:\", temp_file.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_topics_with_justification(topic_text):\n",
        "    # Regular expression pattern for identifying topics with their justifications\n",
        "    topic_pattern = re.compile(r'(\\d+)\\.\\s+([^\\n]+)(\\n\\s+-[^\\n]+)*')\n",
        "    topics = topic_pattern.findall(topic_text)\n",
        "    \n",
        "    extracted_topics = []\n",
        "    for match in topics:\n",
        "        topic = match[1].strip()\n",
        "        justification = ' '.join(match[2].split('\\n')).strip()\n",
        "        # Remove \"Justification:\" if it starts with it\n",
        "        if justification.lower().startswith('- justification:'):\n",
        "            justification = justification[len('- justification:'):].strip()\n",
        "        # Remove the - if it starts with it\n",
        "        if justification.startswith('-'):\n",
        "            justification = justification[1:].strip()\n",
        "        extracted_topics.append({\"topic\": topic, \"justification\": justification})\n",
        "\n",
        "    return extracted_topics\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_topics_with_justification():\n",
        "    # Adjusted topic text\n",
        "    topic_text_list = []\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of Mitochondria in Energy Production, Signaling, and Apoptosis\\n   - Mitochondria as the powerhouse of the cell\\n   - Role of mitochondria in energy production, signaling, and apoptosis\\n   - Significance of studying mitochondrial function and involvement in diseases\\n\\n2. Challenges with Traditional Methods of Mitochondrial Isolation\\n   - Limitations of traditional methods like differential centrifugation\\n   - Potential damage to mitochondrial double membrane and variable viability\\n\\n3. Innovative Techniques for Mitochondrial Isolation\\n   - Nitrogen cavitation for gentle disruption and release of intact mitochondria\\n   - Affinity purification using anti-TOM22 magnetic beads for efficient isolation\\n   - Filtration-based methods to reduce isolation time and improve viability\\n   - Differential isopycnic density gradient centrifugation for separation based on buoyant density\\n\\n4. Quality Control Measures for Validating Mitochondrial Isolation\\n   - Assessment of mitochondrial respiration, metabolic activity, protein import, and membrane fusion\\n   - High-resolution respirometry and bioluminescent measurements of ATP synthesis\\n\\n5. Importance of Continued Refinement and Standardization of Techniques\\n   - Advancing understanding of mitochondrial biology and implications in health and disease\\n   - Need for standardized protocols to facilitate comparisons and translation of research findings into clinical applications\\n\\n**Notes**: The summary provides a comprehensive overview of the importance of mitochondria, challenges with traditional methods of isolation, innovative techniques for isolation, quality control measures, and the need for continued refinement and standardization. The topics cover the main ideas and themes discussed in the summary, providing a clear and comprehensive analysis of the content.\") \n",
        "    topic_text_list.append(\"**Topic List:**\\n\\n1. Challenges in isolating intact mitochondria from plant cells\\n   - Cell walls, mitochondrial membranes, and large amounts of starting material\\n2. Comprehensive protocol for isolating intact mitochondria from plant cells\\n   - Grinding, filtering, centrifuging, and resuspending\\n3. Characterization and analysis of isolated mitochondria\\n   - Purity, integrity, and functionality assessment\\n   - Techniques: protein profiling, enzymatic activity assays, respiratory chain measurements, and oxygen consumption analysis\\n4. Storage of purified mitochondria\\n   - Long-term storage at -80°C\\n5. Adaptation of isolation process for different tissue types and plant species\\n   - Consideration of phenolic compounds and metabolite profiles\\n6. Validation and controls for quality and functionality assurance\\n7. Downstream applications of isolated mitochondria\\n   - Protein and tRNA uptake experiments, enzyme activity assays, Western blot analyses, and mass spectrometry analyses\\n\\n**Notes:**\\n- The revised summary provides a comprehensive overview of the topic, covering various aspects of isolating intact mitochondria from plant cells.\\n- The topics are specific and non-repetitive, ensuring a clear and distinct representation of the core themes.\\n- The summary is focused on the technical process and considerations involved in isolating mitochondria, as well as the analysis and applications of the isolated mitochondria.\")\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction\\n    - Justification: The summary highlights the crucial role of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction.\\n\\n2. Significance of gentle and effective mitochondrial isolation techniques\\n    - Justification: The summary emphasizes the importance of gentle and effective isolation techniques for studying mitochondrial biology and developing mitochondrial-based therapies.\\n\\n3. Overview of macroscale mitochondrial isolation techniques\\n    - Justification: The summary discusses macroscale mitochondrial isolation techniques, such as manual homogenization and differential filtration-based isolation.\\n\\n4. Advancements in microscale and nanoscale mitochondrial isolation techniques\\n    - Justification: The summary mentions microscale and nanoscale techniques, including microfluidic techniques and nanoprobe-based technologies, for mitochondrial isolation.\\n\\n5. Breakthroughs in sub-cellular isolation techniques for mitochondria\\n    - Justification: The summary highlights breakthroughs in sub-cellular isolation techniques that enable the isolation of mitochondria from subcellular compartments with minimal disruption.\\n\\n6. Challenges in mitochondrial isolation techniques\\n    - Justification: The summary mentions challenges such as the presence of whole cell contaminants in mitochondrial isolates and the time sensitivity of isolated mitochondria.\\n\\n7. Emerging therapeutic approach: Autologous mitochondrial transplants\\n    - Justification: The summary discusses the development of autologous mitochondrial transplants as an emerging therapeutic approach.\\n\\n8. Contributions of the London Centre for Nanotechnology and the McCully laboratory\\n    - Justification: The summary mentions the significant contributions of the London Centre for Nanotechnology and the McCully laboratory in optimizing differential filtration-based mitochondrial isolation for use in cellular models.\\n\\n9. Role of Stem Cell Research & Therapy in advancing mitochondrial medicine\\n    - Justification: The summary highlights the role of Stem Cell Research & Therapy in providing in-depth overviews of advancements in mitochondrial research and facilitating the development of novel therapies for mitochondrial diseases.\")\n",
        "    topic_text_list.append(\"Topics:\\n1. Genetic modifications to enhance mitochondrial autonomy\\n   - Justification: The main focus of the report is exploring genetic modifications to enhance the autonomy of mitochondria from nuclear-encoded proteins and functions.\\n2. Role of mitochondria in cellular function\\n   - Justification: The report highlights the crucial role played by mitochondria in cellular function.\\n3. Coordination between mtDNA and nuclear DNA\\n   - Justification: The report discusses the coordination required between mtDNA and nuclear DNA, as most proteins are encoded by nuclear DNA.\\n4. Therapeutic strategies for mitochondrial diseases\\n   - Justification: The report mentions that enhancing mitochondrial autonomy could lead to new therapeutic strategies for mitochondrial diseases.\\n5. Research on genome engineering, programmable nucleases, and base editors\\n   - Justification: The report mentions that recent research in genome engineering, programmable nucleases, and base editors shows promise for treating hereditary mitochondrial diseases.\\n6. Challenges in genetic manipulation of mtDNA\\n   - Justification: The report discusses challenges such as mtDNA mutations, resistance to genetic manipulation, and limitations in mtDNA recombination.\\n7. Advancements in protein-only gene editing platforms\\n   - Justification: The report mentions advancements in protein-only gene editing platforms as potential solutions to the challenges in genetic manipulation of mtDNA.\\n8. Somatic mitochondrial DNA-replaced cells\\n   - Justification: The report mentions the generation of somatic mitochondrial DNA-replaced cells as a potential solution to the challenges in genetic manipulation of mtDNA.\\n9. Mitochondrial nucleoids and their role in maintaining genetic autonomy\\n   - Justification: The report highlights the concept of mitochondrial nucleoids and their role in maintaining genetic autonomy as a key area of study.\\n10. Mitochondrial epigenomics and gene expression regulation\\n    - Justification: The report emphasizes the importance of understanding mitochondrial epigenomics and gene expression regulation in different cellular contexts, including stress conditions, for identifying genetic modifications that could enhance mitochondrial autonomy.\")\n",
        "    for topic_text in topic_text_list:\n",
        "        extracted_topics = extract_topics_with_justification(topic_text)\n",
        "        print(f'Extracted topics: {extracted_topics}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor url in urls:\\n    try:\\n        pdf =  pdfx.PDFx(url)\\n        metadata = pdf.get_metadata()\\n        print(f\\'Metadata: {metadata}\\')\\n        references_list = pdf.get_references()\\n        print(f\\'References: {references_list}\\')\\n        references_dict = pdf.get_references_as_dict()\\n        print(f\\'References dict: {references_dict}\\')\\n        papers = paperscraper.link_to_pdf(url, pdir=\\'downloaded-papers\\')\\n        print(f\\'Papers: {papers}\\')\\n    except:\\n        print(\"Error in extracting references\")\\n        continue\\n#pdf.download_pdfs(\"target-directory\")\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#!pip install pdfx\n",
        "import pdfx\n",
        "#!pip install paperscraper\n",
        "#import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def test_extract_urls():\n",
        "    # Define the reference text\n",
        "    reference_text = \"\"\"\\n\\nAmerican Institute of Physics. (2023). The powerhouse of the future: Artificial cells. Phys.org. Retrieved from https://phys.org/news/2023-03-powerhouse-future-artificial-cells.html\\n\\nNational Institutes of Health. (2023). Artificial mitochondria transfer (AMT) and transplant. PMC. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5511681/\\n\\nNature. (2023). Spatiotemporal simulations of mitochondrial dynamics. Nature.com. Retrieved from https://www.nature.com/articles/s41598-019-54159-1\\n\\nSogang University & Harbin Institute of Technology. (2023). Artificial organelles for sustainable chemical energy conversion and production: Artificial mitochondria and chloroplasts. Biophysics Reviews. Retrieved from https://publishing.aip.org/publications/latest-content/the-powerhouse-of-the-future-artificial-cells/\"\"\"\n",
        "\n",
        "    urls = extract_urls(reference_text)\n",
        "    print(f'Extracted URLs: {urls}')\n",
        "\n",
        "#pdf = pdfx.PDFx(\"filename-or-url.pdf\")\n",
        "#urls = ['/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/2308.00352.pdf']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for url in urls:\n",
        "    try:\n",
        "        pdf =  pdfx.PDFx(url)\n",
        "        metadata = pdf.get_metadata()\n",
        "        print(f'Metadata: {metadata}')\n",
        "        references_list = pdf.get_references()\n",
        "        print(f'References: {references_list}')\n",
        "        references_dict = pdf.get_references_as_dict()\n",
        "        print(f'References dict: {references_dict}')\n",
        "        papers = paperscraper.link_to_pdf(url, pdir='downloaded-papers')\n",
        "        print(f'Papers: {papers}')\n",
        "    except:\n",
        "        print(\"Error in extracting references\")\n",
        "        continue\n",
        "#pdf.download_pdfs(\"target-directory\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted hypothetical questions: [{'question_type': 'Content-Based Question', 'question': 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'}, {'question_type': 'Analytical Question', 'question': 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'}, {'question_type': 'User-Interactive Question', 'question': 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?'}]\n",
            "Extracted hypothetical questions: [{'question_type': 'Analytical Question', 'question': 'How do theoretical models help in understanding mitochondrial ATP production?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'}, {'question_type': 'User-Interactive Question', 'question': 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_hypothetical_questions(hypothetical_questions_text):\n",
        "    # Regular expression pattern for identifying hypothetical questions\n",
        "    question_pattern = re.compile(r'\\d+\\.\\s+([A-Za-z\\/-]+ Question):\\n\\s+-\\s+([^\\n]+)')\n",
        "    questions = question_pattern.findall(hypothetical_questions_text)\n",
        "    #print(f'Questions: {questions}')\n",
        "    if len(questions) == 0:\n",
        "        return hypothetical_questions_text\n",
        "    return [{\"question_type\": question_type, \"question\": question} for question_type, question in questions]\n",
        "def test_extract_hypothetical_questions():\n",
        "    # Example hypothetical questions text\n",
        "    hypothetical_questions_text_1 = \"1. Content-Based Question:\\n   - How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?\\n\\n2. Analytical Question:\\n   - What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?\\n\\n5. User-Interactive Question:\\n   - What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?\"\n",
        "    hypothetical_questions_text_2 = \"1. Content-Based Question: \\n   - What does this report investigate regarding mitochondrial ATP production?\\n   - How does this report contribute to our understanding of mitochondrial function?\\n   - What are the key findings regarding the replication of mitochondrial ATP production outside the cellular environment?\\n\\n2. Analytical Question:\\n   - How do theoretical models help in understanding mitochondrial ATP production?\\n   - What experimental evidence supports the concept of artificial organelles for ATP synthesis?\\n   - What are the implications of studying mitochondrial dynamics and stress responses for ex vivo methods of ATP synthesis?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?\\n   - If artificial organelles capable of ATP synthesis were successfully developed, what potential applications could they have in various industries?\\n   - How might the understanding of mitochondrial dynamics and stress responses lead to the development of innovative approaches for ATP synthesis?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?\\n   - In what ways does the replication of mitochondrial ATP production outside cells build upon previous studies in the field?\\n   - How do the findings in this report align with or challenge existing theories and models of mitochondrial function?\\n\\n5. User-Interactive Question:\\n   - How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?\\n   - Can you think of any potential limitations or ethical considerations in developing artificial organelles for ATP synthesis?\\n   - What questions or areas of research would you like to see explored further in the study of mitochondrial dynamics and stress responses?\"\n",
        "    hypothetical_questions = []\n",
        "    hypothetical_questions.append(hypothetical_questions_text_1)\n",
        "    hypothetical_questions.append(hypothetical_questions_text_2)\n",
        "    for hypothetical_questions_text in hypothetical_questions:\n",
        "        extracted_hypothetical_questions = extract_hypothetical_questions(hypothetical_questions_text)\n",
        "        print(f'Extracted hypothetical questions: {extracted_hypothetical_questions}')\n",
        "\n",
        "test_extract_hypothetical_questions()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'tailored isolation protocol'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimized damage to ensure the integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_entity_relationships(entity_relationships_text):\n",
        "    # Regular expression pattern for identifying entity relationships\n",
        "    entity_pattern = re.compile(r'\\d+\\.\\s+\\((.+?),\\s+(.+?),\\s+(.+?)\\)')\n",
        "    entity_relationships = entity_pattern.findall(entity_relationships_text)\n",
        "    return [{\"subject\": relationship[0], \"relationship\": relationship[1], \"target\": relationship[2]} for relationship in entity_relationships]\n",
        "\n",
        "# Example entity relationships text\n",
        "entity_relationships_text =  \"Entity Relationships:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n30. (mitochondria, isolated with, tailored isolation protocol)\\n31. (mitochondria, isolated with, minimized damage to ensure the integrity)\\n32. (mitochondria, isolated with, reduced contamination from other organelles)\\n33. (mitochondria, isolated with, improved methods)\\n34. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n35. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n36. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n37. (mitochondria, isolated with, minimal contamination from other organelles)\\n38. (mitochondria, isolated with, improved methods)\\n39. (mitochondria, isolated with, slight modifications)\\n40. (mitochondria, isolated with, traditional plant protoplast isolation)\\n41. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n42. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n43. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n44. (mitochondria, isolated with, minimal contamination from other organelles)\\n45. (mitochondria, isolated from, Arabidopsis thaliana)\\n46. (mitochondria, isolated using, continuous colloidal density gradients)\\n47. (mitochondria, isolated at, 4 °C)\\n48. (mitochondria, used for, protein and tRNA uptake experiments)\\n49. (mitochondria, used for, enzyme activity assays)\\n50. (mitochondria, used for, western blot analyses)\\n51. (mitochondria, used for, mass spectrometry analyses)\\n52. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n53. (mitochondria, assessed for, purity and integrity)\\n54. (mitochondria, assessed using, proteinase digestion assays)\\n55. (mitochondria, assessed using, electron microscopy)\\n56. (mitochondria, assessedThe article discusses the protocol for isolating mitochondria from plant cells. Mitochondria are double-membraned organelles responsible for energy production in eukaryotic cells. The isolation of mitochondria is crucial for various studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays.\\n\\nThe isolation process is challenging due to the presence of cell walls, vacuoles, and secondary metabolites in plant cells. The protocol must be tailored to minimize damage to the mitochondria and ensure their integrity. Specificity in isolation protocols is required as different plant species and tissue types have varying phenolic compounds and metabolite profiles. Earlier methods led to contamination with nuclei and chloroplasts, but recent advancements have improved isolation methods, reducing the need for heavy labor, expensive equipment, and large amounts of starting material.\\n\\nThe protocol for isolating intact mitochondria involves several steps. First, the preparation of grinding medium, wash buffer, and gradient solutions is necessary. The plant material is then homogenized in the grinding medium to release the mitochondria, which are then filtered and centrifuged to pellet the mitochondria. The mitochondrial pellet is resuspended in the wash buffer. Oxygen consumption measurements are crucial for determining the intactness and functional capacity of the isolated mitochondria. Evaluation of mitochondrial purity and integrity can be done through proteinase digestion assays, electron microscopy, and checks of mitochondrial membrane potential and electron transport chain activity.\\n\\nOnce purified, the isolated mitochondria can be used for various studies, including protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses. For mass spectrometry analyses, targeted multiple reaction monitoring (MRM) or quantification by dimethyl or other isotope labels can be employed.\\n\\nIn conclusion, the isolation of mitochondria from plant cells is a delicate process that requires careful consideration of the specific requirements of the plant species and tissue type. Recent advancements have made the process more effective and accessible for a range of tissue types and species, allowing for a broader application of mitochondrial studies across different plant species.\\n\\nReferences:\\n- Plant Methods. (2015). https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0099-x\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908444/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7640673/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687074/Extraction and Categorization:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, used for, protein and tRNA uptake experiments)\\n30. (mitochondria, used for, enzyme activity assays)\\n31. (mitochondria, used for, western blot analyses)\\n32. (mitochondria, used for, mass spectrometry analyses)\\n33. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n34. (mitochondria, assessed for, purity and integrity)\\n35. (mitochondria, assessed using, proteinase digestion assays)\\n36. (mitochondria, assessed using, electron microscopy)\\n37. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n38. (mitochondria, assessed using, electron transport chain activity measurement)\\n39. (mitochondria, isolated from, Arabidopsis thaliana)\\n40. (mitochondria, isolated using, continuous colloidal density gradients)\\n41. (mitochondria, isolated at, 4 °C)\"\n",
        "\n",
        "# Clean the entity relationships\n",
        "cleaned_entity_relationships = clean_entity_relationships(entity_relationships_text)\n",
        "\n",
        "# Output the cleaned entity relationships\n",
        "cleaned_entity_relationships\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pydantic 2.6.1\n",
            "Uninstalling pydantic-2.6.1:\n",
            "  Successfully uninstalled pydantic-2.6.1\n",
            "Found existing installation: instructor 0.5.2\n",
            "Uninstalling instructor-0.5.2:\n",
            "  Successfully uninstalled instructor-0.5.2\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Using cached pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
            "Installing collected packages: pydantic\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "agency-swarm 0.1.0 requires instructor==0.3.4, which is not installed.\n",
            "paper-qa 3.13.4 requires openai<1, but you have openai 1.8.0 which is incompatible.\n",
            "paper-qa 3.13.4 requires pydantic<2, but you have pydantic 2.6.1 which is incompatible.\n",
            "agency-swarm 0.1.0 requires openai==1.3.0, but you have openai 1.8.0 which is incompatible.\n",
            "llama-index 0.8.62 requires openai<1, but you have openai 1.8.0 which is incompatible.\n",
            "pymemgpt 0.1.18 requires openai<0.29.0,>=0.28.1, but you have openai 1.8.0 which is incompatible.\n",
            "pyautogen 0.1.14 requires openai<1, but you have openai 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.6.1\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'instructor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'instructor'"
          ]
        }
      ],
      "source": [
        "!pip uninstall pydantic -y\n",
        "!pip uninstall instructor -y\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.5.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.8.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.6.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (8.2.3)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.txt\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge, protocol, organisms):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"mitochondria\": {\"protocol\": protocol, \"organisms\": organisms},\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Isolation Protocol\":\n",
        "        with open(\"get_isolation_protocol.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Organisms\":\n",
        "        with open(\"get_organisms.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0613\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Isolation Protocol\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Isolation Protocol\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Organisms\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Organisms\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0613\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    while True:\n",
        "        topic = request_topics(summary)\n",
        "        if topic:\n",
        "            break\n",
        "    while True:\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "        if questions:\n",
        "            break\n",
        "    return topic, questions\n",
        "\n",
        "def extract_protocol(article):\n",
        "    # NLP logic to extract protocol from the article\n",
        "    protocol = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "    while True:\n",
        "        protocol = generate_summary(text=article, summary_type=\"Get Isolation Protocol\")\n",
        "        if protocol:\n",
        "            break\n",
        "    return protocol\n",
        "\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        #if theres an error use an SPR Compresed knowledge\n",
        "        compressed_knowledge = generate_summary(text=knowledge, summary_type=\"SPR\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=compressed_knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        try:\n",
        "            extract_references_from_pdf(file_path, references_path)\n",
        "            with open(references_path) as f:\n",
        "                references = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting references: {e}\")\n",
        "            references = \"\"\n",
        "    else:\n",
        "        references = extract_references(file_path)  \n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        article_text = pdf_to_text(file_path)\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the Mitochondria Isolation Protocol \n",
        "            protocol = extract_protocol(chunk)\n",
        "\n",
        "            if protocol:\n",
        "                organsims = generate_summary(text=protocol, summary_type=\"Get Organisms\")\n",
        "            else:\n",
        "                organsims = \"\"\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic,\n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets,\n",
        "                        protocol=protocol,\n",
        "                        organisms=organsims\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\") or file_name.endswith(\".mmd\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    # Replace spaces with underscores\n",
        "    \n",
        "    remove_spaces = file_name.replace(\" \", \"_\")\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "#OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/gpt-researcher/outputs/\" # mmd files\n",
        "#OUTPUT_FOLDER = \"/home/epas/Downloads/POB2/\"\n",
        "OUTPUT_FOLDER = \"/home/epas/Documents/MitoMAVEN/full_texts/\"\n",
        "article_list = get_article_list(filetype=\"pdf\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open each of the .json to see the results \n",
        "\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renamed Protocol for mitochondrial isolation and sub-cellular localization assay for mitochondrial prot.pdf to Protocol_for_mitochondrial_isolation_and_sub-cellular_localization_assay_for_mitochondrial_prot.pdf\n",
            "Renamed A mitosome purification protocol based on percoll density gradients and its use in validating t.pdf to A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t.pdf\n",
            "Renamed Isolation and bioenergetic characterization of mitochondria from Pichia pastoris.pdf to Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.pdf\n",
            "Renamed Measurement of mitochondrial respiratory chain enzymatic activities in Drosophila melanogaster .pdf to Measurement_of_mitochondrial_respiratory_chain_enzymatic_activities_in_Drosophila_melanogaster_.pdf\n",
            "Renamed A method for isolating intact mitochondria and nuclei from the same homogenate, and the influen.pdf to A_method_for_isolating_intact_mitochondria_and_nuclei_from_the_same_homogenate,_and_the_influen.pdf\n",
            "Renamed Isolation of mitochondria by gentle cell membrane disruption, and their subsequent characteriza.pdf to Isolation_of_mitochondria_by_gentle_cell_membrane_disruption,_and_their_subsequent_characteriza.pdf\n",
            "Renamed Isolation of Mitochondria From Fresh Mice Lung Tissue.pdf to Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.pdf\n",
            "Renamed A critical comparison between two classical and a kit-based method for mitochondria isolation.pdf to A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.pdf\n",
            "Renamed Isolation of mitochondria and mitochondrial RNA from Crithidia fasciculata.pdf to Isolation_of_mitochondria_and_mitochondrial_RNA_from_Crithidia_fasciculata.pdf\n",
            "Renamed Isolation of functional pure mitochondria by superparamagnetic microbeads.pdf to Isolation_of_functional_pure_mitochondria_by_superparamagnetic_microbeads.pdf\n",
            "Renamed Magnetic nanoparticles an improved method for mitochondrial isolation.pdf to Magnetic_nanoparticles_an_improved_method_for_mitochondrial_isolation.pdf\n",
            "Renamed Isolation and functional analysis of mitochondria from cultured cells and mouse tissue.pdf to Isolation_and_functional_analysis_of_mitochondria_from_cultured_cells_and_mouse_tissue.pdf\n",
            "Renamed A semi-automated method for isolating functionally intact mitochondria from cultured cells and .pdf to A_semi-automated_method_for_isolating_functionally_intact_mitochondria_from_cultured_cells_and_.pdf\n",
            "Renamed Improved method for isolation of mitochondria from chick breast muscle using Nagarse.pdf to Improved_method_for_isolation_of_mitochondria_from_chick_breast_muscle_using_Nagarse.pdf\n",
            "Renamed Isolation and Respiratory Measurements of Mitochondria from Arabidopsis thaliana.pdf to Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.pdf\n",
            "Renamed A high-yield preparative method for isolation of rat liver mitochondria.pdf to A_high-yield_preparative_method_for_isolation_of_rat_liver_mitochondria.pdf\n",
            "Renamed Mitochondrial Isolation and Purification from Mouse Spinal Cord.pdf to Mitochondrial_Isolation_and_Purification_from_Mouse_Spinal_Cord.pdf\n",
            "Renamed The isolation and properties of mitochondria from rat pancreas.pdf to The_isolation_and_properties_of_mitochondria_from_rat_pancreas.pdf\n",
            "Renamed Mitochondrial isolation from skeletal muscle.pdf to Mitochondrial_isolation_from_skeletal_muscle.pdf\n",
            "Renamed Mitochondria and peroxisomes from the cellular slime mould Dictyostelium discoideum. Isolation .pdf to Mitochondria_and_peroxisomes_from_the_cellular_slime_mould_Dictyostelium_discoideum._Isolation_.pdf\n",
            "Renamed Mitochondrial Isolation and Real-Time Monitoring of MOMP.pdf to Mitochondrial_Isolation_and_Real-Time_Monitoring_of_MOMP.pdf\n",
            "Renamed Isolation and quality control of functional mitochondria.pdf to Isolation_and_quality_control_of_functional_mitochondria.pdf\n",
            "Renamed Isolation and Structural Studies of Mitochondria from Pea Roots.pdf to Isolation_and_Structural_Studies_of_Mitochondria_from_Pea_Roots.pdf\n",
            "Renamed Isolation of mitochondria from the CNS.pdf to Isolation_of_mitochondria_from_the_CNS.pdf\n",
            "Renamed Isolation and Characterization of Concanavalin A-labeled Plasma Membranes of Carrot Protoplasts.pdf to Isolation_and_Characterization_of_Concanavalin_A-labeled_Plasma_Membranes_of_Carrot_Protoplasts.pdf\n",
            "Renamed The isolation of coupled mitochondria from Physarum polycephalum and their response to Ca2+.pdf to The_isolation_of_coupled_mitochondria_from_Physarum_polycephalum_and_their_response_to_Ca2+.pdf\n",
            "Renamed Purification of Functional Platelet Mitochondria Using a Discontinuous Percoll Gradient.pdf to Purification_of_Functional_Platelet_Mitochondria_Using_a_Discontinuous_Percoll_Gradient.pdf\n",
            "Renamed Isolation of mitochondria from ascites tumor cells permeabilized with digitonin.pdf to Isolation_of_mitochondria_from_ascites_tumor_cells_permeabilized_with_digitonin.pdf\n",
            "Renamed Isolation of brain mitochondria from neonatal mice.pdf to Isolation_of_brain_mitochondria_from_neonatal_mice.pdf\n",
            "Renamed Optimization of preparation of mitochondria from 25-100 mg skeletal muscle.pdf to Optimization_of_preparation_of_mitochondria_from_25-100_mg_skeletal_muscle.pdf\n",
            "Renamed Preparation of highly coupled rat heart mitochondria.pdf to Preparation_of_highly_coupled_rat_heart_mitochondria.pdf\n",
            "Renamed Organelle isolation functional mitochondria from mouse liver, muscle and cultured fibroblasts.pdf to Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.pdf\n",
            "Renamed Rapid isolation of metabolically active mitochondria from rat brain and subregions using Percol.pdf to Rapid_isolation_of_metabolically_active_mitochondria_from_rat_brain_and_subregions_using_Percol.pdf\n",
            "Renamed A novel, simple and rapid method for the isolation of mitochondria which exhibit respiratory co.pdf to A_novel,_simple_and_rapid_method_for_the_isolation_of_mitochondria_which_exhibit_respiratory_co.pdf\n",
            "Renamed Isolation and Electron Microscopic Analysis of Liver Cancer Cell Mitochondria.pdf to Isolation_and_Electron_Microscopic_Analysis_of_Liver_Cancer_Cell_Mitochondria.pdf\n",
            "Renamed A microcalorimetric study of the effect of La3+ on mitochondria isolated from Star-Cross 288 ch.pdf to A_microcalorimetric_study_of_the_effect_of_La3+_on_mitochondria_isolated_from_Star-Cross_288_ch.pdf\n",
            "Renamed A simplified method to isolate rice mitochondria.pdf to A_simplified_method_to_isolate_rice_mitochondria.pdf\n",
            "Renamed Purification of functional mouse skeletal muscle mitochondria using percoll density gradient ce.pdf to Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_percoll_density_gradient_ce.pdf\n",
            "Renamed Mitochondria isolated from lipid droplets of white adipose tissue reveal functional differences.pdf to Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.pdf\n",
            "Renamed Affordable de novo generation of fish mitogenomes using amplification-free enrichment of mitoch.pdf to Affordable_de_novo_generation_of_fish_mitogenomes_using_amplification-free_enrichment_of_mitoch.pdf\n",
            "Renamed Isolation and Metabolic Assessment of Cancer Cell Mitochondria.pdf to Isolation_and_Metabolic_Assessment_of_Cancer_Cell_Mitochondria.pdf\n",
            "Renamed Isolation of mitochondria from tissue culture cells.pdf to Isolation_of_mitochondria_from_tissue_culture_cells.pdf\n",
            "Renamed Isolation of Mitochondria from Minimal Quantities of Mouse Skeletal Muscle for High Throughput .pdf to Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.pdf\n",
            "Renamed Isolation of mitochondrial subpopulations from skeletal muscle Optimizing recovery and preservi.pdf to Isolation_of_mitochondrial_subpopulations_from_skeletal_muscle_Optimizing_recovery_and_preservi.pdf\n",
            "Renamed Delivery of mitochondria confers cardioprotection through mitochondria replenishment and metabo.pdf to Delivery_of_mitochondria_confers_cardioprotection_through_mitochondria_replenishment_and_metabo.pdf\n",
            "Renamed Simultaneous isolation of pure and intact chloroplasts and mitochondria from moss as the basis .pdf to Simultaneous_isolation_of_pure_and_intact_chloroplasts_and_mitochondria_from_moss_as_the_basis_.pdf\n",
            "Renamed Genome-wide analysis of RNA extracted from isolated mitochondria.pdf to Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria.pdf\n",
            "Renamed Isolation of Intact Mitochondria from Skeletal Muscle by Differential Centrifugation for High-r.pdf to Isolation_of_Intact_Mitochondria_from_Skeletal_Muscle_by_Differential_Centrifugation_for_High-r.pdf\n",
            "Renamed Isolation of Mitochondria from Ustilago maydis Protoplasts.pdf to Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.pdf\n",
            "Renamed Preparation of physiologically active inside-out vesicles from plant inner mitochondrial membra.pdf to Preparation_of_physiologically_active_inside-out_vesicles_from_plant_inner_mitochondrial_membra.pdf\n",
            "Renamed A rapid method for the isolation of intact mitochondria from isolated rat liver cells.pdf to A_rapid_method_for_the_isolation_of_intact_mitochondria_from_isolated_rat_liver_cells.pdf\n",
            "Renamed Isolation of mitochondria with high respiratory control from primary cultures of neurons and as.pdf to Isolation_of_mitochondria_with_high_respiratory_control_from_primary_cultures_of_neurons_and_as.pdf\n",
            "Renamed Rapid isolation of respiring skeletal muscle mitochondria using nitrogen cavitation.pdf to Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.pdf\n",
            "Renamed Optimization of differential filtration-based mitochondrial isolation for mitochondrial transpl.pdf to Optimization_of_differential_filtration-based_mitochondrial_isolation_for_mitochondrial_transpl.pdf\n",
            "Renamed Tightly coupled mitochondria from human early placenta.pdf to Tightly_coupled_mitochondria_from_human_early_placenta.pdf\n",
            "Renamed An Improved Method for Preparation of Uniform and Functional Mitochondria from Fresh Liver.pdf to An_Improved_Method_for_Preparation_of_Uniform_and_Functional_Mitochondria_from_Fresh_Liver.pdf\n",
            "Renamed Efficient isolation of pure and functional mitochondria from mouse tissues using automated tiss.pdf to Efficient_isolation_of_pure_and_functional_mitochondria_from_mouse_tissues_using_automated_tiss.pdf\n",
            "Renamed Two-Step Tag-Free Isolation of Mitochondria for Improved Protein Discovery and Quantification.pdf to Two-Step_Tag-Free_Isolation_of_Mitochondria_for_Improved_Protein_Discovery_and_Quantification.pdf\n",
            "Renamed Isolation of mitochondria from animal tissue.pdf to Isolation_of_mitochondria_from_animal_tissue.pdf\n",
            "Renamed Comparison of three methods for mitochondria isolation from the human liver cell line (HepG2).pdf to Comparison_of_three_methods_for_mitochondria_isolation_from_the_human_liver_cell_line_(HepG2).pdf\n",
            "Renamed Isolation and comparative proteomic analysis of mitochondria from the pulp of ripening citrus f.pdf to Isolation_and_comparative_proteomic_analysis_of_mitochondria_from_the_pulp_of_ripening_citrus_f.pdf\n",
            "Renamed Rapid isolation and purification of functional platelet mitochondria using a discontinuous Perc.pdf to Rapid_isolation_and_purification_of_functional_platelet_mitochondria_using_a_discontinuous_Perc.pdf\n",
            "Renamed Isolation of functionally active and highly purified neuronal mitochondria from human cortex.pdf to Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.pdf\n",
            "Renamed Isolation of Large Amounts of Highly Pure Mitochondria for Omics Studies.pdf to Isolation_of_Large_Amounts_of_Highly_Pure_Mitochondria_for_Omics_Studies.pdf\n",
            "Renamed Rapid isolation and purification of mitochondria for transplantation by tissue dissociation and.pdf to Rapid_isolation_and_purification_of_mitochondria_for_transplantation_by_tissue_dissociation_and.pdf\n",
            "Renamed Purity matters A workflow for the valid high-resolution lipid profiling of mitochondria from ce.pdf to Purity_matters_A_workflow_for_the_valid_high-resolution_lipid_profiling_of_mitochondria_from_ce.pdf\n",
            "Renamed Isolation of mitochondria from cultured cells and liver tissue biopsies for molecular and bioch.pdf to Isolation_of_mitochondria_from_cultured_cells_and_liver_tissue_biopsies_for_molecular_and_bioch.pdf\n",
            "Renamed Isolation of functional mitochondria from rat kidney and skeletal muscle without manual homogen.pdf to Isolation_of_functional_mitochondria_from_rat_kidney_and_skeletal_muscle_without_manual_homogen.pdf\n",
            "Renamed Purification of functional mouse skeletal muscle mitochondria using Percoll density gradient ce.pdf to Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_Percoll_density_gradient_ce.pdf\n",
            "Renamed Scalable Isolation of Mammalian Mitochondria for Nucleic Acid and Nucleoid Analysis.pdf to Scalable_Isolation_of_Mammalian_Mitochondria_for_Nucleic_Acid_and_Nucleoid_Analysis.pdf\n",
            "Renamed An improved technique for the isolation of mitochondria from plant tissue.pdf to An_improved_technique_for_the_isolation_of_mitochondria_from_plant_tissue.pdf\n",
            "Renamed Preservation of mitochondrial functional integrity in mitochondria isolated from small cryopres.pdf to Preservation_of_mitochondrial_functional_integrity_in_mitochondria_isolated_from_small_cryopres.pdf\n",
            "Renamed Isolation and reconstruction of cardiac mitochondria from SBEM images using a deep learning-bas.pdf to Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.pdf\n",
            "Renamed Qualitative Characterization of the Rat Liver Mitochondrial Lipidome Using All Ion Fragmentatio.pdf to Qualitative_Characterization_of_the_Rat_Liver_Mitochondrial_Lipidome_Using_All_Ion_Fragmentatio.pdf\n",
            "Renamed Mouse Liver Mitochondria Isolation, Size Fractionation, and Real-time MOMP Measurement.pdf to Mouse_Liver_Mitochondria_Isolation,_Size_Fractionation,_and_Real-time_MOMP_Measurement.pdf\n",
            "Renamed Isolation and Purification of Mitochondria from Cell Culture for Proteomic Analyses.pdf to Isolation_and_Purification_of_Mitochondria_from_Cell_Culture_for_Proteomic_Analyses.pdf\n",
            "Renamed Isolation of intact, functional mitochondria from the model plant Arabidopsis thaliana.pdf to Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.pdf\n",
            "Renamed Mitochondrial Respiration of Platelets Comparison of Isolation Methods.pdf to Mitochondrial_Respiration_of_Platelets_Comparison_of_Isolation_Methods.pdf\n",
            "Renamed Isolation and functional assessment of mitochondria from small amounts of mouse brain tissue.pdf to Isolation_and_functional_assessment_of_mitochondria_from_small_amounts_of_mouse_brain_tissue.pdf\n",
            "Renamed Assay of succinate dehydrogenase activity by the tetrazolium method evaluation of an improved t.pdf to Assay_of_succinate_dehydrogenase_activity_by_the_tetrazolium_method_evaluation_of_an_improved_t.pdf\n",
            "Renamed An Update on Isolation of Functional Mitochondria from Cells for Bioenergetics Studies.pdf to An_Update_on_Isolation_of_Functional_Mitochondria_from_Cells_for_Bioenergetics_Studies.pdf\n",
            "Renamed Isolation of Physiologically Active and Intact Mitochondria from Chickpea.pdf to Isolation_of_Physiologically_Active_and_Intact_Mitochondria_from_Chickpea.pdf\n",
            "Renamed Rapid isolation techniques for mitochondria technique for rat liver mitochondria.pdf to Rapid_isolation_techniques_for_mitochondria_technique_for_rat_liver_mitochondria.pdf\n",
            "Renamed Mitochondrial structure and function are disrupted by standard isolation methods.pdf to Mitochondrial_structure_and_function_are_disrupted_by_standard_isolation_methods.pdf\n",
            "Renamed Mitochondria from the hepatopancreas of the marine clam Mercenaria mercenaria substrate prefere.pdf to Mitochondria_from_the_hepatopancreas_of_the_marine_clam_Mercenaria_mercenaria_substrate_prefere.pdf\n",
            "Renamed Isolation of mitochondria for biogenetical studies An update.pdf to Isolation_of_mitochondria_for_biogenetical_studies_An_update.pdf\n",
            "Renamed Optimal isolation of mitochondria for proteomic analyses.pdf to Optimal_isolation_of_mitochondria_for_proteomic_analyses.pdf\n",
            "Renamed Isolation of mitochondria from procyclic Trypanosoma brucei.pdf to Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.pdf\n",
            "Renamed Isolation of rat adrenocortical mitochondria.pdf to Isolation_of_rat_adrenocortical_mitochondria.pdf\n",
            "Renamed Characterization of growth plate mitochondria.pdf to Characterization_of_growth_plate_mitochondria.pdf\n",
            "Renamed An improved method with a wider applicability to isolate plant mitochondria for mtDNA extractio.pdf to An_improved_method_with_a_wider_applicability_to_isolate_plant_mitochondria_for_mtDNA_extractio.pdf\n"
          ]
        }
      ],
      "source": [
        "# rename every file \"/home/epas/Documents/MitoMAVEN/full_texts/\" replacing the spaces with underscores \n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "def rename_files(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            new_filename = re.sub(r\"\\s+\", \"_\", filename)\n",
        "            new_filename_with_pdf = new_filename.replace(\".docx\", \".pdf\")\n",
        "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename_with_pdf))\n",
        "            print(f\"Renamed {filename} to {new_filename_with_pdf}\")\n",
        "\n",
        "# Example usage\n",
        "rename_files(\"/home/epas/Documents/extraction_techniques/Extraction Techniques/\")\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/epas/miniconda3/envs/autogen/bin/pdfx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/cli.py\", line 158, in main\n",
            "    pdf = pdfx.PDFx(args.pdf)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/__init__.py\", line 128, in __init__\n",
            "    self.reader = PDFMinerBackend(self.stream)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/backends.py\", line 207, in __init__\n",
            "    self.metadata.update(xmp_to_dict(metadata))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 93, in xmp_to_dict\n",
            "    return XmpParser(xmp).meta\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 43, in __init__\n",
            "    self.tree = ET.XML(xmp)\n",
            "                ^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/xml/etree/ElementTree.py\", line 1338, in XML\n",
            "    parser.feed(text)\n",
            "xml.etree.ElementTree.ParseError: unbound prefix: line 5, column 220\n"
          ]
        }
      ],
      "source": [
        "!pdfx -v '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.pdf' -o '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ChatGPT Message Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def analyze_file(file_path, output_file_path, document_name):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().split('\\n')\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            current_message = \"\"\n",
        "            message_started = False\n",
        "            sender = \"\"\n",
        "            message_number = 0\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line.lower().startswith('user') or line.lower().startswith('chatgpt'):\n",
        "                    if message_started:  # End of a message\n",
        "                        message_number += 1\n",
        "                        word_count = count_words(current_message.strip())\n",
        "                        output_file.write(f\"{sender} Line number {i}, Message number {message_number}, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "                        current_message = \"\"\n",
        "                    message_started = True\n",
        "                    sender = \"User\" if line.lower().startswith('user') else \"ChatGPT\"\n",
        "                    continue\n",
        "                if message_started:\n",
        "                    current_message += \" \" + line\n",
        "\n",
        "            # Add the last message if it exists\n",
        "            if current_message:\n",
        "                message_number += 1\n",
        "                word_count = count_words(current_message.strip())\n",
        "                output_file.write(f\"{sender} Last message, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your text file\n",
        "# Replace 'output_messages.txt' with the path for the output file\n",
        "# Add the document name (e.g., 'ChatGPT_history.txt')\n",
        "file_path = 'ChatGPT_history.txt'\n",
        "output_file_path = 'output_messages.txt'\n",
        "document_name = 'ChatGPT_history'  # This is the document name without the extension\n",
        "analyze_file(file_path, output_file_path, document_name)\n",
        "print(\"Messages have been written to the output file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class MessageAnalysisStore:\n",
        "    def __init__(self, output_file_path):\n",
        "        self.output_file_path = output_file_path\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.output_file_path):\n",
        "            empty_data = []\n",
        "            self._save(empty_data)\n",
        "\n",
        "    def store(self, analyzed_data):\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(analyzed_data)\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.output_file_path):\n",
        "            with open(self.output_file_path, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.output_file_path, \"w\") as file:\n",
        "                json.dump(content, file, indent=4)\n",
        "            print(f\"Successfully saved data to {self.output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.output_file_path}: {e}\")\n",
        "\n",
        "# Assuming the required classes and functions from your new code are already defined and imported\n",
        "# like SummaryStore, generate_summary, get_entities, extract_knowledge, etc.\n",
        "\n",
        "def extract_messages_with_citation(lines: List[str], sender_keyword: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts messages with citation from the given lines based on the sender keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    messages_with_citation = []\n",
        "    current_message = \"\"\n",
        "    message_started = False\n",
        "    citation_info = \"\"\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(sender_keyword):\n",
        "            if message_started:\n",
        "                # End of the current message, add it to the list\n",
        "                messages_with_citation.append((current_message.strip(), citation_info))\n",
        "                current_message = \"\"\n",
        "            message_started = True\n",
        "            citation_info = line  # Capture the line with sender info as citation\n",
        "        elif message_started:\n",
        "            current_message += \" \" + line\n",
        "\n",
        "    # Add the last message if it exists\n",
        "    if current_message:\n",
        "        messages_with_citation.append((current_message.strip(), citation_info))\n",
        "\n",
        "    return messages_with_citation\n",
        "\n",
        "def analyze_conversation(message: str, citation: str, sender_keyword: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyzes a single conversation message, extracting and summarizing information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate an initial summary\n",
        "        if sender_keyword == \"ChatGPT\":\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"Entity Dense\")\n",
        "        else:\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"SPR\")\n",
        "\n",
        "        # Extract entities and knowledge\n",
        "        entities = get_entities(message)\n",
        "        knowledge = extract_knowledge(message, entities[\"clean_entities\"], entities[\"dirty_entities\"])\n",
        "\n",
        "        # Extract the topic and hypothetical questions from the summary\n",
        "        topic, questions = extract_info(initial_summary)\n",
        "\n",
        "        analyzed_data = {\n",
        "            \"id\": citation,\n",
        "            \"sender\": sender_keyword,\n",
        "            \"message\": message,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": questions,\n",
        "            \"clean_entities\": entities[\"clean_entities\"],\n",
        "            \"dirty_entities\": entities[\"dirty_entities\"],\n",
        "            \"summary\": initial_summary,\n",
        "            \"knowledge\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        return analyzed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred in analyzing conversation: {e}\")\n",
        "\n",
        "def extract_and_analyze_messages(file_path: str, output_file_path: str, sender_keyword: str, log_file_path: str):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    messages_with_citation = extract_messages_with_citation(lines, sender_keyword)\n",
        "    store = MessageAnalysisStore(output_file_path)\n",
        "    error_log = []\n",
        "\n",
        "    for message, citation in messages_with_citation:\n",
        "        try:\n",
        "            analyzed_data = analyze_conversation(message, citation, sender_keyword)\n",
        "            store.store(analyzed_data)\n",
        "            time.sleep(15)  # Delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            error_info = {\"citation\": citation, \"error\": str(e), \"timestamp\": datetime.now().isoformat()}\n",
        "            # Appending to the error log\n",
        "            error_log.append(error_info)\n",
        "            with open(log_file_path, \"a\") as log_file:\n",
        "                json.dump(error_info, log_file, indent=4)\n",
        "                log_file.write(\"\\n\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Messages analysis completed. Data saved to {output_file_path}\")\n",
        "    if error_log:\n",
        "        print(f\"Errors logged to {log_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'output_messages.txt'\n",
        "output_file_path_user = 'analyzed_user_messages.json'\n",
        "log_file_path_user = 'error_log_user.json'\n",
        "extract_and_analyze_messages(file_path, output_file_path_user, 'user', log_file_path_user)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install neo4j\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MATCH (n) \n",
        "DETACH DELETE n\n",
        "\n",
        "CREATE CONSTRAINT FOR ()-[r:RELATED_TO]-() REQUIRE r.id IS UNIQUE\n",
        "\n",
        "// Shared Entities  \n",
        "MATCH (a1:ArticleID)-[:MENTIONS]->(e)<-[:MENTIONS]-(a2:ArticleID) \n",
        "WHERE e:CleanEntity OR e:DirtyEntity\n",
        "WITH DISTINCT a1, a2, collect(e.name) AS shared \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Entity'}]->(a2)\n",
        "ON CREATE SET r.entities = shared\n",
        "\n",
        "// Shared Topics\n",
        "MATCH (a1)-[:HAS_TOPIC]->(t1)<-[:HAS_TOPIC]-(a2) \n",
        "WITH a1, a2, split(t1.name, ' ') AS words1\n",
        "UNWIND words1 AS word1\n",
        "MATCH (a3)-[:HAS_TOPIC]->(t2)<-[:HAS_TOPIC]-(a4)\n",
        "WHERE word1 =~ '(?i).*?' + word1  \n",
        "WITH DISTINCT a1, a3, collect(word1) AS shared\n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Topic Word'}]->(a3)\n",
        "ON CREATE SET r.words = shared\n",
        "\n",
        "// Shared Subjects\n",
        "MATCH (a:ArticleID)-[:HAS_TRIPLET]->(s:Subject)<-[:HAS_TRIPLET]-(b:ArticleID)\n",
        "WHERE id(a) < id(b) \n",
        "WITH DISTINCT a, b, collect(s.name) AS shared \n",
        "MERGE (a)-[r:RELATED_TO {type:'Shared Subject'}]->(b)  \n",
        "ON CREATE SET r.subjects = shared\n",
        "\n",
        "// Shared URLs\n",
        "MATCH (a1)-[:HAS_REFERENCE]->(r)<-[:HAS_REFERENCE]-(a2)\n",
        "WHERE r:Reference AND id(a1) < id(a2)  \n",
        "WITH DISTINCT a1, a2, collect(r.url) AS shared  \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared URL'}]->(a2)\n",
        "ON CREATE SET r.urls = shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import dateutil.parser  # You need to install the python-dateutil package\n",
        "# 11226 13863\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp):\n",
        "    # Convert timestamp from string to Unix timestamp (seconds since epoch)\n",
        "    try:\n",
        "        timestamp_unix = dateutil.parser.parse(timestamp).timestamp()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing timestamp: {timestamp}. Error: {e}\")\n",
        "        timestamp_unix = 0  # Default to 0 or some other value in case of parsing failure\n",
        "    # Ensure ArticleID node exists (create if it doesn't)\n",
        "    session.run(f\"MERGE (articleID:ArticleID {{id: '{file_id}'}})\")\n",
        "\n",
        "    # Create or update the Article node and link it to ArticleID\n",
        "    session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) MERGE (article:Article) MERGE (articleID)-[:IDENTIFIES]->(article)\")\n",
        "\n",
        "    # Create a new Chunk node with a unique identifier, timestamp, and link it to ArticleID\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "    create_chunk_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"CREATE (chunk:Chunk {{id: '{chunk_id}', content: '{chunk_content}', timestamp: '{timestamp_unix}'}}) \"\n",
        "        f\"CREATE (articleID)-[:HAS_CHUNK]->(chunk)\"\n",
        "    )\n",
        "    session.run(create_chunk_query)\n",
        "\n",
        "    # Link this chunk to the previous chunk if it's not the first one\n",
        "    if chunk_index > 0:\n",
        "        previous_chunk_id = f\"{file_id}_{chunk_index - 1}\"\n",
        "        link_chunks_query = (\n",
        "            f\"MATCH (prevChunk:Chunk {{id: '{previous_chunk_id}'}}), (currChunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "            f\"CREATE (prevChunk)-[:NEXT]->(currChunk)\"\n",
        "        )\n",
        "        session.run(link_chunks_query)\n",
        "\n",
        "# Functions to link nodes to ArticleID\n",
        "def link_topic_to_article(session, file_id, topic_name, justification):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (topic:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TOPIC]->(topic)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_entity_to_article(session, file_id, entity_name, clean=True):\n",
        "    entity_type = \"CleanEntity\" if clean else \"DirtyEntity\"\n",
        "    # Don't add irrelevant entities: \n",
        "    if entity_name in ['Abstract Concepts:', 'References:', 'Key Phrases:', 'Keywords:', 'Entities:', 'Topics:', 'Concepts:', 'Final Output:', 'Introduction', 'Conclusion', 'Summary']:\n",
        "        return\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (entity:{entity_type} {{name: '{entity_name}'}}) \"\n",
        "        f\"MERGE (articleID)-[:MENTIONS]->(entity)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_question_to_article(session, file_id, question_content, question_type):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (hq:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_QUESTION]->(hq)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_triplet_to_article(session, file_id, subject, target, relationship):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (subj:Subject {{name: '{subject}'}}) \"\n",
        "        f\"MERGE (targ:Target {{name: '{target}'}}) \"\n",
        "        f\"MERGE (rel:Relationship {{type: '{relationship}'}}) \"\n",
        "        f\"MERGE (subj)-[:HAS_RELATIONSHIP]->(rel)-[:TARGETS]->(targ) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TRIPLET]->(subj)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_summary_to_chunk(session, file_id, chunk_index, summary_content):\n",
        "    # Unique identifier for the chunk\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Link the summary to the corresponding chunk\n",
        "    link_summary_query = (\n",
        "        f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "        f\"MERGE (summary:Summary {{content: '{summary_content}'}}) \"\n",
        "        f\"MERGE (chunk)-[:HAS_SUMMARY]->(summary)\"\n",
        "    )\n",
        "    session.run(link_summary_query)\n",
        "def link_url_references_to_article(session, file_id, urls):\n",
        "    for url in urls:\n",
        "        escaped_url = escape_string(url)\n",
        "        query = (\n",
        "            f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "            f\"MERGE (ref:Reference {{url: '{escaped_url}'}}) \"\n",
        "            f\"WITH articleID, ref \"\n",
        "            f\"MERGE (articleID)-[:HAS_REFERENCE]->(ref)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "def link_textual_references_to_article(session, file_id, textual_references):\n",
        "    for reference in textual_references.split('\\n'):  # Assuming each reference is on a new line\n",
        "        if reference.strip():\n",
        "            escaped_reference = escape_string(reference)\n",
        "            query = (\n",
        "                f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "                f\"MERGE (textRef:TextualReference {{text: '{escaped_reference}'}}) \"\n",
        "                f\"WITH articleID, textRef \"\n",
        "                f\"MERGE (articleID)-[:HAS_TEXTUAL_REFERENCE]->(textRef)\"\n",
        "            )\n",
        "            session.run(query)\n",
        "\n",
        "def process_knowledge_triplet(session, file_id, subject, target, relationship):\n",
        "    # Constructing a query to check if the specific triplet combination already exists\n",
        "    check_triplet_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TRIPLET]->\"\n",
        "        f\"(subj:Subject {{name: '{subject}'}})-[:HAS_RELATIONSHIP]->\"\n",
        "        f\"(rel:Relationship {{type: '{relationship}'}})-[:TARGETS]->\"\n",
        "        f\"(targ:Target {{name: '{target}'}}) \"\n",
        "        f\"RETURN subj, rel, targ\"\n",
        "    )\n",
        "    if not session.run(check_triplet_query).single():\n",
        "        # Linking the triplet only if it doesn't exist\n",
        "        link_triplet_to_article(session, file_id, subject, target, relationship)\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        # Group records by file_id\n",
        "        grouped_records = {}\n",
        "        for record in data:\n",
        "            file_id = record['file_id']\n",
        "            grouped_records.setdefault(file_id, []).append(record)\n",
        "\n",
        "        for file_id, records in grouped_records.items():\n",
        "            try:\n",
        "                # Check if the ArticleID already exists\n",
        "                check_query = f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) RETURN articleID\"\n",
        "                result = session.run(check_query).single()\n",
        "                if result:\n",
        "                    logging.info(f\"File ID {file_id} already exists in the database. Processing only new chunks.\")\n",
        "                else:\n",
        "                    logging.info(f\"File ID {file_id} does not exist. Processing all chunks.\")\n",
        "\n",
        "                for index, record in enumerate(records):\n",
        "                    chunk_content = escape_string(record['article'])\n",
        "                    timestamp = record.get('timestamp', '')\n",
        "                    summary_content = escape_string(record['summary']) if \"summary\" in record else \"\"\n",
        "\n",
        "                    # Process each chunk\n",
        "                    process_record_with_chunks(session, file_id, chunk_content, index, timestamp, summary_content, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "def process_record_with_chunks(session, file_id, chunk_content, chunk_index, timestamp, summary_content, record):\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Check and process each chunk and its summary\n",
        "    if not session.run(f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) RETURN chunk\").single():\n",
        "        create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp)\n",
        "        if summary_content:\n",
        "            link_summary_to_chunk(session, file_id, chunk_index, summary_content)\n",
        "\n",
        "    # Process Topics\n",
        "    for topic in record.get(\"topics\", []):\n",
        "        topic_name = escape_string(topic['topic'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TOPIC]->(topic:Topic {{name: '{topic_name}'}}) RETURN topic\").single():\n",
        "            justification = escape_string(topic.get('justification', ''))\n",
        "            link_topic_to_article(session, file_id, topic_name, justification)\n",
        "\n",
        "    # Process Entities (both clean and dirty)\n",
        "    for clean, entities in [(True, record.get(\"clean_entities\", [])), (False, record.get(\"dirty_entities\", []))]:\n",
        "        for entity in entities:\n",
        "            entity_name = escape_string(entity)\n",
        "            if entity_name.strip() and not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:MENTIONS]->(entity {{name: '{entity_name}'}}) RETURN entity\").single():\n",
        "                link_entity_to_article(session, file_id, entity_name, clean)\n",
        "\n",
        "    # Process Hypothetical Questions\n",
        "    for question in record.get(\"hypothetical_questions\", []):\n",
        "        question_content = escape_string(question['question'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_QUESTION]->(hq:HypotheticalQuestion {{content: '{question_content}'}}) RETURN hq\").single():\n",
        "            question_type = escape_string(question.get('question_type', ''))\n",
        "            link_question_to_article(session, file_id, question_content, question_type)\n",
        "\n",
        "    # Process Knowledge Triplets\n",
        "    for triplet in record.get(\"knowledge_triplets\", []):\n",
        "        subject = escape_string(triplet['subject'])\n",
        "        target = escape_string(triplet['target'])\n",
        "        relationship = escape_string(triplet['relationship'])\n",
        "        process_knowledge_triplet(session, file_id, subject, target, relationship)\n",
        "\n",
        "    # Process URL References\n",
        "    if \"references\" in record and \"urls\" in record[\"references\"]:\n",
        "        for url in record[\"references\"][\"urls\"]:\n",
        "            escaped_url = escape_string(url)\n",
        "            if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_REFERENCE]->(ref:Reference {{url: '{escaped_url}'}}) RETURN ref\").single():\n",
        "                link_url_references_to_article(session, file_id, [url])\n",
        "\n",
        "    # Process Textual References\n",
        "    if \"references\" in record and \"textual_references\" in record[\"references\"]:\n",
        "        for reference in record[\"references\"][\"textual_references\"].split('\\n'):\n",
        "            if reference.strip():\n",
        "                escaped_reference = escape_string(reference)\n",
        "                if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TEXTUAL_REFERENCE]->(textRef:TextualReference {{text: '{escaped_reference}'}}) RETURN textRef\").single():\n",
        "                    link_textual_references_to_article(session, file_id, [reference])\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                process_json_file(file_path, session)\n",
        "                print(f\"Successfully processed file: {file_path}\")\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder4 = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder1 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder2 = \"/home/epas/Programming/gpt-researcher/outputs/\"\n",
        "    output_folder3 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\"\n",
        "    for output_folder in [output_folder1, output_folder2, output_folder3, output_folder4]:\n",
        "        try:\n",
        "            add_jsons_to_neo4j(output_folder)\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding jsons to Neo4j database: {e}\")\n",
        "            continue\n",
        "    print(\"Successfully added data to Neo4j database.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding = embeddings.embed_query(\"text\")\n",
        "print(f\"Embedding for text: {embedding}\\n With shape: {len(embedding)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Configuration for HuggingFaceEmbeddings\n",
        "model_name = \"WhereIsAI/UAE-Large-V1\"  # or another model of your choice\n",
        "model_kwargs = {'device': 'mps'}  # use 'cuda' for GPU acceleration if available\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "def LoadEmbedding(node_type, text_properties):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Handle multiple properties (concatenation) for a single embedding\n",
        "        if isinstance(text_properties, list) and len(text_properties) > 1:\n",
        "            properties_str = \" + ' ' + \".join([f\"n.{prop}\" for prop in text_properties])\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, {properties_str} AS text\"\n",
        "        else:\n",
        "            text_property = text_properties[0]\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, n.{text_property} AS text\"\n",
        "\n",
        "        result = session.run(query)\n",
        "        count = 0\n",
        "        for record in result:\n",
        "            id = record[\"id\"]\n",
        "            text = record[\"text\"]\n",
        "\n",
        "            # Generate the embedding\n",
        "            embedding = embeddings.embed_query(text)\n",
        "            print(f\"Embedding for {node_type} with id {id}: With shape: {len(embedding)}\")\n",
        "            cypher = \"MATCH (n) WHERE id(n) = $id SET n.embedding = $embedding\"\n",
        "            session.run(cypher, embedding=embedding, id=id)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Processed {count} {node_type} nodes for property @{' and '.join(text_properties)}.\")\n",
        "    return count\n",
        "\n",
        "# Example usages\n",
        "LoadEmbedding(\"Chunk\", [\"content\"])\n",
        "LoadEmbedding(\"CleanEntity\", [\"name\"])\n",
        "LoadEmbedding(\"DirtyEntity\", [\"name\"])\n",
        "LoadEmbedding(\"Subject\", [\"name\"])\n",
        "LoadEmbedding(\"Target\", [\"name\"])\n",
        "LoadEmbedding(\"Relationship\", [\"type\"])\n",
        "LoadEmbedding(\"Topic\", [\"name\", \"justification\"])  # Concatenating name and justification for topics\n",
        "LoadEmbedding(\"HypotheticalQuestion\", [\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "def create_indexes(uri, user, password):\n",
        "    # Establish a connection to the Neo4j database\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "    # Define Cypher queries for creating the indexes\n",
        "    create_full_text_index_query = \"\"\"\n",
        "    CALL db.index.fulltext.createNodeIndex(\"textIndex\", [\"Chunk\", \"CleanEntity\", \"DirtyEntity\", \"HypotheticalQuestion\"], [\"content\", \"name\"])\n",
        "    \"\"\"\n",
        "    create_vector_index_query = \"\"\"\n",
        "    CALL db.index.vector.createNodeIndex(\"chunkVectorIndex\", \"Embedding\", \"embedding\", 1024, \"COSINE\")\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # Create Full-Text Index#\n",
        "        #session.run(create_full_text_index_query) # Neo.ClientError.Procedure.ProcedureNotFound\n",
        "        #print(\"Full-text index created.\")\n",
        "\n",
        "        # Create Vector Index\n",
        "        session.run(create_vector_index_query)\n",
        "        print(\"Vector index created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Drop the old index\n",
        "        session.run(f\"DROP INDEX {old_index_name}\")\n",
        "\n",
        "        # Create the new vector index\n",
        "        create_vector_index_query = f\"\"\"\n",
        "        CALL db.index.vector.createNodeIndex(\"{new_index_name}\", \"{label}\", \"{property}\", {dimensions}, \"{similarity}\")\n",
        "        \"\"\"\n",
        "        session.run(create_vector_index_query)\n",
        "        print(f\"Vector index {new_index_name} created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "# Usage\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "old_index_name = \"chunkVectorIndex\"  # Replace with the actual name of the index to drop\n",
        "new_index_name = \"chunkVectorIndex\"  # Replace with your new index name\n",
        "label = \"Embedding\"  # Replace with the appropriate label\n",
        "property = \"embedding\"  # Replace with the correct property\n",
        "dimensions = 1024\n",
        "similarity = \"COSINE\"\n",
        "\n",
        "#modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity)\n",
        "\n",
        "\n",
        "create_indexes(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def generate_query_vector(text, model_name=\"WhereIsAI/UAE-Large-V1\", model_kwargs={'device': 'mps'}):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "    return embeddings.embed_query(text)\n",
        "\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def query_similar_nodes(uri, user, password, vector_index_name, query_vector, top_k=10):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    similar_nodes = []\n",
        "    with driver.session() as session:\n",
        "        query = f\"\"\"\n",
        "        CALL db.index.vector.queryNodes('{vector_index_name}', {top_k}, $queryVector)\n",
        "        YIELD node, score\n",
        "        RETURN node, score\n",
        "        \"\"\"\n",
        "        result = session.run(query, queryVector=query_vector)\n",
        "        for record in result:\n",
        "            node = record[\"node\"]\n",
        "            score = record[\"score\"]\n",
        "            similar_nodes.append((node, score))\n",
        "    driver.close()\n",
        "    return similar_nodes\n",
        "\n",
        "\n",
        "query_text = \"X-ray crystallography\"\n",
        "query_vector = generate_query_vector(query_text)\n",
        "print(f\"Query vector: {query_vector}\\n With shape: {len(query_vector)}\")\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "vector_index_name = \"chunkVectorIndex\"  # Replace with your actual vector index name\n",
        "\n",
        "similar_nodes = query_similar_nodes(uri, user, password, vector_index_name, query_vector)\n",
        "for node, score in similar_nodes:\n",
        "    print(node, score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import OllamaEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from typing import List, Any\n",
        "#from utils import BaseLogger\n",
        "from langchain.chains import GraphCypherQAChain \n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" \n",
        "def load_embedding_model(embedding_model_name: str, config={}):\n",
        "    if embedding_model_name == \"ollama\":\n",
        "        embeddings = OllamaEmbeddings(\n",
        "            base_url=config[\"ollama_base_url\"], model=\"llama2\"\n",
        "        )\n",
        "        dimension = 4096\n",
        "        #logger.info(\"Embedding: Using Ollama\")\n",
        "    elif embedding_model_name == \"openai\":\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        dimension = 1536\n",
        "        #logger.info(\"Embedding: Using OpenAI\")\n",
        "    elif embedding_model_name == \"WhereIsAI/UAE-Large-V1\":\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"WhereIsAI/UAE-Large-V1\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 1024\n",
        "        #logger.info(\"Embedding: Using HuggingFace\")\n",
        "    else:\n",
        "        embeddings = SentenceTransformerEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 384\n",
        "        #logger.info(\"Embedding: Using SentenceTransformer\")\n",
        "    return embeddings, dimension\n",
        "\n",
        "\n",
        "def load_llm(llm_name: str, config={}):\n",
        "    if llm_name == \"gpt-4\":\n",
        "        logger.info(\"LLM: Using GPT-4\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True)\n",
        "    elif llm_name == \"gpt-3.5\":\n",
        "        logger.info(\"LLM: Using GPT-3.5\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "    elif len(llm_name):\n",
        "        logger.info(f\"LLM: Using Ollama: {llm_name}\")\n",
        "        return ChatOllama(\n",
        "            temperature=0,\n",
        "            base_url=config[\"ollama_base_url\"],\n",
        "            model=llm_name,\n",
        "            streaming=True,\n",
        "            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.\n",
        "            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.\n",
        "            num_ctx=3072,  # Sets the size of the context window used to generate the next token.\n",
        "        )\n",
        "    logger.info(\"LLM: Using GPT-3.5\")\n",
        "    return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "\n",
        "\n",
        "def configure_llm_only_chain(llm):\n",
        "    # LLM only response\n",
        "    template = \"\"\"\n",
        "    You are a helpful assistant that helps with answering general questions.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    \"\"\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "    human_template = \"{text}\"\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [system_message_prompt, human_message_prompt]\n",
        "    )\n",
        "\n",
        "    def generate_llm_output(\n",
        "        user_input: str, callbacks: List[Any], prompt=chat_prompt\n",
        "    ) -> str:\n",
        "        answer = llm(\n",
        "            prompt.format_prompt(\n",
        "                text=user_input,\n",
        "            ).to_messages(),\n",
        "            callbacks=callbacks,\n",
        "        ).content\n",
        "        return {\"answer\": answer}\n",
        "\n",
        "    return generate_llm_output\n",
        "\n",
        "\n",
        "def configure_qa_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response\n",
        "    general_system_template = \"\"\"\n",
        "    Use the following context to answer the question at the end.\n",
        "    The context contains article summaries, related topics, and hypothetical questions.\n",
        "    Make sure to rely on accurate information from the summaries and topics.\n",
        "    If a particular article or topic in the context is useful, refer to it in your response.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Your response should be concise and based on the given context.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS questionEmb, score\n",
        "        MATCH (questionEmb) <-[:HAS_EMBEDDING]- (article:Article)\n",
        "        OPTIONAL MATCH (article)-[:HAS_SUMMARY]->(summary:Summary)\n",
        "        OPTIONAL MATCH (article)-[:HAS_TOPIC]->(topic:Topic)\n",
        "        RETURN '##Article ID: ' + article.id + '\\n' \n",
        "            + '##Summary: ' + coalesce(summary.content, 'No summary available') + '\\n'\n",
        "            + '##Related Topics: ' + coalesce(topic.name, 'No topics available') AS text, \n",
        "            score AS similarity\n",
        "        ORDER BY similarity DESC LIMIT 5\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=3375,\n",
        "    )\n",
        "    return kg_qa\n",
        "\n",
        "# ADDED\n",
        "# >>>> Extended to support vector search over strucutured chunking\n",
        "\n",
        "def configure_qa_structure_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response based on vector search and retrieval of structured chunks\n",
        "    \n",
        "    sample_query = \"\"\"\n",
        "    // 0 - prepare question and its embedding \n",
        "        MATCH (ch:Chunk) -[:HAS_EMBEDDING]-> (chemb) \n",
        "        WHERE ch.block_idx = 19\n",
        "        WITH ch.sentences AS question, chemb.value AS qemb\n",
        "        // 1 - search chunk vectors\n",
        "        CALL db.index.vector.queryNodes($index_name, $k, qemb) YIELD node, score\n",
        "        // 2 - retrieve connectd chunks, sections and documents\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (answer) -[:HAS_PARENT*]-> (s:Section)\n",
        "        WITH s, score LIMIT 1\n",
        "        MATCH (d:Document) <-[*]- (s) <-[:HAS_PARENT*]- (chunk:Chunk)\n",
        "        WITH d, s, chunk, score ORDER BY chunk.block_idx ASC\n",
        "        // 3 - prepare results\n",
        "        WITH d, collect(chunk) AS chunks, score\n",
        "        RETURN {source: d.url, page: chunks[0].page_idx} AS metadata, \n",
        "            reduce(text = \"\", x IN chunks | text + x.sentences + '.') AS text, score;   \n",
        "    \"\"\"\n",
        "\n",
        "    general_system_template = \"\"\"\n",
        "    You are an assistant providing detailed answers based on specific chunks of articles.\n",
        "    Use the context provided to answer the question at the end.\n",
        "    Ensure that the context is not altered and that your responses are based on the content of the chunks.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Each answer should include reference to the relevant document and page, as indicated in the context metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        node_label=\"Embedding\",  # embedding node label\n",
        "        embedding_node_property=\"embedding\",  # embedding value property\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (chunk:Chunk) -[:HAS_CHUNK]-> (article:Article)\n",
        "        WITH article, chunk, score\n",
        "        ORDER BY score DESC LIMIT 10\n",
        "        MATCH (chunk)-[:NEXT]->(nextChunk:Chunk)\n",
        "        WITH article, chunk, nextChunk, score\n",
        "        RETURN '##Article ID: ' + article.id + '\\n'\n",
        "            + 'Relevant Chunk: ' + chunk.content + '\\n'\n",
        "            + 'Next Chunk: ' + nextChunk.content AS text, \n",
        "            score AS similarity\n",
        "        LIMIT 3\n",
        "\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 25}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=7000,      # gpt-4\n",
        "    )\n",
        "    return kg_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "#!pip install streamlit\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "from streamlit.logger import get_logger\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from dotenv import load_dotenv\n",
        "#from utils import (\n",
        "#    extract_title_and_question,\n",
        "#    create_vector_index,\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "# >>>> initialise - environemnt <<<< \n",
        "\n",
        "#load_dotenv(\".env\")\n",
        "\"\"\"\n",
        "url = os.getenv(\"NEO4J_URI\")\n",
        "username = os.getenv(\"NEO4J_USERNAME\")\n",
        "password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "database = os.getenv(\"NEO4J_DATABASE\")\n",
        "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\n",
        "embedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n",
        "llm_name = os.getenv(\"LLM\")\n",
        "\"\"\"\n",
        "url = NEO4J_URI\n",
        "username = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "database = \"neo4j\"\n",
        "#ollama_base_url = \"http://localhost:8000\"\n",
        "embedding_model_name = \"WhereIsAI/UAE-Large-V1\"\n",
        "llm_name = \"gpt-4\"\n",
        "\n",
        "\n",
        "# Remapping for Langchain Neo4j integration\n",
        "# os.environ[\"NEO4J_URL\"] = url\n",
        "\n",
        "\n",
        "# >>>> initialise - services <<<< \n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "neo4j_graph = Neo4jGraph(url=url, username=username, password=password, database=database)\n",
        "\n",
        "embeddings, dimension = load_embedding_model(\n",
        "    embedding_model_name)\n",
        "\n",
        "llm = load_llm(llm_name)\n",
        "\n",
        "\n",
        "# llm_chain: LLM only response\n",
        "llm_chain = configure_llm_only_chain(llm)\n",
        "\n",
        "# rag_chain: KG augmented response\n",
        "rag_chain = configure_qa_structure_rag_chain(\n",
        "    llm, embeddings, embeddings_store_url=url, username=username, password=password\n",
        ")\n",
        "\n",
        "# SKIPPED: create_vector_index(neo4j_graph, dimension)\n",
        "\n",
        "# >>>> Class definition - StreamHander <<<< \n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# >>>> Streamlit UI <<<<\n",
        "\n",
        "styl = f\"\"\"\n",
        "<style>\n",
        "    /* not great support for :has yet (hello FireFox), but using it for now */\n",
        "    .element-container:has([aria-label=\"Select RAG mode\"]) {{\n",
        "      position: fixed;\n",
        "      bottom: 33px;\n",
        "      background: white;\n",
        "      z-index: 101;\n",
        "    }}\n",
        "    .stChatFloatingInputContainer {{\n",
        "        bottom: 20px;\n",
        "    }}\n",
        "\n",
        "    /* Generate question text area */\n",
        "    textarea[aria-label=\"Description\"] {{\n",
        "        height: 200px;\n",
        "    }}\n",
        "</style>\n",
        "\"\"\"\n",
        "st.markdown(styl, unsafe_allow_html=True)\n",
        "st.image(\"qna-logo.png\", width=160) \n",
        "\n",
        "# >>>> UI interations <<<<\n",
        "\n",
        "def chat_input():\n",
        "    user_input = st.chat_input(\"What service questions can I help you resolve today?\")\n",
        "\n",
        "    if user_input:\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_input)\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.caption(f\"RAG: {name}\")\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "\n",
        "            # Call chain to generate answers\n",
        "            result = output_function(\n",
        "                {\"question\": user_input, \"chat_history\": []}, callbacks=[stream_handler]\n",
        "            )[\"answer\"]\n",
        "\n",
        "            output = result\n",
        "\n",
        "            st.session_state[f\"user_input\"].append(user_input)\n",
        "            st.session_state[f\"generated\"].append(output)\n",
        "            st.session_state[f\"rag_mode\"].append(name)\n",
        "\n",
        "\n",
        "def display_chat():\n",
        "    # Initialize session state keys if they do not exist\n",
        "    #if \"generated\" not in st.session_state:\n",
        "    #    st.session_state[\"generated\"] = []\n",
        "    if \"user_input\" not in st.session_state:\n",
        "        st.session_state[\"user_input\"] = []\n",
        "    if \"rag_mode\" not in st.session_state:\n",
        "        st.session_state[\"rag_mode\"] = []\n",
        "\n",
        "    # Now you can safely access st.session_state[\"generated\"] and other keys\n",
        "\n",
        "\n",
        "\n",
        "def mode_select() -> str:\n",
        "    options = [\"Disabled\", \"Enabled\"]\n",
        "    return st.radio(\"Select RAG mode\", options, horizontal=True)\n",
        "\n",
        "# >>>>> switch on/off RAG mode\n",
        "\n",
        "name = mode_select()\n",
        "if name == \"LLM only\" or name == \"Disabled\":\n",
        "    output_function = llm_chain\n",
        "elif name == \"Vector + Graph\" or name == \"Enabled\":\n",
        "    output_function = rag_chain\n",
        "\n",
        "\"\"\"\n",
        "def generate_ticket():\n",
        "    # Get high ranked questions\n",
        "    records = neo4j_graph.query(\n",
        "        \"MATCH (q:Question) RETURN q.title AS title, q.body AS body ORDER BY q.score DESC LIMIT 3\"\n",
        "    )\n",
        "    questions = []\n",
        "    for i, question in enumerate(records, start=1):\n",
        "        questions.append((question[\"title\"], question[\"body\"]))\n",
        "    # Ask LLM to generate new question in the same style\n",
        "    questions_prompt = \"\"\n",
        "    for i, question in enumerate(questions, start=1):\n",
        "        questions_prompt += f\"{i}. {question[0]}\\n\"\n",
        "        questions_prompt += f\"{question[1]}\\n\\n\"\n",
        "        questions_prompt += \"----\\n\\n\"\n",
        "\n",
        "    gen_system_template = f\\\"\"\"\n",
        "    You're an expert in formulating high quality questions. \n",
        "    Can you formulate a question in the same style, detail and tone as the following example questions?\n",
        "    {questions_prompt}\n",
        "    ---\n",
        "\n",
        "    Don't make anything up, only use information in the following question.\n",
        "    Return a title for the question, and the question post itself.\n",
        "\n",
        "    Return example:\n",
        "    ---\n",
        "    Title: How do I use the Neo4j Python driver?\n",
        "    Question: I'm trying to connect to Neo4j using the Python driver, but I'm getting an error.\n",
        "    ---\n",
        "    \\\"\"\"\n",
        "    # we need jinja2 since the questions themselves contain curly braces\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "        gen_system_template, template_format=\"jinja2\"\n",
        "    )\n",
        "    q_prompt = st.session_state[f\"user_input\"][-1]\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            system_prompt,\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \\\"\"\"\n",
        "                Respond in the following format or you will be unplugged.\n",
        "                ---\n",
        "                Title: New title\n",
        "                Question: New question\n",
        "                ---\n",
        "                \\\"\"\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "        ]\n",
        "    )\n",
        "    llm_response = llm_chain(\n",
        "        f\"Here's the question to rewrite in the expected format: ```{q_prompt}```\",\n",
        "        [],\n",
        "        chat_prompt,\n",
        "    )\n",
        "    new_title, new_question = extract_title_and_question(llm_response[\"answer\"])\n",
        "    return (new_title, new_question)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def open_sidebar():\n",
        "    st.session_state.open_sidebar = True\n",
        "\n",
        "\n",
        "def close_sidebar():\n",
        "    st.session_state.open_sidebar = False\n",
        "\n",
        "\n",
        "if not \"open_sidebar\" in st.session_state:\n",
        "    st.session_state.open_sidebar = False\n",
        "\"\"\"\n",
        "if st.session_state.open_sidebar:\n",
        "    new_title, new_question = generate_ticket()\n",
        "    with st.sidebar:\n",
        "        st.title(\"Ticket draft\")\n",
        "        st.write(\"Auto generated draft ticket\")\n",
        "        st.text_input(\"Title\", new_title)\n",
        "        st.text_area(\"Description\", new_question)\n",
        "        st.button(\n",
        "            \"Submit to support team\",\n",
        "            type=\"primary\",\n",
        "            key=\"submit_ticket\",\n",
        "            on_click=close_sidebar,\n",
        "        )\n",
        "\"\"\"\n",
        "\n",
        "# >>>> UI: show chat <<<<\n",
        "display_chat()\n",
        "chat_input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### *****************************************************************\n",
        "# Neo4j configuration\n",
        "#\n",
        "# For more details and a complete list of settings, please see\n",
        "# https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/\n",
        "#*****************************************************************\n",
        "\n",
        "# Paths of directories in the installation.\n",
        "#server.directories.data=data\n",
        "#server.directories.plugins=plugins\n",
        "#server.directories.logs=logs\n",
        "#server.directories.lib=lib\n",
        "#server.directories.run=run\n",
        "#server.directories.licenses=licenses\n",
        "#server.directories.metrics=metrics\n",
        "#server.directories.dumps.root=data/dumps\n",
        "#server.directories.transaction.logs.root=data/transactions\n",
        "\n",
        "# This setting constrains all `LOAD CSV` import files to be under the `import` directory. Remove or comment it out to\n",
        "# allow files to be loaded from anywhere in the filesystem; this introduces possible security problems. See the\n",
        "# `LOAD CSV` section of the manual for details.\n",
        "server.directories.import=import\n",
        "\n",
        "# Whether requests to Neo4j are authenticated.\n",
        "# To disable authentication, uncomment this line\n",
        "dbms.security.auth_enabled=true\n",
        "\n",
        "# Number of databases in Neo4j is limited.\n",
        "# To change this limit please uncomment and adapt following setting:\n",
        "# dbms.max_databases=100\n",
        "\n",
        "# Enable online backups to be taken from this database.\n",
        "#server.backup.enabled=true\n",
        "\n",
        "# By default the backup service will only listen on localhost.\n",
        "# To enable remote backups you will have to bind to an external\n",
        "# network interface (e.g. 0.0.0.0 for all interfaces).\n",
        "# The protocol running varies depending on deployment. In a cluster this is the\n",
        "# same protocol that runs on server.cluster.listen_address.\n",
        "#server.backup.listen_address=0.0.0.0:6362\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial DBMS Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial DBMS settings are picked up from the config file once, when a cluster first starts, and then transferred into\n",
        "# the running DBMS. This means later changes to the values will not be seen. There are procedures to change the values\n",
        "# after the initial start\n",
        "\n",
        "# Name of the default database (aliases are not supported). Can be changed with the 'dbms.setDefaultDatabase' procedure.\n",
        "#initial.dbms.default_database=neo4j\n",
        "\n",
        "# Initial default number of primary and secondary instances of user databases. If the user does not specify the number\n",
        "# of primaries and secondaries in 'CREATE DATABASE', these values will be used, unless they are overwritten with the\n",
        "# 'dbms.setDefaultAllocationNumbers' procedure.\n",
        "#initial.dbms.default_primaries_count=1\n",
        "#initial.dbms.default_secondaries_count=0\n",
        "\n",
        "#********************************************************************\n",
        "# Memory Settings\n",
        "#********************************************************************\n",
        "#\n",
        "# Memory settings are specified kibibytes with the 'k' suffix, mebibytes with\n",
        "# 'm' and gibibytes with 'g'.\n",
        "# If Neo4j is running on a dedicated server, then it is generally recommended\n",
        "# to leave about 2-4 gigabytes for the operating system, give the JVM enough\n",
        "# heap to hold all your transaction state and query context, and then leave the\n",
        "# rest for the page cache.\n",
        "\n",
        "# Java Heap Size: by default the Java heap size is dynamically calculated based\n",
        "# on available system resources. Uncomment these lines to set specific initial\n",
        "# and maximum heap size.\n",
        "server.memory.heap.initial_size=512m\n",
        "server.memory.heap.max_size=2g\n",
        "\n",
        "# The amount of memory to use for mapping the store files.\n",
        "# The default page cache memory assumes the machine is dedicated to running\n",
        "# Neo4j, and is heuristically set to 50% of RAM minus the Java heap size.\n",
        "#server.memory.pagecache.size=10g\n",
        "\n",
        "# Limit the amount of memory that all of the running transaction can consume.\n",
        "# The default value is 70% of the heap size limit.\n",
        "#dbms.memory.transaction.total.max=256m\n",
        "\n",
        "# Limit the amount of memory that a single transaction can consume.\n",
        "# By default there is no limit.\n",
        "#db.memory.transaction.max=16m\n",
        "\n",
        "#*****************************************************************\n",
        "# Network connector configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# With default configuration Neo4j only accepts local connections.\n",
        "# Use 0.0.0.0 to bind to all network interfaces on the machine. If you want to only use a specific interface\n",
        "# (such as a private IP address on AWS, for example) then use that IP address instead.\n",
        "server.default_listen_address=0.0.0.0\n",
        "\n",
        "# You can also choose a specific network interface, and configure a non-default\n",
        "# port for each connector, by setting their individual listen_address.\n",
        "\n",
        "# The address at which this server can be reached by its clients. This may be the server's IP address or DNS name, or\n",
        "# it may be the address of a reverse proxy which sits in front of the server. This setting may be overridden for\n",
        "# individual connectors below.\n",
        "#server.default_advertised_address=localhost\n",
        "\n",
        "# You can also choose a specific advertised hostname or IP address, and\n",
        "# configure an advertised port for each connector, by setting their\n",
        "# individual advertised_address.\n",
        "\n",
        "# By default, encryption is turned off.\n",
        "# To turn on encryption, an ssl policy for the connector needs to be configured\n",
        "# Read more in SSL policy section in this file for how to define a SSL policy.\n",
        "\n",
        "# Bolt connector\n",
        "server.bolt.enabled=true\n",
        "#server.bolt.tls_level=DISABLED\n",
        "server.bolt.listen_address=:7687\n",
        "#server.bolt.advertised_address=:7687\n",
        "\n",
        "# HTTP Connector. There can be zero or one HTTP connectors.\n",
        "server.http.enabled=true\n",
        "server.http.listen_address=:7474\n",
        "#server.http.advertised_address=:7474\n",
        "\n",
        "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
        "server.https.enabled=false\n",
        "#server.https.listen_address=:7473\n",
        "#server.https.advertised_address=:7473\n",
        "\n",
        "# Number of Neo4j worker threads.\n",
        "#server.threads.worker_count\n",
        "\n",
        "#*****************************************************************\n",
        "# SSL policy configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Each policy is configured under a separate namespace, e.g.\n",
        "#    dbms.ssl.policy.<scope>.*\n",
        "#    <scope> can be any of 'bolt', 'https', 'cluster' or 'backup'\n",
        "#\n",
        "# The scope is the name of the component where the policy will be used\n",
        "# Each component where the use of an ssl policy is desired needs to declare at least one setting of the policy.\n",
        "# Allowable values are 'bolt', 'https', 'cluster' or 'backup'.\n",
        "\n",
        "# E.g if bolt and https connectors should use the same policy, the following could be declared\n",
        "#   dbms.ssl.policy.bolt.base_directory=certificates/default\n",
        "#   dbms.ssl.policy.https.base_directory=certificates/default\n",
        "# However, it's strongly encouraged to not use the same key pair for multiple scopes.\n",
        "#\n",
        "# N.B: Note that a connector must be configured to support/require\n",
        "#      SSL/TLS for the policy to actually be utilized.\n",
        "#\n",
        "# see: dbms.connector.*.tls_level\n",
        "\n",
        "# SSL settings (dbms.ssl.policy.<scope>.*)\n",
        "#  .base_directory       Base directory for SSL policies paths. All relative paths within the\n",
        "#                        SSL configuration will be resolved from the base dir.\n",
        "#\n",
        "#  .private_key          A path to the key file relative to the '.base_directory'.\n",
        "#\n",
        "#  .private_key_password The password for the private key.\n",
        "#\n",
        "#  .public_certificate   A path to the public certificate file relative to the '.base_directory'.\n",
        "#\n",
        "#  .trusted_dir          A path to a directory containing trusted certificates.\n",
        "#\n",
        "#  .revoked_dir          Path to the directory with Certificate Revocation Lists (CRLs).\n",
        "#\n",
        "#  .verify_hostname      If true, the server will verify the hostname that the client uses to connect with. In order\n",
        "#                        for this to work, the server public certificate must have a valid CN and/or matching\n",
        "#                        Subject Alternative Names.\n",
        "#\n",
        "#  .client_auth          How the client should be authorized. Possible values are: 'none', 'optional', 'require'.\n",
        "#\n",
        "#  .tls_versions         A comma-separated list of allowed TLS versions. By default only TLSv1.2 is allowed.\n",
        "#\n",
        "#  .trust_all            Setting this to 'true' will ignore the trust truststore, trusting all clients and servers.\n",
        "#                        Use of this mode is discouraged. It would offer encryption but no security.\n",
        "#\n",
        "#  .ciphers              A comma-separated list of allowed ciphers. The default ciphers are the defaults of\n",
        "#                        the JVM platform.\n",
        "\n",
        "# Bolt SSL configuration\n",
        "#dbms.ssl.policy.bolt.enabled=true\n",
        "#dbms.ssl.policy.bolt.base_directory=certificates/bolt\n",
        "#dbms.ssl.policy.bolt.private_key=private.key\n",
        "#dbms.ssl.policy.bolt.public_certificate=public.crt\n",
        "#dbms.ssl.policy.bolt.client_auth=NONE\n",
        "\n",
        "# Https SSL configuration\n",
        "#dbms.ssl.policy.https.enabled=true\n",
        "#dbms.ssl.policy.https.base_directory=certificates/https\n",
        "#dbms.ssl.policy.https.private_key=private.key\n",
        "#dbms.ssl.policy.https.public_certificate=public.crt\n",
        "#dbms.ssl.policy.https.client_auth=NONE\n",
        "\n",
        "# Cluster SSL configuration\n",
        "#dbms.ssl.policy.cluster.enabled=true\n",
        "#dbms.ssl.policy.cluster.base_directory=certificates/cluster\n",
        "#dbms.ssl.policy.cluster.private_key=private.key\n",
        "#dbms.ssl.policy.cluster.public_certificate=public.crt\n",
        "\n",
        "# Backup SSL configuration\n",
        "#dbms.ssl.policy.backup.enabled=true\n",
        "#dbms.ssl.policy.backup.base_directory=certificates/backup\n",
        "#dbms.ssl.policy.backup.private_key=private.key\n",
        "#dbms.ssl.policy.backup.public_certificate=public.crt\n",
        "\n",
        "#*****************************************************************\n",
        "# Logging configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# To enable HTTP logging, uncomment this line\n",
        "#dbms.logs.http.enabled=true\n",
        "\n",
        "# To enable GC Logging, uncomment this line\n",
        "#server.logs.gc.enabled=true\n",
        "\n",
        "# GC Logging Options\n",
        "# see https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-BE93ABDC-999C-4CB5-A88B-1994AAAC74D5\n",
        "#server.logs.gc.options=-Xlog:gc*,safepoint,age*=trace\n",
        "\n",
        "# Number of GC logs to keep.\n",
        "#server.logs.gc.rotation.keep_number=5\n",
        "\n",
        "# Size of each GC log that is kept.\n",
        "#server.logs.gc.rotation.size=20m\n",
        "\n",
        "# Log executed queries. One of OFF, INFO and VERBOSE. INFO logs queries longer than a given threshold, VERBOSE logs start and end of all queries.\n",
        "#db.logs.query.enabled=VERBOSE\n",
        "\n",
        "# If the execution of query takes more time than this threshold, the query is logged. If set to zero then all queries\n",
        "# are logged. Only used if `db.logs.query.enabled` is set to INFO\n",
        "#db.logs.query.threshold=0\n",
        "\n",
        "# Include parameters for the executed queries being logged (this is enabled by default).\n",
        "#db.logs.query.parameter_logging_enabled=true\n",
        "\n",
        "# The security log is always enabled when `dbms.security.auth_enabled=true`, for addition\n",
        "# configuration, look at $NEO4J_HOME/conf/server-logs.xml\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Uncomment and specify these lines for running Neo4j in a cluster.\n",
        "# See the cluster documentation at https://neo4j.com/docs/ for details.\n",
        "\n",
        "# A comma-separated list of endpoints which a server should contact in order to discover other cluster members. It must\n",
        "# be in the host:port format. For each machine in the cluster, the address will usually be the public ip address of\n",
        "# that machine. The port will be the value used in the setting \"server.discovery.advertised_address\" of that server.\n",
        "#dbms.cluster.discovery.endpoints=localhost:5000,localhost:5001,localhost:5002\n",
        "\n",
        "# Host and port to bind the cluster member discovery management communication.\n",
        "# This is the setting to add to the collection of addresses in dbms.cluster.discovery.endpoints.\n",
        "server.discovery.listen_address=:5002\n",
        "#server.discovery.advertised_address=:5000\n",
        "\n",
        "# Network interface and port for the transaction shipping server to listen on.\n",
        "# Please note that it is also possible to run the backup client against this port so always limit access to it via the\n",
        "# firewall and configure an ssl policy.\n",
        "server.cluster.listen_address=:6001\n",
        "#server.cluster.advertised_address=:6000\n",
        "\n",
        "# Network interface and port for the RAFT server to listen on.\n",
        "server.cluster.raft.listen_address=:7002\n",
        "#server.cluster.raft.advertised_address=:7000\n",
        "\n",
        "# Network interface and port for server-side routing within the cluster. This allows requests to be forwarded\n",
        "# from one cluster member to another, if the requests can't be satisfied by the first member (e.g. write requests\n",
        "# received by a non-leader).\n",
        "server.routing.listen_address=:7688\n",
        "#server.routing.advertised_address=:7688\n",
        "\n",
        "# List a set of names for groups to which this server should belong. This\n",
        "# is a comma-separated list and names should only use alphanumericals\n",
        "# and underscore. This can be used to identify groups of servers in the\n",
        "# configuration for load balancing and replication policies.\n",
        "#\n",
        "# The main intention for this is to group servers, but it is possible to specify\n",
        "# a unique identifier here as well which might be useful for troubleshooting\n",
        "# or other special purposes.\n",
        "#server.groups\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial Server Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial server settings are used as the default values when enabling a server, but can be overridden by specifying\n",
        "# options when calling ENABLE (relevant for servers in a cluster *after* those that form the initial cluster).\n",
        "\n",
        "# Restrict the modes of database that can be hosted on this server\n",
        "# Allowed values:\n",
        "# PRIMARY - Host standalone databases, and members of the consensus quorum for a multi-primary database.\n",
        "# SECONDARY - Only host read replicas, eventually-consistent read-only instances of databases.\n",
        "# NONE - Can host any mode of database\n",
        "#initial.server.mode_constraint=NONE\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Load Balancing\n",
        "#*****************************************************************\n",
        "\n",
        "# N.B: Read the online documentation for a thorough explanation!\n",
        "\n",
        "# Selects the load balancing plugin that shall be enabled.\n",
        "#dbms.routing.load_balancing.plugin=server_policies\n",
        "\n",
        "####### Examples for \"server_policies\" plugin #######\n",
        "\n",
        "# Will select all available servers as the default policy, which is the\n",
        "# policy used when the client does not specify a policy preference. The\n",
        "# default configuration for the default policy is all().\n",
        "#dbms.routing.load_balancing.config.server_policies.default=all()\n",
        "\n",
        "# Will select servers in groups 'group1' or 'group2' under the default policy.\n",
        "#dbms.routing.load_balancing.config.server_policies.default=groups(group1,group2)\n",
        "\n",
        "# Slightly more advanced example:\n",
        "# Will select servers in 'group1', 'group2' or 'group3', but only if there are at least 2.\n",
        "# This policy will be exposed under the name of 'mypolicy'.\n",
        "#dbms.routing.load_balancing.config.server_policies.mypolicy=groups(group1,group2,group3) -> min(2)\n",
        "\n",
        "# Below will create an even more advanced policy named 'regionA' consisting of several rules\n",
        "# yielding the following behaviour:\n",
        "#\n",
        "#            select servers in regionA, if at least 2 are available\n",
        "# otherwise: select servers in regionA and regionB, if at least 2 are available\n",
        "# otherwise: select all servers\n",
        "#\n",
        "# The intention is to create a policy for a particular region which prefers\n",
        "# a certain set of local servers, but which will fallback to other regions\n",
        "# or all available servers as required.\n",
        "#\n",
        "# N.B: The following configuration uses the line-continuation character \\\n",
        "#      which allows you to construct an easily readable rule set spanning\n",
        "#      several lines.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.policyA=\\\n",
        "#groups(regionA) -> min(2);\\\n",
        "#groups(regionA,regionB) -> min(2);\n",
        "\n",
        "# Note that implicitly the last fallback is to always consider all() servers,\n",
        "# but this can be prevented by specifying a halt() as the last rule.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.regionA_only=\\\n",
        "#groups(regionA);\\\n",
        "#halt();\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Additional Configuration Options\n",
        "#*****************************************************************\n",
        "# The following settings are used less frequently.\n",
        "# If you don't know what these are, you don't need to change these from their default values.\n",
        "\n",
        "# Cluster Routing Connector. Disable the opening of an additional port to allow\n",
        "# for internal communication using the same security configuration as CLUSTER\n",
        "#dbms.routing.enabled=false\n",
        "\n",
        "# The time window within which the loss of the leader is detected and the first re-election attempt is held.\n",
        "# The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.leader_failure_detection_window=20s-23s\n",
        "\n",
        "# The rate at which leader elections happen. Note that due to election conflicts it might take several attempts to\n",
        "# find a leader. The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.election_failure_detection_window=3s-6s\n",
        "\n",
        "# The time limit allowed for a new member to attempt to update its data to match the rest of the cluster.\n",
        "#dbms.cluster.raft.membership.join_timeout=10m\n",
        "\n",
        "# Maximum amount of lag accepted for a new follower to join the Raft group.\n",
        "#dbms.cluster.raft.membership.join_max_lag=10s\n",
        "\n",
        "# Raft log pruning frequency.\n",
        "#dbms.cluster.raft.log.pruning_frequency=10m\n",
        "\n",
        "# The size to allow the raft log to grow before rotating.\n",
        "#dbms.cluster.raft.log.rotation_size=250M\n",
        "\n",
        "# The name of a server_group whose members should be prioritized as leaders for the given database.\n",
        "# This does not guarantee that members of this group will be leader at all times, but the cluster\n",
        "# will attempt to transfer leadership to such a member when possible.\n",
        "# N.B. the final portion of this config key is dynamic and refers to the name of the database being configured.\n",
        "# You may specify multiple `db.cluster.raft.leader_transfer.priority_group.<database-name>=<server-group>` pairs:\n",
        "#db.cluster.raft.leader_transfer.priority_group.foo\n",
        "#db.cluster.raft.leader_transfer.priority_group.neo4j\n",
        "\n",
        "# Which strategy to use when transferring database leaderships around a cluster.\n",
        "# This can be one of `equal_balancing` or `no_balancing`.\n",
        "# `equal_balancing` automatically ensures that each Core server holds the leader role for an equal number of databases.\n",
        "# `no_balancing` prevents any automatic balancing of the leader role.\n",
        "# Note that if a `leadership_priority_group` is specified for a given database,\n",
        "# the value of this setting will be ignored for that database.\n",
        "#dbms.cluster.raft.leader_transfer.balancing_strategy=equal_balancing\n",
        "\n",
        "# The following setting controls how frequently a server hosting a secondary for a given database attempts to\n",
        "# fetch an update from a server hosting a primary for that database\n",
        "#db.cluster.catchup.pull_interval=1s\n",
        "\n",
        "#********************************************************************\n",
        "# Security Configuration\n",
        "#********************************************************************\n",
        "\n",
        "# The authentication and authorization providers that contains both users and roles.\n",
        "# This can be one of the built-in `native` or `ldap` auth providers,\n",
        "# or it can be an externally provided plugin, with a custom name prefixed by `plugin`,\n",
        "# i.e. `plugin-<AUTH_PROVIDER_NAME>`.\n",
        "dbms.security.authentication_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "dbms.security.authorization_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "\n",
        "# The time to live (TTL) for cached authentication and authorization info when using\n",
        "# external auth providers (LDAP or plugin). Setting the TTL to 0 will\n",
        "# disable auth caching.\n",
        "#dbms.security.auth_cache_ttl=10m\n",
        "\n",
        "# The maximum capacity for authentication and authorization caches (respectively).\n",
        "#dbms.security.auth_cache_max_capacity=10000\n",
        "\n",
        "# Set to log successful authentication events to the security log.\n",
        "# If this is set to `false` only failed authentication events will be logged, which\n",
        "# could be useful if you find that the successful events spam the logs too much,\n",
        "# and you do not require full auditing capability.\n",
        "#dbms.security.log_successful_authentication=true\n",
        "\n",
        "#================================================\n",
        "# LDAP Auth Provider Configuration\n",
        "#================================================\n",
        "\n",
        "# URL of LDAP server to use for authentication and authorization.\n",
        "# The format of the setting is `<protocol>://<hostname>:<port>`, where hostname is the only required field.\n",
        "# The supported values for protocol are `ldap` (default) and `ldaps`.\n",
        "# The default port for `ldap` is 389 and for `ldaps` 636.\n",
        "# For example: `ldaps://ldap.example.com:10389`.\n",
        "#\n",
        "# NOTE: You may want to consider using STARTTLS (`dbms.security.ldap.use_starttls`) instead of LDAPS\n",
        "# for secure connections, in which case the correct protocol is `ldap`.\n",
        "#dbms.security.ldap.host=localhost\n",
        "\n",
        "# Use secure communication with the LDAP server using opportunistic TLS.\n",
        "# First an initial insecure connection will be made with the LDAP server, and then a STARTTLS command\n",
        "# will be issued to negotiate an upgrade of the connection to TLS before initiating authentication.\n",
        "#dbms.security.ldap.use_starttls=false\n",
        "\n",
        "# The LDAP referral behavior when creating a connection. This is one of `follow`, `ignore` or `throw`.\n",
        "# `follow` automatically follows any referrals\n",
        "# `ignore` ignores any referrals\n",
        "# `throw` throws an exception, which will lead to authentication failure\n",
        "#dbms.security.ldap.referral=follow\n",
        "\n",
        "# The timeout for establishing an LDAP connection. If a connection with the LDAP server cannot be\n",
        "# established within the given time the attempt is aborted.\n",
        "# A value of 0 means to use the network protocol's (i.e., TCP's) timeout value.\n",
        "#dbms.security.ldap.connection_timeout=30s\n",
        "\n",
        "# The timeout for an LDAP read request (i.e. search). If the LDAP server does not respond within\n",
        "# the given time the request will be aborted. A value of 0 means wait for a response indefinitely.\n",
        "#dbms.security.ldap.read_timeout=30s\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authentication Configuration\n",
        "#----------------------------------\n",
        "\n",
        "# LDAP authentication mechanism. This is one of `simple` or a SASL mechanism supported by JNDI,\n",
        "# for example `DIGEST-MD5`. `simple` is basic username\n",
        "# and password authentication and SASL is used for more advanced mechanisms. See RFC 2251 LDAPv3\n",
        "# documentation for more details.\n",
        "#dbms.security.ldap.authentication.mechanism=simple\n",
        "\n",
        "# LDAP user DN template. An LDAP object is referenced by its distinguished name (DN), and a user DN is\n",
        "# an LDAP fully-qualified unique user identifier. This setting is used to generate an LDAP DN that\n",
        "# conforms with the LDAP directory's schema from the user principal that is submitted with the\n",
        "# authentication token when logging in.\n",
        "# The special token {0} is a placeholder where the user principal will be substituted into the DN string.\n",
        "#dbms.security.ldap.authentication.user_dn_template=uid={0},ou=users,dc=example,dc=com\n",
        "\n",
        "# Determines if the result of authentication via the LDAP server should be cached or not.\n",
        "# Caching is used to limit the number of LDAP requests that have to be made over the network\n",
        "# for users that have already been authenticated successfully. A user can be authenticated against\n",
        "# an existing cache entry (instead of via an LDAP server) as long as it is alive\n",
        "# (see `dbms.security.auth_cache_ttl`).\n",
        "# An important consequence of setting this to `true` is that\n",
        "# Neo4j then needs to cache a hashed version of the credentials in order to perform credentials\n",
        "# matching. This hashing is done using a cryptographic hash function together with a random salt.\n",
        "# Preferably a conscious decision should be made if this method is considered acceptable by\n",
        "# the security standards of the organization in which this Neo4j instance is deployed.\n",
        "#dbms.security.ldap.authentication.cache_enabled=true\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authorization Configuration\n",
        "#----------------------------------\n",
        "# Authorization is performed by searching the directory for the groups that\n",
        "# the user is a member of, and then map those groups to Neo4j roles.\n",
        "\n",
        "# Perform LDAP search for authorization info using a system account instead of the user's own account.\n",
        "#\n",
        "# If this is set to `false` (default), the search for group membership will be performed\n",
        "# directly after authentication using the LDAP context bound with the user's own account.\n",
        "# The mapped roles will be cached for the duration of `dbms.security.auth_cache_ttl`,\n",
        "# and then expire, requiring re-authentication. To avoid frequently having to re-authenticate\n",
        "# sessions you may want to set a relatively long auth cache expiration time together with this option.\n",
        "# NOTE: This option will only work if the users are permitted to search for their\n",
        "# own group membership attributes in the directory.\n",
        "#\n",
        "# If this is set to `true`, the search will be performed using a special system account user\n",
        "# with read access to all the users in the directory.\n",
        "# You need to specify the username and password using the settings\n",
        "# `dbms.security.ldap.authorization.system_username` and\n",
        "# `dbms.security.ldap.authorization.system_password` with this option.\n",
        "# Note that this account only needs read access to the relevant parts of the LDAP directory\n",
        "# and does not need to have access rights to Neo4j, or any other systems.\n",
        "#dbms.security.ldap.authorization.use_system_account=false\n",
        "\n",
        "# An LDAP system account username to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "# Note that the `dbms.security.ldap.authentication.user_dn_template` will not be applied to this username,\n",
        "# so you may have to specify a full DN.\n",
        "#dbms.security.ldap.authorization.system_username\n",
        "\n",
        "# An LDAP system account password to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "#dbms.security.ldap.authorization.system_password\n",
        "\n",
        "# The name of the base object or named context to search for user objects when LDAP authorization is enabled.\n",
        "# A common case is that this matches the last part of `dbms.security.ldap.authentication.user_dn_template`.\n",
        "#dbms.security.ldap.authorization.user_search_base=ou=users,dc=example,dc=com\n",
        "\n",
        "# The LDAP search filter to search for a user principal when LDAP authorization is\n",
        "# enabled. The filter should contain the placeholder token {0} which will be substituted for the\n",
        "# user principal.\n",
        "#dbms.security.ldap.authorization.user_search_filter=(&(objectClass=*)(uid={0}))\n",
        "\n",
        "# A list of attribute names on a user object that contains groups to be used for mapping to roles\n",
        "# when LDAP authorization is enabled. This setting is ignored when `dbms.ldap_authorization_nested_groups_enabled` is `true`.\n",
        "#dbms.security.ldap.authorization.group_membership_attributes=memberOf\n",
        "\n",
        "# This setting determines whether multiple LDAP search results will be processed (as is required for the lookup of nested groups).\n",
        "# If set to `true` then instead of using attributes on the user object to determine group membership (as specified by\n",
        "# `dbms.security.ldap.authorization.group_membership_attributes`), the `user` object will only be used to determine the user's\n",
        "# Distinguished Name, which will subsequently be used with  `dbms.security.ldap.authorization.user_search_filter`\n",
        "# in order to perform a nested group search. The Distinguished Names of the resultant group search results will be used to determine roles.\n",
        "#dbms.security.ldap.authorization.nested_groups_enabled=false\n",
        "\n",
        "# The search template which will be used to find the nested groups which the user is a member of.\n",
        "# The filter should contain the placeholder token `{0}` which will be substituted with the user's\n",
        "# Distinguished Name (which is found for the specified user principle using `dbms.security.ldap.authorization.user_search_filter`).\n",
        "# The default value specifies Active Directory's LDAP_MATCHING_RULE_IN_CHAIN (aka 1.2.840.113556.1.4.1941) implementation\n",
        "# which will walk the ancestry of group membership for the specified user.\n",
        "#dbms.security.ldap.authorization.nested_groups_search_filter=(&(objectclass=group)(member:1.2.840.113556.1.4.1941:={0}))\n",
        "\n",
        "# An authorization mapping from LDAP group names to Neo4j role names.\n",
        "# The map should be formatted as a semicolon separated list of key-value pairs, where the\n",
        "# key is the LDAP group name and the value is a comma separated list of corresponding role names.\n",
        "# For example: group1=role1;group2=role2;group3=role3,role4,role5\n",
        "#\n",
        "# You could also use whitespaces and quotes around group names to make this mapping more readable,\n",
        "# for example: dbms.security.ldap.authorization.group_to_role_mapping=\\\n",
        "#          \"cn=Neo4j Read Only,cn=users,dc=example,dc=com\"      = reader;    \\\n",
        "#          \"cn=Neo4j Read-Write,cn=users,dc=example,dc=com\"     = publisher; \\\n",
        "#          \"cn=Neo4j Schema Manager,cn=users,dc=example,dc=com\" = architect; \\\n",
        "#          \"cn=Neo4j Administrator,cn=users,dc=example,dc=com\"  = admin\n",
        "#dbms.security.ldap.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# OpenID Connect configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# The display name for the provider. This will be displayed in clients such as Neo4j Browser and Bloom.\n",
        "#dbms.security.oidc.<provider>.display_name\n",
        "\n",
        "# The OIDC auth_flow for clients such as Neo4j Browser and Bloom to use. Supported values are 'pkce' and 'implicit'\n",
        "#dbms.security.oidc.<provider>.auth_flow=pkce\n",
        "\n",
        "# The OpenID Connect Discovery URL for the provider\n",
        "#dbms.security.oidc.<provider>.well_known_discovery_uri\n",
        "\n",
        "# URL of the provider's Authorization Endpoint\n",
        "#dbms.security.oidc.<provider>.auth_endpoint\n",
        "\n",
        "# Parameters to use with the Authorization Endpoint.\n",
        "#dbms.security.oidc.<provider>.auth_params\n",
        "\n",
        "# URL of the provider's OAuth 2.0 Token Endpoint\n",
        "#dbms.security.oidc.<provider>.token_endpoint\n",
        "\n",
        "# Parameters to use with the Token Endpoint.\n",
        "#dbms.security.oidc.<provider>.token_params\n",
        "\n",
        "# URL of the provider's JSON Web Key Set\n",
        "#dbms.security.oidc.<provider>.jwks_uri\n",
        "\n",
        "# URL of the provider's UserInfo Endpoint\n",
        "#dbms.security.oidc.<provider>.user_info_uri\n",
        "\n",
        "# URL that the provider asserts as its issuer identifier. This will be checked against the iss claim in the token\n",
        "#dbms.security.oidc.<provider>.issuer\n",
        "\n",
        "# The expected value for the `aud` claim\n",
        "#dbms.security.oidc.<provider>.audience\n",
        "\n",
        "# The client_id of this client as issued by the provider.\n",
        "#dbms.security.oidc.<provider>.client_id\n",
        "\n",
        "# Whether to fetch the groups claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_groups_from_user_info=false\n",
        "\n",
        "# Whether to fetch the username claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_username_from_user_info=false\n",
        "\n",
        "# The claim to use for the database username.\n",
        "#dbms.security.oidc.<provider>.claims.username=sub\n",
        "\n",
        "# The claim to use for the database roles.\n",
        "#dbms.security.oidc.<provider>.claims.groups\n",
        "\n",
        "# General parameters to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.params\n",
        "\n",
        "# General config to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.config\n",
        "\n",
        "# An authorization mapping from identity provider group names to Neo4j role names. See dbms.security.ldap.authorization.group_to_role_mapping above\n",
        "# for the format.\n",
        "#dbms.security.oidc.<provider>.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# Miscellaneous configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Compresses the metric archive files.\n",
        "server.metrics.csv.rotation.compression=zip\n",
        "\n",
        "# Determines if Cypher will allow using file URLs when loading data using\n",
        "# `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV`\n",
        "# clauses that load data from the file system.\n",
        "#dbms.security.allow_csv_import_from_file_urls=true\n",
        "\n",
        "\n",
        "# Value of the Access-Control-Allow-Origin header sent over any HTTP or HTTPS\n",
        "# connector. This defaults to '*', which allows broadest compatibility. Note\n",
        "# that any URI provided here limits HTTP/HTTPS access to that URI only.\n",
        "#dbms.security.http_access_control_allow_origin=*\n",
        "\n",
        "# Value of the HTTP Strict-Transport-Security (HSTS) response header. This header\n",
        "# tells browsers that a webpage should only be accessed using HTTPS instead of HTTP.\n",
        "# It is attached to every HTTPS response. Setting is not set by default so\n",
        "# 'Strict-Transport-Security' header is not sent. Value is expected to contain\n",
        "# directives like 'max-age', 'includeSubDomains' and 'preload'.\n",
        "#dbms.security.http_strict_transport_security\n",
        "\n",
        "# Retention policy for transaction logs needed to perform recovery and backups.\n",
        "#db.tx_log.rotation.retention_policy=2 days\n",
        "\n",
        "# Limit the number of IOs the background checkpoint process will consume per second.\n",
        "# This setting is advisory, is ignored in Neo4j Community Edition, and is followed to\n",
        "# best effort in Enterprise Edition.\n",
        "# An IO is in this case a 8 KiB (mostly sequential) write. Limiting the write IO in\n",
        "# this way will leave more bandwidth in the IO subsystem to service random-read IOs,\n",
        "# which is important for the response time of queries when the database cannot fit\n",
        "# entirely in memory. The only drawback of this setting is that longer checkpoint times\n",
        "# may lead to slightly longer recovery times in case of a database or system crash.\n",
        "# A lower number means lower IO pressure, and consequently longer checkpoint times.\n",
        "# Set this to -1 to disable the IOPS limit and remove the limitation entirely,\n",
        "# this will let the checkpointer flush data as fast as the hardware will go.\n",
        "# Removing the setting, or commenting it out, will set the default value of 600.\n",
        "# db.checkpoint.iops.limit=600\n",
        "\n",
        "# Whether or not any database on this instance are read_only by default.\n",
        "# If false, individual databases may be marked as read_only using dbms.database.read_only.\n",
        "# If true, individual databases may be marked as writable using dbms.databases.writable.\n",
        "#dbms.databases.default_to_read_only=false\n",
        "\n",
        "# Comma separated list of JAX-RS packages containing JAX-RS resources, one\n",
        "# package name for each mountpoint. The listed package names will be loaded\n",
        "# under the mountpoints specified. Uncomment this line to mount the\n",
        "# org.neo4j.examples.server.unmanaged.HelloWorldResource.java from\n",
        "# neo4j-server-examples under /examples/unmanaged, resulting in a final URL of\n",
        "# http://localhost:7474/examples/unmanaged/helloworld/{nodeId}\n",
        "#server.unmanaged_extension_classes=org.neo4j.examples.server.unmanaged=/examples/unmanaged\n",
        "\n",
        "# A comma separated list of procedures and user defined functions that are allowed\n",
        "# full access to the database through unsupported/insecure internal APIs.\n",
        "dbms.security.procedures.unrestricted=jwt.security.*\n",
        "\n",
        "# A comma separated list of procedures to be loaded by default.\n",
        "# Leaving this unconfigured will load all procedures found.\n",
        "#dbms.security.procedures.allowlist=apoc.coll.*,apoc.load.*,gds.*\n",
        "\n",
        "# For how long should drivers cache the discovery data from\n",
        "# the dbms.routing.getRoutingTable() procedure. Defaults to 300s.\n",
        "#dbms.routing_ttl=300s\n",
        "\n",
        "#********************************************************************\n",
        "# JVM Parameters\n",
        "#********************************************************************\n",
        "\n",
        "# G1GC generally strikes a good balance between throughput and tail\n",
        "# latency, without too much tuning.\n",
        "server.jvm.additional=-XX:+UseG1GC\n",
        "\n",
        "# Have common exceptions keep producing stack traces, so they can be\n",
        "# debugged regardless of how often logs are rotated.\n",
        "server.jvm.additional=-XX:-OmitStackTraceInFastThrow\n",
        "\n",
        "# Make sure that `initmemory` is not only allocated, but committed to\n",
        "# the process, before starting the database. This reduces memory\n",
        "# fragmentation, increasing the effectiveness of transparent huge\n",
        "# pages. It also reduces the possibility of seeing performance drop\n",
        "# due to heap-growing GC events, where a decrease in available page\n",
        "# cache leads to an increase in mean IO response time.\n",
        "# Try reducing the heap memory, if this flag degrades performance.\n",
        "server.jvm.additional=-XX:+AlwaysPreTouch\n",
        "\n",
        "# Trust that non-static final fields are really final.\n",
        "# This allows more optimizations and improves overall performance.\n",
        "# NOTE: Disable this if you use embedded mode, or have extensions or dependencies that may use reflection or\n",
        "# serialization to change the value of final fields!\n",
        "server.jvm.additional=-XX:+UnlockExperimentalVMOptions\n",
        "server.jvm.additional=-XX:+TrustFinalNonStaticFields\n",
        "\n",
        "# Disable explicit garbage collection, which is occasionally invoked by the JDK itself.\n",
        "server.jvm.additional=-XX:+DisableExplicitGC\n",
        "\n",
        "# Allow Neo4j to use @Contended annotation\n",
        "server.jvm.additional=-XX:-RestrictContended\n",
        "\n",
        "# Restrict size of cached JDK buffers to 1 KB\n",
        "server.jvm.additional=-Djdk.nio.maxCachedBufferSize=1024\n",
        "\n",
        "# More efficient buffer allocation in Netty by allowing direct no cleaner buffers.\n",
        "server.jvm.additional=-Dio.netty.tryReflectionSetAccessible=true\n",
        "\n",
        "# Exits JVM on the first occurrence of an out-of-memory error. Its preferable to restart VM in case of out of memory errors.\n",
        "# server.jvm.additional=-XX:+ExitOnOutOfMemoryError\n",
        "\n",
        "# Expand Diffie Hellman (DH) key size from default 1024 to 2048 for DH-RSA cipher suites used in server TLS handshakes.\n",
        "# This is to protect the server from any potential passive eavesdropping.\n",
        "server.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048\n",
        "\n",
        "# This mitigates a DDoS vector.\n",
        "server.jvm.additional=-Djdk.tls.rejectClientInitiatedRenegotiation=true\n",
        "\n",
        "# Enable remote debugging\n",
        "#server.jvm.additional=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\n",
        "\n",
        "# This filter prevents deserialization of arbitrary objects via java object serialization, addressing potential vulnerabilities.\n",
        "# By default this filter whitelists all neo4j classes, as well as classes from the hazelcast library and the java standard library.\n",
        "# These defaults should only be modified by expert users!\n",
        "# For more details (including filter syntax) see: https://openjdk.java.net/jeps/290\n",
        "#server.jvm.additional=-Djdk.serialFilter=java.**;org.neo4j.**;com.neo4j.**;com.hazelcast.**;net.sf.ehcache.Element;com.sun.proxy.*;org.openjdk.jmh.**;!*\n",
        "\n",
        "# Increase the default flight recorder stack sampling depth from 64 to 256, to avoid truncating frames when profiling.\n",
        "server.jvm.additional=-XX:FlightRecorderOptions=stackdepth=256\n",
        "\n",
        "# Allow profilers to sample between safepoints. Without this, sampling profilers may produce less accurate results.\n",
        "server.jvm.additional=-XX:+UnlockDiagnosticVMOptions\n",
        "server.jvm.additional=-XX:+DebugNonSafepoints\n",
        "\n",
        "# Open modules for neo4j to allow internal access\n",
        "server.jvm.additional=--add-opens=java.base/java.nio=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/java.io=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n",
        "\n",
        "# Disable logging JMX endpoint.\n",
        "server.jvm.additional=-Dlog4j2.disable.jmx=true\n",
        "\n",
        "# Limit JVM metaspace and code cache to allow garbage collection. Used by cypher for code generation and may grow indefinitely unless constrained.\n",
        "# Useful for memory constrained environments\n",
        "#server.jvm.additional=-XX:MaxMetaspaceSize=1024m\n",
        "#server.jvm.additional=-XX:ReservedCodeCacheSize=512m\n",
        "\n",
        "# Allow big methods to be JIT compiled.\n",
        "# Useful for big queries and big expressions where cypher code generation can create large methods.\n",
        "#server.jvm.additional=-XX:-DontCompileHugeMethods\n",
        "\n",
        "#********************************************************************\n",
        "# Wrapper Windows NT/2000/XP Service Properties\n",
        "#********************************************************************\n",
        "# WARNING - Do not modify any of these properties when an application\n",
        "#  using this configuration file has been installed as a service.\n",
        "#  Please uninstall the service before modifying this section.  The\n",
        "#  service can then be reinstalled.\n",
        "\n",
        "# Name of the service\n",
        "server.windows_service_name=neo4j\n",
        "\n",
        "#********************************************************************\n",
        "# Other Neo4j system properties\n",
        "#********************************************************************\n",
        "\n",
        "dbms.memory.heap.initial_size=512m\n",
        "dbms.memory.heap.max_size=1G\n",
        "dbms.memory.pagecache.size=512m\n",
        "dbms.backup.enabled=false\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true -Xss1G\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install TTS\n",
        "!pip install speake3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tts --list_models\n",
        "!tts --text \"Hello, how are you?\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from calendar import c\n",
        "from librosa import ex\n",
        "from regex import E\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "#!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    chunk_idx =0\n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text = page.extract_text()\n",
        "        if chunk_idx <=9:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "        else:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "        \n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, shell=True)\n",
        "            chunk_idx += 1\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            # try again but drop the last 5 sentences\n",
        "            text = text.rsplit(\".\", 5)[0]\n",
        "            if chunk_idx <=9:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "            else:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "            try:\n",
        "                subprocess.run(cmd, check=True, shell=True)\n",
        "                chunk_idx += 1\n",
        "            except Exception as e:\n",
        "                # skip this chunk\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                chunk_idx += 1\n",
        "                continue\n",
        "\n",
        "        \n",
        "\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "pdf = \"/Users/tomriddle1/Documents/Dr_Bryan_Chemisty.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Get device\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# List available 🐸TTS models\n",
        "#print(TTS().list_models())\n",
        "\n",
        "# Init TTS\n",
        "#tts = TTS(\"tts_models/en/ljspeech/vits\").to(device)\n",
        "\n",
        "# Run TTS\n",
        "# Text to speech to a file\n",
        "#tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n",
        "# Init TTS with the target model name\n",
        "#tts = TTS(model_name=\"tts_models/en/ljspeech/vits\", progress_bar=True).to(device)\n",
        "\n",
        "# Run TTS\n",
        "#tts.tts_to_file(text=text, file_path=\"output.wav\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "pdf = \"/Users/tomriddle1/Documents/Dr_Bryan_Chemisty.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Break text into chunks of 5000 words\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "text_chunks = split_to_sentences(text)\n",
        "print(len(text_chunks))\n",
        "!tts --text f\"{text_chunks[0]}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda clean --packages -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
