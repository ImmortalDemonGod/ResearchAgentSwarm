{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperQA - A Question Answering Dataset for Academic Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting paper-qa\n",
            "  Using cached paper_qa-3.13.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pypdf (from paper-qa)\n",
            "  Using cached pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pydantic<2 (from paper-qa)\n",
            "  Downloading pydantic-1.10.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m955.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting langchain>=0.0.303 (from paper-qa)\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting openai<1 (from paper-qa)\n",
            "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting faiss-cpu (from paper-qa)\n",
            "  Downloading faiss_cpu-1.7.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting PyCryptodome (from paper-qa)\n",
            "  Using cached pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting html2text (from paper-qa)\n",
            "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting tiktoken>=0.4.0 (from paper-qa)\n",
            "  Downloading tiktoken-0.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading SQLAlchemy-2.0.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading aiohttp-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain>=0.0.303->paper-qa)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain>=0.0.303->paper-qa)\n",
            "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->paper-qa)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.13 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.9 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading langchain_core-0.1.11-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading langsmith-0.0.81-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy<2,>=1 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3,>=2 (from langchain>=0.0.303->paper-qa)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain>=0.0.303->paper-qa)\n",
            "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting tqdm (from openai<1->paper-qa)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from pydantic<2->paper-qa) (4.9.0)\n",
            "Collecting regex>=2022.1.18 (from tiktoken>=0.4.0->paper-qa)\n",
            "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa)\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa)\n",
            "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa)\n",
            "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa)\n",
            "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->paper-qa)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa)\n",
            "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (23.2)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain>=0.0.303->paper-qa)\n",
            "  Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain>=0.0.303->paper-qa)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain>=0.0.303->paper-qa)\n",
            "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain>=0.0.303->paper-qa)\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain>=0.0.303->paper-qa)\n",
            "  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Using cached paper_qa-3.13.4-py3-none-any.whl (31 kB)\n",
            "Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "Downloading pydantic-1.10.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "Downloading aiohttp-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.11-py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.0.81-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m470.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.9/738.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.4/773.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Downloading SQLAlchemy-2.0.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.3/142.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.3/614.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu, urllib3, tqdm, tenacity, sniffio, regex, PyYAML, pypdf, pydantic, PyCryptodome, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, html2text, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, typing-inspect, SQLAlchemy, requests, jsonpatch, anyio, aiosignal, tiktoken, langsmith, dataclasses-json, aiohttp, openai, langchain-core, langchain-community, langchain, paper-qa\n",
            "Successfully installed PyCryptodome-3.20.0 PyYAML-6.0.1 SQLAlchemy-2.0.25 aiohttp-3.9.1 aiosignal-1.3.1 anyio-4.2.0 async-timeout-4.0.3 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 dataclasses-json-0.6.3 faiss-cpu-1.7.4 frozenlist-1.4.1 greenlet-3.0.3 html2text-2020.1.16 idna-3.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.13 langchain-core-0.1.11 langsmith-0.0.81 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 numpy-1.26.3 openai-0.28.1 paper-qa-3.13.4 pydantic-1.10.13 pypdf-3.17.4 regex-2023.12.25 requests-2.31.0 sniffio-1.3.0 tenacity-8.2.3 tiktoken-0.5.2 tqdm-4.66.1 typing-inspect-0.9.0 urllib3-2.1.0 yarl-1.9.4\n",
            "Collecting openai==0.28\n",
            "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.1\n",
            "    Uninstalling openai-0.28.1:\n",
            "      Successfully uninstalled openai-0.28.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped empty or invalid JSON file: 3c79dc2968c14a4181afe666f854b7f0.json\n",
            "Skipped empty or invalid JSON file: 893c5fc3459a4f8c87323cbc8e73e2a3.json\n",
            "Skipped empty or invalid JSON file: 0efbb24ca8384c42a7884b125b493fd0.json\n",
            "Skipped empty or invalid JSON file: 55766967cea04bdcbed482d7f083f0f9.json\n",
            "Skipped empty or invalid JSON file: f399750b560643d4811198433afdef69.json\n",
            "Skipped empty or invalid JSON file: f1a85ce0b384417e9cdd30a1bbf1e587.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/epas/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `arun` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use ainvoke instead.\n",
            "  warn_deprecated(\n",
            "Retrying langchain_community.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            "Retrying langchain_community.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b4d584aa4d4fddca31fdf5c6b47bdaf in your message.) {\n",
            "  \"error\": {\n",
            "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b4d584aa4d4fddca31fdf5c6b47bdaf in your message.)\",\n",
            "    \"type\": \"server_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": null\n",
            "  }\n",
            "}\n",
            " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b4d584aa4d4fddca31fdf5c6b47bdaf in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 17 Jan 2024 02:56:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-gq9obq8rqddxdy5wl4ugofjt', 'openai-processing-ms': '90029', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '4999985', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': '1b4d584aa4d4fddca31fdf5c6b47bdaf', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '846b508b4f375108-MSP', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
            "Retrying langchain_community.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09cb9b109251803ccfce93dcdb81724b in your message.) {\n",
            "  \"error\": {\n",
            "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09cb9b109251803ccfce93dcdb81724b in your message.)\",\n",
            "    \"type\": \"server_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": null\n",
            "  }\n",
            "}\n",
            " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09cb9b109251803ccfce93dcdb81724b in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 17 Jan 2024 03:00:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-gq9obq8rqddxdy5wl4ugofjt', 'openai-processing-ms': '90035', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '4999984', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': '09cb9b109251803ccfce93dcdb81724b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '846b5673087526f2-MSP', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
          ]
        },
        {
          "ename": "InvalidRequestError",
          "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 6606 tokens. Please reduce the length of the messages.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m     write_responses(responses, responses_file_path)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m process_research_papers(output_folder)\n\u001b[1;32m     66\u001b[0m questions \u001b[38;5;241m=\u001b[39m read_questions(questions_file_path)\n\u001b[0;32m---> 67\u001b[0m responses \u001b[38;5;241m=\u001b[39m [docs\u001b[38;5;241m.\u001b[39mquery(question) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions]\n\u001b[1;32m     68\u001b[0m write_responses(responses, responses_file_path)\n",
            "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m process_research_papers(output_folder)\n\u001b[1;32m     66\u001b[0m questions \u001b[38;5;241m=\u001b[39m read_questions(questions_file_path)\n\u001b[0;32m---> 67\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\u001b[43mdocs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions]\n\u001b[1;32m     68\u001b[0m write_responses(responses, responses_file_path)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/paperqa/docs.py:584\u001b[0m, in \u001b[0;36mDocs.query\u001b[0;34m(self, query, k, max_sources, length_prompt, marginal_relevance, answer, key_filter, get_callbacks)\u001b[0m\n\u001b[1;32m    582\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    583\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarginal_relevance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_relevance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
            "File \u001b[0;32m/usr/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/paperqa/docs.py:652\u001b[0m, in \u001b[0;36mDocs.aquery\u001b[0;34m(self, query, k, max_sources, length_prompt, marginal_relevance, answer, key_filter, get_callbacks)\u001b[0m\n\u001b[1;32m    645\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m get_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m     qa_chain \u001b[38;5;241m=\u001b[39m make_chain(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mqa,\n\u001b[1;32m    648\u001b[0m         cast(BaseLanguageModel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm),\n\u001b[1;32m    649\u001b[0m         memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_model,\n\u001b[1;32m    650\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39msystem,\n\u001b[1;32m    651\u001b[0m     )\n\u001b[0;32m--> 652\u001b[0m     answer_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m qa_chain\u001b[38;5;241m.\u001b[39marun(\n\u001b[1;32m    653\u001b[0m         context\u001b[38;5;241m=\u001b[39manswer\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[1;32m    654\u001b[0m         answer_length\u001b[38;5;241m=\u001b[39manswer\u001b[38;5;241m.\u001b[39manswer_length,\n\u001b[1;32m    655\u001b[0m         question\u001b[38;5;241m=\u001b[39manswer\u001b[38;5;241m.\u001b[39mquestion,\n\u001b[1;32m    656\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    657\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# it still happens\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Example2012)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m answer_text:\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/base.py:620\u001b[0m, in \u001b[0;36mChain.arun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macall(\n\u001b[1;32m    614\u001b[0m             args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m     )[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macall(\n\u001b[1;32m    621\u001b[0m             kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata\n\u001b[1;32m    622\u001b[0m         )\n\u001b[1;32m    623\u001b[0m     )[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but not both. Got args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/base.py:413\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously execute the chain.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m    414\u001b[0m     inputs,\n\u001b[1;32m    415\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[1;32m    416\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    417\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    418\u001b[0m )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/base.py:209\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    211\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    212\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    213\u001b[0m )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/base.py:203\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    197\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    198\u001b[0m     inputs,\n\u001b[1;32m    199\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/llm.py:275\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acall\u001b[39m(\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    272\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    273\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 275\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/paperqa/chains.py:38\u001b[0m, in \u001b[0;36mFallbackLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     run_manager \u001b[38;5;241m=\u001b[39m cast(AsyncCallbackManagerForChainRun, run_manager)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39magenerate(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     run_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManagerForChainRun, run_manager)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain/chains/llm.py:142\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    140\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    143\u001b[0m         prompts,\n\u001b[1;32m    144\u001b[0m         stop,\n\u001b[1;32m    145\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mabatch(\n\u001b[1;32m    150\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    151\u001b[0m     )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:553\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    547\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    551\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    552\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    554\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    555\u001b[0m     )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:513\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    503\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    504\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m             ]\n\u001b[1;32m    512\u001b[0m         )\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    514\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    515\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    517\u001b[0m ]\n\u001b[1;32m    518\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
            "File \u001b[0;32m/usr/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:616\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    617\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:530\u001b[0m, in \u001b[0;36mChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    525\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    529\u001b[0m }\n\u001b[0;32m--> 530\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:114\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:112\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:217\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macreate\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    202\u001b[0m ):\n\u001b[1;32m    203\u001b[0m     (\n\u001b[1;32m    204\u001b[0m         deployment_id,\n\u001b[1;32m    205\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m requestor\u001b[38;5;241m.\u001b[39marequest(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[1;32m    220\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    221\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    222\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/openai/api_requestor.py:382\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marequest_raw(\n\u001b[1;32m    373\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    374\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[0;32m--> 382\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/openai/api_requestor.py:728\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    726\u001b[0m     util\u001b[38;5;241m.\u001b[39mlog_warn(e, body\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 728\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    735\u001b[0m )\n",
            "File \u001b[0;32m~/Programming/ResearchAgentSwarm/.venv/lib/python3.9/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 6606 tokens. Please reduce the length of the messages."
          ]
        }
      ],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "!pip install paper-qa\n",
        "!pip install openai==0.28\n",
        "#!pip install langchain=0.1.1\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "\n",
        "# Configuration\n",
        "#output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "output_folder = \"/home/epas/Programming/gpt-researcher/outputs/\" \n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    process_json_files(output_folder)\n",
        "    process_research_papers(output_folder)\n",
        "    questions = read_questions(questions_file_path)\n",
        "    responses = [docs.query(question) for question in questions]\n",
        "    write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed responses are saved in JSON format to structured_responses.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "def parse_qa_responses(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    responses = []\n",
        "    response = {}\n",
        "    references_lines = []\n",
        "    capturing_references = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('Question:'):\n",
        "            if response:  # Add the previous response with its references to the list\n",
        "                response['references'] = ''.join(references_lines).strip()\n",
        "                responses.append(response)\n",
        "                references_lines = []\n",
        "            response = {'question': line.split('    ')[1].strip(), 'answer': '', 'references': ''}\n",
        "            capturing_references = False\n",
        "        elif line.strip().startswith('I cannot answer') or 'The provided context does not contain' in line:\n",
        "            response['answer'] = line.strip()\n",
        "        elif line.strip().startswith('References'):\n",
        "            capturing_references = True\n",
        "        elif capturing_references:\n",
        "            references_lines.append(line)\n",
        "    \n",
        "    if response:  # Add the last response with its references to the list\n",
        "        response['references'] = ''.join(references_lines).strip()\n",
        "        responses.append(response)\n",
        "    # Add timestamp to the responses\n",
        "    \n",
        "    for response in responses:\n",
        "        response[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        # Extract URLs from the references if any\n",
        "        if response['references']:\n",
        "            response['references_urls'] = extract_urls(response['references'])\n",
        "    return responses\n",
        "\n",
        "def save_json_append(responses, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    combined_data = existing_data + responses\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "file_path =\"responses_file.txt\"\n",
        "output_json_file = 'structured_responses.json'\n",
        "\n",
        "responses = parse_qa_responses(file_path)\n",
        "save_json_append(responses, output_json_file)\n",
        "\n",
        "print(f\"Processed responses are saved in JSON format to {output_json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"/home/epas/Documents/docs.pickle\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"/home/epas/Documents/docs.pickle\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Generate Search Queries\":\n",
        "        with open(\"generate_search_queries.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.7.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.7.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: \"Descriptions of traditional plant protoplast isolation methods for mitochondria\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the traditional methods for isolating plant protoplasts?\",\n",
            "  \"How are plant protoplasts traditionally isolated from mitochondria?\",\n",
            "  \"Describe the procedure of isolating plant protoplasts using traditional methods.\",\n",
            "  \"What are the steps involved in traditional plant protoplast isolation for mitochondria?\",\n",
            "  \"What are the commonly used techniques for isolating plant protoplasts?\",\n",
            "  \"How has the process of isolating plant protoplasts evolved over time?\",\n",
            "  \"Are there any specific challenges or limitations associated with traditional plant protoplast isolation methods for mitochondria?\",\n",
            "  \"What are the advantages and disadvantages of using traditional methods for isolating plant protoplasts from mitochondria?\",\n",
            "  \"Can you provide a comparison of traditional plant protoplast isolation methods for mitochondria and newer techniques?\",\n",
            "  \"Are there any alternative methods for isolating plant protoplasts that are more efficient than the traditional approaches?\"\n",
            "]\n",
            "\n",
            "Question: \"Effectiveness of traditional mitochondrial isolation methods in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the traditional methods used for mitochondrial isolation in Arabidopsis thaliana?\",\n",
            "  \"How effective are the traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"Are there any limitations or drawbacks associated with the traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"What are the advantages of using traditional methods for mitochondrial isolation in Arabidopsis thaliana compared to newer techniques?\",\n",
            "  \"Are there any studies or research papers comparing the effectiveness of traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"What are the key steps involved in the traditional mitochondrial isolation methods for Arabidopsis thaliana?\",\n",
            "  \"Can the traditional mitochondrial isolation methods in Arabidopsis thaliana be modified or optimized for better results?\",\n",
            "  \"Are there any alternative techniques or approaches to mitochondrial isolation in Arabidopsis thaliana that have been developed?\",\n",
            "  \"What are the recommended protocols or guidelines for performing mitochondrial isolation in Arabidopsis thaliana using traditional methods?\",\n",
            "  \"Are there any specific considerations or precautions that need to be taken when using traditional mitochondrial isolation methods in Arabidopsis thaliana?\"\n",
            "]\n",
            "\n",
            "Question: \"Limitations of traditional plant protoplast methods in mitochondrial integrity and functionality\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Traditional plant protoplast methods for studying mitochondrial integrity and functionality\"\n",
            "2. \"Challenges and drawbacks of traditional plant protoplast methods in assessing mitochondrial integrity\"\n",
            "3. \"Alternatives to traditional plant protoplast methods for studying mitochondrial functionality\"\n",
            "4. \"Comparative analysis of different plant protoplast methods in assessing mitochondrial integrity\"\n",
            "5. \"Advantages and disadvantages of traditional plant protoplast methods in studying mitochondrial functionality\"\n",
            "6. \"Improvements and advancements in plant protoplast methods for assessing mitochondrial integrity\"\n",
            "7. \"Exploring the limitations of traditional plant protoplast methods in studying mitochondrial functionality\"\n",
            "8. \"Investigating the impact of traditional plant protoplast methods on mitochondrial integrity\"\n",
            "9. \"Emerging techniques for assessing mitochondrial functionality in plant protoplasts\"\n",
            "10. \"Critique of traditional plant protoplast methods in measuring mitochondrial integrity and functionality\"\n",
            "\n",
            "Question: \"Detailed protocols for using continuous colloidal density gradients in mitochondrial isolation\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Continuous colloidal density gradients protocol for mitochondrial isolation\"\n",
            "2. \"Step-by-step guide for using continuous colloidal density gradients in mitochondrial isolation\"\n",
            "3. \"Best practices for using continuous colloidal density gradients in mitochondrial isolation\"\n",
            "4. \"Optimal conditions for conducting mitochondrial isolation with continuous colloidal density gradients\"\n",
            "5. \"Troubleshooting tips for continuous colloidal density gradients in mitochondrial isolation\"\n",
            "6. \"Comparative analysis of continuous colloidal density gradients vs other methods for mitochondrial isolation\"\n",
            "7. \"Recent advancements in continuous colloidal density gradients for mitochondrial isolation\"\n",
            "8. \"Applications of continuous colloidal density gradients in mitochondrial research\"\n",
            "9. \"Limitations and challenges of continuous colloidal density gradients in mitochondrial isolation\"\n",
            "10. \"Review articles on the use of continuous colloidal density gradients for mitochondrial isolation\"\n",
            "\n",
            "Question: \"Adjustments in isolation medium composition for Arabidopsis thaliana mitochondria\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the optimal medium compositions for isolating Arabidopsis thaliana mitochondria?\",\n",
            "  \"How does the composition of the isolation medium affect the isolation of Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any specific adjustments that can be made to the isolation medium for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the common components used in isolation medium for Arabidopsis thaliana mitochondria?\",\n",
            "  \"Can the isolation medium composition be optimized for better yield of Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any studies on the effects of different isolation medium compositions on Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the best practices for adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any protocols available for optimizing the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the challenges in adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the potential benefits of adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\"\n",
            "]\n",
            "\n",
            "Question: \"Impact of temperature control at 4 °C on mitochondrial isolation in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Effects of temperature control at 4 °C on mitochondrial isolation in Arabidopsis thaliana\"\n",
            "2. \"Influence of temperature regulation at 4 °C on the isolation process of mitochondria in Arabidopsis thaliana\"\n",
            "3. \"How does temperature control at 4 °C affect the isolation of mitochondria in Arabidopsis thaliana?\"\n",
            "4. \"Exploring the relationship between temperature control at 4 °C and mitochondrial isolation in Arabidopsis thaliana\"\n",
            "5. \"Investigating the impact of maintaining a temperature of 4 °C on the process of isolating mitochondria in Arabidopsis thaliana\"\n",
            "6. \"The role of temperature control at 4 °C in the efficiency of mitochondrial isolation in Arabidopsis thaliana\"\n",
            "7. \"Temperature manipulation at 4 °C and its effects on the isolation yield of mitochondria in Arabidopsis thaliana\"\n",
            "8. \"Understanding the influence of temperature regulation at 4 °C on mitochondrial isolation in Arabidopsis thaliana\"\n",
            "9. \"Comparing the effects of different temperature conditions on the isolation of mitochondria in Arabidopsis thaliana\"\n",
            "10. \"Exploring the optimal temperature for mitochondrial isolation in Arabidopsis thaliana: A focus on 4 °C\"\n",
            "\n",
            "Question: \"Control variables important for comparing mitochondrial isolation methods\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the key control variables to consider when comparing mitochondrial isolation methods?\"\n",
            "2. \"Which factors should be controlled for when evaluating different techniques for mitochondrial isolation?\"\n",
            "3. \"What are the important variables that need to be standardized when comparing different methods of isolating mitochondria?\"\n",
            "4. \"What are the common confounding factors that can affect the results when comparing mitochondrial isolation protocols?\"\n",
            "5. \"What are the potential sources of variability that should be controlled for when comparing different methods of isolating mitochondria?\"\n",
            "6. \"Are there any specific parameters that need to be kept constant when comparing the efficiency of mitochondrial isolation techniques?\"\n",
            "7. \"What are the critical factors that need to be taken into account when evaluating the accuracy and reproducibility of mitochondrial isolation methods?\"\n",
            "8. \"Which variables should be controlled to ensure the reliability and validity of comparisons between different mitochondrial isolation procedures?\"\n",
            "9. \"What are the potential sources of bias that need to be minimized when comparing the effectiveness of various mitochondrial isolation methods?\"\n",
            "10. \"What are the standard control measures that should be implemented when comparing the performance of different mitochondrial isolation techniques?\"\n",
            "\n",
            "Question: \"Influence of plant age and tissue type on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Factors affecting mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Effect of plant age on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Impact of tissue type on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Optimal plant age for mitochondrial isolation in Arabidopsis thaliana\",\n",
            "  \"Comparison of mitochondrial isolation quality in different tissue types of Arabidopsis thaliana\",\n",
            "  \"Mitochondrial isolation protocols for Arabidopsis thaliana\",\n",
            "  \"Methods to improve mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Mitochondrial isolation techniques for plant research\",\n",
            "  \"Mitochondrial isolation challenges in Arabidopsis thaliana\",\n",
            "  \"Mitochondrial purity assessment in Arabidopsis thaliana\"\n",
            "]\n",
            "\n",
            "Question: \"Protocols for assessing mitochondrial integrity using proteinase digestion assays\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the different protocols available for assessing mitochondrial integrity using proteinase digestion assays?\",\n",
            "  \"How can proteinase digestion assays be used to assess mitochondrial integrity?\",\n",
            "  \"Are there any established methods or guidelines for assessing mitochondrial integrity using proteinase digestion assays?\",\n",
            "  \"What are the advantages and limitations of proteinase digestion assays in assessing mitochondrial integrity?\",\n",
            "  \"Are there any alternative methods or techniques for assessing mitochondrial integrity besides proteinase digestion assays?\",\n",
            "  \"What are the key considerations when performing proteinase digestion assays to assess mitochondrial integrity?\",\n",
            "  \"Are there any specific markers or indicators that can be used in proteinase digestion assays to evaluate mitochondrial integrity?\",\n",
            "  \"What are the best practices for analyzing and interpreting the results of proteinase digestion assays in assessing mitochondrial integrity?\",\n",
            "  \"Can proteinase digestion assays be used to differentiate between healthy and damaged mitochondria?\",\n",
            "  \"Are there any studies or research papers that have used proteinase digestion assays to assess mitochondrial integrity?\"\n",
            "]\n",
            "\n",
            "Question: \"Techniques for measuring mitochondrial membrane potential in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Methods for measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "2. \"Techniques for quantifying mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "3. \"Procedures for assessing mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "4. \"Tools and assays for detecting mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "5. \"Comparative analysis of different methods to measure mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "6. \"Best practices for measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "7. \"Validation techniques for mitochondrial membrane potential measurements in Arabidopsis thaliana\"\n",
            "8. \"Advancements in measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "9. \"Challenges and limitations in assessing mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "10. \"Review of studies on mitochondrial membrane potential measurement in Arabidopsis thaliana\"\n",
            "\n",
            "Question: \"Comparative studies on mitochondrial isolation methods in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Mitochondrial isolation methods in Arabidopsis thaliana: a comparative study\"\n",
            "2. \"Comparison of techniques for isolating mitochondria in Arabidopsis thaliana\"\n",
            "3. \"Efficiency and effectiveness of mitochondrial isolation techniques in Arabidopsis thaliana\"\n",
            "4. \"Optimal methods for isolating mitochondria in Arabidopsis thaliana: a review\"\n",
            "5. \"Advantages and disadvantages of different mitochondrial isolation protocols in Arabidopsis thaliana\"\n",
            "6. \"Evaluation of mitochondrial isolation techniques in Arabidopsis thaliana: a systematic review\"\n",
            "7. \"Comparing different approaches for isolating mitochondria in Arabidopsis thaliana\"\n",
            "8. \"Mitochondrial isolation methods in Arabidopsis thaliana: a comprehensive analysis\"\n",
            "9. \"Exploring the most reliable techniques for isolating mitochondria in Arabidopsis thaliana\"\n",
            "10. \"Comparative analysis of mitochondrial isolation protocols in Arabidopsis thaliana\"\n",
            "\n",
            "Question: \"Advantages and disadvantages of various mitochondrial isolation techniques in plant biology\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Advantages of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Disadvantages of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Comparison of different mitochondrial isolation techniques in plant biology\",\n",
            "  \"Effectiveness of various mitochondrial isolation techniques in plant biology\",\n",
            "  \"Limitations of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Improvements in mitochondrial isolation techniques in plant biology\",\n",
            "  \"Challenges in isolating mitochondria in plant biology\",\n",
            "  \"Innovations in mitochondrial isolation techniques in plant biology\",\n",
            "  \"Optimal mitochondrial isolation techniques in plant biology\",\n",
            "  \"Methods for isolating mitochondria in plant biology\"\n",
            "]\n",
            "\n",
            "Question: \"Statistical methods for analyzing mitochondrial isolation outcomes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Statistical methods for analyzing mitochondrial isolation outcomes\"\n",
            "2. \"Experimental design for studying mitochondrial isolation outcomes\"\n",
            "3. \"Factors influencing the success of mitochondrial isolation\"\n",
            "4. \"Comparison of statistical approaches for analyzing mitochondrial isolation data\"\n",
            "5. \"Best practices for analyzing mitochondrial isolation outcomes\"\n",
            "6. \"Challenges in analyzing mitochondrial isolation data\"\n",
            "7. \"Role of sample size in statistical analysis of mitochondrial isolation outcomes\"\n",
            "8. \"Multivariate analysis techniques for studying mitochondrial isolation outcomes\"\n",
            "9. \"Statistical software for analyzing mitochondrial isolation data\"\n",
            "10. \"Meta-analysis of studies on mitochondrial isolation outcomes\"\n",
            "\n",
            "Question: \"Criteria for successful mitochondrial isolation in terms of purity and functionality\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Expert reviews on advancements in mitochondrial isolation techniques\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Advancements in mitochondrial isolation techniques\",\n",
            "  \"Current methods for isolating mitochondria\",\n",
            "  \"Review articles on mitochondrial isolation techniques\",\n",
            "  \"Comparative analysis of different mitochondrial isolation protocols\",\n",
            "  \"Challenges in mitochondrial isolation and potential solutions\",\n",
            "  \"New approaches for isolating mitochondria\",\n",
            "  \"Emerging technologies in mitochondrial isolation\",\n",
            "  \"Cutting-edge techniques for mitochondrial isolation\",\n",
            "  \"Best practices for isolating mitochondria\",\n",
            "  \"Optimizing mitochondrial isolation protocols\",\n",
            "  \"Expert opinions on mitochondrial isolation techniques\",\n",
            "  \"Recent studies on improving mitochondrial isolation\",\n",
            "  \"Innovations in mitochondrial isolation methods\",\n",
            "  \"Methods for isolating mitochondria from different cell types\",\n",
            "  \"Efficient and reliable techniques for isolating mitochondria\",\n",
            "  \"Troubleshooting common issues in mitochondrial isolation\",\n",
            "  \"Factors influencing the success of mitochondrial isolation\",\n",
            "  \"Mitochondrial isolation protocols for specific research applications\",\n",
            "  \"Comparison of manual vs automated mitochondrial isolation methods\",\n",
            "  \"Protocols for high-throughput mitochondrial isolation\",\n",
            "  \"Mitochondrial isolation kits and their performance evaluation\"\n",
            "]\n",
            "\n",
            "Question: \"Meta-analyses on mitochondrial isolation methods in plant biology\"\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the different methods used for isolating mitochondria in plant biology?\",\n",
            "  \"Comparison of mitochondrial isolation methods in plant biology\",\n",
            "  \"Meta-analysis of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Advantages and disadvantages of different methods for isolating mitochondria in plant biology\",\n",
            "  \"Optimal conditions for isolating mitochondria in plant biology\",\n",
            "  \"Efficiency of different mitochondrial isolation techniques in plant biology\",\n",
            "  \"Validation of mitochondrial isolation methods in plant biology\",\n",
            "  \"Common challenges in isolating mitochondria from plant cells\",\n",
            "  \"Improvements in mitochondrial isolation methods for plant biology\",\n",
            "  \"Recent advancements in mitochondrial isolation techniques for plant research\"\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "def generate_search_queries(question: str):\n",
        "    # Build the prompt\n",
        "    prompt = build_system_prompt(\"Generate Search Queries\")\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "\n",
        "    # Generate the response from gpt-3.5-turbo\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    response = client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo-0613\",\n",
        "                    temperature=0.7,\n",
        "                    max_retries=3,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": \"Search Querys:\"},\n",
        "                    ],\n",
        "                )\n",
        "    response_text = parse_response(response)\n",
        "    return response_text\n",
        "\n",
        "def check_unanswered_questions(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    unanswered_questions = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Checking for phrases that indicate an unanswered question\n",
        "        if \"cannot answer\" in entry[\"answer\"] or \"does not contain\" in entry[\"answer\"] or \"no answer\" in entry[\"answer\"] or \"no results\" in entry[\"answer\"] or \"no information\" in entry[\"answer\"]:\n",
        "            unanswered = True\n",
        "        else:\n",
        "            unanswered = False\n",
        "\n",
        "        # Building the result entry\n",
        "        result_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"answerable\": not unanswered,\n",
        "            \"timestamp\": entry.get(\"timestamp\", \"Unknown timestamp\")\n",
        "        }\n",
        "\n",
        "        if entry.get(\"references\"):\n",
        "            result_entry[\"references\"] = entry[\"references\"]\n",
        "\n",
        "        if entry.get(\"references_urls\"):  # Using .get to avoid KeyError\n",
        "            result_entry[\"references_urls\"] = entry[\"references_urls\"]\n",
        "\n",
        "        unanswered_questions.append(result_entry)\n",
        "\n",
        "    return unanswered_questions\n",
        "\n",
        "json_file = 'structured_responses.json'\n",
        "unanswered_questions = check_unanswered_questions(json_file)\n",
        "\n",
        "# Display the results\n",
        "for item in unanswered_questions:\n",
        "    print(f\"Question: {item['question']}\\nAnswerable: {item['answerable']}\\n\")\n",
        "    if item.get(\"references\") and item.get(\"references_urls\"):\n",
        "        print(f\"Url(s): {item['references_urls']}\\n\")\n",
        "        #print(f\"Reference(s): {item['references']}\\n\")\n",
        "    # Generate search queries for unanswered questions\n",
        "    if item['answerable'] == False:\n",
        "        if item.get(\"references\"):\n",
        "            search_queries = generate_search_queries(f\"{item['question']}\\n{item['references']}\")\n",
        "        else:\n",
        "            search_queries = generate_search_queries(item[\"question\"])\n",
        "        # save search queries to json file\n",
        "        with open(\"search_queries.json\", \"a\") as outfile:\n",
        "            json.dump(search_queries, outfile)\n",
        "        print(f\"Search Queries: {search_queries}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install paper-qa\n",
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lee2018.pdf\n",
            "s41556-023-01246-1.pdf\n",
            "s00068-018-0954-3.pdf\n",
            "s13578-022-00805-7.pdf\n",
            "Advanced Science - 2023 - Maffeis - Synthetic Cells Revisited Artificial Cells Construction Using Polymeric Building.pdf\n",
            "elife-70899-v2.pdf\n",
            "izawa2017.pdf\n",
            "fonc-11-672781.pdf\n",
            "s41392-020-00440-z.pdf\n",
            "Mitochondria and cell signalling - PMC.pdf\n",
            "nihms158858.pdf\n",
            "nihms-1621944.pdf\n",
            "s13619-023-00158-7.pdf\n",
            "nihms804627.pdf\n",
            "elife-70899-figures-v2.pdf\n"
          ]
        }
      ],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('/home/epas/Programming/ResearchAgentSwarm/Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    #docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "    print(paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaperqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Docs\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     docs \u001b[38;5;241m=\u001b[39m Docs(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, Docs, PromptCollection, Doc, Text, Context\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:42\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, CallbackFactory, Context, Doc, DocKey, PromptCollection, Text\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     gather_with_concurrency,\n\u001b[1;32m     31\u001b[0m     get_llm_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     strip_citations,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDocs\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marbitrary_types_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmart_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"A collection of documents to be used for answering questions.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:50\u001b[0m, in \u001b[0;36mDocs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m texts_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m doc_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m llm: Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel] \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m summary_llm: Optional[Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 4\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mpaperscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbayesian model selection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded-papers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:578\u001b[0m, in \u001b[0;36msearch_papers\u001b[0;34m(query, limit, pdir, semantic_scholar_api_key, _paths, _limit, _offset, logger, year, verbose, scraper, batch_size, search_type)\u001b[0m\n\u001b[1;32m    576\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    577\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_search_papers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:350\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    203\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:495\u001b[0m, in \u001b[0;36ma_search_papers.<locals>.process_paper\u001b[0;34m(paper, i)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_paper\u001b[39m(paper, i):\n\u001b[1;32m    494\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaperId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape(paper, path, i\u001b[38;5;241m=\u001b[39mi, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    497\u001b[0m         bibtex \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitationStyles\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbibtex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/scraper.py:63\u001b[0m, in \u001b[0;36mScraper.scrape\u001b[0;34m(self, paper, path, i, logger)\u001b[0m\n\u001b[1;32m     61\u001b[0m scraper \u001b[38;5;241m=\u001b[39m scrapers[j]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mfunction(paper, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscraper\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mcheck_pdf \u001b[38;5;129;01mor\u001b[39;00m check_pdf(path)):\n\u001b[1;32m     65\u001b[0m         scrape_result[scraper\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:239\u001b[0m, in \u001b[0;36mopenaccess_scraper\u001b[0;34m(paper, path, session)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m link_to_pdf(url, path, session)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:103\u001b[0m, in \u001b[0;36mlink_to_pdf\u001b[0;34m(url, path, session)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=1,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Research Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Function to clean entities based on new lines and remove leading hyphens\n",
        "def clean_and_separate_entities(entities_list):\n",
        "    entities_str = '\\n'.join(entities_list)\n",
        "    cleaned_entities = []\n",
        "    dirty_entities = []\n",
        "\n",
        "    for line in entities_str.split('\\n'):\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line.startswith('-'):\n",
        "            # Remove the leading hyphen and any extra space after it\n",
        "            cleaned_entities.append(stripped_line.lstrip('-').strip())\n",
        "        else:\n",
        "            dirty_entities.append(stripped_line)\n",
        "\n",
        "    return cleaned_entities, dirty_entities\n",
        "def test_clean_and_separate_entities():\n",
        "    \n",
        "    # Define the summary JSON file path\n",
        "    SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "    # Read the summaries.json file\n",
        "    with open(SUMMARY_JSON, \"r\") as file:\n",
        "        summaries_json = json.load(file)\n",
        "\n",
        "    # Extract the first entities entry\n",
        "    first_entities_list = summaries_json[0][\"entities\"][0]\n",
        "\n",
        "    # Clean the entities and separate the uncleaned ones\n",
        "    cleaned_entities, dirty_entities = clean_and_separate_entities(first_entities_list)\n",
        "\n",
        "    # Save the results to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp_file:\n",
        "        json.dump({\n",
        "            \"cleaned_entities\": cleaned_entities,\n",
        "            \"dirty_entities\": dirty_entities\n",
        "        }, temp_file, indent=4)\n",
        "\n",
        "    print(\"Results saved in:\", temp_file.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_topics_with_justification(topic_text):\n",
        "    # Regular expression pattern for identifying topics with their justifications\n",
        "    topic_pattern = re.compile(r'(\\d+)\\.\\s+([^\\n]+)(\\n\\s+-[^\\n]+)*')\n",
        "    topics = topic_pattern.findall(topic_text)\n",
        "    \n",
        "    extracted_topics = []\n",
        "    for match in topics:\n",
        "        topic = match[1].strip()\n",
        "        justification = ' '.join(match[2].split('\\n')).strip()\n",
        "        # Remove \"Justification:\" if it starts with it\n",
        "        if justification.lower().startswith('- justification:'):\n",
        "            justification = justification[len('- justification:'):].strip()\n",
        "        # Remove the - if it starts with it\n",
        "        if justification.startswith('-'):\n",
        "            justification = justification[1:].strip()\n",
        "        extracted_topics.append({\"topic\": topic, \"justification\": justification})\n",
        "\n",
        "    return extracted_topics\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_topics_with_justification():\n",
        "    # Adjusted topic text\n",
        "    topic_text_list = []\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of Mitochondria in Energy Production, Signaling, and Apoptosis\\n   - Mitochondria as the powerhouse of the cell\\n   - Role of mitochondria in energy production, signaling, and apoptosis\\n   - Significance of studying mitochondrial function and involvement in diseases\\n\\n2. Challenges with Traditional Methods of Mitochondrial Isolation\\n   - Limitations of traditional methods like differential centrifugation\\n   - Potential damage to mitochondrial double membrane and variable viability\\n\\n3. Innovative Techniques for Mitochondrial Isolation\\n   - Nitrogen cavitation for gentle disruption and release of intact mitochondria\\n   - Affinity purification using anti-TOM22 magnetic beads for efficient isolation\\n   - Filtration-based methods to reduce isolation time and improve viability\\n   - Differential isopycnic density gradient centrifugation for separation based on buoyant density\\n\\n4. Quality Control Measures for Validating Mitochondrial Isolation\\n   - Assessment of mitochondrial respiration, metabolic activity, protein import, and membrane fusion\\n   - High-resolution respirometry and bioluminescent measurements of ATP synthesis\\n\\n5. Importance of Continued Refinement and Standardization of Techniques\\n   - Advancing understanding of mitochondrial biology and implications in health and disease\\n   - Need for standardized protocols to facilitate comparisons and translation of research findings into clinical applications\\n\\n**Notes**: The summary provides a comprehensive overview of the importance of mitochondria, challenges with traditional methods of isolation, innovative techniques for isolation, quality control measures, and the need for continued refinement and standardization. The topics cover the main ideas and themes discussed in the summary, providing a clear and comprehensive analysis of the content.\") \n",
        "    topic_text_list.append(\"**Topic List:**\\n\\n1. Challenges in isolating intact mitochondria from plant cells\\n   - Cell walls, mitochondrial membranes, and large amounts of starting material\\n2. Comprehensive protocol for isolating intact mitochondria from plant cells\\n   - Grinding, filtering, centrifuging, and resuspending\\n3. Characterization and analysis of isolated mitochondria\\n   - Purity, integrity, and functionality assessment\\n   - Techniques: protein profiling, enzymatic activity assays, respiratory chain measurements, and oxygen consumption analysis\\n4. Storage of purified mitochondria\\n   - Long-term storage at -80°C\\n5. Adaptation of isolation process for different tissue types and plant species\\n   - Consideration of phenolic compounds and metabolite profiles\\n6. Validation and controls for quality and functionality assurance\\n7. Downstream applications of isolated mitochondria\\n   - Protein and tRNA uptake experiments, enzyme activity assays, Western blot analyses, and mass spectrometry analyses\\n\\n**Notes:**\\n- The revised summary provides a comprehensive overview of the topic, covering various aspects of isolating intact mitochondria from plant cells.\\n- The topics are specific and non-repetitive, ensuring a clear and distinct representation of the core themes.\\n- The summary is focused on the technical process and considerations involved in isolating mitochondria, as well as the analysis and applications of the isolated mitochondria.\")\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction\\n    - Justification: The summary highlights the crucial role of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction.\\n\\n2. Significance of gentle and effective mitochondrial isolation techniques\\n    - Justification: The summary emphasizes the importance of gentle and effective isolation techniques for studying mitochondrial biology and developing mitochondrial-based therapies.\\n\\n3. Overview of macroscale mitochondrial isolation techniques\\n    - Justification: The summary discusses macroscale mitochondrial isolation techniques, such as manual homogenization and differential filtration-based isolation.\\n\\n4. Advancements in microscale and nanoscale mitochondrial isolation techniques\\n    - Justification: The summary mentions microscale and nanoscale techniques, including microfluidic techniques and nanoprobe-based technologies, for mitochondrial isolation.\\n\\n5. Breakthroughs in sub-cellular isolation techniques for mitochondria\\n    - Justification: The summary highlights breakthroughs in sub-cellular isolation techniques that enable the isolation of mitochondria from subcellular compartments with minimal disruption.\\n\\n6. Challenges in mitochondrial isolation techniques\\n    - Justification: The summary mentions challenges such as the presence of whole cell contaminants in mitochondrial isolates and the time sensitivity of isolated mitochondria.\\n\\n7. Emerging therapeutic approach: Autologous mitochondrial transplants\\n    - Justification: The summary discusses the development of autologous mitochondrial transplants as an emerging therapeutic approach.\\n\\n8. Contributions of the London Centre for Nanotechnology and the McCully laboratory\\n    - Justification: The summary mentions the significant contributions of the London Centre for Nanotechnology and the McCully laboratory in optimizing differential filtration-based mitochondrial isolation for use in cellular models.\\n\\n9. Role of Stem Cell Research & Therapy in advancing mitochondrial medicine\\n    - Justification: The summary highlights the role of Stem Cell Research & Therapy in providing in-depth overviews of advancements in mitochondrial research and facilitating the development of novel therapies for mitochondrial diseases.\")\n",
        "    topic_text_list.append(\"Topics:\\n1. Genetic modifications to enhance mitochondrial autonomy\\n   - Justification: The main focus of the report is exploring genetic modifications to enhance the autonomy of mitochondria from nuclear-encoded proteins and functions.\\n2. Role of mitochondria in cellular function\\n   - Justification: The report highlights the crucial role played by mitochondria in cellular function.\\n3. Coordination between mtDNA and nuclear DNA\\n   - Justification: The report discusses the coordination required between mtDNA and nuclear DNA, as most proteins are encoded by nuclear DNA.\\n4. Therapeutic strategies for mitochondrial diseases\\n   - Justification: The report mentions that enhancing mitochondrial autonomy could lead to new therapeutic strategies for mitochondrial diseases.\\n5. Research on genome engineering, programmable nucleases, and base editors\\n   - Justification: The report mentions that recent research in genome engineering, programmable nucleases, and base editors shows promise for treating hereditary mitochondrial diseases.\\n6. Challenges in genetic manipulation of mtDNA\\n   - Justification: The report discusses challenges such as mtDNA mutations, resistance to genetic manipulation, and limitations in mtDNA recombination.\\n7. Advancements in protein-only gene editing platforms\\n   - Justification: The report mentions advancements in protein-only gene editing platforms as potential solutions to the challenges in genetic manipulation of mtDNA.\\n8. Somatic mitochondrial DNA-replaced cells\\n   - Justification: The report mentions the generation of somatic mitochondrial DNA-replaced cells as a potential solution to the challenges in genetic manipulation of mtDNA.\\n9. Mitochondrial nucleoids and their role in maintaining genetic autonomy\\n   - Justification: The report highlights the concept of mitochondrial nucleoids and their role in maintaining genetic autonomy as a key area of study.\\n10. Mitochondrial epigenomics and gene expression regulation\\n    - Justification: The report emphasizes the importance of understanding mitochondrial epigenomics and gene expression regulation in different cellular contexts, including stress conditions, for identifying genetic modifications that could enhance mitochondrial autonomy.\")\n",
        "    for topic_text in topic_text_list:\n",
        "        extracted_topics = extract_topics_with_justification(topic_text)\n",
        "        print(f'Extracted topics: {extracted_topics}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor url in urls:\\n    try:\\n        pdf =  pdfx.PDFx(url)\\n        metadata = pdf.get_metadata()\\n        print(f\\'Metadata: {metadata}\\')\\n        references_list = pdf.get_references()\\n        print(f\\'References: {references_list}\\')\\n        references_dict = pdf.get_references_as_dict()\\n        print(f\\'References dict: {references_dict}\\')\\n        papers = paperscraper.link_to_pdf(url, pdir=\\'downloaded-papers\\')\\n        print(f\\'Papers: {papers}\\')\\n    except:\\n        print(\"Error in extracting references\")\\n        continue\\n#pdf.download_pdfs(\"target-directory\")\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#!pip install pdfx\n",
        "import pdfx\n",
        "#!pip install paperscraper\n",
        "#import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def test_extract_urls():\n",
        "    # Define the reference text\n",
        "    reference_text = \"\"\"\\n\\nAmerican Institute of Physics. (2023). The powerhouse of the future: Artificial cells. Phys.org. Retrieved from https://phys.org/news/2023-03-powerhouse-future-artificial-cells.html\\n\\nNational Institutes of Health. (2023). Artificial mitochondria transfer (AMT) and transplant. PMC. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5511681/\\n\\nNature. (2023). Spatiotemporal simulations of mitochondrial dynamics. Nature.com. Retrieved from https://www.nature.com/articles/s41598-019-54159-1\\n\\nSogang University & Harbin Institute of Technology. (2023). Artificial organelles for sustainable chemical energy conversion and production: Artificial mitochondria and chloroplasts. Biophysics Reviews. Retrieved from https://publishing.aip.org/publications/latest-content/the-powerhouse-of-the-future-artificial-cells/\"\"\"\n",
        "\n",
        "    urls = extract_urls(reference_text)\n",
        "    print(f'Extracted URLs: {urls}')\n",
        "\n",
        "#pdf = pdfx.PDFx(\"filename-or-url.pdf\")\n",
        "#urls = ['/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/2308.00352.pdf']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for url in urls:\n",
        "    try:\n",
        "        pdf =  pdfx.PDFx(url)\n",
        "        metadata = pdf.get_metadata()\n",
        "        print(f'Metadata: {metadata}')\n",
        "        references_list = pdf.get_references()\n",
        "        print(f'References: {references_list}')\n",
        "        references_dict = pdf.get_references_as_dict()\n",
        "        print(f'References dict: {references_dict}')\n",
        "        papers = paperscraper.link_to_pdf(url, pdir='downloaded-papers')\n",
        "        print(f'Papers: {papers}')\n",
        "    except:\n",
        "        print(\"Error in extracting references\")\n",
        "        continue\n",
        "#pdf.download_pdfs(\"target-directory\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Questions: [('Content-Based Question', 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'), ('Analytical Question', 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'), ('Creative/Scenario-Based Question', 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'), ('Contextual/Relational Question', 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'), ('User-Interactive Question', 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Content-Based Question', 'question': 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'}, {'question_type': 'Analytical Question', 'question': 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'}, {'question_type': 'User-Interactive Question', 'question': 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?'}]\n",
            "Questions: [('Analytical Question', 'How do theoretical models help in understanding mitochondrial ATP production?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'), ('Contextual/Relational Question', 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'), ('User-Interactive Question', 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Analytical Question', 'question': 'How do theoretical models help in understanding mitochondrial ATP production?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'}, {'question_type': 'User-Interactive Question', 'question': 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_hypothetical_questions(hypothetical_questions_text):\n",
        "    # Regular expression pattern for identifying hypothetical questions\n",
        "    question_pattern = re.compile(r'\\d+\\.\\s+([A-Za-z\\/-]+ Question):\\n\\s+-\\s+([^\\n]+)')\n",
        "    questions = question_pattern.findall(hypothetical_questions_text)\n",
        "    print(f'Questions: {questions}')\n",
        "    if len(questions) == 0:\n",
        "        return hypothetical_questions_text\n",
        "    return [{\"question_type\": question_type, \"question\": question} for question_type, question in questions]\n",
        "def test_extract_hypothetical_questions():\n",
        "    # Example hypothetical questions text\n",
        "    hypothetical_questions_text_1 = \"1. Content-Based Question:\\n   - How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?\\n\\n2. Analytical Question:\\n   - What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?\\n\\n5. User-Interactive Question:\\n   - What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?\"\n",
        "    hypothetical_questions_text_2 = \"1. Content-Based Question: \\n   - What does this report investigate regarding mitochondrial ATP production?\\n   - How does this report contribute to our understanding of mitochondrial function?\\n   - What are the key findings regarding the replication of mitochondrial ATP production outside the cellular environment?\\n\\n2. Analytical Question:\\n   - How do theoretical models help in understanding mitochondrial ATP production?\\n   - What experimental evidence supports the concept of artificial organelles for ATP synthesis?\\n   - What are the implications of studying mitochondrial dynamics and stress responses for ex vivo methods of ATP synthesis?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?\\n   - If artificial organelles capable of ATP synthesis were successfully developed, what potential applications could they have in various industries?\\n   - How might the understanding of mitochondrial dynamics and stress responses lead to the development of innovative approaches for ATP synthesis?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?\\n   - In what ways does the replication of mitochondrial ATP production outside cells build upon previous studies in the field?\\n   - How do the findings in this report align with or challenge existing theories and models of mitochondrial function?\\n\\n5. User-Interactive Question:\\n   - How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?\\n   - Can you think of any potential limitations or ethical considerations in developing artificial organelles for ATP synthesis?\\n   - What questions or areas of research would you like to see explored further in the study of mitochondrial dynamics and stress responses?\"\n",
        "    hypothetical_questions = []\n",
        "    hypothetical_questions.append(hypothetical_questions_text_1)\n",
        "    hypothetical_questions.append(hypothetical_questions_text_2)\n",
        "    for hypothetical_questions_text in hypothetical_questions:\n",
        "        extracted_hypothetical_questions = extract_hypothetical_questions(hypothetical_questions_text)\n",
        "        print(f'Extracted hypothetical questions: {extracted_hypothetical_questions}')\n",
        "\n",
        "test_extract_hypothetical_questions()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'tailored isolation protocol'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimized damage to ensure the integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_entity_relationships(entity_relationships_text):\n",
        "    # Regular expression pattern for identifying entity relationships\n",
        "    entity_pattern = re.compile(r'\\d+\\.\\s+\\((.+?),\\s+(.+?),\\s+(.+?)\\)')\n",
        "    entity_relationships = entity_pattern.findall(entity_relationships_text)\n",
        "    return [{\"subject\": relationship[0], \"relationship\": relationship[1], \"target\": relationship[2]} for relationship in entity_relationships]\n",
        "\n",
        "# Example entity relationships text\n",
        "entity_relationships_text =  \"Entity Relationships:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n30. (mitochondria, isolated with, tailored isolation protocol)\\n31. (mitochondria, isolated with, minimized damage to ensure the integrity)\\n32. (mitochondria, isolated with, reduced contamination from other organelles)\\n33. (mitochondria, isolated with, improved methods)\\n34. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n35. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n36. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n37. (mitochondria, isolated with, minimal contamination from other organelles)\\n38. (mitochondria, isolated with, improved methods)\\n39. (mitochondria, isolated with, slight modifications)\\n40. (mitochondria, isolated with, traditional plant protoplast isolation)\\n41. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n42. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n43. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n44. (mitochondria, isolated with, minimal contamination from other organelles)\\n45. (mitochondria, isolated from, Arabidopsis thaliana)\\n46. (mitochondria, isolated using, continuous colloidal density gradients)\\n47. (mitochondria, isolated at, 4 °C)\\n48. (mitochondria, used for, protein and tRNA uptake experiments)\\n49. (mitochondria, used for, enzyme activity assays)\\n50. (mitochondria, used for, western blot analyses)\\n51. (mitochondria, used for, mass spectrometry analyses)\\n52. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n53. (mitochondria, assessed for, purity and integrity)\\n54. (mitochondria, assessed using, proteinase digestion assays)\\n55. (mitochondria, assessed using, electron microscopy)\\n56. (mitochondria, assessedThe article discusses the protocol for isolating mitochondria from plant cells. Mitochondria are double-membraned organelles responsible for energy production in eukaryotic cells. The isolation of mitochondria is crucial for various studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays.\\n\\nThe isolation process is challenging due to the presence of cell walls, vacuoles, and secondary metabolites in plant cells. The protocol must be tailored to minimize damage to the mitochondria and ensure their integrity. Specificity in isolation protocols is required as different plant species and tissue types have varying phenolic compounds and metabolite profiles. Earlier methods led to contamination with nuclei and chloroplasts, but recent advancements have improved isolation methods, reducing the need for heavy labor, expensive equipment, and large amounts of starting material.\\n\\nThe protocol for isolating intact mitochondria involves several steps. First, the preparation of grinding medium, wash buffer, and gradient solutions is necessary. The plant material is then homogenized in the grinding medium to release the mitochondria, which are then filtered and centrifuged to pellet the mitochondria. The mitochondrial pellet is resuspended in the wash buffer. Oxygen consumption measurements are crucial for determining the intactness and functional capacity of the isolated mitochondria. Evaluation of mitochondrial purity and integrity can be done through proteinase digestion assays, electron microscopy, and checks of mitochondrial membrane potential and electron transport chain activity.\\n\\nOnce purified, the isolated mitochondria can be used for various studies, including protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses. For mass spectrometry analyses, targeted multiple reaction monitoring (MRM) or quantification by dimethyl or other isotope labels can be employed.\\n\\nIn conclusion, the isolation of mitochondria from plant cells is a delicate process that requires careful consideration of the specific requirements of the plant species and tissue type. Recent advancements have made the process more effective and accessible for a range of tissue types and species, allowing for a broader application of mitochondrial studies across different plant species.\\n\\nReferences:\\n- Plant Methods. (2015). https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0099-x\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908444/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7640673/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687074/Extraction and Categorization:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, used for, protein and tRNA uptake experiments)\\n30. (mitochondria, used for, enzyme activity assays)\\n31. (mitochondria, used for, western blot analyses)\\n32. (mitochondria, used for, mass spectrometry analyses)\\n33. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n34. (mitochondria, assessed for, purity and integrity)\\n35. (mitochondria, assessed using, proteinase digestion assays)\\n36. (mitochondria, assessed using, electron microscopy)\\n37. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n38. (mitochondria, assessed using, electron transport chain activity measurement)\\n39. (mitochondria, isolated from, Arabidopsis thaliana)\\n40. (mitochondria, isolated using, continuous colloidal density gradients)\\n41. (mitochondria, isolated at, 4 °C)\"\n",
        "\n",
        "# Clean the entity relationships\n",
        "cleaned_entity_relationships = clean_entity_relationships(entity_relationships_text)\n",
        "\n",
        "# Output the cleaned entity relationships\n",
        "cleaned_entity_relationships\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.8.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n",
            "Summarizing /home/epas/Programming/gpt-researcher/outputs/a99ec0db582e4341aed9d4084e4a000d.md\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/a99ec0db582e4341aed9d4084e4a000d.json\n",
            "Questions: [('Content-Based Question', 'How does virtual screening contribute to advancements in chemical and pharmaceutical research?'), ('Analytical Question', \"What factors contribute to North America's dominance in the chemoinformatics market?\"), ('Creative/Scenario-Based Question', 'Imagine a scenario where a new breakthrough technology replaces virtual screening in drug discovery. How might this impact the chemoinformatics market?'), ('Contextual/Relational Question', 'How do strategic partnerships, acquisitions, and product innovations help key players expand their market presence in the chemoinformatics industry?'), ('User-Interactive Question', 'As a chemoinformatics professional, how would you address the challenges posed by competition from international and local players in the market?')]\n",
            "Storing data for file_id: a99ec0db582e4341aed9d4084e4a000d\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/a99ec0db582e4341aed9d4084e4a000d.json\n",
            "Successfully summarized /home/epas/Programming/gpt-researcher/outputs/a99ec0db582e4341aed9d4084e4a000d.md\n",
            "Summarizing /home/epas/Programming/gpt-researcher/outputs/e7edb1b2743d49c1ab86217dcabb9523.md\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/e7edb1b2743d49c1ab86217dcabb9523.json\n",
            "Questions: [('Content-Based Question', 'What are some of the hazards that laboratory personnel may encounter when working with thiolate compounds?'), ('Analytical Question', 'How does the Occupational Safety and Health Administration (OSHA) regulate laboratory safety and health practices related to thiolate compounds?'), ('Creative/Scenario-Based Question', 'Imagine you are a laboratory worker handling thiolate compounds. What personal protective equipment (PPE) would you need to ensure your safety?'), ('Contextual/Relational Question', 'How do exposure monitoring and medical surveillance, based on Permissible Exposure Limits (PELs), contribute to the protection of laboratory workers handling thiolate compounds?'), ('User-Interactive Question', 'Have you ever encountered any chemical hazards in a laboratory setting? If so, how were you trained to handle them safely?')]\n",
            "Storing data for file_id: e7edb1b2743d49c1ab86217dcabb9523\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/e7edb1b2743d49c1ab86217dcabb9523.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How might the new OSHA regulatory updates impact workplace safety practices?'), ('Analytical Question', 'What are the potential benefits and drawbacks of the OSHA regulatory updates on businesses and employees?'), ('Creative/Scenario-Based Question', 'Imagine you are a safety manager in a manufacturing company. How would you adapt your safety protocols to comply with the new OSHA regulatory updates?'), ('Contextual/Relational Question', 'How do the recent OSHA regulatory updates align with the broader national or international standards for workplace safety?'), ('User-Interactive Question', 'Have you ever encountered a workplace safety issue that could have been prevented with the implementation of the new OSHA regulatory updates? Share your experience and how the updates might have made a difference.')]\n",
            "Storing data for file_id: e7edb1b2743d49c1ab86217dcabb9523\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/e7edb1b2743d49c1ab86217dcabb9523.json\n",
            "Successfully summarized /home/epas/Programming/gpt-researcher/outputs/e7edb1b2743d49c1ab86217dcabb9523.md\n",
            "Summarizing /home/epas/Programming/gpt-researcher/outputs/8cb5d3be1eb94bcdababdc5b31dc0c0a.md\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/8cb5d3be1eb94bcdababdc5b31dc0c0a.json\n",
            "Questions: [('Content-Based Question', 'What are some of the best practices for approaching emergencies in laboratories?'), ('Analytical Question', 'How do hazard assessments and risk minimization contribute to laboratory safety?'), ('Creative/Scenario-Based Question', 'Imagine a laboratory where a chemical spill occurs. How would you handle the situation using spill response plans and kits?'), ('Contextual/Relational Question', 'How do the emergency planning approaches of the University of Washington and West Virginia University differ, and what impact does this have on laboratory safety?'), ('User-Interactive Question', 'Have you ever been in a laboratory emergency situation? How did you handle it, and what safety protocols were in place to assist you?')]\n",
            "Storing data for file_id: 8cb5d3be1eb94bcdababdc5b31dc0c0a\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/8cb5d3be1eb94bcdababdc5b31dc0c0a.json\n",
            "Successfully summarized /home/epas/Programming/gpt-researcher/outputs/8cb5d3be1eb94bcdababdc5b31dc0c0a.md\n",
            "Summarizing /home/epas/Programming/gpt-researcher/outputs/b2c7f076a9444b579b9392b8f31e66b1.md\n",
            "Successfully saved data to /home/epas/Programming/gpt-researcher/outputs/b2c7f076a9444b579b9392b8f31e66b1.json\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0613\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0613\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    while True:\n",
        "        topic = request_topics(summary)\n",
        "        if topic:\n",
        "            break\n",
        "    while True:\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "        if questions:\n",
        "            break\n",
        "    return topic, questions\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        extract_references_from_pdf(file_path, references_path)\n",
        "        with open(references_path) as f:\n",
        "            references = f.read()\n",
        "    else:\n",
        "        references = extract_references(file_path)  \n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        article_text = pdf_to_text(file_path)\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic, \n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\") or file_name.endswith(\".mmd\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "#OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/\"\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/gpt-researcher/outputs/\" # mmd files\n",
        "article_list = get_article_list(filetype=\"md\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open summary.json to see the results \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ChatGPT Message Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def analyze_file(file_path, output_file_path, document_name):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().split('\\n')\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            current_message = \"\"\n",
        "            message_started = False\n",
        "            sender = \"\"\n",
        "            message_number = 0\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line.lower().startswith('user') or line.lower().startswith('chatgpt'):\n",
        "                    if message_started:  # End of a message\n",
        "                        message_number += 1\n",
        "                        word_count = count_words(current_message.strip())\n",
        "                        output_file.write(f\"{sender} Line number {i}, Message number {message_number}, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "                        current_message = \"\"\n",
        "                    message_started = True\n",
        "                    sender = \"User\" if line.lower().startswith('user') else \"ChatGPT\"\n",
        "                    continue\n",
        "                if message_started:\n",
        "                    current_message += \" \" + line\n",
        "\n",
        "            # Add the last message if it exists\n",
        "            if current_message:\n",
        "                message_number += 1\n",
        "                word_count = count_words(current_message.strip())\n",
        "                output_file.write(f\"{sender} Last message, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your text file\n",
        "# Replace 'output_messages.txt' with the path for the output file\n",
        "# Add the document name (e.g., 'ChatGPT_history.txt')\n",
        "file_path = 'ChatGPT_history.txt'\n",
        "output_file_path = 'output_messages.txt'\n",
        "document_name = 'ChatGPT_history'  # This is the document name without the extension\n",
        "analyze_file(file_path, output_file_path, document_name)\n",
        "print(\"Messages have been written to the output file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class MessageAnalysisStore:\n",
        "    def __init__(self, output_file_path):\n",
        "        self.output_file_path = output_file_path\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.output_file_path):\n",
        "            empty_data = []\n",
        "            self._save(empty_data)\n",
        "\n",
        "    def store(self, analyzed_data):\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(analyzed_data)\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.output_file_path):\n",
        "            with open(self.output_file_path, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.output_file_path, \"w\") as file:\n",
        "                json.dump(content, file, indent=4)\n",
        "            print(f\"Successfully saved data to {self.output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.output_file_path}: {e}\")\n",
        "\n",
        "# Assuming the required classes and functions from your new code are already defined and imported\n",
        "# like SummaryStore, generate_summary, get_entities, extract_knowledge, etc.\n",
        "\n",
        "def extract_messages_with_citation(lines: List[str], sender_keyword: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts messages with citation from the given lines based on the sender keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    messages_with_citation = []\n",
        "    current_message = \"\"\n",
        "    message_started = False\n",
        "    citation_info = \"\"\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(sender_keyword):\n",
        "            if message_started:\n",
        "                # End of the current message, add it to the list\n",
        "                messages_with_citation.append((current_message.strip(), citation_info))\n",
        "                current_message = \"\"\n",
        "            message_started = True\n",
        "            citation_info = line  # Capture the line with sender info as citation\n",
        "        elif message_started:\n",
        "            current_message += \" \" + line\n",
        "\n",
        "    # Add the last message if it exists\n",
        "    if current_message:\n",
        "        messages_with_citation.append((current_message.strip(), citation_info))\n",
        "\n",
        "    return messages_with_citation\n",
        "\n",
        "def analyze_conversation(message: str, citation: str, sender_keyword: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyzes a single conversation message, extracting and summarizing information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate an initial summary\n",
        "        if sender_keyword == \"ChatGPT\":\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"Entity Dense\")\n",
        "        else:\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"SPR\")\n",
        "\n",
        "        # Extract entities and knowledge\n",
        "        entities = get_entities(message)\n",
        "        knowledge = extract_knowledge(message, entities[\"clean_entities\"], entities[\"dirty_entities\"])\n",
        "\n",
        "        # Extract the topic and hypothetical questions from the summary\n",
        "        topic, questions = extract_info(initial_summary)\n",
        "\n",
        "        analyzed_data = {\n",
        "            \"id\": citation,\n",
        "            \"sender\": sender_keyword,\n",
        "            \"message\": message,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": questions,\n",
        "            \"clean_entities\": entities[\"clean_entities\"],\n",
        "            \"dirty_entities\": entities[\"dirty_entities\"],\n",
        "            \"summary\": initial_summary,\n",
        "            \"knowledge\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        return analyzed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred in analyzing conversation: {e}\")\n",
        "\n",
        "def extract_and_analyze_messages(file_path: str, output_file_path: str, sender_keyword: str, log_file_path: str):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    messages_with_citation = extract_messages_with_citation(lines, sender_keyword)\n",
        "    store = MessageAnalysisStore(output_file_path)\n",
        "    error_log = []\n",
        "\n",
        "    for message, citation in messages_with_citation:\n",
        "        try:\n",
        "            analyzed_data = analyze_conversation(message, citation, sender_keyword)\n",
        "            store.store(analyzed_data)\n",
        "            time.sleep(15)  # Delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            error_info = {\"citation\": citation, \"error\": str(e), \"timestamp\": datetime.now().isoformat()}\n",
        "            # Appending to the error log\n",
        "            error_log.append(error_info)\n",
        "            with open(log_file_path, \"a\") as log_file:\n",
        "                json.dump(error_info, log_file, indent=4)\n",
        "                log_file.write(\"\\n\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Messages analysis completed. Data saved to {output_file_path}\")\n",
        "    if error_log:\n",
        "        print(f\"Errors logged to {log_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'output_messages.txt'\n",
        "output_file_path_user = 'analyzed_user_messages.json'\n",
        "log_file_path_user = 'error_log_user.json'\n",
        "extract_and_analyze_messages(file_path, output_file_path_user, 'user', log_file_path_user)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install neo4j\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"neo4j+s://d0cccc82.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"pkHFLmEdqhftN2n5BcR362MTrG4RomuLgnkp3GR7yEQ\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7617/1621462424.py:131: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session() as session:\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 0.9157391045379122s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.9635494431935174s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 4.010955571213135s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:root:Failed to process file: /home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/6829ccbe621c4f47a47e02bb959c7136.json. Error: {code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.0747247988968547s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:neo4j.io:Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n"
          ]
        },
        {
          "ename": "TransientError",
          "evalue": "{code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 135\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    134\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, file_name)\n\u001b[0;32m--> 135\u001b[0m             \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
            "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(file_path, session)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcypher_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:757\u001b[0m, in \u001b[0;36mSession.execute_write\u001b[0;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute a unit of work in a managed write transaction.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m.. versionadded:: 5.0\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWRITE_ACCESS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTX_FUNC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransaction_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:552\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36mprocess_json_file.<locals>.<lambda>\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     session\u001b[38;5;241m.\u001b[39mexecute_write(\u001b[38;5;28;01mlambda\u001b[39;00m tx: \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcypher_query\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/transaction.py:169\u001b[0m, in \u001b[0;36mTransactionBase.run\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    168\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwparameters)\n\u001b[0;32m--> 169\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tx_ready_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:131\u001b[0m, in \u001b[0;36mResult._tx_ready_run\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tx_ready_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# BEGIN+RUN does not carry any extra on the RUN message.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# BEGIN {extra}\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# RUN \"query\" {parameters} {extra}\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:298\u001b[0m, in \u001b[0;36mResult._attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:846\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(tag, fields)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:72\u001b[0m, in \u001b[0;36mInbox.pop\u001b[0;34m(self, hydration_hooks)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:51\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chunk_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mpop_u16()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:326\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[0;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mused \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m--> 326\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:493\u001b[0m, in \u001b[0;36mBoltSocket.recv_into\u001b[0;34m(self, buffer, nbytes)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecv_into\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer, nbytes):\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:468\u001b[0m, in \u001b[0;36mBoltSocket._wait_for_io\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mgettimeout()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTransientError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    139\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     \u001b[43madd_jsons_to_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 131\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_jsons_to_neo4j\u001b[39m(output_folder):\n\u001b[0;32m--> 131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:124\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:207\u001b[0m, in \u001b[0;36mSession.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\u001b[38;5;241m.\u001b[39m_closed() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# roll back the transaction if it is not closed\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/transaction.py:221\u001b[0m, in \u001b[0;36mTransactionBase._rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mrollback(on_success\u001b[38;5;241m=\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_cancel()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:863\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcomplete:\n\u001b[0;32m--> 863\u001b[0m     detail_delta, summary_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     detail_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m detail_delta\n\u001b[1;32m    865\u001b[0m     summary_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary_delta\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
            "\u001b[0;31mTransientError\u001b[0m: {code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def aggregate_article_content(data):\n",
        "    # Aggregates content for articles with the same file_id\n",
        "    aggregated_content = {}\n",
        "    for record in data:\n",
        "        file_id = record['file_id']\n",
        "        if file_id not in aggregated_content:\n",
        "            aggregated_content[file_id] = {\n",
        "                \"content\": record['article'],\n",
        "                \"timestamp\": record.get('timestamp', '')\n",
        "            }\n",
        "        else:\n",
        "            aggregated_content[file_id]['content'] += \"\\n\" + record['article']\n",
        "    return aggregated_content\n",
        "\n",
        "def construct_cypher_query(data, aggregated_content):\n",
        "    cypher_query = \"\"\n",
        "    article_ids = set()\n",
        "    \n",
        "    for record in data:\n",
        "        file_id = record['file_id']\n",
        "        if file_id not in article_ids:\n",
        "            article_content = escape_string(aggregated_content[file_id]['content'])\n",
        "            timestamp = escape_string(aggregated_content[file_id]['timestamp'])\n",
        "\n",
        "            # Create or update the Article node\n",
        "            cypher_query += (\n",
        "                f\"MERGE (a:Article {{id: '{file_id}'}}) \"\n",
        "                f\"ON CREATE SET a.content = '{article_content}', a.timestamp = '{timestamp}' \"\n",
        "                f\"ON MATCH SET a.content = '{article_content}', a.timestamp = '{timestamp}' \"\n",
        "            )\n",
        "            article_ids.add(file_id)\n",
        "\n",
        "        # Add Summary\n",
        "        if \"summary\" in record:\n",
        "            summary_content = escape_string(record['summary'])\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (s:Summary {{content: '{summary_content}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_SUMMARY]->(s) \"\n",
        "            )\n",
        "\n",
        "        # Add Entities (both clean and dirty)\n",
        "        for entity in record.get(\"clean_entities\", []):\n",
        "            entity_name = escape_string(entity)\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (e:Entity {{name: '{entity_name}'}}) \"\n",
        "                f\"MERGE (a)-[:MENTIONS]->(e) \"\n",
        "            )\n",
        "\n",
        "        for entity in record.get(\"dirty_entities\", []):\n",
        "            entity_name = escape_string(entity)\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (de:DirtyEntity {{name: '{entity_name}'}}) \"\n",
        "                f\"MERGE (a)-[:MENTIONS_DIRTY]->(de) \"\n",
        "            )\n",
        "\n",
        "        # Add Topics with Justifications\n",
        "        for topic in record.get(\"topics\", []):\n",
        "            topic_name = escape_string(topic['topic'])\n",
        "            justification = escape_string(topic.get('justification', ''))\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (t:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_TOPIC]->(t) \"\n",
        "            )\n",
        "\n",
        "        # Add Hypothetical Questions\n",
        "        for question in record.get(\"hypothetical_questions\", []):\n",
        "            question_content = escape_string(question['question'])\n",
        "            question_type = escape_string(question.get('question_type', ''))\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (hq:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_QUESTION]->(hq) \"\n",
        "            )\n",
        "\n",
        "        # Add Knowledge Triplets\n",
        "        for triplet in record.get(\"knowledge_triplets\", []):\n",
        "            subject_name = escape_string(triplet['subject'])\n",
        "            target_name = escape_string(triplet['target'])\n",
        "            relationship = escape_string(triplet['relationship'])\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (subj:Subject {{name: '{subject_name}'}}) \"\n",
        "                f\"MERGE (targ:Target {{name: '{target_name}'}}) \"\n",
        "                f\"MERGE (subj)-[:RELATIONSHIP {{type: '{relationship}'}}]->(targ) \"\n",
        "            )\n",
        "\n",
        "        # Add References (URLs and Reference Text) only once per file_id\n",
        "        if file_id not in article_ids:\n",
        "            references = record.get(\"references\", {})\n",
        "            if isinstance(references, dict):\n",
        "                for url in references.get(\"urls\", []):\n",
        "                    url = escape_string(url)\n",
        "                    cypher_query += (\n",
        "                        f\"WITH a MERGE (r:Reference {{url: '{url}'}}) \"\n",
        "                        f\"MERGE (a)-[:HAS_REFERENCE]->(r) \"\n",
        "                    )\n",
        "                ref_text = references.get(\"references\", \"\")\n",
        "                if ref_text:\n",
        "                    ref_text = escape_string(ref_text)\n",
        "                    cypher_query += (\n",
        "                        f\"WITH a MERGE (rText:Reference {{text: '{ref_text}'}}) \"\n",
        "                        f\"MERGE (a)-[:HAS_REFERENCE]->(rText) \"\n",
        "                    )\n",
        "\n",
        "    # Return the complete Cypher query\n",
        "    return cypher_query\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        aggregated_content = aggregate_article_content(data)\n",
        "        cypher_query = construct_cypher_query(data, aggregated_content)\n",
        "        try:\n",
        "            session.execute_write(lambda tx: tx.run(cypher_query))\n",
        "            logging.info(f\"Successfully processed file: {file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                process_json_file(file_path, session)\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def update_article_content(session, record):\n",
        "    file_id = record['file_id']\n",
        "    article_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists and update or create accordingly\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a.content AS existing_content\")\n",
        "    existing_record = result.single()\n",
        "\n",
        "    if existing_record:\n",
        "        # Article exists, update it with new content\n",
        "        updated_content = existing_record['existing_content'] + \"\\n\" + article_content\n",
        "        update_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "            f\"SET a.content = '{updated_content}', a.timestamp = '{timestamp}'\"\n",
        "        )\n",
        "        session.run(update_query)\n",
        "    else:\n",
        "        # Article does not exist, create new\n",
        "        create_query = (\n",
        "            f\"CREATE (a:Article {{id: '{file_id}', content: '{article_content}', timestamp: '{timestamp}'}})\"\n",
        "        )\n",
        "        session.run(create_query)\n",
        "\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article content from file: {file_path}\")\n",
        "                update_article_content(session, record)\n",
        "\n",
        "                logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "                process_additional_elements(session, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                logging.info(f\"Starting processing file: {file_path}\")\n",
        "                process_json_file(file_path, session)\n",
        "                logging.info(f\"Finished processing file: {file_path}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def update_article_chunk(session, record):\n",
        "    file_id = record['file_id']\n",
        "    chunk_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a.content AS existing_content\")\n",
        "    existing_record = result.single()\n",
        "\n",
        "    if existing_record:\n",
        "        # Article exists, append the chunk to the existing content\n",
        "        updated_content = existing_record['existing_content'] + \"\\n\" + chunk_content\n",
        "        update_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "            f\"SET a.content = '{updated_content}', a.timestamp = '{timestamp}'\"\n",
        "        )\n",
        "    else:\n",
        "        # Article does not exist, create new with the chunk content\n",
        "        update_query = (\n",
        "            f\"CREATE (a:Article {{id: '{file_id}', content: '{chunk_content}', timestamp: '{timestamp}'}})\"\n",
        "        )\n",
        "\n",
        "    session.run(update_query)\n",
        "\n",
        "def process_additional_elements(session, record):\n",
        "    file_id = record['file_id']\n",
        "\n",
        "    # Construct query for additional elements linked to the article\n",
        "    cypher_query = f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "\n",
        "    # Add other elements (entities, topics, questions, etc.) similar to the previous approach\n",
        "\n",
        "    # Execute the query\n",
        "    session.run(cypher_query)\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article chunk from file: {file_path}\")\n",
        "                update_article_chunk(session, record)\n",
        "\n",
        "                # Additional elements processing can be done here or after all chunks are processed\n",
        "                logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "                process_additional_elements(session, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed chunk from file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process chunk in file: {file_path}. Error: {e}\")\n",
        "\n",
        "        # Process additional elements after all chunks are processed\n",
        "        logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "        process_additional_elements(session, data[0])  # Assuming additional elements are same across chunks\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                logging.info(f\"Starting processing file: {file_path}\")\n",
        "                process_json_file(file_path, session)\n",
        "                logging.info(f\"Finished processing file: {file_path}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10555/120211380.py:128: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session() as session:\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.0987203224723447s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:neo4j.io:Failed to read from defunct connection IPv4Address(('d0cccc82.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.884448562985088s (Failed to read from defunct connection IPv4Address(('d0cccc82.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687))))\n",
            "ERROR:neo4j.io:Failed to read from defunct connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 4.294914787174731s (Unable to retrieve routing information)\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 9.218245426261193s (Unable to retrieve routing information)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    147\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m     \u001b[43madd_jsons_to_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Close the driver after all sessions are complete\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     driver\u001b[38;5;241m.\u001b[39mclose()\n",
            "Cell \u001b[0;32mIn[3], line 131\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(output_folder):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 131\u001b[0m         \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(file_path, session)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing article chunks from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_or_update_article_with_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:757\u001b[0m, in \u001b[0;36mSession.execute_write\u001b[0;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39mnon_concurrent_method\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_write\u001b[39m(\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs,  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    713\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _R:\n\u001b[1;32m    714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute a unit of work in a managed write transaction.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 5.0\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWRITE_ACCESS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTX_FUNC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransaction_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:582\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransaction failed and will be retried in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124ms (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(delay, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margs)))\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    584\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[#0000]  _: <SESSION> retry cancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "import json\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "def create_or_update_article_with_chunks(session, record):\n",
        "    file_id = record['file_id']\n",
        "    chunk_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a\")\n",
        "    article_exists = result.single() is not None\n",
        "\n",
        "    if not article_exists:\n",
        "        # Create new Article node\n",
        "        session.run(f\"CREATE (a:Article {{id: '{file_id}', timestamp: '{timestamp}'}})\")\n",
        "\n",
        "    # Create new Chunk node and link it to the Article\n",
        "    create_chunk_query = (\n",
        "        f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "        f\"CREATE (c:Chunk {{content: '{chunk_content}'}}) \"\n",
        "        f\"CREATE (a)-[:HAS_CHUNK]->(c)\"\n",
        "    )\n",
        "    session.run(create_chunk_query)\n",
        "\n",
        "    # If there are existing chunks, link the new chunk to the last chunk\n",
        "    if article_exists:\n",
        "        link_chunks_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}})-[:HAS_CHUNK]->(lastChunk:Chunk) \"\n",
        "            f\"WHERE NOT (lastChunk)-[:NEXT_CHUNK]->() \"\n",
        "            f\"MATCH (newChunk:Chunk) \"\n",
        "            f\"WHERE newChunk.content = '{chunk_content}' \"\n",
        "            f\"CREATE (lastChunk)-[:NEXT_CHUNK]->(newChunk)\"\n",
        "        )\n",
        "        session.run(link_chunks_query)\n",
        "    \n",
        "    file_id = record['file_id']\n",
        "\n",
        "    # Start constructing the Cypher query\n",
        "    cypher_query = f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "\n",
        "    # Add Summary\n",
        "    if \"summary\" in record:\n",
        "        summary_content = escape_string(record['summary'])\n",
        "        cypher_query += (\n",
        "            f\"MERGE (s:Summary {{content: '{summary_content}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_SUMMARY]->(s) \"\n",
        "        )\n",
        "\n",
        "    # Add Entities (both clean and dirty)\n",
        "    for i, entity in enumerate(record.get(\"clean_entities\", [])):\n",
        "        entity_name = escape_string(entity)\n",
        "        cypher_query += (\n",
        "            f\"MERGE (ce{i}:Entity {{name: '{entity_name}'}}) \"\n",
        "            f\"MERGE (a)-[:MENTIONS]->(ce{i}) \"\n",
        "        )\n",
        "\n",
        "    for i, entity in enumerate(record.get(\"dirty_entities\", [])):\n",
        "        entity_name = escape_string(entity)\n",
        "        cypher_query += (\n",
        "            f\"MERGE (de{i}:DirtyEntity {{name: '{entity_name}'}}) \"\n",
        "            f\"MERGE (a)-[:MENTIONS_DIRTY]->(de{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Topics with Justifications\n",
        "    for i, topic in enumerate(record.get(\"topics\", [])):\n",
        "        topic_name = escape_string(topic['topic'])\n",
        "        justification = escape_string(topic.get('justification', ''))\n",
        "        cypher_query += (\n",
        "            f\"MERGE (t{i}:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_TOPIC]->(t{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Hypothetical Questions\n",
        "    for i, question in enumerate(record.get(\"hypothetical_questions\", [])):\n",
        "        question_content = escape_string(question['question'])\n",
        "        question_type = escape_string(question.get('question_type', ''))\n",
        "        cypher_query += (\n",
        "            f\"MERGE (hq{i}:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_QUESTION]->(hq{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Knowledge Triplets\n",
        "    for i, triplet in enumerate(record.get(\"knowledge_triplets\", [])):\n",
        "        subject_name = escape_string(triplet['subject'])\n",
        "        target_name = escape_string(triplet['target'])\n",
        "        relationship = escape_string(triplet['relationship'])\n",
        "        cypher_query += (\n",
        "            f\"MERGE (subj{i}:Subject {{name: '{subject_name}'}}) \"\n",
        "            f\"MERGE (targ{i}:Target {{name: '{target_name}'}}) \"\n",
        "            f\"MERGE (subj{i})-[:RELATIONSHIP {{type: '{relationship}'}}]->(targ{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add References (URLs and Reference Text)\n",
        "    references = record.get(\"references\", {})\n",
        "    if isinstance(references, dict):\n",
        "        for i, url in enumerate(references.get(\"urls\", [])):\n",
        "            url = escape_string(url)\n",
        "            cypher_query += (\n",
        "                f\"MERGE (r{i}:Reference {{url: '{url}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_REFERENCE]->(r{i}) \"\n",
        "            )\n",
        "        ref_text = references.get(\"references\", \"\")\n",
        "        if ref_text:\n",
        "            ref_text = escape_string(ref_text)\n",
        "            cypher_query += (\n",
        "                f\"MERGE (rText{i}:Reference {{text: '{ref_text}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_REFERENCE]->(rText{i}) \"\n",
        "            )\n",
        "\n",
        "    # Append WITH statement to carry forward 'a' for the next set of operations\n",
        "    #cypher_query += \"RETURN a\"\n",
        "\n",
        "    # Execute the query\n",
        "    session.run(cypher_query)\n",
        "\n",
        "\n",
        "\n",
        "# Rest of the script for add_jsons_to_neo4j, process_json_file, and the main function remains the same\n",
        "\n",
        "\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    # Ensure the session is correctly opened and closed\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                process_json_file(os.path.join(output_folder, file_name), session)\n",
        "\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article chunks from file: {file_path}\")\n",
        "                session.execute_write(lambda tx: create_or_update_article_with_chunks(tx, record))\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n",
        "    # Close the driver after all sessions are complete\n",
        "    driver.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "uri = \"bolt://localhost:7687\"  # Replace with your Neo4j instance URI\n",
        "username = \"neo4j\"  # Replace with your username\n",
        "password = \"password\"  # Replace with your password\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "def load_summaries_to_neo4j(summaries, driver):\n",
        "    with driver.session() as session:\n",
        "        for summary in summaries:\n",
        "            # Call the add_summary function within a Neo4j transaction\n",
        "            session.write_transaction(add_summary, summary)\n",
        "\n",
        "def add_summary(tx, summary_data):\n",
        "    # Create Article node\n",
        "    tx.run(\"MERGE (a:Article {id: $file_id, content: $article})\", summary_data)\n",
        "\n",
        "    # Create Summary node and link to Article\n",
        "    tx.run(\"MERGE (s:Summary {content: $summary}) \"\n",
        "           \"MERGE (a)-[:HAS_SUMMARY]->(s)\", summary_data)\n",
        "\n",
        "    # Handling arrays in summary_data\n",
        "    handle_arrays(tx, summary_data, 'clean_entities', 'Entity', 'MENTIONS', 'e', 'a')\n",
        "    handle_arrays(tx, summary_data, 'dirty_entities', 'DirtyEntity', 'MENTIONS_DIRTY', 'de', 'a')\n",
        "    handle_arrays(tx, summary_data, 'topics', 'Topic', 'HAS_TOPIC', 't', 'a')\n",
        "    handle_arrays(tx, summary_data, 'hypothetical_questions', 'HypotheticalQuestion', 'HAS_QUESTION', 'q', 'a')\n",
        "    \n",
        "    # Handle knowledge triplets\n",
        "    if 'knowledge_triplets' in summary_data:\n",
        "        for triplet in summary_data[\"knowledge_triplets\"]:\n",
        "            tx.run(\"MERGE (s:Subject {name: $subject}) \"\n",
        "                   \"MERGE (t:Target {name: $target}) \"\n",
        "                   \"MERGE (s)-[:RELATIONSHIP {type: $relationship}]->(t)\",\n",
        "                   {'subject': triplet['subject'], 'relationship': triplet['relationship'], 'target': triplet['target']})\n",
        "\n",
        "    # Add references (assuming references are a list of URLs or texts)\n",
        "    if 'references' in summary_data and 'urls' in summary_data['references']:\n",
        "        for url in summary_data[\"references\"][\"urls\"]:\n",
        "            tx.run(\"MERGE (r:Reference {url: $url}) \"\n",
        "                   \"MERGE (a)-[:HAS_REFERENCE]->(r)\", {'url': url, 'article': summary_data['article']})\n",
        "\n",
        "def handle_arrays(tx, summary_data, key, node_type, relationship, node_alias, article_alias):\n",
        "    if key in summary_data:\n",
        "        for item in summary_data[key]:\n",
        "            # For topics with justifications\n",
        "            if key == 'topics' and isinstance(item, dict):\n",
        "                tx.run(\"MERGE (t:Topic {name: $topic}) \"\n",
        "                       \"MERGE (s)-[:HAS_TOPIC {justification: $justification}]->(t)\",\n",
        "                       {'topic': item['topic'], 'justification': item['justification'], 's': summary_data['summary']})\n",
        "\n",
        "            # For hypothetical questions with question types\n",
        "            elif key == 'hypothetical_questions' and isinstance(item, dict):\n",
        "                tx.run(\"MERGE (hq:HypotheticalQuestion {content: $question}) \"\n",
        "                       \"MERGE (s)-[:HAS_QUESTION {type: $question_type}]->(hq)\",\n",
        "                       {'question': item['question'], 'question_type': item['question_type'], 's': summary_data['summary']})\n",
        "\n",
        "            # For simple string lists like clean and dirty entities\n",
        "            elif isinstance(item, str):\n",
        "                tx.run(f\"MERGE ({node_alias}:{node_type} {{name: $name}}) \"\n",
        "                       f\"MERGE (a)-[:{relationship}]->({node_alias})\",\n",
        "                       {'name': item, 'a': summary_data['article']})\n",
        "\n",
        "\n",
        "# Load summaries from JSON folder\n",
        "summaries = []\n",
        "for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "    file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "    if file_name.endswith(\".json\"):\n",
        "        with open(file_path) as f:\n",
        "            summaries.extend(json.load(f))\n",
        "\n",
        "# Load summaries to Neo4j\n",
        "load_summaries_to_neo4j(summaries, driver)\n",
        "\n",
        "# Close the driver connection\n",
        "\n",
        "driver.close()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
