{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperQA - A Question Answering Dataset for Academic Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped empty or invalid JSON file: 7fe118e00411442ea91c951d5fa62338.json\n",
            "Skipped empty or invalid JSON file: 641d140e02854f2f9cd77da5d347168d.json\n",
            "Skipped empty or invalid JSON file: dd803c16e6e14ac6b279b29e3b40c229.json\n",
            "Skipped empty or invalid JSON file: fa591421670b49f4bdffc9b3e53be8c2.json\n",
            "Skipped empty or invalid JSON file: 9a4469c247bc48ac84dda8b05a6e350d.json\n",
            "Skipped empty or invalid JSON file: f885211ff5164954a13d99ef9cfa039f.json\n"
          ]
        }
      ],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "#!pip install paper-qa\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "\n",
        "# Configuration\n",
        "output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    #process_json_files(output_folder)\n",
        "    #process_research_papers(output_folder)\n",
        "    questions = read_questions(questions_file_path)\n",
        "    responses = [docs.query(question) for question in questions]\n",
        "    write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed responses are saved in JSON format to structured_responses.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "def parse_qa_responses(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    responses = []\n",
        "    response = {}\n",
        "    references_lines = []\n",
        "    capturing_references = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('Question:'):\n",
        "            if response:  # Add the previous response with its references to the list\n",
        "                response['references'] = ''.join(references_lines).strip()\n",
        "                responses.append(response)\n",
        "                references_lines = []\n",
        "            response = {'question': line.split('    ')[1].strip(), 'answer': '', 'references': ''}\n",
        "            capturing_references = False\n",
        "        elif line.strip().startswith('I cannot answer') or 'The provided context does not contain' in line:\n",
        "            response['answer'] = line.strip()\n",
        "        elif line.strip().startswith('References'):\n",
        "            capturing_references = True\n",
        "        elif capturing_references:\n",
        "            references_lines.append(line)\n",
        "    \n",
        "    if response:  # Add the last response with its references to the list\n",
        "        response['references'] = ''.join(references_lines).strip()\n",
        "        responses.append(response)\n",
        "    # Add timestamp to the responses\n",
        "    \n",
        "    for response in responses:\n",
        "        response[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        # Extract URLs from the references if any\n",
        "        if response['references']:\n",
        "            response['references_urls'] = extract_urls(response['references'])\n",
        "    return responses\n",
        "\n",
        "def save_json_append(responses, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    combined_data = existing_data + responses\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "file_path =\"responses_file.txt\"\n",
        "output_json_file = 'structured_responses.json'\n",
        "\n",
        "responses = parse_qa_responses(file_path)\n",
        "save_json_append(responses, output_json_file)\n",
        "\n",
        "print(f\"Processed responses are saved in JSON format to {output_json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"/home/epas/Documents/docs.pickle\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"/home/epas/Documents/docs.pickle\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Generate Search Queries\":\n",
        "        with open(\"generate_search_queries.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.7.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.7.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: \"Descriptions of traditional plant protoplast isolation methods for mitochondria\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the traditional methods for isolating plant protoplasts?\",\n",
            "  \"How are plant protoplasts traditionally isolated from mitochondria?\",\n",
            "  \"Describe the procedure of isolating plant protoplasts using traditional methods.\",\n",
            "  \"What are the steps involved in traditional plant protoplast isolation for mitochondria?\",\n",
            "  \"What are the commonly used techniques for isolating plant protoplasts?\",\n",
            "  \"How has the process of isolating plant protoplasts evolved over time?\",\n",
            "  \"Are there any specific challenges or limitations associated with traditional plant protoplast isolation methods for mitochondria?\",\n",
            "  \"What are the advantages and disadvantages of using traditional methods for isolating plant protoplasts from mitochondria?\",\n",
            "  \"Can you provide a comparison of traditional plant protoplast isolation methods for mitochondria and newer techniques?\",\n",
            "  \"Are there any alternative methods for isolating plant protoplasts that are more efficient than the traditional approaches?\"\n",
            "]\n",
            "\n",
            "Question: \"Effectiveness of traditional mitochondrial isolation methods in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the traditional methods used for mitochondrial isolation in Arabidopsis thaliana?\",\n",
            "  \"How effective are the traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"Are there any limitations or drawbacks associated with the traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"What are the advantages of using traditional methods for mitochondrial isolation in Arabidopsis thaliana compared to newer techniques?\",\n",
            "  \"Are there any studies or research papers comparing the effectiveness of traditional mitochondrial isolation methods in Arabidopsis thaliana?\",\n",
            "  \"What are the key steps involved in the traditional mitochondrial isolation methods for Arabidopsis thaliana?\",\n",
            "  \"Can the traditional mitochondrial isolation methods in Arabidopsis thaliana be modified or optimized for better results?\",\n",
            "  \"Are there any alternative techniques or approaches to mitochondrial isolation in Arabidopsis thaliana that have been developed?\",\n",
            "  \"What are the recommended protocols or guidelines for performing mitochondrial isolation in Arabidopsis thaliana using traditional methods?\",\n",
            "  \"Are there any specific considerations or precautions that need to be taken when using traditional mitochondrial isolation methods in Arabidopsis thaliana?\"\n",
            "]\n",
            "\n",
            "Question: \"Limitations of traditional plant protoplast methods in mitochondrial integrity and functionality\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Traditional plant protoplast methods for studying mitochondrial integrity and functionality\"\n",
            "2. \"Challenges and drawbacks of traditional plant protoplast methods in assessing mitochondrial integrity\"\n",
            "3. \"Alternatives to traditional plant protoplast methods for studying mitochondrial functionality\"\n",
            "4. \"Comparative analysis of different plant protoplast methods in assessing mitochondrial integrity\"\n",
            "5. \"Advantages and disadvantages of traditional plant protoplast methods in studying mitochondrial functionality\"\n",
            "6. \"Improvements and advancements in plant protoplast methods for assessing mitochondrial integrity\"\n",
            "7. \"Exploring the limitations of traditional plant protoplast methods in studying mitochondrial functionality\"\n",
            "8. \"Investigating the impact of traditional plant protoplast methods on mitochondrial integrity\"\n",
            "9. \"Emerging techniques for assessing mitochondrial functionality in plant protoplasts\"\n",
            "10. \"Critique of traditional plant protoplast methods in measuring mitochondrial integrity and functionality\"\n",
            "\n",
            "Question: \"Detailed protocols for using continuous colloidal density gradients in mitochondrial isolation\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Continuous colloidal density gradients protocol for mitochondrial isolation\"\n",
            "2. \"Step-by-step guide for using continuous colloidal density gradients in mitochondrial isolation\"\n",
            "3. \"Best practices for using continuous colloidal density gradients in mitochondrial isolation\"\n",
            "4. \"Optimal conditions for conducting mitochondrial isolation with continuous colloidal density gradients\"\n",
            "5. \"Troubleshooting tips for continuous colloidal density gradients in mitochondrial isolation\"\n",
            "6. \"Comparative analysis of continuous colloidal density gradients vs other methods for mitochondrial isolation\"\n",
            "7. \"Recent advancements in continuous colloidal density gradients for mitochondrial isolation\"\n",
            "8. \"Applications of continuous colloidal density gradients in mitochondrial research\"\n",
            "9. \"Limitations and challenges of continuous colloidal density gradients in mitochondrial isolation\"\n",
            "10. \"Review articles on the use of continuous colloidal density gradients for mitochondrial isolation\"\n",
            "\n",
            "Question: \"Adjustments in isolation medium composition for Arabidopsis thaliana mitochondria\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the optimal medium compositions for isolating Arabidopsis thaliana mitochondria?\",\n",
            "  \"How does the composition of the isolation medium affect the isolation of Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any specific adjustments that can be made to the isolation medium for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the common components used in isolation medium for Arabidopsis thaliana mitochondria?\",\n",
            "  \"Can the isolation medium composition be optimized for better yield of Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any studies on the effects of different isolation medium compositions on Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the best practices for adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"Are there any protocols available for optimizing the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the challenges in adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\",\n",
            "  \"What are the potential benefits of adjusting the isolation medium composition for Arabidopsis thaliana mitochondria?\"\n",
            "]\n",
            "\n",
            "Question: \"Impact of temperature control at 4 °C on mitochondrial isolation in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Effects of temperature control at 4 °C on mitochondrial isolation in Arabidopsis thaliana\"\n",
            "2. \"Influence of temperature regulation at 4 °C on the isolation process of mitochondria in Arabidopsis thaliana\"\n",
            "3. \"How does temperature control at 4 °C affect the isolation of mitochondria in Arabidopsis thaliana?\"\n",
            "4. \"Exploring the relationship between temperature control at 4 °C and mitochondrial isolation in Arabidopsis thaliana\"\n",
            "5. \"Investigating the impact of maintaining a temperature of 4 °C on the process of isolating mitochondria in Arabidopsis thaliana\"\n",
            "6. \"The role of temperature control at 4 °C in the efficiency of mitochondrial isolation in Arabidopsis thaliana\"\n",
            "7. \"Temperature manipulation at 4 °C and its effects on the isolation yield of mitochondria in Arabidopsis thaliana\"\n",
            "8. \"Understanding the influence of temperature regulation at 4 °C on mitochondrial isolation in Arabidopsis thaliana\"\n",
            "9. \"Comparing the effects of different temperature conditions on the isolation of mitochondria in Arabidopsis thaliana\"\n",
            "10. \"Exploring the optimal temperature for mitochondrial isolation in Arabidopsis thaliana: A focus on 4 °C\"\n",
            "\n",
            "Question: \"Control variables important for comparing mitochondrial isolation methods\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the key control variables to consider when comparing mitochondrial isolation methods?\"\n",
            "2. \"Which factors should be controlled for when evaluating different techniques for mitochondrial isolation?\"\n",
            "3. \"What are the important variables that need to be standardized when comparing different methods of isolating mitochondria?\"\n",
            "4. \"What are the common confounding factors that can affect the results when comparing mitochondrial isolation protocols?\"\n",
            "5. \"What are the potential sources of variability that should be controlled for when comparing different methods of isolating mitochondria?\"\n",
            "6. \"Are there any specific parameters that need to be kept constant when comparing the efficiency of mitochondrial isolation techniques?\"\n",
            "7. \"What are the critical factors that need to be taken into account when evaluating the accuracy and reproducibility of mitochondrial isolation methods?\"\n",
            "8. \"Which variables should be controlled to ensure the reliability and validity of comparisons between different mitochondrial isolation procedures?\"\n",
            "9. \"What are the potential sources of bias that need to be minimized when comparing the effectiveness of various mitochondrial isolation methods?\"\n",
            "10. \"What are the standard control measures that should be implemented when comparing the performance of different mitochondrial isolation techniques?\"\n",
            "\n",
            "Question: \"Influence of plant age and tissue type on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Factors affecting mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Effect of plant age on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Impact of tissue type on mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Optimal plant age for mitochondrial isolation in Arabidopsis thaliana\",\n",
            "  \"Comparison of mitochondrial isolation quality in different tissue types of Arabidopsis thaliana\",\n",
            "  \"Mitochondrial isolation protocols for Arabidopsis thaliana\",\n",
            "  \"Methods to improve mitochondrial isolation quality in Arabidopsis thaliana\",\n",
            "  \"Mitochondrial isolation techniques for plant research\",\n",
            "  \"Mitochondrial isolation challenges in Arabidopsis thaliana\",\n",
            "  \"Mitochondrial purity assessment in Arabidopsis thaliana\"\n",
            "]\n",
            "\n",
            "Question: \"Protocols for assessing mitochondrial integrity using proteinase digestion assays\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the different protocols available for assessing mitochondrial integrity using proteinase digestion assays?\",\n",
            "  \"How can proteinase digestion assays be used to assess mitochondrial integrity?\",\n",
            "  \"Are there any established methods or guidelines for assessing mitochondrial integrity using proteinase digestion assays?\",\n",
            "  \"What are the advantages and limitations of proteinase digestion assays in assessing mitochondrial integrity?\",\n",
            "  \"Are there any alternative methods or techniques for assessing mitochondrial integrity besides proteinase digestion assays?\",\n",
            "  \"What are the key considerations when performing proteinase digestion assays to assess mitochondrial integrity?\",\n",
            "  \"Are there any specific markers or indicators that can be used in proteinase digestion assays to evaluate mitochondrial integrity?\",\n",
            "  \"What are the best practices for analyzing and interpreting the results of proteinase digestion assays in assessing mitochondrial integrity?\",\n",
            "  \"Can proteinase digestion assays be used to differentiate between healthy and damaged mitochondria?\",\n",
            "  \"Are there any studies or research papers that have used proteinase digestion assays to assess mitochondrial integrity?\"\n",
            "]\n",
            "\n",
            "Question: \"Techniques for measuring mitochondrial membrane potential in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Methods for measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "2. \"Techniques for quantifying mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "3. \"Procedures for assessing mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "4. \"Tools and assays for detecting mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "5. \"Comparative analysis of different methods to measure mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "6. \"Best practices for measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "7. \"Validation techniques for mitochondrial membrane potential measurements in Arabidopsis thaliana\"\n",
            "8. \"Advancements in measuring mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "9. \"Challenges and limitations in assessing mitochondrial membrane potential in Arabidopsis thaliana\"\n",
            "10. \"Review of studies on mitochondrial membrane potential measurement in Arabidopsis thaliana\"\n",
            "\n",
            "Question: \"Comparative studies on mitochondrial isolation methods in Arabidopsis thaliana\",\n",
            "Answerable: False\n",
            "\n",
            "Url(s): ['https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-99']\n",
            "\n",
            "Search Queries: 1. \"Mitochondrial isolation methods in Arabidopsis thaliana: a comparative study\"\n",
            "2. \"Comparison of techniques for isolating mitochondria in Arabidopsis thaliana\"\n",
            "3. \"Efficiency and effectiveness of mitochondrial isolation techniques in Arabidopsis thaliana\"\n",
            "4. \"Optimal methods for isolating mitochondria in Arabidopsis thaliana: a review\"\n",
            "5. \"Advantages and disadvantages of different mitochondrial isolation protocols in Arabidopsis thaliana\"\n",
            "6. \"Evaluation of mitochondrial isolation techniques in Arabidopsis thaliana: a systematic review\"\n",
            "7. \"Comparing different approaches for isolating mitochondria in Arabidopsis thaliana\"\n",
            "8. \"Mitochondrial isolation methods in Arabidopsis thaliana: a comprehensive analysis\"\n",
            "9. \"Exploring the most reliable techniques for isolating mitochondria in Arabidopsis thaliana\"\n",
            "10. \"Comparative analysis of mitochondrial isolation protocols in Arabidopsis thaliana\"\n",
            "\n",
            "Question: \"Advantages and disadvantages of various mitochondrial isolation techniques in plant biology\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Advantages of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Disadvantages of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Comparison of different mitochondrial isolation techniques in plant biology\",\n",
            "  \"Effectiveness of various mitochondrial isolation techniques in plant biology\",\n",
            "  \"Limitations of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Improvements in mitochondrial isolation techniques in plant biology\",\n",
            "  \"Challenges in isolating mitochondria in plant biology\",\n",
            "  \"Innovations in mitochondrial isolation techniques in plant biology\",\n",
            "  \"Optimal mitochondrial isolation techniques in plant biology\",\n",
            "  \"Methods for isolating mitochondria in plant biology\"\n",
            "]\n",
            "\n",
            "Question: \"Statistical methods for analyzing mitochondrial isolation outcomes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Statistical methods for analyzing mitochondrial isolation outcomes\"\n",
            "2. \"Experimental design for studying mitochondrial isolation outcomes\"\n",
            "3. \"Factors influencing the success of mitochondrial isolation\"\n",
            "4. \"Comparison of statistical approaches for analyzing mitochondrial isolation data\"\n",
            "5. \"Best practices for analyzing mitochondrial isolation outcomes\"\n",
            "6. \"Challenges in analyzing mitochondrial isolation data\"\n",
            "7. \"Role of sample size in statistical analysis of mitochondrial isolation outcomes\"\n",
            "8. \"Multivariate analysis techniques for studying mitochondrial isolation outcomes\"\n",
            "9. \"Statistical software for analyzing mitochondrial isolation data\"\n",
            "10. \"Meta-analysis of studies on mitochondrial isolation outcomes\"\n",
            "\n",
            "Question: \"Criteria for successful mitochondrial isolation in terms of purity and functionality\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Expert reviews on advancements in mitochondrial isolation techniques\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Advancements in mitochondrial isolation techniques\",\n",
            "  \"Current methods for isolating mitochondria\",\n",
            "  \"Review articles on mitochondrial isolation techniques\",\n",
            "  \"Comparative analysis of different mitochondrial isolation protocols\",\n",
            "  \"Challenges in mitochondrial isolation and potential solutions\",\n",
            "  \"New approaches for isolating mitochondria\",\n",
            "  \"Emerging technologies in mitochondrial isolation\",\n",
            "  \"Cutting-edge techniques for mitochondrial isolation\",\n",
            "  \"Best practices for isolating mitochondria\",\n",
            "  \"Optimizing mitochondrial isolation protocols\",\n",
            "  \"Expert opinions on mitochondrial isolation techniques\",\n",
            "  \"Recent studies on improving mitochondrial isolation\",\n",
            "  \"Innovations in mitochondrial isolation methods\",\n",
            "  \"Methods for isolating mitochondria from different cell types\",\n",
            "  \"Efficient and reliable techniques for isolating mitochondria\",\n",
            "  \"Troubleshooting common issues in mitochondrial isolation\",\n",
            "  \"Factors influencing the success of mitochondrial isolation\",\n",
            "  \"Mitochondrial isolation protocols for specific research applications\",\n",
            "  \"Comparison of manual vs automated mitochondrial isolation methods\",\n",
            "  \"Protocols for high-throughput mitochondrial isolation\",\n",
            "  \"Mitochondrial isolation kits and their performance evaluation\"\n",
            "]\n",
            "\n",
            "Question: \"Meta-analyses on mitochondrial isolation methods in plant biology\"\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the different methods used for isolating mitochondria in plant biology?\",\n",
            "  \"Comparison of mitochondrial isolation methods in plant biology\",\n",
            "  \"Meta-analysis of mitochondrial isolation techniques in plant biology\",\n",
            "  \"Advantages and disadvantages of different methods for isolating mitochondria in plant biology\",\n",
            "  \"Optimal conditions for isolating mitochondria in plant biology\",\n",
            "  \"Efficiency of different mitochondrial isolation techniques in plant biology\",\n",
            "  \"Validation of mitochondrial isolation methods in plant biology\",\n",
            "  \"Common challenges in isolating mitochondria from plant cells\",\n",
            "  \"Improvements in mitochondrial isolation methods for plant biology\",\n",
            "  \"Recent advancements in mitochondrial isolation techniques for plant research\"\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "def generate_search_queries(question: str):\n",
        "    # Build the prompt\n",
        "    prompt = build_system_prompt(\"Generate Search Queries\")\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "\n",
        "    # Generate the response from gpt-3.5-turbo\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    response = client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo-0613\",\n",
        "                    temperature=0.7,\n",
        "                    max_retries=3,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": \"Search Querys:\"},\n",
        "                    ],\n",
        "                )\n",
        "    response_text = parse_response(response)\n",
        "    return response_text\n",
        "\n",
        "def check_unanswered_questions(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    unanswered_questions = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Checking for phrases that indicate an unanswered question\n",
        "        if \"cannot answer\" in entry[\"answer\"] or \"does not contain\" in entry[\"answer\"] or \"no answer\" in entry[\"answer\"] or \"no results\" in entry[\"answer\"] or \"no information\" in entry[\"answer\"]:\n",
        "            unanswered = True\n",
        "        else:\n",
        "            unanswered = False\n",
        "\n",
        "        # Building the result entry\n",
        "        result_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"answerable\": not unanswered,\n",
        "            \"timestamp\": entry.get(\"timestamp\", \"Unknown timestamp\")\n",
        "        }\n",
        "\n",
        "        if entry.get(\"references\"):\n",
        "            result_entry[\"references\"] = entry[\"references\"]\n",
        "\n",
        "        if entry.get(\"references_urls\"):  # Using .get to avoid KeyError\n",
        "            result_entry[\"references_urls\"] = entry[\"references_urls\"]\n",
        "\n",
        "        unanswered_questions.append(result_entry)\n",
        "\n",
        "    return unanswered_questions\n",
        "\n",
        "json_file = 'structured_responses.json'\n",
        "unanswered_questions = check_unanswered_questions(json_file)\n",
        "\n",
        "# Display the results\n",
        "for item in unanswered_questions:\n",
        "    print(f\"Question: {item['question']}\\nAnswerable: {item['answerable']}\\n\")\n",
        "    if item.get(\"references\") and item.get(\"references_urls\"):\n",
        "        print(f\"Url(s): {item['references_urls']}\\n\")\n",
        "        #print(f\"Reference(s): {item['references']}\\n\")\n",
        "    # Generate search queries for unanswered questions\n",
        "    if item['answerable'] == False:\n",
        "        if item.get(\"references\"):\n",
        "            search_queries = generate_search_queries(f\"{item['question']}\\n{item['references']}\")\n",
        "        else:\n",
        "            search_queries = generate_search_queries(item[\"question\"])\n",
        "        # save search queries to json file\n",
        "        with open(\"search_queries.json\", \"a\") as outfile:\n",
        "            json.dump(search_queries, outfile)\n",
        "        print(f\"Search Queries: {search_queries}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install paper-qa\n",
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lee2018.pdf\n",
            "s41556-023-01246-1.pdf\n",
            "s00068-018-0954-3.pdf\n",
            "s13578-022-00805-7.pdf\n",
            "Advanced Science - 2023 - Maffeis - Synthetic Cells Revisited Artificial Cells Construction Using Polymeric Building.pdf\n",
            "elife-70899-v2.pdf\n",
            "izawa2017.pdf\n",
            "fonc-11-672781.pdf\n",
            "s41392-020-00440-z.pdf\n",
            "Mitochondria and cell signalling - PMC.pdf\n",
            "nihms158858.pdf\n",
            "nihms-1621944.pdf\n",
            "s13619-023-00158-7.pdf\n",
            "nihms804627.pdf\n",
            "elife-70899-figures-v2.pdf\n"
          ]
        }
      ],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('/home/epas/Programming/ResearchAgentSwarm/Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    #docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "    print(paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaperqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Docs\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     docs \u001b[38;5;241m=\u001b[39m Docs(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, Docs, PromptCollection, Doc, Text, Context\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:42\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, CallbackFactory, Context, Doc, DocKey, PromptCollection, Text\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     gather_with_concurrency,\n\u001b[1;32m     31\u001b[0m     get_llm_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     strip_citations,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDocs\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marbitrary_types_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmart_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"A collection of documents to be used for answering questions.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:50\u001b[0m, in \u001b[0;36mDocs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m texts_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m doc_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m llm: Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel] \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m summary_llm: Optional[Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 4\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mpaperscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbayesian model selection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded-papers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:578\u001b[0m, in \u001b[0;36msearch_papers\u001b[0;34m(query, limit, pdir, semantic_scholar_api_key, _paths, _limit, _offset, logger, year, verbose, scraper, batch_size, search_type)\u001b[0m\n\u001b[1;32m    576\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    577\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_search_papers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:350\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    203\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:495\u001b[0m, in \u001b[0;36ma_search_papers.<locals>.process_paper\u001b[0;34m(paper, i)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_paper\u001b[39m(paper, i):\n\u001b[1;32m    494\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaperId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape(paper, path, i\u001b[38;5;241m=\u001b[39mi, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    497\u001b[0m         bibtex \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitationStyles\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbibtex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/scraper.py:63\u001b[0m, in \u001b[0;36mScraper.scrape\u001b[0;34m(self, paper, path, i, logger)\u001b[0m\n\u001b[1;32m     61\u001b[0m scraper \u001b[38;5;241m=\u001b[39m scrapers[j]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mfunction(paper, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscraper\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mcheck_pdf \u001b[38;5;129;01mor\u001b[39;00m check_pdf(path)):\n\u001b[1;32m     65\u001b[0m         scrape_result[scraper\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:239\u001b[0m, in \u001b[0;36mopenaccess_scraper\u001b[0;34m(paper, path, session)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m link_to_pdf(url, path, session)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:103\u001b[0m, in \u001b[0;36mlink_to_pdf\u001b[0;34m(url, path, session)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=1,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Research Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Function to clean entities based on new lines and remove leading hyphens\n",
        "def clean_and_separate_entities(entities_list):\n",
        "    entities_str = '\\n'.join(entities_list)\n",
        "    cleaned_entities = []\n",
        "    dirty_entities = []\n",
        "\n",
        "    for line in entities_str.split('\\n'):\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line.startswith('-'):\n",
        "            # Remove the leading hyphen and any extra space after it\n",
        "            cleaned_entities.append(stripped_line.lstrip('-').strip())\n",
        "        else:\n",
        "            dirty_entities.append(stripped_line)\n",
        "\n",
        "    return cleaned_entities, dirty_entities\n",
        "def test_clean_and_separate_entities():\n",
        "    \n",
        "    # Define the summary JSON file path\n",
        "    SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "    # Read the summaries.json file\n",
        "    with open(SUMMARY_JSON, \"r\") as file:\n",
        "        summaries_json = json.load(file)\n",
        "\n",
        "    # Extract the first entities entry\n",
        "    first_entities_list = summaries_json[0][\"entities\"][0]\n",
        "\n",
        "    # Clean the entities and separate the uncleaned ones\n",
        "    cleaned_entities, dirty_entities = clean_and_separate_entities(first_entities_list)\n",
        "\n",
        "    # Save the results to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp_file:\n",
        "        json.dump({\n",
        "            \"cleaned_entities\": cleaned_entities,\n",
        "            \"dirty_entities\": dirty_entities\n",
        "        }, temp_file, indent=4)\n",
        "\n",
        "    print(\"Results saved in:\", temp_file.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_topics_with_justification(topic_text):\n",
        "    # Regular expression pattern for identifying topics with their justifications\n",
        "    topic_pattern = re.compile(r'(\\d+)\\.\\s+([^\\n]+)(\\n\\s+-[^\\n]+)*')\n",
        "    topics = topic_pattern.findall(topic_text)\n",
        "    \n",
        "    extracted_topics = []\n",
        "    for match in topics:\n",
        "        topic = match[1].strip()\n",
        "        justification = ' '.join(match[2].split('\\n')).strip()\n",
        "        # Remove \"Justification:\" if it starts with it\n",
        "        if justification.lower().startswith('- justification:'):\n",
        "            justification = justification[len('- justification:'):].strip()\n",
        "        # Remove the - if it starts with it\n",
        "        if justification.startswith('-'):\n",
        "            justification = justification[1:].strip()\n",
        "        extracted_topics.append({\"topic\": topic, \"justification\": justification})\n",
        "\n",
        "    return extracted_topics\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_topics_with_justification():\n",
        "    # Adjusted topic text\n",
        "    topic_text_list = []\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of Mitochondria in Energy Production, Signaling, and Apoptosis\\n   - Mitochondria as the powerhouse of the cell\\n   - Role of mitochondria in energy production, signaling, and apoptosis\\n   - Significance of studying mitochondrial function and involvement in diseases\\n\\n2. Challenges with Traditional Methods of Mitochondrial Isolation\\n   - Limitations of traditional methods like differential centrifugation\\n   - Potential damage to mitochondrial double membrane and variable viability\\n\\n3. Innovative Techniques for Mitochondrial Isolation\\n   - Nitrogen cavitation for gentle disruption and release of intact mitochondria\\n   - Affinity purification using anti-TOM22 magnetic beads for efficient isolation\\n   - Filtration-based methods to reduce isolation time and improve viability\\n   - Differential isopycnic density gradient centrifugation for separation based on buoyant density\\n\\n4. Quality Control Measures for Validating Mitochondrial Isolation\\n   - Assessment of mitochondrial respiration, metabolic activity, protein import, and membrane fusion\\n   - High-resolution respirometry and bioluminescent measurements of ATP synthesis\\n\\n5. Importance of Continued Refinement and Standardization of Techniques\\n   - Advancing understanding of mitochondrial biology and implications in health and disease\\n   - Need for standardized protocols to facilitate comparisons and translation of research findings into clinical applications\\n\\n**Notes**: The summary provides a comprehensive overview of the importance of mitochondria, challenges with traditional methods of isolation, innovative techniques for isolation, quality control measures, and the need for continued refinement and standardization. The topics cover the main ideas and themes discussed in the summary, providing a clear and comprehensive analysis of the content.\") \n",
        "    topic_text_list.append(\"**Topic List:**\\n\\n1. Challenges in isolating intact mitochondria from plant cells\\n   - Cell walls, mitochondrial membranes, and large amounts of starting material\\n2. Comprehensive protocol for isolating intact mitochondria from plant cells\\n   - Grinding, filtering, centrifuging, and resuspending\\n3. Characterization and analysis of isolated mitochondria\\n   - Purity, integrity, and functionality assessment\\n   - Techniques: protein profiling, enzymatic activity assays, respiratory chain measurements, and oxygen consumption analysis\\n4. Storage of purified mitochondria\\n   - Long-term storage at -80°C\\n5. Adaptation of isolation process for different tissue types and plant species\\n   - Consideration of phenolic compounds and metabolite profiles\\n6. Validation and controls for quality and functionality assurance\\n7. Downstream applications of isolated mitochondria\\n   - Protein and tRNA uptake experiments, enzyme activity assays, Western blot analyses, and mass spectrometry analyses\\n\\n**Notes:**\\n- The revised summary provides a comprehensive overview of the topic, covering various aspects of isolating intact mitochondria from plant cells.\\n- The topics are specific and non-repetitive, ensuring a clear and distinct representation of the core themes.\\n- The summary is focused on the technical process and considerations involved in isolating mitochondria, as well as the analysis and applications of the isolated mitochondria.\")\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction\\n    - Justification: The summary highlights the crucial role of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction.\\n\\n2. Significance of gentle and effective mitochondrial isolation techniques\\n    - Justification: The summary emphasizes the importance of gentle and effective isolation techniques for studying mitochondrial biology and developing mitochondrial-based therapies.\\n\\n3. Overview of macroscale mitochondrial isolation techniques\\n    - Justification: The summary discusses macroscale mitochondrial isolation techniques, such as manual homogenization and differential filtration-based isolation.\\n\\n4. Advancements in microscale and nanoscale mitochondrial isolation techniques\\n    - Justification: The summary mentions microscale and nanoscale techniques, including microfluidic techniques and nanoprobe-based technologies, for mitochondrial isolation.\\n\\n5. Breakthroughs in sub-cellular isolation techniques for mitochondria\\n    - Justification: The summary highlights breakthroughs in sub-cellular isolation techniques that enable the isolation of mitochondria from subcellular compartments with minimal disruption.\\n\\n6. Challenges in mitochondrial isolation techniques\\n    - Justification: The summary mentions challenges such as the presence of whole cell contaminants in mitochondrial isolates and the time sensitivity of isolated mitochondria.\\n\\n7. Emerging therapeutic approach: Autologous mitochondrial transplants\\n    - Justification: The summary discusses the development of autologous mitochondrial transplants as an emerging therapeutic approach.\\n\\n8. Contributions of the London Centre for Nanotechnology and the McCully laboratory\\n    - Justification: The summary mentions the significant contributions of the London Centre for Nanotechnology and the McCully laboratory in optimizing differential filtration-based mitochondrial isolation for use in cellular models.\\n\\n9. Role of Stem Cell Research & Therapy in advancing mitochondrial medicine\\n    - Justification: The summary highlights the role of Stem Cell Research & Therapy in providing in-depth overviews of advancements in mitochondrial research and facilitating the development of novel therapies for mitochondrial diseases.\")\n",
        "    topic_text_list.append(\"Topics:\\n1. Genetic modifications to enhance mitochondrial autonomy\\n   - Justification: The main focus of the report is exploring genetic modifications to enhance the autonomy of mitochondria from nuclear-encoded proteins and functions.\\n2. Role of mitochondria in cellular function\\n   - Justification: The report highlights the crucial role played by mitochondria in cellular function.\\n3. Coordination between mtDNA and nuclear DNA\\n   - Justification: The report discusses the coordination required between mtDNA and nuclear DNA, as most proteins are encoded by nuclear DNA.\\n4. Therapeutic strategies for mitochondrial diseases\\n   - Justification: The report mentions that enhancing mitochondrial autonomy could lead to new therapeutic strategies for mitochondrial diseases.\\n5. Research on genome engineering, programmable nucleases, and base editors\\n   - Justification: The report mentions that recent research in genome engineering, programmable nucleases, and base editors shows promise for treating hereditary mitochondrial diseases.\\n6. Challenges in genetic manipulation of mtDNA\\n   - Justification: The report discusses challenges such as mtDNA mutations, resistance to genetic manipulation, and limitations in mtDNA recombination.\\n7. Advancements in protein-only gene editing platforms\\n   - Justification: The report mentions advancements in protein-only gene editing platforms as potential solutions to the challenges in genetic manipulation of mtDNA.\\n8. Somatic mitochondrial DNA-replaced cells\\n   - Justification: The report mentions the generation of somatic mitochondrial DNA-replaced cells as a potential solution to the challenges in genetic manipulation of mtDNA.\\n9. Mitochondrial nucleoids and their role in maintaining genetic autonomy\\n   - Justification: The report highlights the concept of mitochondrial nucleoids and their role in maintaining genetic autonomy as a key area of study.\\n10. Mitochondrial epigenomics and gene expression regulation\\n    - Justification: The report emphasizes the importance of understanding mitochondrial epigenomics and gene expression regulation in different cellular contexts, including stress conditions, for identifying genetic modifications that could enhance mitochondrial autonomy.\")\n",
        "    for topic_text in topic_text_list:\n",
        "        extracted_topics = extract_topics_with_justification(topic_text)\n",
        "        print(f'Extracted topics: {extracted_topics}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor url in urls:\\n    try:\\n        pdf =  pdfx.PDFx(url)\\n        metadata = pdf.get_metadata()\\n        print(f\\'Metadata: {metadata}\\')\\n        references_list = pdf.get_references()\\n        print(f\\'References: {references_list}\\')\\n        references_dict = pdf.get_references_as_dict()\\n        print(f\\'References dict: {references_dict}\\')\\n        papers = paperscraper.link_to_pdf(url, pdir=\\'downloaded-papers\\')\\n        print(f\\'Papers: {papers}\\')\\n    except:\\n        print(\"Error in extracting references\")\\n        continue\\n#pdf.download_pdfs(\"target-directory\")\\n\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#!pip install pdfx\n",
        "import pdfx\n",
        "#!pip install paperscraper\n",
        "#import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def test_extract_urls():\n",
        "    # Define the reference text\n",
        "    reference_text = \"\"\"\\n\\nAmerican Institute of Physics. (2023). The powerhouse of the future: Artificial cells. Phys.org. Retrieved from https://phys.org/news/2023-03-powerhouse-future-artificial-cells.html\\n\\nNational Institutes of Health. (2023). Artificial mitochondria transfer (AMT) and transplant. PMC. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5511681/\\n\\nNature. (2023). Spatiotemporal simulations of mitochondrial dynamics. Nature.com. Retrieved from https://www.nature.com/articles/s41598-019-54159-1\\n\\nSogang University & Harbin Institute of Technology. (2023). Artificial organelles for sustainable chemical energy conversion and production: Artificial mitochondria and chloroplasts. Biophysics Reviews. Retrieved from https://publishing.aip.org/publications/latest-content/the-powerhouse-of-the-future-artificial-cells/\"\"\"\n",
        "\n",
        "    urls = extract_urls(reference_text)\n",
        "    print(f'Extracted URLs: {urls}')\n",
        "\n",
        "#pdf = pdfx.PDFx(\"filename-or-url.pdf\")\n",
        "#urls = ['/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/2308.00352.pdf']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for url in urls:\n",
        "    try:\n",
        "        pdf =  pdfx.PDFx(url)\n",
        "        metadata = pdf.get_metadata()\n",
        "        print(f'Metadata: {metadata}')\n",
        "        references_list = pdf.get_references()\n",
        "        print(f'References: {references_list}')\n",
        "        references_dict = pdf.get_references_as_dict()\n",
        "        print(f'References dict: {references_dict}')\n",
        "        papers = paperscraper.link_to_pdf(url, pdir='downloaded-papers')\n",
        "        print(f'Papers: {papers}')\n",
        "    except:\n",
        "        print(\"Error in extracting references\")\n",
        "        continue\n",
        "#pdf.download_pdfs(\"target-directory\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Questions: [('Content-Based Question', 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'), ('Analytical Question', 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'), ('Creative/Scenario-Based Question', 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'), ('Contextual/Relational Question', 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'), ('User-Interactive Question', 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Content-Based Question', 'question': 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'}, {'question_type': 'Analytical Question', 'question': 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'}, {'question_type': 'User-Interactive Question', 'question': 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?'}]\n",
            "Questions: [('Analytical Question', 'How do theoretical models help in understanding mitochondrial ATP production?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'), ('Contextual/Relational Question', 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'), ('User-Interactive Question', 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Analytical Question', 'question': 'How do theoretical models help in understanding mitochondrial ATP production?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'}, {'question_type': 'User-Interactive Question', 'question': 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_hypothetical_questions(hypothetical_questions_text):\n",
        "    # Regular expression pattern for identifying hypothetical questions\n",
        "    question_pattern = re.compile(r'\\d+\\.\\s+([A-Za-z\\/-]+ Question):\\n\\s+-\\s+([^\\n]+)')\n",
        "    questions = question_pattern.findall(hypothetical_questions_text)\n",
        "    print(f'Questions: {questions}')\n",
        "    if len(questions) == 0:\n",
        "        return hypothetical_questions_text\n",
        "    return [{\"question_type\": question_type, \"question\": question} for question_type, question in questions]\n",
        "def test_extract_hypothetical_questions():\n",
        "    # Example hypothetical questions text\n",
        "    hypothetical_questions_text_1 = \"1. Content-Based Question:\\n   - How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?\\n\\n2. Analytical Question:\\n   - What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?\\n\\n5. User-Interactive Question:\\n   - What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?\"\n",
        "    hypothetical_questions_text_2 = \"1. Content-Based Question: \\n   - What does this report investigate regarding mitochondrial ATP production?\\n   - How does this report contribute to our understanding of mitochondrial function?\\n   - What are the key findings regarding the replication of mitochondrial ATP production outside the cellular environment?\\n\\n2. Analytical Question:\\n   - How do theoretical models help in understanding mitochondrial ATP production?\\n   - What experimental evidence supports the concept of artificial organelles for ATP synthesis?\\n   - What are the implications of studying mitochondrial dynamics and stress responses for ex vivo methods of ATP synthesis?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?\\n   - If artificial organelles capable of ATP synthesis were successfully developed, what potential applications could they have in various industries?\\n   - How might the understanding of mitochondrial dynamics and stress responses lead to the development of innovative approaches for ATP synthesis?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?\\n   - In what ways does the replication of mitochondrial ATP production outside cells build upon previous studies in the field?\\n   - How do the findings in this report align with or challenge existing theories and models of mitochondrial function?\\n\\n5. User-Interactive Question:\\n   - How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?\\n   - Can you think of any potential limitations or ethical considerations in developing artificial organelles for ATP synthesis?\\n   - What questions or areas of research would you like to see explored further in the study of mitochondrial dynamics and stress responses?\"\n",
        "    hypothetical_questions = []\n",
        "    hypothetical_questions.append(hypothetical_questions_text_1)\n",
        "    hypothetical_questions.append(hypothetical_questions_text_2)\n",
        "    for hypothetical_questions_text in hypothetical_questions:\n",
        "        extracted_hypothetical_questions = extract_hypothetical_questions(hypothetical_questions_text)\n",
        "        print(f'Extracted hypothetical questions: {extracted_hypothetical_questions}')\n",
        "\n",
        "test_extract_hypothetical_questions()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'tailored isolation protocol'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimized damage to ensure the integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_entity_relationships(entity_relationships_text):\n",
        "    # Regular expression pattern for identifying entity relationships\n",
        "    entity_pattern = re.compile(r'\\d+\\.\\s+\\((.+?),\\s+(.+?),\\s+(.+?)\\)')\n",
        "    entity_relationships = entity_pattern.findall(entity_relationships_text)\n",
        "    return [{\"subject\": relationship[0], \"relationship\": relationship[1], \"target\": relationship[2]} for relationship in entity_relationships]\n",
        "\n",
        "# Example entity relationships text\n",
        "entity_relationships_text =  \"Entity Relationships:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n30. (mitochondria, isolated with, tailored isolation protocol)\\n31. (mitochondria, isolated with, minimized damage to ensure the integrity)\\n32. (mitochondria, isolated with, reduced contamination from other organelles)\\n33. (mitochondria, isolated with, improved methods)\\n34. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n35. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n36. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n37. (mitochondria, isolated with, minimal contamination from other organelles)\\n38. (mitochondria, isolated with, improved methods)\\n39. (mitochondria, isolated with, slight modifications)\\n40. (mitochondria, isolated with, traditional plant protoplast isolation)\\n41. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n42. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n43. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n44. (mitochondria, isolated with, minimal contamination from other organelles)\\n45. (mitochondria, isolated from, Arabidopsis thaliana)\\n46. (mitochondria, isolated using, continuous colloidal density gradients)\\n47. (mitochondria, isolated at, 4 °C)\\n48. (mitochondria, used for, protein and tRNA uptake experiments)\\n49. (mitochondria, used for, enzyme activity assays)\\n50. (mitochondria, used for, western blot analyses)\\n51. (mitochondria, used for, mass spectrometry analyses)\\n52. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n53. (mitochondria, assessed for, purity and integrity)\\n54. (mitochondria, assessed using, proteinase digestion assays)\\n55. (mitochondria, assessed using, electron microscopy)\\n56. (mitochondria, assessedThe article discusses the protocol for isolating mitochondria from plant cells. Mitochondria are double-membraned organelles responsible for energy production in eukaryotic cells. The isolation of mitochondria is crucial for various studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays.\\n\\nThe isolation process is challenging due to the presence of cell walls, vacuoles, and secondary metabolites in plant cells. The protocol must be tailored to minimize damage to the mitochondria and ensure their integrity. Specificity in isolation protocols is required as different plant species and tissue types have varying phenolic compounds and metabolite profiles. Earlier methods led to contamination with nuclei and chloroplasts, but recent advancements have improved isolation methods, reducing the need for heavy labor, expensive equipment, and large amounts of starting material.\\n\\nThe protocol for isolating intact mitochondria involves several steps. First, the preparation of grinding medium, wash buffer, and gradient solutions is necessary. The plant material is then homogenized in the grinding medium to release the mitochondria, which are then filtered and centrifuged to pellet the mitochondria. The mitochondrial pellet is resuspended in the wash buffer. Oxygen consumption measurements are crucial for determining the intactness and functional capacity of the isolated mitochondria. Evaluation of mitochondrial purity and integrity can be done through proteinase digestion assays, electron microscopy, and checks of mitochondrial membrane potential and electron transport chain activity.\\n\\nOnce purified, the isolated mitochondria can be used for various studies, including protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses. For mass spectrometry analyses, targeted multiple reaction monitoring (MRM) or quantification by dimethyl or other isotope labels can be employed.\\n\\nIn conclusion, the isolation of mitochondria from plant cells is a delicate process that requires careful consideration of the specific requirements of the plant species and tissue type. Recent advancements have made the process more effective and accessible for a range of tissue types and species, allowing for a broader application of mitochondrial studies across different plant species.\\n\\nReferences:\\n- Plant Methods. (2015). https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0099-x\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908444/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7640673/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687074/Extraction and Categorization:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, used for, protein and tRNA uptake experiments)\\n30. (mitochondria, used for, enzyme activity assays)\\n31. (mitochondria, used for, western blot analyses)\\n32. (mitochondria, used for, mass spectrometry analyses)\\n33. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n34. (mitochondria, assessed for, purity and integrity)\\n35. (mitochondria, assessed using, proteinase digestion assays)\\n36. (mitochondria, assessed using, electron microscopy)\\n37. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n38. (mitochondria, assessed using, electron transport chain activity measurement)\\n39. (mitochondria, isolated from, Arabidopsis thaliana)\\n40. (mitochondria, isolated using, continuous colloidal density gradients)\\n41. (mitochondria, isolated at, 4 °C)\"\n",
        "\n",
        "# Clean the entity relationships\n",
        "cleaned_entity_relationships = clean_entity_relationships(entity_relationships_text)\n",
        "\n",
        "# Output the cleaned entity relationships\n",
        "cleaned_entity_relationships\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.7.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.7.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.pdf\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "References extracted successfully to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.txt\n",
            "Questions: [('Content-Based Question', 'How does AutoGen2 enable the creation of Next-Gen LLM applications?'), ('Analytical Question', 'What are the key features of AutoGen2 that make it effective in various domains?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where AutoGen2 is used in an online decision-making application. How can the framework facilitate conversations between agents and humans in this context?'), ('Contextual/Relational Question', 'How does AutoGen2 leverage GPT-4 to incorporate feedback and enhance conversations between agents and humans?'), ('User-Interactive Question', 'As a developer, how can you customize and build reusable agents using the AutoGen2 framework?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does AutoGen simplify the development of complex applications through conversation programming?'), ('Analytical Question', 'What are the key benefits of using AutoGen in terms of streamlining and consolidating multi-agent workflows?'), ('Creative/Scenario-Based Question', 'Imagine you are a developer using AutoGen. How would you customize and configure an agent to display complex behavior in a multi-agent conversation?'), ('Contextual/Relational Question', 'How does AutoGen support the interaction between conversable agents and various entities such as LLMs, humans, and tools?'), ('User-Interactive Question', 'As a user of AutoGen, how would you leverage the different capabilities of agents to enhance the effectiveness of multi-agent conversations?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Questions: [('Content-Based Question', 'How do the AssistantAgent and UserProxyAgent work together in a typical scenario?'), ('Analytical Question', 'What are the key features of AutoGen that ensure a natural conversation flow in multi-agent conversations?'), ('Creative/Scenario-Based Question', 'Imagine you are using the AssistantAgent and UserProxyAgent to solve a complex task. How would you customize the LLM-backed assistant agent to enable conversation-driven control flow?'), ('Contextual/Relational Question', 'How does conversation programming in AutoGen involve control flow and conversation-centric computation?'), ('User-Interactive Question', 'In what ways can the UserProxyAgent initiate and control conversations in multi-agent settings?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Questions: [('Content-Based Question', 'How does AutoGen enable control through the fusion of programming and natural language?'), ('Analytical Question', 'What are the potential benefits of using AutoGen in the development of high-performance multi-agent systems?'), ('Creative/Scenario-Based Question', \"Imagine you are a developer working on a math problem-solving application. How could you leverage AutoGen's features to improve the performance and flexibility of your system?\"), ('Contextual/Relational Question', 'How does AutoGen compare to other approaches, such as Multi-Agent Debate, LangChain Re-Act, GPT-4, ChatGPT + Code Interpreter, and ChatGPT + Plugin, in terms of solving math problems on the MATH dataset?'), ('User-Interactive Question', \"As a user, how would you feel about interacting with AutoGen's built-in agents, such as AssistantAgent and GroupChatManager, while solving math problems? How do you think these agents contribute to the overall problem-solving experience?\")]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does AutoGen incorporate human feedback into its problem-solving process?'), ('Analytical Question', \"What are the advantages of using AutoGen's Retrieval-Augmented Generation (RAG) system compared to traditional language models?\"), ('Creative/Scenario-Based Question', 'Imagine you are a user participating in the problem-solving process with AutoGen. How would you provide your input to the system?'), ('Contextual/Relational Question', \"How does AutoGen's interactive retrieval feature enhance the retrieval process compared to traditional methods?\"), ('User-Interactive Question', \"If you were to use AutoGen to generate code based on a codebase not included in GPT-4's training data, how would you expect the system to perform?\")]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', \"How does the integration of Re-Act prompting enhance the system's ability to leverage common sense knowledge?\"), ('Analytical Question', 'What are the key benefits of introducing a grounding agent in the system?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where the system does not have a grounding agent. How might this impact its performance and ability to mitigate errors?'), ('Contextual/Relational Question', 'How does AutoGen contribute to reducing code length and improving productivity in the multi-agent coding system?'), ('User-Interactive Question', 'In what ways can the dynamic group chat feature supported by AutoGen facilitate flexible collaboration among agents?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does utilizing role-play prompts in complex tasks contribute to the consideration of conversation context and role alignment?'), ('Analytical Question', 'What are the advantages of using the Conversational Chess game developed with AutoGen in terms of success rates and LLM calls reduction?'), ('Creative/Scenario-Based Question', 'Imagine you are a game developer. How would you utilize the modular design of AutoGen to create a unique and engaging multi-agent game?'), ('Contextual/Relational Question', 'How does the AutoGen framework contribute to the improvement of performance, reduction of development code, and decrease of manual burden in creating multi-agent systems?'), ('User-Interactive Question', 'As a user, what privacy and data protection measures would you expect to be in place when using the AutoGen framework?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How can biases in LLMs be identified and addressed to promote fairness and inclusivity in the AutoGen framework?'), ('Analytical Question', 'What are the key mechanisms that can be implemented to ensure accountability and transparency in multi-agent conversations?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where a user interacts with an AI system that consistently provides biased responses. How can trust and reliance on AI systems be restored in such situations?'), ('Contextual/Relational Question', 'How does the impact of human-AI interaction on user experience vary across different contexts, and how can this impact be effectively managed?'), ('User-Interactive Question', 'In your opinion, what are some potential unintended consequences that could arise from the use of multi-agent conversations and automation, and how can these risks be mitigated?')]\n",
            "Storing data for file_id: 2308.08155\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4116 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting dirty knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4116 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.pdf: Error in extracting knowledge\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.08155.pdf\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.pdf\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "References extracted successfully to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.txt\n",
            "Questions: [('Content-Based Question', 'How does MetaGPT enhance collaboration in multi-agent systems and what role do Standardized Operating Procedures (SOPs) play in this framework?'), ('Analytical Question', 'What are the key differences between MetaGPT and previous chat-based multi-agent systems in terms of generating coherent solutions?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where MetaGPT is used in a software development team. How can the framework help in breaking down complex tasks into subtasks and assigning roles to agents?'), ('Contextual/Relational Question', 'How does MetaGPT address the issues of logic inconsistencies and cascading hallucinations that can be present in large language models (LLMs)?'), ('User-Interactive Question', 'How can MetaGPT be applied in real-world scenarios, such as Product Managers creating Product Requirements Documents (PRDs)? Can you think of any other potential applications for this framework?')]\n",
            "Storing data for file_id: 2308.00352\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does MetaGPT enhance collaboration and code generation quality in multi-agent systems?'), ('Analytical Question', 'What are the key features of MetaGPT that contribute to its state-of-the-art performance?'), ('Creative/Scenario-Based Question', \"Imagine you are working on a complex software project. How could MetaGPT's role definition, message sharing, and human-like SOPs improve the efficiency and effectiveness of your team's collaboration?\"), ('Contextual/Relational Question', 'How does MetaGPT compare to other frameworks in terms of its potential for handling complex software projects?'), ('User-Interactive Question', \"Have you ever worked on a multi-agent system? How do you think MetaGPT's capabilities could benefit your current or future projects?\")]\n",
            "Storing data for file_id: 2308.00352\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How do agents in a software company use a shared message pool for publishing and subscribing to structured messages?'), ('Analytical Question', 'What role does the Engineer agent play in the software development process, and how does it compare past messages with the PRD, system design, and code files?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where an error occurs in the initial code. How can the agent use executable feedback to enhance the quality of code generation during runtime?'), ('Contextual/Relational Question', 'How do the specialized roles in a software company, such as Product Manager, Architect, Project Manager, Engineer, and QA Engineer, contribute to the overall software development process?'), ('User-Interactive Question', 'As an agent in a software company, how would you monitor the message pool to identify important observations and use them to trigger actions or assist in completing tasks?')]\n",
            "Storing data for file_id: 2308.00352\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "Questions: [('Content-Based Question', 'How does the subscription mechanism for information sharing enhance communication efficiency?'), ('Analytical Question', 'What are the evaluation metrics used to assess the software development tasks in the SoftwareDev dataset?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where an Engineer is struggling to pass a test after three retries. How can they improve their code iteratively in this situation?'), ('Contextual/Relational Question', 'How do domain-specific LLMs compare to general domain LLMs in terms of their performance in software development tasks?'), ('User-Interactive Question', 'As an Architect, how would you prioritize the PRDs provided by the Product Manager in the management and dissemination of information?')]\n",
            "Storing data for file_id: 2308.00352\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "Questions: [('Content-Based Question', 'How does MetaGPT compare to AutoGPT, LangChain, AgentVerse, and ChatDev in the SoftwareDev benchmark?'), ('Analytical Question', \"What are the factors that contribute to MetaGPT's high scores in the HumanEval and MBPP benchmarks?\"), ('Creative/Scenario-Based Question', \"Imagine you are a software developer. How could MetaGPT's incorporation of SOPs and different roles enhance your code revisions and executability?\"), ('Contextual/Relational Question', \"How does MetaGPT's token requirement and code generation compare to ChatDev in the SoftwareDev dataset?\"), ('User-Interactive Question', 'As a user, how do you think the executable feedback mechanism in MetaGPT could improve the quality of your code?')]\n",
            "Storing data for file_id: 2308.00352\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.json\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5200 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5200 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.pdf: Error in extracting knowledge\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2308.00352.pdf\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.pdf\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "References extracted successfully to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.txt\n",
            "Questions: [('Content-Based Question', 'How does MemGPT address the challenges posed by the limited fixed-length context windows of LLMs?'), ('Analytical Question', 'What are some key features of MemGPT that enable it to handle large documents and long conversations?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where MemGPT is used in a multi-session chat. How could the ability to remember, reflect, and evolve dynamically through long-term interactions enhance the user experience?'), ('Contextual/Relational Question', \"How does MemGPT's memory hierarchy, inspired by traditional operating systems, contribute to its ability to handle more information effectively?\"), ('User-Interactive Question', \"If you were to design a conversational agent using MemGPT, how would you leverage its virtual memory capabilities to enhance the agent's performance and user interaction?\")]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does MemGPT manage its memory to enable unbounded context?'), ('Analytical Question', 'What are the advantages and disadvantages of using a multi-level memory architecture in MemGPT?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where MemGPT is used in a real-time conversation. How could the limitation of conversation length due to the preprompt be overcome?'), ('Contextual/Relational Question', 'How does the use of recursive summarization in MemGPT impact its ability to retain memory?'), ('User-Interactive Question', 'As a user, how would you feel about MemGPT automatically managing its memory without your intervention? How could this feature enhance or hinder your experience?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Questions: [('Content-Based Question', 'How do MemGPT and LLM processors efficiently process information in conversational systems?'), ('Analytical Question', 'What are the different components of the memory hierarchy utilized by Chad and Brenda in managing memory for effective multi-session chat and document analysis?'), ('Creative/Scenario-Based Question', 'Imagine you are using a conversational system with memory management capabilities. How would you utilize the memory hierarchy to ensure efficient recall and storage of information?'), ('Contextual/Relational Question', 'How does the external context in the memory hierarchy differ from the conversational and working context, and how does it contribute to the overall memory management process?'), ('User-Interactive Question', 'As a user of a conversational system, how would you prefer the memory management to be implemented to enhance your experience and facilitate efficient information retrieval?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Questions: [('Content-Based Question', \"How might Samantha's interest in Formula 1 racing and sailing reflect her desire for adrenaline rush activities and thrill-seeking?\"), ('Analytical Question', \"How can MemGPT's memory mechanism for context management and learning enhance its ability to process events and retain knowledge?\"), ('Creative/Scenario-Based Question', 'Imagine Samantha decides to combine her love for speed and romance by participating in a Formula 1 race with her significant other. How might this unique experience unfold?'), ('Contextual/Relational Question', \"In what ways can Samantha's attraction to intense gaming sessions and romantic comedies be related to her desire for excitement and emotional connection?\"), ('User-Interactive Question', 'What other adrenaline rush activities or genres do you think Samantha might enjoy based on her interests in Formula 1 racing, sailing, and romantic comedies?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does MemGPT aim to achieve consistency and engagement in conversations?'), ('Analytical Question', \"What are the key criteria for evaluating MemGPT's performance in maintaining conversation consistency and producing engaging dialogue?\"), ('Creative/Scenario-Based Question', 'Imagine you are using MemGPT as a conversational agent. How would it leverage your long-term knowledge to personalize responses and reference prior conversations?'), ('Contextual/Relational Question', \"How does MemGPT's utilization of memory improve conversation consistency compared to fixed-context baselines?\"), ('User-Interactive Question', \"In what ways do you think MemGPT's ability to align new facts, preferences, and events with prior statements from both the user and the agent can enhance user engagement in conversations?\")]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does GPT-4 compare to GPT-3.5 in terms of accuracy and ROUGE-L scores?'), ('Analytical Question', 'What are the key factors that contribute to MemGPT outperforming GPT-4 and GPT-3.5?'), ('Creative/Scenario-Based Question', \"Imagine a conversation between MemGPT and a human. How would MemGPT's conversation openers, based on accumulated knowledge, differ from those of a human?\"), ('Contextual/Relational Question', 'How does the performance of MemGPT depend on the working context and recall storage?'), ('User-Interactive Question', \"In what ways can MemGPT's more verbose and comprehensive conversation openers enhance user engagement compared to human baselines?\")]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Questions: [('Content-Based Question', 'How does MemGPT address the limitations of limited context windows in transformer models like GPT?'), ('Analytical Question', 'What is the token limit for GPT models like the one behind ChatGPT, and why is it insufficient for lengthy documents?'), ('Creative/Scenario-Based Question', \"Imagine you are working with a legal document that has millions of tokens. How can MemGPT's flexible memory architecture be beneficial in this scenario?\"), ('Contextual/Relational Question', 'How does MemGPT outperform fixed-context baselines in the retriever-reader document QA task?'), ('User-Interactive Question', 'As a user, how would you feel about MemGPT actively retrieving documents from its archival storage instead of relying on fixed-context baselines? How might this impact your experience with the model?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Questions: [('Content-Based Question', 'How does the retriever performance impact the accuracy of GPT-3.5 and GPT-4?'), ('Analytical Question', \"What are the advantages and disadvantages of MemGPT's ability to handle larger numbers of documents compared to GPT-4?\"), ('Creative/Scenario-Based Question', 'Imagine a scenario where the retriever performance is perfect. How would this impact the overall accuracy of GPT-3.5 and GPT-4?'), ('Contextual/Relational Question', \"How does MemGPT's performance in the nested key-value retrieval task compare to that of GPT-3.5 and GPT-4?\"), ('User-Interactive Question', 'As a user, how would you prioritize retriever performance and document truncation in order to optimize accuracy in the document retrieval process?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does MemGPT address the limitations of context length in Large Language Models (LLMs)?'), ('Analytical Question', \"What are the key components of MemGPT's memory hierarchy inspired by operating systems?\"), ('Creative/Scenario-Based Question', \"Imagine a scenario where MemGPT is applied to a domain with massive contexts, such as analyzing a large dataset. How could MemGPT's memory management capabilities enhance the performance of the language model?\"), ('Contextual/Relational Question', \"How does MemGPT's hierarchical memory management and interrupts contribute to maintaining long-term memory in conversational agents?\"), ('User-Interactive Question', \"How do you think MemGPT's ability to process lengthy texts beyond context limits could impact the development of chatbots or virtual assistants?\")]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'What are the limitations of the reference implementation using OpenAI GPT-4 models for function calling?'), ('Analytical Question', 'How do the GPT-4 models differ from GPT-3.5 models in terms of making errors on the MemGPT function set?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where the Llama 2 70B model variants are fine-tuned for function calling. How might this impact the generation of function calls?'), ('Contextual/Relational Question', 'How does the reliance on proprietary closed-source models affect the overall limitations of the work on function calling?'), ('User-Interactive Question', 'As a developer, how might you address the limitations of the reference implementation using OpenAI GPT-4 models for function calling?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Questions: [('Content-Based Question', 'How do generative agents, as discussed in the paper, contribute to character design and open-domain conversation?'), ('Analytical Question', 'What are the potential applications of generative agents, and how do they simulate human behavior?'), ('Creative/Scenario-Based Question', 'Imagine a world where generative agents have advanced to the point where they are indistinguishable from humans. How would this impact various industries, such as customer service or entertainment?'), ('Contextual/Relational Question', 'How does the concept of interleaving retrieval with chain-of-thought reasoning, as explored in the paper, enhance knowledge-intensive multi-step questions?'), ('User-Interactive Question', 'How would you judge the effectiveness of a language model that combines reasoning and acting in reacting to prompts? What criteria would you use to evaluate its performance?')]\n",
            "Storing data for file_id: 2310.08560\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.json\n",
            "Successfully summarized /home/epas/Programming/ResearchAgentSwarm/testPDFs/2310.08560.pdf\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2309.17288.pdf\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2309.17288.json\n",
            "References extracted successfully to /home/epas/Programming/ResearchAgentSwarm/testPDFs/2309.17288.txt\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4543 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4543 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2309.17288.pdf: Error in extracting knowledge\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/testPDFs/2309.17288.pdf\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0613\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0613\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    while True:\n",
        "        topic = request_topics(summary)\n",
        "        if topic:\n",
        "            break\n",
        "    while True:\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "        if questions:\n",
        "            break\n",
        "    return topic, questions\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        extract_references_from_pdf(file_path, references_path)\n",
        "        with open(references_path) as f:\n",
        "            references = f.read()\n",
        "    else:\n",
        "        references = extract_references(file_path)  \n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        article_text = pdf_to_text(file_path)\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic, \n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/testPDFs/\"\n",
        "article_list = get_article_list(filetype=\"pdf\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open summary.json to see the results \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ChatGPT Message Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def analyze_file(file_path, output_file_path, document_name):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().split('\\n')\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            current_message = \"\"\n",
        "            message_started = False\n",
        "            sender = \"\"\n",
        "            message_number = 0\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line.lower().startswith('user') or line.lower().startswith('chatgpt'):\n",
        "                    if message_started:  # End of a message\n",
        "                        message_number += 1\n",
        "                        word_count = count_words(current_message.strip())\n",
        "                        output_file.write(f\"{sender} Line number {i}, Message number {message_number}, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "                        current_message = \"\"\n",
        "                    message_started = True\n",
        "                    sender = \"User\" if line.lower().startswith('user') else \"ChatGPT\"\n",
        "                    continue\n",
        "                if message_started:\n",
        "                    current_message += \" \" + line\n",
        "\n",
        "            # Add the last message if it exists\n",
        "            if current_message:\n",
        "                message_number += 1\n",
        "                word_count = count_words(current_message.strip())\n",
        "                output_file.write(f\"{sender} Last message, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your text file\n",
        "# Replace 'output_messages.txt' with the path for the output file\n",
        "# Add the document name (e.g., 'ChatGPT_history.txt')\n",
        "file_path = 'ChatGPT_history.txt'\n",
        "output_file_path = 'output_messages.txt'\n",
        "document_name = 'ChatGPT_history'  # This is the document name without the extension\n",
        "analyze_file(file_path, output_file_path, document_name)\n",
        "print(\"Messages have been written to the output file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class MessageAnalysisStore:\n",
        "    def __init__(self, output_file_path):\n",
        "        self.output_file_path = output_file_path\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.output_file_path):\n",
        "            empty_data = []\n",
        "            self._save(empty_data)\n",
        "\n",
        "    def store(self, analyzed_data):\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(analyzed_data)\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.output_file_path):\n",
        "            with open(self.output_file_path, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.output_file_path, \"w\") as file:\n",
        "                json.dump(content, file, indent=4)\n",
        "            print(f\"Successfully saved data to {self.output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.output_file_path}: {e}\")\n",
        "\n",
        "# Assuming the required classes and functions from your new code are already defined and imported\n",
        "# like SummaryStore, generate_summary, get_entities, extract_knowledge, etc.\n",
        "\n",
        "def extract_messages_with_citation(lines: List[str], sender_keyword: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts messages with citation from the given lines based on the sender keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    messages_with_citation = []\n",
        "    current_message = \"\"\n",
        "    message_started = False\n",
        "    citation_info = \"\"\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(sender_keyword):\n",
        "            if message_started:\n",
        "                # End of the current message, add it to the list\n",
        "                messages_with_citation.append((current_message.strip(), citation_info))\n",
        "                current_message = \"\"\n",
        "            message_started = True\n",
        "            citation_info = line  # Capture the line with sender info as citation\n",
        "        elif message_started:\n",
        "            current_message += \" \" + line\n",
        "\n",
        "    # Add the last message if it exists\n",
        "    if current_message:\n",
        "        messages_with_citation.append((current_message.strip(), citation_info))\n",
        "\n",
        "    return messages_with_citation\n",
        "\n",
        "def analyze_conversation(message: str, citation: str, sender_keyword: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyzes a single conversation message, extracting and summarizing information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate an initial summary\n",
        "        if sender_keyword == \"ChatGPT\":\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"Entity Dense\")\n",
        "        else:\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"SPR\")\n",
        "\n",
        "        # Extract entities and knowledge\n",
        "        entities = get_entities(message)\n",
        "        knowledge = extract_knowledge(message, entities[\"clean_entities\"], entities[\"dirty_entities\"])\n",
        "\n",
        "        # Extract the topic and hypothetical questions from the summary\n",
        "        topic, questions = extract_info(initial_summary)\n",
        "\n",
        "        analyzed_data = {\n",
        "            \"id\": citation,\n",
        "            \"sender\": sender_keyword,\n",
        "            \"message\": message,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": questions,\n",
        "            \"clean_entities\": entities[\"clean_entities\"],\n",
        "            \"dirty_entities\": entities[\"dirty_entities\"],\n",
        "            \"summary\": initial_summary,\n",
        "            \"knowledge\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        return analyzed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred in analyzing conversation: {e}\")\n",
        "\n",
        "def extract_and_analyze_messages(file_path: str, output_file_path: str, sender_keyword: str, log_file_path: str):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    messages_with_citation = extract_messages_with_citation(lines, sender_keyword)\n",
        "    store = MessageAnalysisStore(output_file_path)\n",
        "    error_log = []\n",
        "\n",
        "    for message, citation in messages_with_citation:\n",
        "        try:\n",
        "            analyzed_data = analyze_conversation(message, citation, sender_keyword)\n",
        "            store.store(analyzed_data)\n",
        "            time.sleep(15)  # Delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            error_info = {\"citation\": citation, \"error\": str(e), \"timestamp\": datetime.now().isoformat()}\n",
        "            # Appending to the error log\n",
        "            error_log.append(error_info)\n",
        "            with open(log_file_path, \"a\") as log_file:\n",
        "                json.dump(error_info, log_file, indent=4)\n",
        "                log_file.write(\"\\n\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Messages analysis completed. Data saved to {output_file_path}\")\n",
        "    if error_log:\n",
        "        print(f\"Errors logged to {log_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'output_messages.txt'\n",
        "output_file_path_user = 'analyzed_user_messages.json'\n",
        "log_file_path_user = 'error_log_user.json'\n",
        "extract_and_analyze_messages(file_path, output_file_path_user, 'user', log_file_path_user)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install neo4j\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"neo4j+s://d0cccc82.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"pkHFLmEdqhftN2n5BcR362MTrG4RomuLgnkp3GR7yEQ\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7617/1621462424.py:131: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session() as session:\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 0.9157391045379122s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.9635494431935174s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 4.010955571213135s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:root:Failed to process file: /home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/6829ccbe621c4f47a47e02bb959c7136.json. Error: {code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.0747247988968547s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:neo4j.io:Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n"
          ]
        },
        {
          "ename": "TransientError",
          "evalue": "{code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 135\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    134\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, file_name)\n\u001b[0;32m--> 135\u001b[0m             \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
            "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(file_path, session)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcypher_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:757\u001b[0m, in \u001b[0;36mSession.execute_write\u001b[0;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute a unit of work in a managed write transaction.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m.. versionadded:: 5.0\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWRITE_ACCESS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTX_FUNC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransaction_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:552\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36mprocess_json_file.<locals>.<lambda>\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     session\u001b[38;5;241m.\u001b[39mexecute_write(\u001b[38;5;28;01mlambda\u001b[39;00m tx: \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcypher_query\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/transaction.py:169\u001b[0m, in \u001b[0;36mTransactionBase.run\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    168\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwparameters)\n\u001b[0;32m--> 169\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tx_ready_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:131\u001b[0m, in \u001b[0;36mResult._tx_ready_run\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tx_ready_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# BEGIN+RUN does not carry any extra on the RUN message.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# BEGIN {extra}\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# RUN \"query\" {parameters} {extra}\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/result.py:298\u001b[0m, in \u001b[0;36mResult._attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:846\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(tag, fields)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:72\u001b[0m, in \u001b[0;36mInbox.pop\u001b[0;34m(self, hydration_hooks)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:51\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chunk_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mpop_u16()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:326\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[0;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mused \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m--> 326\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:493\u001b[0m, in \u001b[0;36mBoltSocket.recv_into\u001b[0;34m(self, buffer, nbytes)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecv_into\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer, nbytes):\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:468\u001b[0m, in \u001b[0;36mBoltSocket._wait_for_io\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mgettimeout()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTransientError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    139\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     \u001b[43madd_jsons_to_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 131\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_jsons_to_neo4j\u001b[39m(output_folder):\n\u001b[0;32m--> 131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:124\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:207\u001b[0m, in \u001b[0;36mSession.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\u001b[38;5;241m.\u001b[39m_closed() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# roll back the transaction if it is not closed\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/transaction.py:221\u001b[0m, in \u001b[0;36mTransactionBase._rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mrollback(on_success\u001b[38;5;241m=\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_cancel()\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:863\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcomplete:\n\u001b[0;32m--> 863\u001b[0m     detail_delta, summary_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     detail_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m detail_delta\n\u001b[1;32m    865\u001b[0m     summary_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary_delta\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
            "\u001b[0;31mTransientError\u001b[0m: {code: Neo.TransientError.General.StackOverFlowError} {message: There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def aggregate_article_content(data):\n",
        "    # Aggregates content for articles with the same file_id\n",
        "    aggregated_content = {}\n",
        "    for record in data:\n",
        "        file_id = record['file_id']\n",
        "        if file_id not in aggregated_content:\n",
        "            aggregated_content[file_id] = {\n",
        "                \"content\": record['article'],\n",
        "                \"timestamp\": record.get('timestamp', '')\n",
        "            }\n",
        "        else:\n",
        "            aggregated_content[file_id]['content'] += \"\\n\" + record['article']\n",
        "    return aggregated_content\n",
        "\n",
        "def construct_cypher_query(data, aggregated_content):\n",
        "    cypher_query = \"\"\n",
        "    article_ids = set()\n",
        "    \n",
        "    for record in data:\n",
        "        file_id = record['file_id']\n",
        "        if file_id not in article_ids:\n",
        "            article_content = escape_string(aggregated_content[file_id]['content'])\n",
        "            timestamp = escape_string(aggregated_content[file_id]['timestamp'])\n",
        "\n",
        "            # Create or update the Article node\n",
        "            cypher_query += (\n",
        "                f\"MERGE (a:Article {{id: '{file_id}'}}) \"\n",
        "                f\"ON CREATE SET a.content = '{article_content}', a.timestamp = '{timestamp}' \"\n",
        "                f\"ON MATCH SET a.content = '{article_content}', a.timestamp = '{timestamp}' \"\n",
        "            )\n",
        "            article_ids.add(file_id)\n",
        "\n",
        "        # Add Summary\n",
        "        if \"summary\" in record:\n",
        "            summary_content = escape_string(record['summary'])\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (s:Summary {{content: '{summary_content}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_SUMMARY]->(s) \"\n",
        "            )\n",
        "\n",
        "        # Add Entities (both clean and dirty)\n",
        "        for entity in record.get(\"clean_entities\", []):\n",
        "            entity_name = escape_string(entity)\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (e:Entity {{name: '{entity_name}'}}) \"\n",
        "                f\"MERGE (a)-[:MENTIONS]->(e) \"\n",
        "            )\n",
        "\n",
        "        for entity in record.get(\"dirty_entities\", []):\n",
        "            entity_name = escape_string(entity)\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (de:DirtyEntity {{name: '{entity_name}'}}) \"\n",
        "                f\"MERGE (a)-[:MENTIONS_DIRTY]->(de) \"\n",
        "            )\n",
        "\n",
        "        # Add Topics with Justifications\n",
        "        for topic in record.get(\"topics\", []):\n",
        "            topic_name = escape_string(topic['topic'])\n",
        "            justification = escape_string(topic.get('justification', ''))\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (t:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_TOPIC]->(t) \"\n",
        "            )\n",
        "\n",
        "        # Add Hypothetical Questions\n",
        "        for question in record.get(\"hypothetical_questions\", []):\n",
        "            question_content = escape_string(question['question'])\n",
        "            question_type = escape_string(question.get('question_type', ''))\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (hq:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_QUESTION]->(hq) \"\n",
        "            )\n",
        "\n",
        "        # Add Knowledge Triplets\n",
        "        for triplet in record.get(\"knowledge_triplets\", []):\n",
        "            subject_name = escape_string(triplet['subject'])\n",
        "            target_name = escape_string(triplet['target'])\n",
        "            relationship = escape_string(triplet['relationship'])\n",
        "            cypher_query += (\n",
        "                f\"WITH a MERGE (subj:Subject {{name: '{subject_name}'}}) \"\n",
        "                f\"MERGE (targ:Target {{name: '{target_name}'}}) \"\n",
        "                f\"MERGE (subj)-[:RELATIONSHIP {{type: '{relationship}'}}]->(targ) \"\n",
        "            )\n",
        "\n",
        "        # Add References (URLs and Reference Text) only once per file_id\n",
        "        if file_id not in article_ids:\n",
        "            references = record.get(\"references\", {})\n",
        "            if isinstance(references, dict):\n",
        "                for url in references.get(\"urls\", []):\n",
        "                    url = escape_string(url)\n",
        "                    cypher_query += (\n",
        "                        f\"WITH a MERGE (r:Reference {{url: '{url}'}}) \"\n",
        "                        f\"MERGE (a)-[:HAS_REFERENCE]->(r) \"\n",
        "                    )\n",
        "                ref_text = references.get(\"references\", \"\")\n",
        "                if ref_text:\n",
        "                    ref_text = escape_string(ref_text)\n",
        "                    cypher_query += (\n",
        "                        f\"WITH a MERGE (rText:Reference {{text: '{ref_text}'}}) \"\n",
        "                        f\"MERGE (a)-[:HAS_REFERENCE]->(rText) \"\n",
        "                    )\n",
        "\n",
        "    # Return the complete Cypher query\n",
        "    return cypher_query\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        aggregated_content = aggregate_article_content(data)\n",
        "        cypher_query = construct_cypher_query(data, aggregated_content)\n",
        "        try:\n",
        "            session.execute_write(lambda tx: tx.run(cypher_query))\n",
        "            logging.info(f\"Successfully processed file: {file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                process_json_file(file_path, session)\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def update_article_content(session, record):\n",
        "    file_id = record['file_id']\n",
        "    article_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists and update or create accordingly\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a.content AS existing_content\")\n",
        "    existing_record = result.single()\n",
        "\n",
        "    if existing_record:\n",
        "        # Article exists, update it with new content\n",
        "        updated_content = existing_record['existing_content'] + \"\\n\" + article_content\n",
        "        update_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "            f\"SET a.content = '{updated_content}', a.timestamp = '{timestamp}'\"\n",
        "        )\n",
        "        session.run(update_query)\n",
        "    else:\n",
        "        # Article does not exist, create new\n",
        "        create_query = (\n",
        "            f\"CREATE (a:Article {{id: '{file_id}', content: '{article_content}', timestamp: '{timestamp}'}})\"\n",
        "        )\n",
        "        session.run(create_query)\n",
        "\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article content from file: {file_path}\")\n",
        "                update_article_content(session, record)\n",
        "\n",
        "                logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "                process_additional_elements(session, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                logging.info(f\"Starting processing file: {file_path}\")\n",
        "                process_json_file(file_path, session)\n",
        "                logging.info(f\"Finished processing file: {file_path}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(filename='neo4j_import.log', level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def update_article_chunk(session, record):\n",
        "    file_id = record['file_id']\n",
        "    chunk_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a.content AS existing_content\")\n",
        "    existing_record = result.single()\n",
        "\n",
        "    if existing_record:\n",
        "        # Article exists, append the chunk to the existing content\n",
        "        updated_content = existing_record['existing_content'] + \"\\n\" + chunk_content\n",
        "        update_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "            f\"SET a.content = '{updated_content}', a.timestamp = '{timestamp}'\"\n",
        "        )\n",
        "    else:\n",
        "        # Article does not exist, create new with the chunk content\n",
        "        update_query = (\n",
        "            f\"CREATE (a:Article {{id: '{file_id}', content: '{chunk_content}', timestamp: '{timestamp}'}})\"\n",
        "        )\n",
        "\n",
        "    session.run(update_query)\n",
        "\n",
        "def process_additional_elements(session, record):\n",
        "    file_id = record['file_id']\n",
        "\n",
        "    # Construct query for additional elements linked to the article\n",
        "    cypher_query = f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "\n",
        "    # Add other elements (entities, topics, questions, etc.) similar to the previous approach\n",
        "\n",
        "    # Execute the query\n",
        "    session.run(cypher_query)\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article chunk from file: {file_path}\")\n",
        "                update_article_chunk(session, record)\n",
        "\n",
        "                # Additional elements processing can be done here or after all chunks are processed\n",
        "                logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "                process_additional_elements(session, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed chunk from file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process chunk in file: {file_path}. Error: {e}\")\n",
        "\n",
        "        # Process additional elements after all chunks are processed\n",
        "        logging.info(f\"Processing additional elements from file: {file_path}\")\n",
        "        process_additional_elements(session, data[0])  # Assuming additional elements are same across chunks\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                logging.info(f\"Starting processing file: {file_path}\")\n",
        "                process_json_file(file_path, session)\n",
        "                logging.info(f\"Finished processing file: {file_path}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10555/120211380.py:128: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session() as session:\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.0987203224723447s (There is not enough stack size to perform the current task. This is generally considered to be a database error, so please contact Neo4j support. You could try increasing the stack size: for example to set the stack size to 2M, add `server.jvm.additional=-Xss2M' to in the neo4j configuration (normally in 'conf/neo4j.conf' or, if you are using Neo4j Desktop, found through the user interface) or if you are running an embedded installation just add -Xss2M as command line flag.)\n",
            "ERROR:neo4j.io:Failed to read from defunct connection IPv4Address(('d0cccc82.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.884448562985088s (Failed to read from defunct connection IPv4Address(('d0cccc82.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687))))\n",
            "ERROR:neo4j.io:Failed to read from defunct connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 4.294914787174731s (Unable to retrieve routing information)\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 9.218245426261193s (Unable to retrieve routing information)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    147\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m     \u001b[43madd_jsons_to_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Close the driver after all sessions are complete\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     driver\u001b[38;5;241m.\u001b[39mclose()\n",
            "Cell \u001b[0;32mIn[3], line 131\u001b[0m, in \u001b[0;36madd_jsons_to_neo4j\u001b[0;34m(output_folder)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(output_folder):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 131\u001b[0m         \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(file_path, session)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing article chunks from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_or_update_article_with_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:757\u001b[0m, in \u001b[0;36mSession.execute_write\u001b[0;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39mnon_concurrent_method\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_write\u001b[39m(\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs,  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    713\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _R:\n\u001b[1;32m    714\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute a unit of work in a managed write transaction.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 5.0\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWRITE_ACCESS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTX_FUNC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransaction_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/neo4j/_sync/work/session.py:582\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransaction failed and will be retried in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124ms (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(delay, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margs)))\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    584\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[#0000]  _: <SESSION> retry cancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "from neo4j import GraphDatabase\n",
        "import json\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "def create_or_update_article_with_chunks(session, record):\n",
        "    file_id = record['file_id']\n",
        "    chunk_content = escape_string(record['article'])\n",
        "    timestamp = record.get('timestamp', '')\n",
        "\n",
        "    # Check if the Article already exists\n",
        "    result = session.run(f\"MATCH (a:Article {{id: '{file_id}'}}) RETURN a\")\n",
        "    article_exists = result.single() is not None\n",
        "\n",
        "    if not article_exists:\n",
        "        # Create new Article node\n",
        "        session.run(f\"CREATE (a:Article {{id: '{file_id}', timestamp: '{timestamp}'}})\")\n",
        "\n",
        "    # Create new Chunk node and link it to the Article\n",
        "    create_chunk_query = (\n",
        "        f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "        f\"CREATE (c:Chunk {{content: '{chunk_content}'}}) \"\n",
        "        f\"CREATE (a)-[:HAS_CHUNK]->(c)\"\n",
        "    )\n",
        "    session.run(create_chunk_query)\n",
        "\n",
        "    # If there are existing chunks, link the new chunk to the last chunk\n",
        "    if article_exists:\n",
        "        link_chunks_query = (\n",
        "            f\"MATCH (a:Article {{id: '{file_id}'}})-[:HAS_CHUNK]->(lastChunk:Chunk) \"\n",
        "            f\"WHERE NOT (lastChunk)-[:NEXT_CHUNK]->() \"\n",
        "            f\"MATCH (newChunk:Chunk) \"\n",
        "            f\"WHERE newChunk.content = '{chunk_content}' \"\n",
        "            f\"CREATE (lastChunk)-[:NEXT_CHUNK]->(newChunk)\"\n",
        "        )\n",
        "        session.run(link_chunks_query)\n",
        "    \n",
        "    file_id = record['file_id']\n",
        "\n",
        "    # Start constructing the Cypher query\n",
        "    cypher_query = f\"MATCH (a:Article {{id: '{file_id}'}}) \"\n",
        "\n",
        "    # Add Summary\n",
        "    if \"summary\" in record:\n",
        "        summary_content = escape_string(record['summary'])\n",
        "        cypher_query += (\n",
        "            f\"MERGE (s:Summary {{content: '{summary_content}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_SUMMARY]->(s) \"\n",
        "        )\n",
        "\n",
        "    # Add Entities (both clean and dirty)\n",
        "    for i, entity in enumerate(record.get(\"clean_entities\", [])):\n",
        "        entity_name = escape_string(entity)\n",
        "        cypher_query += (\n",
        "            f\"MERGE (ce{i}:Entity {{name: '{entity_name}'}}) \"\n",
        "            f\"MERGE (a)-[:MENTIONS]->(ce{i}) \"\n",
        "        )\n",
        "\n",
        "    for i, entity in enumerate(record.get(\"dirty_entities\", [])):\n",
        "        entity_name = escape_string(entity)\n",
        "        cypher_query += (\n",
        "            f\"MERGE (de{i}:DirtyEntity {{name: '{entity_name}'}}) \"\n",
        "            f\"MERGE (a)-[:MENTIONS_DIRTY]->(de{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Topics with Justifications\n",
        "    for i, topic in enumerate(record.get(\"topics\", [])):\n",
        "        topic_name = escape_string(topic['topic'])\n",
        "        justification = escape_string(topic.get('justification', ''))\n",
        "        cypher_query += (\n",
        "            f\"MERGE (t{i}:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_TOPIC]->(t{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Hypothetical Questions\n",
        "    for i, question in enumerate(record.get(\"hypothetical_questions\", [])):\n",
        "        question_content = escape_string(question['question'])\n",
        "        question_type = escape_string(question.get('question_type', ''))\n",
        "        cypher_query += (\n",
        "            f\"MERGE (hq{i}:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "            f\"MERGE (a)-[:HAS_QUESTION]->(hq{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add Knowledge Triplets\n",
        "    for i, triplet in enumerate(record.get(\"knowledge_triplets\", [])):\n",
        "        subject_name = escape_string(triplet['subject'])\n",
        "        target_name = escape_string(triplet['target'])\n",
        "        relationship = escape_string(triplet['relationship'])\n",
        "        cypher_query += (\n",
        "            f\"MERGE (subj{i}:Subject {{name: '{subject_name}'}}) \"\n",
        "            f\"MERGE (targ{i}:Target {{name: '{target_name}'}}) \"\n",
        "            f\"MERGE (subj{i})-[:RELATIONSHIP {{type: '{relationship}'}}]->(targ{i}) \"\n",
        "        )\n",
        "\n",
        "    # Add References (URLs and Reference Text)\n",
        "    references = record.get(\"references\", {})\n",
        "    if isinstance(references, dict):\n",
        "        for i, url in enumerate(references.get(\"urls\", [])):\n",
        "            url = escape_string(url)\n",
        "            cypher_query += (\n",
        "                f\"MERGE (r{i}:Reference {{url: '{url}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_REFERENCE]->(r{i}) \"\n",
        "            )\n",
        "        ref_text = references.get(\"references\", \"\")\n",
        "        if ref_text:\n",
        "            ref_text = escape_string(ref_text)\n",
        "            cypher_query += (\n",
        "                f\"MERGE (rText{i}:Reference {{text: '{ref_text}'}}) \"\n",
        "                f\"MERGE (a)-[:HAS_REFERENCE]->(rText{i}) \"\n",
        "            )\n",
        "\n",
        "    # Append WITH statement to carry forward 'a' for the next set of operations\n",
        "    #cypher_query += \"RETURN a\"\n",
        "\n",
        "    # Execute the query\n",
        "    session.run(cypher_query)\n",
        "\n",
        "\n",
        "\n",
        "# Rest of the script for add_jsons_to_neo4j, process_json_file, and the main function remains the same\n",
        "\n",
        "\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    # Ensure the session is correctly opened and closed\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                process_json_file(os.path.join(output_folder, file_name), session)\n",
        "\n",
        "\n",
        "\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "        for record in data:\n",
        "            try:\n",
        "                logging.info(f\"Processing article chunks from file: {file_path}\")\n",
        "                session.execute_write(lambda tx: create_or_update_article_with_chunks(tx, record))\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "    add_jsons_to_neo4j(output_folder)\n",
        "    # Close the driver after all sessions are complete\n",
        "    driver.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
