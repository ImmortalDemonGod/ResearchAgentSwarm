{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperQA - A Question Answering Dataset for Academic Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: paper-qa in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.13.3)\n",
            "Requirement already satisfied: pypdf in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.17.4)\n",
            "Collecting pydantic<2 (from paper-qa)\n",
            "  Downloading pydantic-1.10.14-cp311-cp311-macosx_11_0_arm64.whl.metadata (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.2/150.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=0.0.303 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.1.1)\n",
            "Requirement already satisfied: openai<1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.28.0)\n",
            "Requirement already satisfied: faiss-cpu in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (1.7.4)\n",
            "Requirement already satisfied: PyCryptodome in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.19.0)\n",
            "Requirement already satisfied: html2text in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (2020.1.16)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.1.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.0.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.26.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<1->paper-qa) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<2->paper-qa) (4.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tiktoken>=0.4.0->paper-qa) (2023.10.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->paper-qa) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (2023.7.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (1.0.0)\n",
            "Downloading pydantic-1.10.14-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.5.3\n",
            "    Uninstalling pydantic-2.5.3:\n",
            "      Successfully uninstalled pydantic-2.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "instructor 0.4.7 requires openai<2.0.0,>=1.1.0, but you have openai 0.28.0 which is incompatible.\n",
            "instructor 0.4.7 requires pydantic<3.0.0,>=2.0.2, but you have pydantic 1.10.14 which is incompatible.\n",
            "gradio 4.12.0 requires pydantic>=2.0, but you have pydantic 1.10.14 which is incompatible.\n",
            "agency-swarm 0.1.0 requires instructor==0.3.4, but you have instructor 0.4.7 which is incompatible.\n",
            "agency-swarm 0.1.0 requires openai==1.3.0, but you have openai 0.28.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires certifi==2023.11.17, but you have certifi 2023.7.22 which is incompatible.\n",
            "aider-chat 0.19.1 requires charset-normalizer==3.3.2, but you have charset-normalizer 3.2.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires idna==3.6, but you have idna 3.4 which is incompatible.\n",
            "aider-chat 0.19.1 requires networkx==3.2.1, but you have networkx 2.8.8 which is incompatible.\n",
            "aider-chat 0.19.1 requires numpy==1.26.2, but you have numpy 1.26.3 which is incompatible.\n",
            "aider-chat 0.19.1 requires openai==1.3.7, but you have openai 0.28.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic==2.5.2, but you have pydantic 1.10.14 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic-core==2.14.5, but you have pydantic-core 2.14.6 which is incompatible.\n",
            "aider-chat 0.19.1 requires tiktoken==0.5.2, but you have tiktoken 0.5.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires urllib3==2.1.0, but you have urllib3 1.26.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.10.14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (0.1.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (0.0.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (1.26.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain==0.1.1) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.1) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain==0.1.1) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.1) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.1) (2023.7.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (4.36.2)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: torchvision in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.16.2)\n",
            "Requirement already satisfied: numpy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: scikit-learn in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: filelock in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
            "Requirement already satisfied: requests in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /Users/tomriddle1/.cache/torch/sentence_transformers/WhereIsAI_UAE-Large-V1. Creating a new one with MEAN pooling.\n",
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m     write_responses(responses, responses_file_path)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mprocess_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     process_research_papers(output_folder_pdf)\n\u001b[1;32m     82\u001b[0m     questions \u001b[38;5;241m=\u001b[39m read_questions(questions_file_path)\n",
            "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mprocess_json_files\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Check if file_id and citation are not empty\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id:\n\u001b[0;32m---> 56\u001b[0m         \u001b[43mdocs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdockey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped empty or invalid JSON file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperqa/docs.py:234\u001b[0m, in \u001b[0;36mDocs.add\u001b[0;34m(self, path, citation, docname, disable_check, dockey, chunk_chars)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m disable_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maybe_is_text(texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m    230\u001b[0m ):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis does not look like a text document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Path disable_check to ignore this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docname\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperqa/docs.py:257\u001b[0m, in \u001b[0;36mDocs.add_texts\u001b[0;34m(self, texts, doc)\u001b[0m\n\u001b[1;32m    255\u001b[0m     doc\u001b[38;5;241m.\u001b[39mdocname \u001b[38;5;241m=\u001b[39m new_docname\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m    259\u001b[0m         t\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m text_embeddings[i]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:91\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     89\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "!pip install paper-qa\n",
        "#!pip install openai==1.7.2\n",
        "!pip install openai==0.28\n",
        "!pip install langchain==0.1.1\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "!pip install sentence-transformers\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "#model_name = \"ggrn/e5-small-v2\" # fast\n",
        "model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "# Configuration\n",
        "#output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "output_folder_json = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\" \n",
        "output_folder_pdf = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_pdf/\" \n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, embeddings=embeddings)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    process_json_files(output_folder_json)\n",
        "    process_research_papers(output_folder_pdf)\n",
        "    questions = read_questions(questions_file_path)\n",
        "    responses = [docs.query(question) for question in questions]\n",
        "    write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: openai 0.28.0\n",
            "Uninstalling openai-0.28.0:\n",
            "  Successfully uninstalled openai-0.28.0\n",
            "\u001b[33mWARNING: Skipping paperqa as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openai==0.28\n",
            "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "agency-swarm 0.1.0 requires instructor==0.3.4, but you have instructor 0.4.5 which is incompatible.\n",
            "agency-swarm 0.1.0 requires openai==1.3.0, but you have openai 0.28.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires certifi==2023.11.17, but you have certifi 2023.7.22 which is incompatible.\n",
            "aider-chat 0.19.1 requires charset-normalizer==3.3.2, but you have charset-normalizer 3.2.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires idna==3.6, but you have idna 3.4 which is incompatible.\n",
            "aider-chat 0.19.1 requires numpy==1.26.2, but you have numpy 1.26.3 which is incompatible.\n",
            "aider-chat 0.19.1 requires openai==1.3.7, but you have openai 0.28.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic==2.5.2, but you have pydantic 1.10.13 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic-core==2.14.5, but you have pydantic-core 2.14.6 which is incompatible.\n",
            "aider-chat 0.19.1 requires tiktoken==0.5.2, but you have tiktoken 0.5.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires urllib3==2.1.0, but you have urllib3 1.26.16 which is incompatible.\n",
            "instructor 0.4.5 requires openai<2.0.0,>=1.1.0, but you have openai 0.28.0 which is incompatible.\n",
            "instructor 0.4.5 requires pydantic<3.0.0,>=2.0.2, but you have pydantic 1.10.13 which is incompatible.\n",
            "fleet-context 1.1.9 requires openai>1.1.0, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall openai -y\n",
        "!pip uninstall paperqa -y\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed responses are saved in JSON format to structured_responses.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "def parse_qa_responses(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    responses = []\n",
        "    response = {}\n",
        "    references_lines = []\n",
        "    capturing_references = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('Question:'):\n",
        "            if response:  # Add the previous response with its references to the list\n",
        "                response['references'] = ''.join(references_lines).strip()\n",
        "                responses.append(response)\n",
        "                references_lines = []\n",
        "            # Handling split operation\n",
        "            split_line = line.split('    ')\n",
        "            if len(split_line) >= 2:\n",
        "                response = {'question': split_line[1].strip(), 'answer': '', 'references': ''}\n",
        "            else:\n",
        "                response = {'question': '', 'answer': '', 'references': ''}\n",
        "            capturing_references = False\n",
        "        elif 'I cannot answer' in line.strip() or 'The provided context does not contain' in line.strip():\n",
        "            response['answer'] = line.strip()\n",
        "        elif line.strip().startswith('References'):\n",
        "            capturing_references = True\n",
        "        elif capturing_references:\n",
        "            references_lines.append(line)\n",
        "    \n",
        "    if response:  # Add the last response with its references to the list\n",
        "        response['references'] = ''.join(references_lines).strip()\n",
        "        responses.append(response)\n",
        "\n",
        "    # Add timestamp and extract URLs from references\n",
        "    for response in responses:\n",
        "        response[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if response['references']:\n",
        "            response['references_urls'] = extract_urls(response['references'])\n",
        "    return responses\n",
        "\n",
        "\n",
        "# You would then continue with your original code for saving the JSON.\n",
        "\n",
        "\n",
        "def save_json_append(responses, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    combined_data = existing_data + responses\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "file_path =\"responses_file.txt\"\n",
        "output_json_file = 'structured_responses.json'\n",
        "\n",
        "responses = parse_qa_responses(file_path)\n",
        "save_json_append(responses, output_json_file)\n",
        "\n",
        "print(f\"Processed responses are saved in JSON format to {output_json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "laptop= \"/home/epas/Documents/docs.pickle\"\n",
        "desktop = \"/Users/tomriddle1/Documents/GitHub/docs.pickle\"\n",
        "# save\n",
        "with open(desktop, \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(desktop, \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Generate Search Queries\":\n",
        "        with open(\"generate_search_queries.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting instructor\n",
            "  Using cached instructor-0.4.7-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (1.8.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.17.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.6)\n",
            "Requirement already satisfied: certifi in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Using cached instructor-0.4.7-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: instructor\n",
            "Successfully installed instructor-0.4.7\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'validate_call' from 'pydantic' (/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages/pydantic/__init__.cpython-311-darwin.so)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages/instructor/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FinetuneFormat, Instructions\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     CitationMixin,\n\u001b[1;32m      4\u001b[0m     Maybe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     openai_moderation,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAISchema, openai_schema, Mode\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages/instructor/distil.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, List, Optional\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, validate_call\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openai_schema\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'validate_call' from 'pydantic' (/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages/pydantic/__init__.cpython-311-darwin.so)"
          ]
        }
      ],
      "source": [
        "#!pip install pydantic==2.0.3\n",
        "!pip install instructor\n",
        "#\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Question: \"What is H-bond donation in molecular structures?\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Definition and role of thiolate ligands in chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Overview of metal-thiolate complex formation and properties\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introduction to density functional theory (DFT) in chemical analysis\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Applications of H-bond donation in metal-thiolate complexes\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Understanding the discontinuum between thiolate and thiol ligands\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the difference between thiolate and thiol ligands?\n",
            "2. What are the properties and characteristics of thiolate ligands?\n",
            "3. How do thiolate and thiol ligands interact with metal complexes?\n",
            "4. Can thiolate and thiol ligands be used interchangeably in coordination chemistry?\n",
            "5. What are the advantages and disadvantages of using thiolate ligands compared to thiol ligands?\n",
            "6. Are there any specific examples or case studies where the difference between thiolate and thiol ligands is significant?\n",
            "7. How does the coordination chemistry of thiolate and thiol ligands affect their biological activity?\n",
            "8. Are there any known reactions or transformations that are specific to thiolate or thiol ligands?\n",
            "9. What is the historical background and development of thiolate and thiol ligands in coordination chemistry?\n",
            "10. Are there any ongoing research projects or recent advancements related to understanding the discontinuum between thiolate and thiol ligands?\n",
            "\n",
            "Question: \"Examples of structure reports on novel synthesized compounds\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Academic papers on hydrogen bond donation to thiolate ligands\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Hydrogen bond donation to thiolate ligands in academic papers\",\n",
            "  \"Research articles on hydrogen bond donation to thiolate ligands\",\n",
            "  \"Studies on the interaction between hydrogen bonds and thiolate ligands\",\n",
            "  \"Hydrogen bonding to thiolate ligands: a literature review\",\n",
            "  \"Investigating the role of hydrogen bond donation in thiolate ligand complexes\",\n",
            "  \"Hydrogen bonding interactions with thiolate ligands: recent advancements\",\n",
            "  \"Exploring the mechanism of hydrogen bond donation to thiolate ligands\",\n",
            "  \"Hydrogen bond stability in thiolate ligand systems: an overview\",\n",
            "  \"Hydrogen bond strength in thiolate ligand complexes: a comparative study\",\n",
            "  \"Analyzing the effect of hydrogen bond donation on thiolate ligand reactivity\"\n",
            "]\n",
            "\n",
            "Question: \"Impact of protonation on the electronic structure of metal-thiolate complexes\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Case studies using density functional theory in molecular structure analysis\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Techniques in single-crystal X-ray crystallography for molecular structure determination\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Comparing the effects of H-bond donation and protonation in metal complexes\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Guidelines for writing a structure report in chemistry\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Writing a structure report in chemistry step by step\",\n",
            "  \"Key elements to include in a structure report in chemistry\",\n",
            "  \"Examples of successful structure reports in chemistry\",\n",
            "  \"Tips for organizing and presenting data in a structure report in chemistry\",\n",
            "  \"Best practices for interpreting and analyzing results in a structure report in chemistry\",\n",
            "  \"Common mistakes to avoid in writing a structure report in chemistry\",\n",
            "  \"How to write an effective abstract for a structure report in chemistry\",\n",
            "  \"Impact of structure reports on scientific research in chemistry\",\n",
            "  \"Comparison of different structure report formats in chemistry\",\n",
            "  \"Importance of proper referencing in a structure report in chemistry\",\n",
            "  \"Innovative techniques for visualizing molecular structures in a structure report in chemistry\",\n",
            "  \"Evaluation criteria for assessing the quality of a structure report in chemistry\"\n",
            "]\n",
            "\n",
            "Question: \"Sources explaining the electronic structure of metal-thiolate complexes\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Analyzing the structure of academic papers in chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Resources for understanding spectroscopy and DFT calculations in chemical research\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Introduction to spectroscopy\",\n",
            "  \"Principles of spectroscopy\",\n",
            "  \"Types of spectroscopy techniques\",\n",
            "  \"How does spectroscopy work?\",\n",
            "  \"Applications of spectroscopy in chemical research\",\n",
            "  \"Introduction to DFT calculations\",\n",
            "  \"Principles of DFT calculations\",\n",
            "  \"How do DFT calculations work?\",\n",
            "  \"Advantages and limitations of DFT calculations\",\n",
            "  \"Applications of DFT calculations in chemical research\",\n",
            "  \"Comparison between different spectroscopy techniques\",\n",
            "  \"Comparison between experimental and theoretical spectroscopy techniques\",\n",
            "  \"Challenges in spectroscopy and DFT calculations in chemical research\",\n",
            "  \"Recent advancements in spectroscopy and DFT calculations\",\n",
            "  \"Resources for learning spectroscopy and DFT calculations in chemical research\"\n",
            "]\n",
            "\n",
            "Question: \"Exploring the role of acid-base equilibrium in metal-ligand interactions\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the relationship between acid-base equilibrium and metal-ligand interactions?\n",
            "2. How does acid-base equilibrium affect the formation of metal-ligand complexes?\n",
            "3. What are the different types of acids and bases involved in metal-ligand interactions?\n",
            "4. How do pH and pKa values influence metal-ligand interactions?\n",
            "5. Can acid-base reactions be used to control the stability of metal-ligand complexes?\n",
            "6. Are there any specific acid-base theories or models that explain metal-ligand interactions?\n",
            "7. What experimental techniques are commonly used to study acid-base equilibrium in metal-ligand interactions?\n",
            "8. Are there any examples of metal-ligand systems where acid-base interactions play a significant role?\n",
            "9. How do different ligands affect the acid-base properties of metal complexes?\n",
            "10. What is the role of acid-base equilibrium in the redox behavior of metal-ligand complexes?\n",
            "\n",
            "Question: \"How to assess donor properties of thiolate ligands in chemical complexes\"\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Historical development of research on metal-thiolate complexes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Historical timeline of research on metal-thiolate complexes\"\n",
            "2. \"Key discoveries in the research on metal-thiolate complexes\"\n",
            "3. \"Pioneering scientists in the field of metal-thiolate complex research\"\n",
            "4. \"Evolution of metal-thiolate complex studies over time\"\n",
            "5. \"Historical milestones in the understanding of metal-thiolate complexes\"\n",
            "6. \"Influential research papers on metal-thiolate complex studies\"\n",
            "7. \"How has research on metal-thiolate complexes evolved since its inception?\"\n",
            "8. \"Historical advancements in the synthesis and characterization of metal-thiolate complexes\"\n",
            "9. \"Impact of early research on metal-thiolate complexes on current scientific understanding\"\n",
            "10. \"Overview of the development of metal-thiolate complex research in the last decade\"\n",
            "\n",
            "Question: \"Comparative studies of thiolate and thiol ligands in different compounds\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What are the differences between thiolate and thiol ligands in various compounds?\n",
            "2. What are some compounds that contain thiolate ligands and thiol ligands?\n",
            "3. How do thiolate and thiol ligands affect the properties and reactivity of different compounds?\n",
            "4. Are there any studies comparing the stability and coordination behavior of thiolate and thiol ligands in different compounds?\n",
            "5. Are there any known cases where the use of thiolate ligands is favored over thiol ligands, or vice versa, in different compounds?\n",
            "6. What are the advantages and disadvantages of using thiolate ligands compared to thiol ligands in different compounds?\n",
            "7. Can you provide examples of applications where thiolate and thiol ligands are used differently in various compounds?\n",
            "8. What are the main factors influencing the choice between thiolate and thiol ligands in the synthesis of different compounds?\n",
            "9. How do the electronic and steric properties of thiolate and thiol ligands vary in different compounds?\n",
            "10. Are there any studies investigating the role of thiolate and thiol ligands in catalysis of different compounds?\n",
            "\n",
            "Question: \"Advanced spectroscopic methods used in molecular structure analysis\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Role of computational chemistry in modern chemical research\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Case studies on the synthesis of novel compounds in inorganic chemistry\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Case studies on the synthesis of novel compounds in inorganic chemistry\",\n",
            "  \"Examples of successful synthesis of novel compounds in inorganic chemistry\",\n",
            "  \"Methods for synthesizing novel compounds in inorganic chemistry\",\n",
            "  \"Strategies for designing and synthesizing new compounds in inorganic chemistry\",\n",
            "  \"Techniques used in the synthesis of novel compounds in inorganic chemistry\",\n",
            "  \"Challenges and solutions in synthesizing novel inorganic compounds\",\n",
            "  \"Innovative approaches to synthesizing new compounds in inorganic chemistry\",\n",
            "  \"Recent advances in the synthesis of novel compounds in inorganic chemistry\",\n",
            "  \"Key publications on the synthesis of novel compounds in inorganic chemistry\",\n",
            "  \"Reviews of case studies on the synthesis of novel compounds in inorganic chemistry\"\n",
            "]\n",
            "\n",
            "Question: \"Review articles on the latest advancements in thiolate chemistry\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "   \"Latest advancements in thiolate chemistry\",\n",
            "   \"Recent developments in thiolate chemistry\",\n",
            "   \"New research in thiolate chemistry\",\n",
            "   \"Advances in thiolate chemistry\",\n",
            "   \"Cutting-edge discoveries in thiolate chemistry\",\n",
            "   \"Breakthroughs in thiolate chemistry\",\n",
            "   \"Emerging trends in thiolate chemistry\",\n",
            "   \"State-of-the-art in thiolate chemistry\",\n",
            "   \"Thiolate chemistry review articles\",\n",
            "   \"Thiolate chemistry literature survey\"\n",
            "]\n",
            "\n",
            "Question: \"Impact of molecular structure analysis on pharmaceutical applications\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Environmental implications of synthesizing metal-thiolate complexes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What are the environmental impacts of synthesizing metal-thiolate complexes?\n",
            "2. How does the synthesis of metal-thiolate complexes affect the environment?\n",
            "3. What are the ecological consequences of producing metal-thiolate complexes?\n",
            "4. Are there any negative effects on the environment due to the synthesis of metal-thiolate complexes?\n",
            "5. How does the production of metal-thiolate complexes contribute to environmental pollution?\n",
            "6. What are the environmental risks associated with the synthesis of metal-thiolate complexes?\n",
            "7. Are there any studies on the environmental implications of manufacturing metal-thiolate complexes?\n",
            "8. How do metal-thiolate complexes influence local ecosystems?\n",
            "9. What are the long-term effects of metal-thiolate complex synthesis on environmental health?\n",
            "10. Are there any regulations in place to mitigate the environmental impact of metal-thiolate complex production?\n",
            "\n",
            "Question: \"Laboratory safety protocols for handling thiolate compounds\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Statistical methods in analyzing chemical structure data\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Interdisciplinary approaches in studying metal-ligand interactions\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Ethical considerations in chemical research and publication\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the ethical considerations in chemical research?\",\n",
            "  \"What are the ethical concerns in chemical publication?\",\n",
            "  \"How is ethical responsibility addressed in chemical research?\",\n",
            "  \"What ethical guidelines should be followed in chemical publication?\",\n",
            "  \"What are the potential ethical issues in chemical experimentation?\",\n",
            "  \"How do researchers ensure ethical practices in chemical research?\",\n",
            "  \"Are there any specific ethical codes for chemical research?\",\n",
            "  \"What is the role of an ethics committee in chemical publications?\",\n",
            "  \"How are conflicts of interest managed in chemical research?\",\n",
            "  \"What are the consequences of unethical behavior in chemical research?\"\n",
            "]\n",
            "\n",
            "Question: \"Funding sources and grants for undergraduate chemical research projects\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Collaborative research opportunities in molecular chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Career paths in inorganic and computational chemistry for undergraduates\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "   \"What are the career paths for undergraduates in inorganic chemistry?\",\n",
            "   \"What are the career paths for undergraduates in computational chemistry?\",\n",
            "   \"How can undergraduate students pursue a career in inorganic chemistry?\",\n",
            "   \"What job opportunities are available for undergraduates in computational chemistry?\",\n",
            "   \"What are the skills and qualifications required for a career in inorganic chemistry?\",\n",
            "   \"What are the skills and qualifications required for a career in computational chemistry?\",\n",
            "   \"What industries hire undergraduates with a background in inorganic chemistry?\",\n",
            "   \"What industries hire undergraduates with a background in computational chemistry?\",\n",
            "   \"What research opportunities are available for undergraduates in inorganic chemistry?\",\n",
            "   \"What research opportunities are available for undergraduates in computational chemistry?\",\n",
            "   \"Are there any internships or co-op programs for undergraduates in inorganic chemistry?\",\n",
            "   \"Are there any internships or co-op programs for undergraduates in computational chemistry?\",\n",
            "   \"What universities offer specialized programs for undergraduates in inorganic chemistry?\",\n",
            "   \"What universities offer specialized programs for undergraduates in computational chemistry?\",\n",
            "   \"What professional societies or organizations are relevant to a career in inorganic chemistry?\",\n",
            "   \"What professional societies or organizations are relevant to a career in computational chemistry?\",\n",
            "   \"What conferences or events should undergraduates in inorganic chemistry attend?\",\n",
            "   \"What conferences or events should undergraduates in computational chemistry attend?\",\n",
            "   \"What are some notable advancements or breakthroughs in inorganic chemistry?\",\n",
            "   \"What are some notable advancements or breakthroughs in computational chemistry?\"\n",
            "]\n",
            "\n",
            "Question: \"Professional organizations and conferences for chemistry students\"\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Comprehensive guide to basic chemistry concepts for beginners\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Chemistry fundamentals for beginners\"\n",
            "2. \"Basic principles of chemistry explained\"\n",
            "3. \"Introduction to chemistry for beginners\"\n",
            "4. \"Chemistry basics for new learners\"\n",
            "5. \"Key concepts in chemistry for beginners\"\n",
            "6. \"Chemistry 101: A beginner's guide\"\n",
            "7. \"Exploring the foundations of chemistry\"\n",
            "8. \"Chemistry concepts made easy for beginners\"\n",
            "9. \"Beginner's crash course in chemistry\"\n",
            "10. \"Understanding the essentials of chemistry for beginners\"\n",
            "\n",
            "Question: \"Essentials of molecular bonds and interactions in chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introductory resources on metal-ligand bonding principles\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\"Introduction to metal-ligand bonding principles\",\n",
            "\"Basic concepts of metal-ligand bonding\",\n",
            "\"Principles of metal-ligand interactions\",\n",
            "\"Overview of metal complexes and ligands\",\n",
            "\"Fundamentals of metal-ligand coordination chemistry\",\n",
            "\"Key principles of metal-ligand bonding\",\n",
            "\"Understanding metal-ligand bonding\",\n",
            "\"Introduction to the chemistry of metal-ligand complexes\",\n",
            "\"Exploring the basics of metal-ligand bonding\",\n",
            "\"Overview of metal-ligand interactions\"]\n",
            "\n",
            "Question: \"Beginner's guide to understanding thiol and thiolate chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Fundamentals of X-ray crystallography for molecular structure analysis\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Overview and introduction to inorganic chemistry for undergraduates\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Basic chemical nomenclature and terminology in molecular chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introductory guides to computational chemistry tools and their use\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is computational chemistry?\",\n",
            "  \"How is computational chemistry used in research?\",\n",
            "  \"What are the main computational chemistry tools?\",\n",
            "  \"What is the role of computational chemistry in drug discovery?\",\n",
            "  \"What are some beginner-friendly computational chemistry tools?\",\n",
            "  \"How can computational chemistry be applied to materials science?\",\n",
            "  \"What are the limitations of computational chemistry?\",\n",
            "  \"What are the advantages of using computational chemistry tools?\",\n",
            "  \"Are there any online courses or tutorials on computational chemistry?\",\n",
            "  \"How can I get started with computational chemistry?\"\n",
            "]\n",
            "\n",
            "Question: \"How to read and interpret different chemical structure representations\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Step-by-step guide to chemical synthesis and compound isolation\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Chemical synthesis tutorial\"\n",
            "2. \"Isolation techniques for compounds\"\n",
            "3. \"Step-by-step procedure for chemical synthesis\"\n",
            "4. \"Methods for compound purification\"\n",
            "5. \"Chemical synthesis protocols\"\n",
            "6. \"Isolation of organic compounds\"\n",
            "7. \"Techniques for compound separation\"\n",
            "8. \"Synthesis and isolation of specific compounds\"\n",
            "9. \"Chemical synthesis and purification methods\"\n",
            "10. \"Isolation procedures for natural products\"\n",
            "\n",
            "Question: \"Understanding the role of hydrogen bonding in molecular structures\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Fundamentals of spectroscopic techniques in chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Basic principles of density functional theory (DFT) and its applications\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introduction to molecular orbital theory in chemistry\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is molecular orbital theory and how is it used in chemistry?\",\n",
            "  \"Key concepts and principles of molecular orbital theory\",\n",
            "  \"Applications of molecular orbital theory in chemical reactions\",\n",
            "  \"Overview of the mathematical and quantum mechanical basis of molecular orbital theory\",\n",
            "  \"Comparison between molecular orbital theory and valence bond theory in chemistry\",\n",
            "  \"Experimental techniques used to study molecular orbitals\",\n",
            "  \"Limitations and criticisms of molecular orbital theory\",\n",
            "  \"Historical development of molecular orbital theory in chemistry\",\n",
            "  \"Impact of molecular orbital theory on our understanding of chemical bonding\",\n",
            "  \"Current trends and advancements in molecular orbital theory research and applications\"\n",
            "]\n",
            "\n",
            "Question: \"Essential laboratory techniques for analyzing metal-thiolate complexes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Introduction to laboratory techniques for analyzing metal-thiolate complexes\"\n",
            "2. \"Methods for analyzing metal-thiolate complexes in the laboratory\"\n",
            "3. \"Common laboratory techniques for studying metal-thiolate complexes\"\n",
            "4. \"Analytical methods for metal-thiolate complex analysis\"\n",
            "5. \"Overview of techniques for analyzing metal-thiolate complexes\"\n",
            "6. \"Experimental methods for characterizing metal-thiolate complexes\"\n",
            "7. \"Specific laboratory techniques used in the analysis of metal-thiolate complexes\"\n",
            "8. \"Comparative analysis of different laboratory techniques for metal-thiolate complex analysis\"\n",
            "9. \"Step-by-step guide to analyzing metal-thiolate complexes in the laboratory\"\n",
            "10. \"Innovative approaches in the analysis of metal-thiolate complexes\"\n",
            "\n",
            "Question: \"Overview of chemical reactions involving thiolate ligands\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Guides on safe handling and ethical considerations in chemical research\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Resources for learning about chemical compound characterization\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introductory guide to basic organic chemistry principles\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Understanding acid-base chemistry in molecular interactions\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Basics of quantum chemistry for electron distribution analysis\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is quantum chemistry and how does it relate to electron distribution analysis?\n",
            "2. What are the fundamental principles and theories of quantum chemistry?\n",
            "3. How is electron distribution analyzed in the field of quantum chemistry?\n",
            "4. What tools and methods are used for electron distribution analysis in quantum chemistry?\n",
            "5. Are there any specific applications of electron distribution analysis in quantum chemistry?\n",
            "6. What are the main challenges and limitations of electron distribution analysis in quantum chemistry?\n",
            "7. Are there any recent advancements or breakthroughs in the field of quantum chemistry for electron distribution analysis?\n",
            "8. Can you provide examples of electron distribution analysis in real-world applications using quantum chemistry?\n",
            "9. Are there any alternative approaches or theories to electron distribution analysis in quantum chemistry?\n",
            "10. What are the key factors that influence electron distribution in quantum chemistry?\n",
            "\n",
            "Question: \"Principles of thermodynamics in chemical reactions explained\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Fundamentals of chemical kinetics in compound synthesis\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Introduction to chemical kinetics in compound synthesis\"\n",
            "2. \"Key principles of chemical kinetics in compound synthesis\"\n",
            "3. \"Fundamental concepts of chemical reactions in compound synthesis\"\n",
            "4. \"Exploring the role of chemical kinetics in compound synthesis\"\n",
            "5. \"Understanding reaction rates and kinetics in compound synthesis\"\n",
            "6. \"How does chemical kinetics impact compound synthesis?\"\n",
            "7. \"Application of chemical kinetics in compound synthesis\"\n",
            "8. \"Factors influencing chemical reaction rates in compound synthesis\"\n",
            "9. \"Investigating the kinetics of specific reactions in compound synthesis\"\n",
            "10. \"Advancements in chemical kinetics for compound synthesis\"\n",
            "\n",
            "Question: \"Overview of environmental chemistry in chemical synthesis\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Introduction to polymer chemistry for beginners\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is polymer chemistry?\",\n",
            "  \"Basic concepts of polymer chemistry\",\n",
            "  \"Polymerization processes in polymer chemistry\",\n",
            "  \"Properties of polymers and their applications\",\n",
            "  \"Polymer chemistry tutorials for beginners\",\n",
            "  \"Polymer chemistry books for beginners\",\n",
            "  \"Polymer chemistry courses for beginners\",\n",
            "  \"Polymer chemistry resources for beginners\",\n",
            "  \"Polymer chemistry experiments for beginners\",\n",
            "  \"Recent advancements in polymer chemistry\"\n",
            "]\n",
            "\n",
            "Question: \"Effective chemical literature search techniques for students\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Chemical literature search techniques for students\"\n",
            "2. \"Best practices for conducting literature searches in chemistry\"\n",
            "3. \"Tips for effective chemical literature searching\"\n",
            "4. \"How to find relevant chemical literature for student research\"\n",
            "5. \"Strategies for efficient literature search in chemistry\"\n",
            "6. \"Recommended databases for chemical literature searches\"\n",
            "7. \"Boolean operators for refining chemical literature searches\"\n",
            "8. \"Advanced search techniques for chemical literature\"\n",
            "9. \"How to critically evaluate sources in chemical literature\"\n",
            "10. \"Comparison of different citation databases for chemical research\"\n",
            "\n",
            "Question: \"Basics of patent search and intellectual property in chemistry\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is a patent and how does it work in the field of chemistry?\",\n",
            "  \"How does the patent system protect intellectual property in the chemical industry?\",\n",
            "  \"What are the key components of a patent in chemistry?\",\n",
            "  \"What is the process of conducting a patent search for chemical inventions?\",\n",
            "  \"What are the different types of patents available for chemical innovations?\",\n",
            "  \"What are the requirements for obtaining a patent in chemistry?\",\n",
            "  \"What are the benefits of patent protection in the field of chemistry?\",\n",
            "  \"Are there any limitations or restrictions to patent protection for chemical inventions?\",\n",
            "  \"What is the role of intellectual property rights in promoting innovation in the chemical industry?\",\n",
            "  \"What are some notable examples of patented chemical inventions?\",\n",
            "  \"What are the potential challenges or considerations in enforcing patent rights for chemical innovations?\"\n",
            "]\n",
            "\n",
            "Question: \"Essential concepts in biochemistry for chemistry students\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Relation between organic and inorganic chemistry in compound analysis\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the relationship between organic chemistry and inorganic chemistry in compound analysis?\n",
            "2. How do organic and inorganic chemistry intersect in the analysis of compounds?\n",
            "3. What are the similarities and differences between organic and inorganic chemistry in the context of compound analysis?\n",
            "4. How does the understanding of organic chemistry contribute to the analysis of compounds?\n",
            "5. How does the understanding of inorganic chemistry contribute to the analysis of compounds?\n",
            "6. What techniques and methods from organic chemistry are commonly used in compound analysis?\n",
            "7. What techniques and methods from inorganic chemistry are commonly used in compound analysis?\n",
            "8. Can organic and inorganic chemistry be combined to achieve more accurate compound analysis?\n",
            "9. Are there any limitations or challenges when combining knowledge from organic and inorganic chemistry in compound analysis?\n",
            "10. What advancements or research have been made in the integration of organic and inorganic chemistry in compound analysis?\n",
            "\n",
            "Question: \"Acid-base reactions in metal-thiolate complex formation\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the role of acid-base reactions in the formation of metal-thiolate complexes?\n",
            "2. How do acid-base reactions affect the stability of metal-thiolate complexes?\n",
            "3. Can acid-base reactions be used to control the synthesis of metal-thiolate complexes?\n",
            "4. What are the factors that influence acid-base reactions in the formation of metal-thiolate complexes?\n",
            "5. Are there any specific acid-base catalysts that are commonly used in the formation of metal-thiolate complexes?\n",
            "6. What are the mechanisms involved in acid-base reactions during the formation of metal-thiolate complexes?\n",
            "7. Are there any known limitations or challenges associated with acid-base reactions in metal-thiolate complex formation?\n",
            "8. How do different metal ions and thiols interact in acid-base reactions during complex formation?\n",
            "9. Can acid-base reactions in metal-thiolate complexes be tuned for specific applications?\n",
            "10. What are the current research trends in understanding acid-base reactions in metal-thiolate complex formation?\n",
            "\n",
            "Question: \"Quantum chemical methods in studying metal-ligand bonding\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Thermodynamic stability of synthesized compounds in chemistry\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Rate determining steps in chemical synthesis of novel compounds\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the concept of rate determining steps in chemical synthesis?\n",
            "2. How are rate determining steps determined in the synthesis of novel compounds?\n",
            "3. Can rate determining steps vary depending on the specific compound being synthesized?\n",
            "4. What are the factors that influence the rate determining steps in chemical synthesis?\n",
            "5. Are there any experimental techniques or approaches for identifying rate determining steps in synthesis reactions?\n",
            "6. How do researchers optimize the rate determining steps in the synthesis of novel compounds?\n",
            "7. Are there any computational methods or tools available for predicting rate determining steps in chemical synthesis?\n",
            "8. What is the significance of understanding rate determining steps in the design and production of new compounds?\n",
            "9. Can rate determining steps be manipulated to enhance the efficiency of chemical synthesis?\n",
            "10. What are some examples of rate determining steps in the synthesis of specific types of compounds?\n",
            "\n",
            "Question: \"Environmental impact assessment of chemical compounds\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the process of conducting an environmental impact assessment for chemical compounds?\",\n",
            "  \"What are the key components of an environmental impact assessment for chemical compounds?\",\n",
            "  \"How does an environmental impact assessment evaluate the effects of chemical compounds on the environment?\",\n",
            "  \"What are the potential environmental impacts of chemical compounds?\",\n",
            "  \"What are the criteria for assessing the environmental impact of chemical compounds?\",\n",
            "  \"Are there any regulations or guidelines for conducting environmental impact assessments for chemical compounds?\",\n",
            "  \"What methods are used to measure the environmental impact of chemical compounds?\",\n",
            "  \"How are the risks associated with chemical compounds evaluated in an environmental impact assessment?\",\n",
            "  \"Are there any case studies or examples of environmental impact assessments conducted for specific chemical compounds?\",\n",
            "  \"What are the challenges and limitations of conducting environmental impact assessments for chemical compounds?\"\n",
            "]\n",
            "\n",
            "Question: \"Polymer applications of metal-thiolate complexes\",\n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"Polymer applications of metal-thiolate complexes\",\n",
            "  \"Synthesis of metal-thiolate complexes for polymer applications\",\n",
            "  \"Effect of metal-thiolate complexes on polymer properties\",\n",
            "  \"Polymerization catalysts based on metal-thiolate complexes\",\n",
            "  \"Polymer nanocomposites with metal-thiolate complexes\",\n",
            "  \"Polymer stabilization using metal-thiolate complexes\",\n",
            "  \"Polymer modification with metal-thiolate complexes\",\n",
            "  \"Advantages and limitations of metal-thiolate complex-based polymers\",\n",
            "  \"Recent advancements in metal-thiolate complex-polymer research\",\n",
            "  \"Biomedical applications of metal-thiolate complex-polymer composites\"\n",
            "]\n",
            "\n",
            "Question: \"How to navigate and utilize scientific journals for chemical research\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Understanding IP rights and patenting in chemical discoveries\",\n",
            "Answerable: True\n",
            "\n",
            "Question: \"Biochemical interactions of metal-thiolate compounds\"\n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the causes of climate change?\",\n",
            "  \"How does climate change impact the environment?\",\n",
            "  \"What measures can be taken to mitigate climate change?\",\n",
            "  \"What are the effects of climate change on human health?\",\n",
            "  \"What are the economic impacts of climate change?\",\n",
            "  \"What policies are in place to address climate change?\",\n",
            "  \"What is the role of renewable energy in combating climate change?\",\n",
            "  \"What are the long-term implications of climate change?\",\n",
            "  \"How does deforestation contribute to climate change?\",\n",
            "  \"What are the potential solutions for reducing greenhouse gas emissions?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What are the causes and symptoms of migraines?\n",
            "2. How can migraines be treated or managed?\n",
            "3. Are there any natural remedies or alternative therapies for migraines?\n",
            "4. What is the difference between migraines and tension headaches?\n",
            "5. Can migraines be hereditary? \n",
            "6. Are there any specific triggers that can cause migraines?\n",
            "7. What are some lifestyle changes that may help prevent migraines?\n",
            "8. Are there any specific diets or foods that can help reduce the frequency of migraines?\n",
            "9. What medications are commonly prescribed for migraines?\n",
            "10. Are there any new or experimental treatments for migraines that show promising results?\n",
            "11. What are some non-pharmacological interventions for migraines, such as relaxation techniques or acupuncture?\n",
            "12. Can migraines be a symptom of an underlying medical condition?\n",
            "13. Are there any complementary therapies that can be used alongside conventional medical treatments for migraines?\n",
            "14. Is there a link between hormonal changes and migraines?\n",
            "15. Are there any specific risk factors for developing migraines, such as age or gender?\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the best way to train a puppy?\",\n",
            "  \"How can I potty train my new puppy?\",\n",
            "  \"What are some tips for crate training a puppy?\",\n",
            "  \"Are there any specific techniques for leash training a puppy?\",\n",
            "  \"What are the best treats to use for training a puppy?\",\n",
            "  \"Are there any recommended books or resources for puppy training?\",\n",
            "  \"What are some common mistakes to avoid when training a puppy?\",\n",
            "  \"How long does it usually take to train a puppy?\",\n",
            "  \"Are there any specialized training programs or classes for puppies?\",\n",
            "  \"What are some signs that my puppy is responding well to training?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What causes migraines?\n",
            "2. How to prevent migraines?\n",
            "3. What are the common triggers for migraines?\n",
            "4. Are migraines hereditary?\n",
            "5. What are the different types of migraines?\n",
            "6. What are the common symptoms of migraines?\n",
            "7. Is there a cure for migraines?\n",
            "8. What are the available treatments for migraines?\n",
            "9. How does stress contribute to migraines?\n",
            "10. Can dietary changes help with migraines?\n",
            "11. What are some natural remedies for migraines?\n",
            "12. Are there any alternative therapies for migraines?\n",
            "13. Can hormonal changes trigger migraines?\n",
            "14. What is the role of weather changes in migraines?\n",
            "15. How do migraines affect daily life and productivity?\n",
            "16. Can exercise help in reducing the frequency of migraines?\n",
            "17. Can certain medications worsen migraines?\n",
            "18. What are some lifestyle changes that can help manage migraines?\n",
            "19. How does sleep deprivation impact migraines?\n",
            "20. Can relaxation techniques provide relief from migraines?\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the impact of climate change on the polar ice caps?\",\n",
            "  \"How is climate change affecting Arctic wildlife?\",\n",
            "  \"What are the major causes of melting of the polar ice caps?\",\n",
            "  \"Can the melting of the polar ice caps lead to a rise in sea levels?\",\n",
            "  \"What are the current efforts to mitigate the effects of polar ice melting?\",\n",
            "  \"What are the potential consequences of the loss of polar ice caps?\",\n",
            "  \"What are the solutions proposed to slow down the melting of the polar ice caps?\",\n",
            "  \"What is the role of greenhouse gases in the melting of polar ice caps?\",\n",
            "  \"How does the melting of polar ice caps contribute to climate change?\",\n",
            "  \"What are the different types of polar ice caps and their characteristics?\",\n",
            "  \"What are the primary sources of data used to monitor changes in the polar ice caps?\",\n",
            "  \"What are some historical examples of sudden and significant melting of polar ice caps?\",\n",
            "  \"How is the melting of polar ice caps affecting indigenous communities in the Arctic?\",\n",
            "  \"What is the impact of melting polar ice caps on marine ecosystems?\",\n",
            "  \"What are the long-term implications of the shrinking polar ice caps?\",\n",
            "  \"What are some potential strategies for adaptation to the changes caused by melting polar ice caps?\",\n",
            "  \"Are there any known positive effects of melting polar ice caps?\",\n",
            "  \"How do climate change projections predict the future status of the polar ice caps?\",\n",
            "  \"What are the effects of polar ice melting on global weather patterns?\",\n",
            "  \"What are some technological advancements in monitoring and studying polar ice caps?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the causes of climate change?\",\n",
            "  \"How does climate change affect the environment?\",\n",
            "  \"What are the long-term implications of climate change?\",\n",
            "  \"What are the possible solutions to address climate change?\",\n",
            "  \"What is the role of humans in causing climate change?\",\n",
            "  \"How can individuals take action to combat climate change?\",\n",
            "  \"What policies are in place to mitigate climate change?\",\n",
            "  \"What are the economic impacts of climate change?\",\n",
            "  \"What are the social impacts of climate change?\",\n",
            "  \"What are the environmental impacts of climate change?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"How to cook spaghetti\",\n",
            "  \"Spaghetti cooking tips\",\n",
            "  \"Best spaghetti recipes\",\n",
            "  \"Traditional spaghetti sauce recipe\",\n",
            "  \"How to make homemade spaghetti noodles\",\n",
            "  \"Spaghetti cooking time\",\n",
            "  \"Different types of spaghetti noodles\",\n",
            "  \"Spaghetti sauce from scratch\",\n",
            "  \"Spaghetti ingredients\",\n",
            "  \"Spaghetti cooking techniques\",\n",
            "  \"Spaghetti with meatballs recipe\",\n",
            "  \"Healthy spaghetti alternatives\",\n",
            "  \"Gluten-free spaghetti options\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the symptoms of the flu?\",\n",
            "  \"How is the flu diagnosed?\",\n",
            "  \"What are the available treatments for the flu?\",\n",
            "  \"What are the complications of the flu?\",\n",
            "  \"Are there any natural remedies for the flu?\",\n",
            "  \"Can the flu be prevented?\",\n",
            "  \"What is the difference between the flu and a cold?\",\n",
            "  \"Does the flu vaccine have any side effects?\",\n",
            "  \"Are there any home remedies for flu relief?\",\n",
            "  \"Is it necessary to see a doctor for the flu?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the impact of climate change on coral reefs?\",\n",
            "  \"Causes of coral reef degradation\",\n",
            "  \"Effects of ocean acidification on coral reefs\",\n",
            "  \"How are coral reefs affected by rising sea temperatures?\",\n",
            "  \"Solutions to protect coral reefs from climate change\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the causes of climate change?\",\n",
            "  \"How does climate change impact ecosystems?\",\n",
            "  \"What are the solutions to mitigate climate change?\",\n",
            "  \"What is the role of greenhouse gases in climate change?\",\n",
            "  \"What are the long-term effects of climate change on human health?\",\n",
            "  \"How does deforestation contribute to climate change?\",\n",
            "  \"What are the economic implications of climate change?\",\n",
            "  \"What are the potential future scenarios for climate change?\",\n",
            "  \"What policies have been implemented to address climate change?\",\n",
            "  \"What are the impacts of climate change on agriculture?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "\"naive queries\", \n",
            "\"information foraging\", \n",
            "\"information literacy\", \n",
            "\"search query generation\", \n",
            "\"comprehensive search queries\", \n",
            "\"counterfactual search queries\", \n",
            "\"notes accumulation\", \n",
            "\"diverse search queries\", \n",
            "\"breadth and depth of search\", \n",
            "\"alternative search scenarios\", \n",
            "\"balance between precision and recall\", \n",
            "\"iterative improvement of queries\", \n",
            "\"feedback incorporation in query formulation\", \n",
            "\"contextual awareness in query generation\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the impact of climate change on biodiversity?\",\n",
            "  \"How does global warming affect plant and animal species?\",\n",
            "  \"What are the potential consequences of climate change on ecosystems?\",\n",
            "  \"How is climate change impacting endangered species?\",\n",
            "  \"What are the effects of rising temperatures on biodiversity?\",\n",
            "  \"What are the causes and effects of climate change on wildlife?\",\n",
            "  \"How is climate change affecting species extinction?\",\n",
            "  \"What are the ecological impacts of climate change?\",\n",
            "  \"What are the long-term effects of climate change on biodiversity?\",\n",
            "  \"How is climate change threatening the survival of certain species?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the main causes of air pollution?\"\n",
            "2. \"How does air pollution affect human health?\"\n",
            "3. \"What are the different types of air pollutants?\"\n",
            "4. \"What are the health risks associated with air pollution?\"\n",
            "5. \"What are the long-term effects of air pollution on the environment?\"\n",
            "6. \"How can air pollution be reduced or prevented?\"\n",
            "7. \"What are some regulations or policies in place to control air pollution?\"\n",
            "8. \"What are the sources of indoor air pollution?\"\n",
            "9. \"What are the effects of air pollution on wildlife and ecosystems?\"\n",
            "10. \"Are there any natural solutions to reducing air pollution?\"\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the definition of search query generation?\n",
            "2. Techniques for generating search queries.\n",
            "3. How do search engines interpret search queries?\n",
            "4. Why is query refinement important for search results?\n",
            "5. What are some best practices for creating effective search queries?\n",
            "6. How can information literacy skills be applied to search query generation?\n",
            "7. How does the precision vs recall tradeoff affect search query formulation?\n",
            "8. What are some strategies for generating counterfactual search queries?\n",
            "9. How can feedback from previous search outcomes be used to improve search queries?\n",
            "10. What role does contextual awareness play in search query generation?\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the benefits of exercise for cardiovascular health?\",\n",
            "  \"What are the different types of exercises that can improve cardiovascular health?\",\n",
            "  \"How often should one exercise to maintain cardiovascular health?\",\n",
            "  \"Are there any specific exercises that are more effective for improving cardiovascular health?\",\n",
            "  \"What are the potential risks or complications of exercising for cardiovascular health?\",\n",
            "  \"Can exercise alone prevent or treat cardiovascular diseases?\",\n",
            "  \"What is the role of diet in maintaining cardiovascular health?\",\n",
            "  \"Are there any specific foods or nutrients that are beneficial for cardiovascular health?\",\n",
            "  \"What is the impact of lifestyle factors, such as smoking and alcohol consumption, on cardiovascular health?\",\n",
            "  \"Are there any alternative treatments or complementary therapies that can support cardiovascular health?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the symptoms of COVID-19?\",\n",
            "  \"How is COVID-19 transmitted?\",\n",
            "  \"What are the risk factors for COVID-19?\",\n",
            "  \"What is the mortality rate of COVID-19?\",\n",
            "  \"How can COVID-19 be prevented?\",\n",
            "  \"What treatments are available for COVID-19?\",\n",
            "  \"What is the current status of COVID-19 vaccine development?\",\n",
            "  \"Are there any long-term effects of COVID-19?\",\n",
            "  \"Can COVID-19 be transmitted through food or packaging?\",\n",
            "  \"How accurate are COVID-19 tests?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Comprehensive overview of the impact of climate change on biodiversity\"\n",
            "\n",
            "2. \"Recent research on the relationship between climate change and species extinction\"\n",
            "\n",
            "3. \"Methods used to assess climate change effects on wildlife populations\"\n",
            "\n",
            "4. \"Case studies of specific species affected by climate change\"\n",
            "\n",
            "5. \"Government policies and initiatives aimed at conserving biodiversity in the face of climate change\"\n",
            "\n",
            "6. \"Expert opinions on the potential long-term consequences of climate change on global biodiversity\"\n",
            "\n",
            "7. \"Comparison of climate change impacts on terrestrial and marine ecosystems\"\n",
            "\n",
            "8. \"Climate change mitigation strategies to protect vulnerable wildlife populations\"\n",
            "\n",
            "9. \"Historical patterns of species adaptation to climate change\"\n",
            "\n",
            "10. \"Empirical evidence supporting the existence of climate change-induced range shifts in plant and animal populations\"\n",
            "\n",
            "11. \"Effects of climate change on ecosystem services provided by biodiversity\"\n",
            "\n",
            "12. \"Potential implications of climate change on invasive species distribution and spread\"\n",
            "\n",
            "13. \"Long-term monitoring programs for tracking climate change impacts on biodiversity\"\n",
            "\n",
            "14. \"Projected impacts of climate change on keystone species and ecological stability\"\n",
            "\n",
            "15. \"Community-based conservation efforts for preserving biodiversity in the face of climate change\"\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What are the causes of deforestation in the Amazon rainforest?\n",
            "2. How does deforestation affect biodiversity in the Amazon rainforest?\n",
            "3. Can reforestation efforts help mitigate the impacts of deforestation in the Amazon?\n",
            "4. What are the economic factors driving deforestation in the Amazon rainforest?\n",
            "5. How does deforestation in the Amazon contribute to climate change?\n",
            "6. What are the social and cultural impacts of deforestation on indigenous communities in the Amazon?\n",
            "7. Are there any sustainable alternatives to deforestation in the Amazon?\n",
            "8. What are the long-term consequences of deforestation in the Amazon rainforest?\n",
            "9. How are international organizations and governments working to prevent deforestation in the Amazon?\n",
            "10. What are the current policies or regulations in place to address deforestation in the Amazon rainforest?\n",
            "\n",
            "Informed Queries:\n",
            "11. What are the specific industries responsible for deforestation in the Amazon?\n",
            "12. Has there been any progress in reducing deforestation rates in the Amazon in recent years?\n",
            "13. Are there any successful case studies of reforestation projects in the Amazon?\n",
            "14. What are the social, environmental, and economic benefits of preserving the Amazon rainforest?\n",
            "15. How does illegal logging contribute to deforestation in the Amazon and what measures are being taken to combat it?\n",
            "\n",
            "Counterfactual Queries:\n",
            "16. What would be the environmental impact if deforestation in the Amazon continues unabated?\n",
            "17. What would be the consequences of completely halting deforestation in the Amazon?\n",
            "18. Are there any alternative methods to meet the demand for resources without resorting to deforestation in the Amazon?\n",
            "19. What would be the effect of stricter regulations on deforestation in the Amazon on the local economy?\n",
            "20. How would the Amazon region be affected if deforestation was entirely reversed and the forest was restored to its original state?\n",
            "\n",
            "Feedback-Informed Queries:\n",
            "21. What are the recent findings or scientific studies related to the impact of deforestation on the Amazon's ecosystem?\n",
            "22. What are the leading conservation organizations working on protecting the Amazon and what initiatives are they currently involved in?\n",
            "23. Are there any ongoing international agreements or initiatives aimed at reducing deforestation in the Amazon?\n",
            "24. What are the major challenges faced in enforcing deforestation regulations in the Amazon?\n",
            "25. How do local communities in the Amazon region perceive and respond to deforestation issues?\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the cause of global warming?\",\n",
            "  \"Effects of climate change on wildlife\",\n",
            "  \"How can we mitigate climate change?\",\n",
            "  \"Renewable energy sources\",\n",
            "  \"Impacts of deforestation on the environment\",\n",
            "  \"The role of human activities in climate change\",\n",
            "  \"Solutions to reduce carbon emissions\",\n",
            "  \"The influence of greenhouse gases on climate\",\n",
            "  \"Consequences of rising sea levels\",\n",
            "  \"Effects of air pollution on human health\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the latest research on renewable energy sources?\n",
            "2. How do advancements in renewable energy technologies impact global energy consumption?\n",
            "3. What are the key challenges and barriers in implementing renewable energy systems?\n",
            "4. What government policies and incentives are in place to promote the use of renewable energy?\n",
            "5. Are there any case studies of successful renewable energy projects?\n",
            "6. What are the environmental benefits and drawbacks of different types of renewable energy?\n",
            "7. How do renewable energy sources compare in terms of cost and efficiency?\n",
            "8. Is there ongoing research on improving the storage and distribution of renewable energy?\n",
            "9. How does the adoption of renewable energy contribute to reducing greenhouse gas emissions?\n",
            "10. What are the economic implications of transitioning to a renewable energy-based economy?\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the symptoms of COVID-19?\"\n",
            "2. \"How is COVID-19 spreading?\"\n",
            "3. \"Preventive measures for COVID-19\"\n",
            "4. \"COVID-19 treatment options\"\n",
            "5. \"Impact of COVID-19 on the economy\"\n",
            "6. \"COVID-19 vaccination progress\"\n",
            "7. \"Long-term effects of COVID-19\"\n",
            "8. \"COVID-19 variants and their implications\"\n",
            "9. \"COVID-19 testing availability and procedure\"\n",
            "10. \"COVID-19 case statistics and trend analysis\"\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the symptoms of a common cold?\"\n",
            "2. \"Home remedies for a sore throat\"\n",
            "3. \"Is it possible to prevent catching a cold?\"\n",
            "4. \"Over-the-counter medications for cold relief\"\n",
            "5. \"How long does a cold typically last?\"\n",
            "6. \"Can a cold lead to complications?\"\n",
            "7. \"Difference between a cold and the flu\"\n",
            "8. \"Natural remedies for boosting the immune system\"\n",
            "9. \"Can you catch a cold from being in cold weather?\"\n",
            "10. \"Managing cold symptoms while at work/school\"\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "   \"What are the symptoms of COVID-19?\",\n",
            "   \"Is COVID-19 airborne?\",\n",
            "   \"How is COVID-19 transmitted?\",\n",
            "   \"What is the incubation period for COVID-19?\",\n",
            "   \"What are the risk factors for severe COVID-19 illness?\",\n",
            "   \"Are there any preventive measures against COVID-19?\",\n",
            "   \"What treatments are available for COVID-19?\",\n",
            "   \"What are the long-term effects of COVID-19?\",\n",
            "   \"Can you get reinfected with COVID-19?\",\n",
            "   \"What is the current status of COVID-19 vaccine development?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the symptoms of COVID-19?\",\n",
            "  \"How is COVID-19 transmitted?\",\n",
            "  \"What are the preventive measures for COVID-19?\",\n",
            "  \"How is COVID-19 different from the common flu?\",\n",
            "  \"What treatments are available for COVID-19?\",\n",
            "  \"What are the long-term effects of COVID-19?\",\n",
            "  \"How effective are the COVID-19 vaccines?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: {\n",
            "  \"queries\": [\n",
            "    \"What causes global warming?\",\n",
            "    \"Effects of global warming on the environment\",\n",
            "    \"Mitigation strategies for global warming\",\n",
            "    \"How do greenhouse gases contribute to global warming?\",\n",
            "    \"Natural factors impacting global warming\",\n",
            "    \"Global warming and its impact on weather patterns\",\n",
            "    \"Current levels of carbon dioxide in the atmosphere\",\n",
            "    \"Renewable energy sources to reduce global warming\",\n",
            "    \"Historical climate data and global warming\",\n",
            "    \"Government policies to address global warming\",\n",
            "    \"Is global warming reversible?\",\n",
            "    \"Climate change and its relationship to global warming\",\n",
            "    \"Predictions for future temperature increases due to global warming\",\n",
            "    \"Impact of global warming on wildlife and ecosystems\",\n",
            "    \"Technological innovations to combat global warming\",\n",
            "    \"Is global warming a natural cycle or human-induced?\",\n",
            "    \"The role of deforestation in global warming\",\n",
            "    \"Economic consequences of global warming\",\n",
            "    \"Global warming and sea level rise\",\n",
            "    \"Is global warming causing more frequent natural disasters?\",\n",
            "    \"Health impacts of global warming on humans\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the causes of climate change?\"\n",
            "2. \"Effects of climate change on ecosystems\"\n",
            "3. \"How does climate change impact the economy?\"\n",
            "4. \"Climate change and human health\"\n",
            "5. \"Mitigation strategies for climate change\"\n",
            "6. \"Social and cultural impacts of climate change\"\n",
            "7. \"Technological advancements to combat climate change\"\n",
            "8. \"Political responses to climate change\"\n",
            "9. \"Historical perspectives on climate change\"\n",
            "10. \"Future projections of climate change\"\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: Naive Queries:\n",
            "1. What is the best way to lose weight?\n",
            "2. How can I improve my diet to be healthier?\n",
            "3. What are some effective exercises for weight loss?\n",
            "4. Are there any natural remedies for weight loss?\n",
            "5. Can you recommend any weight loss supplements?\n",
            "\n",
            "Informed Queries:\n",
            "6. What are some scientifically-proven methods for weight loss?\n",
            "7. How does calorie counting impact weight loss?\n",
            "8. Can you provide examples of successful weight loss stories and their strategies?\n",
            "9. What is the role of genetics in weight loss?\n",
            "10. Are there any specific diets or eating plans that are effective for weight loss?\n",
            "\n",
            "Counterfactual Queries:\n",
            "11. What would happen if I don't exercise but only focus on reducing calorie intake?\n",
            "12. Are there any weight loss methods that do not restrict certain food groups?\n",
            "13. What if I only focus on strength training exercises for weight loss?\n",
            "14. Can meditation or stress reduction techniques help with weight loss?\n",
            "15. Are there any weight loss strategies that prioritize mental and emotional well-being?\n",
            "\n",
            "Expanded Queries:\n",
            "16. What are the long-term effects of rapid weight loss?\n",
            "17. How does sleep quality and duration affect weight loss?\n",
            "18. Can you provide examples of weight loss programs that offer support and accountability?\n",
            "19. Are there any weight loss strategies specifically tailored to different age groups?\n",
            "20. What are the potential risks or side effects of weight loss surgery?\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What is the best way to lose weight?\"\n",
            "2. \"Effective weight loss strategies\"\n",
            "3. \"Top diets for weight loss\"\n",
            "4. \"How to lose weight fast\"\n",
            "5. \"Healthy weight loss tips\"\n",
            "6. \"Exercises for weight loss\"\n",
            "7. \"Weight loss programs\"\n",
            "8. \"Weight loss supplements\"\n",
            "9. \"Meal plans for weight loss\"\n",
            "10. \"Weight loss success stories\"\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: - What causes climate change?\n",
            "- How does climate change affect the environment?\n",
            "- What are the consequences of climate change?\n",
            "- How can we mitigate climate change?\n",
            "- What are the current efforts to address climate change?\n",
            "- What are the possible solutions to combat climate change?\n",
            "- What policies and regulations are in place to reduce greenhouse gas emissions?\n",
            "- How does deforestation contribute to climate change?\n",
            "- What are the impacts of climate change on wildlife and biodiversity?\n",
            "- How does climate change affect human health?\n",
            "- What are the economic implications of climate change?\n",
            "- How does climate change affect global food production?\n",
            "- What are the predicted future impacts of climate change?\n",
            "- How does climate change relate to extreme weather events?\n",
            "- What are the effects of melting ice caps and rising sea levels due to climate change?\n",
            "- How does climate change affect vulnerable populations and developing countries?\n",
            "- What are the latest research findings and studies on climate change?\n",
            "- How does climate change interact with other environmental issues, such as pollution and loss of habitat?\n",
            "- What are the roles of governments, businesses, and individuals in addressing climate change?\n",
            "- How can we promote renewable energy sources and reduce reliance on fossil fuels?\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the impact of climate change on biodiversity?\",\n",
            "  \"How does climate change affect different species?\",\n",
            "  \"What are the consequences of climate change on ecosystems?\",\n",
            "  \"Can climate change lead to species extinction?\",\n",
            "  \"What are the long-term effects of climate change on biodiversity?\",\n",
            "  \"How are habitats and ecosystems affected by climate change?\",\n",
            "  \"What are the implications of climate change on endangered species?\",\n",
            "  \"What are the indirect effects of climate change on biodiversity?\",\n",
            "  \"How does climate change disrupt food chains and food webs?\",\n",
            "  \"What are the adaptation strategies of species to climate change?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: {\n",
            "  \"queries\": [\n",
            "    \"What is the root cause of climate change?\",\n",
            "    \"How does human activity contribute to climate change?\",\n",
            "    \"What are the effects of climate change on ecosystems?\",\n",
            "    \"What are the potential solutions to mitigate climate change?\",\n",
            "    \"How can renewable energy sources help reduce climate change?\",\n",
            "    \"What is the role of deforestation in climate change?\",\n",
            "    \"What are the economic impacts of climate change?\",\n",
            "    \"What are the political barriers to addressing climate change?\",\n",
            "    \"What are the social implications of climate change?\",\n",
            "    \"How does climate change affect global food security?\",\n",
            "    \"What are the health risks associated with climate change?\",\n",
            "    \"What are the technological innovations to combat climate change?\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the benefits of regular exercise?\"\n",
            "2. \"How does exercise affect mental health?\"\n",
            "3. \"Can exercise help prevent chronic diseases?\"\n",
            "4. \"What are some effective exercise routines for weight loss?\"\n",
            "5. \"Are there specific exercises that can improve flexibility?\"\n",
            "6. \"What types of exercises are recommended for older adults?\"\n",
            "7. \"Are there exercises that can strengthen the core muscles?\"\n",
            "8. \"Can exercise improve sleep quality?\"\n",
            "9. \"What are the potential risks of exercising too much?\"\n",
            "10. \"Are there any studies on the relationship between exercise and longevity?\"\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the causes of climate change and global warming?\"\n",
            "2. \"How does deforestation contribute to climate change?\"\n",
            "3. \"What is the impact of greenhouse gas emissions on the Earth's climate?\"\n",
            "4. \"What are the long-term effects of climate change on ecosystems and biodiversity?\"\n",
            "5. \"Can renewable energy sources like solar and wind power mitigate climate change?\"\n",
            "6. \"What are the potential solutions to mitigate the effects of climate change on coastal areas?\"\n",
            "7. \"How has climate change affected agricultural practices and food production?\"\n",
            "8. \"What are the economic implications of climate change for different industries and countries?\"\n",
            "9. \"Are there any natural climate change cycles that could account for the current warming trend?\"\n",
            "10. \"What are the social and political factors that hinder effective climate change mitigation efforts?\"\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\"What causes insomnia?\", \"How to treat insomnia?\", \"Natural remedies for better sleep\", \"Understanding sleep disorders\", \"Sleep hygiene tips for good sleep\"]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the symptoms of COVID-19?\",\n",
            "  \"How is COVID-19 transmitted?\",\n",
            "  \"What is the incubation period of COVID-19?\",\n",
            "  \"What are the risk factors for severe COVID-19?\",\n",
            "  \"What are the long-term effects of COVID-19?\",\n",
            "  \"What are the current treatment options for COVID-19?\",\n",
            "  \"How is COVID-19 diagnosed?\",\n",
            "  \"What are the different variants of COVID-19?\",\n",
            "  \"How effective are the COVID-19 vaccines?\",\n",
            "  \"What are the prevention measures for COVID-19?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the highest mountain in the world?\",\n",
            "  \"How tall is Mount Everest?\",\n",
            "  \"When was Mount Everest discovered?\",\n",
            "  \"Who was the first person to climb Mount Everest?\",\n",
            "  \"What are the dangers of climbing Mount Everest?\",\n",
            "  \"Are there any records of people dying on Mount Everest?\",\n",
            "  \"What is the success rate of reaching the summit of Mount Everest?\",\n",
            "  \"What is the weather like on Mount Everest?\",\n",
            "  \"How long does it take to climb Mount Everest?\",\n",
            "  \"Are there any alternative routes to climb Mount Everest?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: []\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"Brainstorming techniques for generating comprehensive search queries\"\n",
            "2. \"Information literacy skills for formulating effective search queries\"\n",
            "3. \"Strategies for developing naive search queries\"\n",
            "4. \"Methods for refining and improving search queries iteratively\"\n",
            "5. \"Techniques to balance between precision and recall in search queries\"\n",
            "6. \"How to generate counterfactual search queries to explore alternative scenarios\"\n",
            "7. \"Considerations for crafting informed search queries based on accumulated information\"\n",
            "8. \"Contextual awareness in search query formulation to meet the user's information need\"\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What are the symptoms of COVID-19?\n",
            "2. How is COVID-19 transmitted?\n",
            "3. What is the incubation period of COVID-19?\n",
            "4. What are the risk factors for severe illness from COVID-19?\n",
            "5. What are the treatment options for COVID-19?\n",
            "6. Can COVID-19 be prevented through vaccination?\n",
            "7. What is the difference between COVID-19 and the flu?\n",
            "8. How long does it take to recover from COVID-19?\n",
            "9. What is the current global COVID-19 case count?\n",
            "10. What are the long-term effects of COVID-19?\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What are the causes of climate change?\",\n",
            "  \"How does climate change affect the environment?\",\n",
            "  \"What are the current efforts to combat climate change?\",\n",
            "  \"What are the long-term effects of climate change?\",\n",
            "  \"What are some sustainable practices to address climate change?\",\n",
            "  \"How can individuals reduce their carbon footprint?\",\n",
            "  \"What are the economic impacts of climate change?\",\n",
            "  \"How does climate change affect biodiversity?\",\n",
            "  \"What are the natural factors that contribute to climate change?\",\n",
            "  \"What are the social implications of climate change?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "    \"What are the causes of climate change?\",\n",
            "    \"How does climate change impact the environment?\",\n",
            "    \"What are the effects of climate change on biodiversity?\",\n",
            "    \"What are the solutions to mitigate climate change?\",\n",
            "    \"What is the role of governments in addressing climate change?\",\n",
            "    \"How does climate change affect human health?\",\n",
            "    \"What are the economic consequences of climate change?\",\n",
            "    \"What are the ethical implications of climate change?\",\n",
            "    \"What are the social impacts of climate change on vulnerable communities?\",\n",
            "    \"What is the current state of international climate change agreements?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the impact of climate change on coral reefs?\",\n",
            "  \"How does climate change affect coral reef ecosystems?\",\n",
            "  \"What are the consequences of global warming on coral reefs?\",\n",
            "  \"What are the effects of rising sea temperatures on coral reefs?\",\n",
            "  \"How is ocean acidification linked to climate change and coral reefs?\",\n",
            "  \"What are the long-term implications of climate change for coral reef biodiversity?\",\n",
            "  \"What are the potential solutions to protect coral reefs from the impacts of climate change?\",\n",
            "  \"What are the adaptation strategies for coral reef ecosystems under climate change?\",\n",
            "  \"How can individuals contribute to mitigating the effects of climate change on coral reefs?\",\n",
            "  \"What is the role of coral reef conservation in mitigating climate change effects?\",\n",
            "  \"What scientific studies have been conducted on the relationship between climate change and coral reef health?\",\n",
            "  \"What are the current conservation efforts to address the impact of climate change on coral reefs?\",\n",
            "  \"What are the economic implications of the decline of coral reefs due to climate change?\",\n",
            "  \"How do climate change and coral bleaching events interact and impact the health of coral reefs?\",\n",
            "  \"What are the potential cascading effects of climate change on coral reefs and marine ecosystems?\",\n",
            "  \"What recent research has been conducted on ecosystem-based approaches to climate change adaptation for coral reefs?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [\n",
            "  \"What is the purpose of vaccination?\",\n",
            "  \"How do vaccines work in the body?\",\n",
            "  \"What are the benefits of vaccination?\",\n",
            "  \"What are the side effects of vaccination?\",\n",
            "  \"What are the different types of vaccines?\",\n",
            "  \"How do vaccines stimulate an immune response?\",\n",
            "  \"What are the ingredients in vaccines?\",\n",
            "  \"Can vaccines cause autism?\",\n",
            "  \"Are vaccines effective in preventing diseases?\",\n",
            "  \"Do vaccines provide lifelong immunity?\"\n",
            "]\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. What is the latest research on artificial intelligence?\n",
            "2. How does artificial intelligence impact various industries?\n",
            "3. What are the ethical implications of artificial intelligence?\n",
            "4. What are the current challenges in artificial intelligence?\n",
            "5. Can artificial intelligence replace human intelligence?\n",
            "6. What are the different types of artificial intelligence algorithms?\n",
            "7. How is artificial intelligence being used in healthcare?\n",
            "8. What are the long-term implications of artificial intelligence on the job market?\n",
            "9. What are the potential risks and dangers of artificial intelligence?\n",
            "10. Can artificial intelligence be used to solve complex societal problems?\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: [{\"query\": \"What is the definition of search query?\"}, \n",
            " {\"query\": \"How to generate search queries effectively?\"}, \n",
            " {\"query\": \"What are the different types of search queries?\"}, \n",
            " {\"query\": \"What are some best practices for search query generation?\"}, \n",
            " {\"query\": \"Can search queries be refined based on user feedback?\"}, \n",
            " {\"query\": \"What are some common mistakes to avoid when generating search queries?\"}, \n",
            " {\"query\": \"How can counterfactual queries be used to improve search results?\"}, \n",
            " {\"query\": \"What is the role of information literacy in search query generation?\"}, \n",
            " {\"query\": \"How can precision and recall be balanced in search query generation?\"}, \n",
            " {\"query\": \"What factors should be considered when generating search queries?\"}]\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n",
            "Search Queries: 1. \"What are the symptoms of COVID-19?\"\n",
            "2. \"How to prevent the spread of COVID-19?\"\n",
            "3. \"What is the current global status of COVID-19?\"\n",
            "4. \"What are the different variants of COVID-19?\"\n",
            "5. \"How effective are COVID-19 vaccines?\"\n",
            "6. \"What are the long-term effects of COVID-19?\"\n",
            "7. \"What is the difference between COVID-19 and the flu?\"\n",
            "8. \"How is COVID-19 diagnosed?\"\n",
            "9. \"What is the mortality rate of COVID-19?\"\n",
            "10. \"How does COVID-19 affect different age groups?\"\n",
            "\n",
            "Question: \n",
            "Answerable: True\n",
            "\n",
            "Question: \n",
            "Answerable: False\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m     search_queries \u001b[38;5;241m=\u001b[39m generate_search_queries(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     search_queries \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_search_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# save search queries to json file\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_queries.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n",
            "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mgenerate_search_queries\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize the OpenAI client without explicitly passing the API key\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#client = OpenAI(api_key=api_key)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m  \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-0613\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSearch Queries:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(response)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "!pip install openai==0.28\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import json\n",
        "#import instructor\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "import openai\n",
        "\n",
        "\n",
        "def generate_search_queries(question: str):\n",
        "    prompt = build_system_prompt(\"Generate Search Queries\")\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "\n",
        "    # Initialize the OpenAI client without explicitly passing the API key\n",
        "    #client = OpenAI(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        response =  openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": \"Search Queries:\"},\n",
        "            ]\n",
        "        )\n",
        "        return parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_unanswered_questions(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    unanswered_questions = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Checking for phrases that indicate an unanswered question\n",
        "        if \"cannot answer\" in entry[\"answer\"] or \"does not contain\" in entry[\"answer\"] or \"no answer\" in entry[\"answer\"] or \"no results\" in entry[\"answer\"] or \"no information\" in entry[\"answer\"]:\n",
        "            unanswered = True\n",
        "        else:\n",
        "            unanswered = False\n",
        "\n",
        "        # Building the result entry\n",
        "        result_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"answerable\": not unanswered,\n",
        "            \"timestamp\": entry.get(\"timestamp\", \"Unknown timestamp\")\n",
        "        }\n",
        "\n",
        "        if entry.get(\"references\"):\n",
        "            result_entry[\"references\"] = entry[\"references\"]\n",
        "\n",
        "        if entry.get(\"references_urls\"):  # Using .get to avoid KeyError\n",
        "            result_entry[\"references_urls\"] = entry[\"references_urls\"]\n",
        "\n",
        "        unanswered_questions.append(result_entry)\n",
        "\n",
        "    return unanswered_questions\n",
        "\n",
        "json_file = 'structured_responses.json'\n",
        "unanswered_questions = check_unanswered_questions(json_file)\n",
        "\n",
        "# Display the results\n",
        "for item in unanswered_questions:\n",
        "    print(f\"Question: {item['question']}\\nAnswerable: {item['answerable']}\\n\")\n",
        "    if item.get(\"references\") and item.get(\"references_urls\"):\n",
        "        print(f\"Url(s): {item['references_urls']}\\n\")\n",
        "        #print(f\"Reference(s): {item['references']}\\n\")\n",
        "    # Generate search queries for unanswered questions\n",
        "    if item['answerable'] == False:\n",
        "        if item.get(\"references\"):\n",
        "            search_queries = generate_search_queries(f\"{item['question']}\\n{item['references']}\")\n",
        "        else:\n",
        "            search_queries = generate_search_queries(item[\"question\"])\n",
        "        # save search queries to json file\n",
        "        with open(\"search_queries.json\", \"a\") as outfile:\n",
        "            json.dump(search_queries, outfile)\n",
        "        print(f\"Search Queries: {search_queries}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "create_model() takes exactly 1 positional argument (0 given)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoneType, Transport, ProxiesTypes\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __title__, __version__\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_response\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m APIResponse \u001b[38;5;28;01mas\u001b[39;00m APIResponse, AsyncAPIResponse \u001b[38;5;28;01mas\u001b[39;00m AsyncAPIResponse\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_client.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self, override\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resources, _exceptions\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Querystring\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     NOT_GIVEN,\n\u001b[1;32m     15\u001b[0m     Omit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     RequestOptions,\n\u001b[1;32m     21\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Beta,\n\u001b[1;32m      5\u001b[0m     AsyncBeta,\n\u001b[1;32m      6\u001b[0m     BetaWithRawResponse,\n\u001b[1;32m      7\u001b[0m     AsyncBetaWithRawResponse,\n\u001b[1;32m      8\u001b[0m     BetaWithStreamingResponse,\n\u001b[1;32m      9\u001b[0m     AsyncBetaWithStreamingResponse,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Chat,\n\u001b[1;32m     13\u001b[0m     AsyncChat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     AsyncChatWithStreamingResponse,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Audio,\n\u001b[1;32m     21\u001b[0m     AsyncAudio,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     AsyncAudioWithStreamingResponse,\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Beta,\n\u001b[1;32m      5\u001b[0m     AsyncBeta,\n\u001b[1;32m      6\u001b[0m     BetaWithRawResponse,\n\u001b[1;32m      7\u001b[0m     AsyncBetaWithRawResponse,\n\u001b[1;32m      8\u001b[0m     BetaWithStreamingResponse,\n\u001b[1;32m      9\u001b[0m     AsyncBetaWithStreamingResponse,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Threads,\n\u001b[1;32m     13\u001b[0m     AsyncThreads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     AsyncThreadsWithStreamingResponse,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Assistants,\n\u001b[1;32m     21\u001b[0m     AsyncAssistants,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     AsyncAssistantsWithStreamingResponse,\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/beta.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     Threads,\n\u001b[1;32m      7\u001b[0m     AsyncThreads,\n\u001b[1;32m      8\u001b[0m     ThreadsWithRawResponse,\n\u001b[1;32m      9\u001b[0m     AsyncThreadsWithRawResponse,\n\u001b[1;32m     10\u001b[0m     ThreadsWithStreamingResponse,\n\u001b[1;32m     11\u001b[0m     AsyncThreadsWithStreamingResponse,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_property\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     Assistants,\n\u001b[1;32m     16\u001b[0m     AsyncAssistants,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     AsyncAssistantsWithStreamingResponse,\n\u001b[1;32m     21\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/threads/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Runs,\n\u001b[1;32m      5\u001b[0m     AsyncRuns,\n\u001b[1;32m      6\u001b[0m     RunsWithRawResponse,\n\u001b[1;32m      7\u001b[0m     AsyncRunsWithRawResponse,\n\u001b[1;32m      8\u001b[0m     RunsWithStreamingResponse,\n\u001b[1;32m      9\u001b[0m     AsyncRunsWithStreamingResponse,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Threads,\n\u001b[1;32m     13\u001b[0m     AsyncThreads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     AsyncThreadsWithStreamingResponse,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Messages,\n\u001b[1;32m     21\u001b[0m     AsyncMessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     AsyncMessagesWithStreamingResponse,\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/threads/runs/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Runs,\n\u001b[1;32m      5\u001b[0m     AsyncRuns,\n\u001b[1;32m      6\u001b[0m     RunsWithRawResponse,\n\u001b[1;32m      7\u001b[0m     AsyncRunsWithRawResponse,\n\u001b[1;32m      8\u001b[0m     RunsWithStreamingResponse,\n\u001b[1;32m      9\u001b[0m     AsyncRunsWithStreamingResponse,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msteps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Steps,\n\u001b[1;32m     13\u001b[0m     AsyncSteps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     AsyncStepsWithStreamingResponse,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncRunsWithStreamingResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m ]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/threads/runs/runs.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _legacy_response\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msteps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Steps,\n\u001b[1;32m     13\u001b[0m     AsyncSteps,\n\u001b[1;32m     14\u001b[0m     StepsWithRawResponse,\n\u001b[1;32m     15\u001b[0m     AsyncStepsWithRawResponse,\n\u001b[1;32m     16\u001b[0m     StepsWithStreamingResponse,\n\u001b[1;32m     17\u001b[0m     AsyncStepsWithStreamingResponse,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, Body, Query, Headers, NotGiven\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maybe_transform\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/beta/threads/runs/steps.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SyncAPIResource, AsyncAPIResource\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_response\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_streamed_response_wrapper, async_to_streamed_response_wrapper\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpagination\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SyncCursorPage, AsyncCursorPage\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AsyncPaginator,\n\u001b[1;32m     18\u001b[0m     make_request_options,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunStep, step_list_params\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/pagination.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Generic, TypeVar, Optional, cast\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Protocol, override, runtime_checkable\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasePage, PageInfo, BaseSyncPage, BaseAsyncPage\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncPage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncPage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncCursorPage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncCursorPage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_base_client.py:196\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m options\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected PageInfo state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseSyncPage\u001b[39;00m(\u001b[43mBasePage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_T\u001b[49m\u001b[43m]\u001b[49m, Generic[_T]):\n\u001b[1;32m    197\u001b[0m     _client: SyncAPIClient \u001b[38;5;241m=\u001b[39m pydantic\u001b[38;5;241m.\u001b[39mPrivateAttr()\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_private_attributes\u001b[39m(\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    201\u001b[0m         client: SyncAPIClient,\n\u001b[1;32m    202\u001b[0m         model: Type[_T],\n\u001b[1;32m    203\u001b[0m         options: FinalRequestOptions,\n\u001b[1;32m    204\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/generics.py:46\u001b[0m, in \u001b[0;36mGenericModel.__class_getitem__\u001b[0;34m(cls, params)\u001b[0m\n\u001b[1;32m     42\u001b[0m validators \u001b[38;5;241m=\u001b[39m gather_all_validators(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     43\u001b[0m fields: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[Type[Any], Any]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     k: (v, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__[k]\u001b[38;5;241m.\u001b[39mfield_info) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m concrete_type_hints\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\n\u001b[1;32m     45\u001b[0m }\n\u001b[0;32m---> 46\u001b[0m created_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__base__\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__config__\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__validators__\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m created_model\u001b[38;5;241m.\u001b[39mConfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mConfig\n\u001b[1;32m     55\u001b[0m created_model\u001b[38;5;241m.\u001b[39m__concrete__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/main.py:952\u001b[0m, in \u001b[0;36mpydantic.main.create_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_model() takes exactly 1 positional argument (0 given)"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(api_key= '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')\n",
        "\n",
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": \"You are a kind helpfull assistant\"}\n",
        "]\n",
        "\n",
        "while True:\n",
        "    message = input(\"user:\")\n",
        "    if message:\n",
        "        messages.append(\n",
        "            {\"role\":\"user\", \"content\": message},\n",
        "        )\n",
        "        completion = client.chat.completions.create(\n",
        "            messages = messages,\n",
        "            model = \"gpt-3.5-turbo\"\n",
        "        )\n",
        "\n",
        "    reply = completion.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply}\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install paper-qa\n",
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lee2018.pdf\n",
            "s41556-023-01246-1.pdf\n",
            "s00068-018-0954-3.pdf\n",
            "s13578-022-00805-7.pdf\n",
            "Advanced Science - 2023 - Maffeis - Synthetic Cells Revisited Artificial Cells Construction Using Polymeric Building.pdf\n",
            "elife-70899-v2.pdf\n",
            "izawa2017.pdf\n",
            "fonc-11-672781.pdf\n",
            "s41392-020-00440-z.pdf\n",
            "Mitochondria and cell signalling - PMC.pdf\n",
            "nihms158858.pdf\n",
            "nihms-1621944.pdf\n",
            "s13619-023-00158-7.pdf\n",
            "nihms804627.pdf\n",
            "elife-70899-figures-v2.pdf\n"
          ]
        }
      ],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('/home/epas/Programming/ResearchAgentSwarm/Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    #docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "    print(paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaperqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Docs\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     docs \u001b[38;5;241m=\u001b[39m Docs(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, Docs, PromptCollection, Doc, Text, Context\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:42\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, CallbackFactory, Context, Doc, DocKey, PromptCollection, Text\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     gather_with_concurrency,\n\u001b[1;32m     31\u001b[0m     get_llm_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     strip_citations,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDocs\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marbitrary_types_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmart_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"A collection of documents to be used for answering questions.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/paperqa/docs.py:50\u001b[0m, in \u001b[0;36mDocs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m texts_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m doc_index: Optional[VectorStore] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m llm: Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel] \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m summary_llm: Optional[Union[\u001b[38;5;28mstr\u001b[39m, BaseLanguageModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 4\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mpaperscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbayesian model selection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded-papers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:578\u001b[0m, in \u001b[0;36msearch_papers\u001b[0;34m(query, limit, pdir, semantic_scholar_api_key, _paths, _limit, _offset, logger, year, verbose, scraper, batch_size, search_type)\u001b[0m\n\u001b[1;32m    576\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    577\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_search_papers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:350\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    203\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:495\u001b[0m, in \u001b[0;36ma_search_papers.<locals>.process_paper\u001b[0;34m(paper, i)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_paper\u001b[39m(paper, i):\n\u001b[1;32m    494\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaperId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape(paper, path, i\u001b[38;5;241m=\u001b[39mi, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    497\u001b[0m         bibtex \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitationStyles\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbibtex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/scraper.py:63\u001b[0m, in \u001b[0;36mScraper.scrape\u001b[0;34m(self, paper, path, i, logger)\u001b[0m\n\u001b[1;32m     61\u001b[0m scraper \u001b[38;5;241m=\u001b[39m scrapers[j]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mfunction(paper, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscraper\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mcheck_pdf \u001b[38;5;129;01mor\u001b[39;00m check_pdf(path)):\n\u001b[1;32m     65\u001b[0m         scrape_result[scraper\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:239\u001b[0m, in \u001b[0;36mopenaccess_scraper\u001b[0;34m(paper, path, session)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m link_to_pdf(url, path, session)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:103\u001b[0m, in \u001b[0;36mlink_to_pdf\u001b[0;34m(url, path, session)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=1,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Research Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Function to clean entities based on new lines and remove leading hyphens\n",
        "def clean_and_separate_entities(entities_list):\n",
        "    entities_str = '\\n'.join(entities_list)\n",
        "    cleaned_entities = []\n",
        "    dirty_entities = []\n",
        "\n",
        "    for line in entities_str.split('\\n'):\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line.startswith('-'):\n",
        "            # Remove the leading hyphen and any extra space after it\n",
        "            cleaned_entities.append(stripped_line.lstrip('-').strip())\n",
        "        else:\n",
        "            dirty_entities.append(stripped_line)\n",
        "\n",
        "    return cleaned_entities, dirty_entities\n",
        "def test_clean_and_separate_entities():\n",
        "    \n",
        "    # Define the summary JSON file path\n",
        "    SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "    # Read the summaries.json file\n",
        "    with open(SUMMARY_JSON, \"r\") as file:\n",
        "        summaries_json = json.load(file)\n",
        "\n",
        "    # Extract the first entities entry\n",
        "    first_entities_list = summaries_json[0][\"entities\"][0]\n",
        "\n",
        "    # Clean the entities and separate the uncleaned ones\n",
        "    cleaned_entities, dirty_entities = clean_and_separate_entities(first_entities_list)\n",
        "\n",
        "    # Save the results to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp_file:\n",
        "        json.dump({\n",
        "            \"cleaned_entities\": cleaned_entities,\n",
        "            \"dirty_entities\": dirty_entities\n",
        "        }, temp_file, indent=4)\n",
        "\n",
        "    print(\"Results saved in:\", temp_file.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_topics_with_justification(topic_text):\n",
        "    # Regular expression pattern for identifying topics with their justifications\n",
        "    topic_pattern = re.compile(r'(\\d+)\\.\\s+([^\\n]+)(\\n\\s+-[^\\n]+)*')\n",
        "    topics = topic_pattern.findall(topic_text)\n",
        "    \n",
        "    extracted_topics = []\n",
        "    for match in topics:\n",
        "        topic = match[1].strip()\n",
        "        justification = ' '.join(match[2].split('\\n')).strip()\n",
        "        # Remove \"Justification:\" if it starts with it\n",
        "        if justification.lower().startswith('- justification:'):\n",
        "            justification = justification[len('- justification:'):].strip()\n",
        "        # Remove the - if it starts with it\n",
        "        if justification.startswith('-'):\n",
        "            justification = justification[1:].strip()\n",
        "        extracted_topics.append({\"topic\": topic, \"justification\": justification})\n",
        "\n",
        "    return extracted_topics\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_topics_with_justification():\n",
        "    # Adjusted topic text\n",
        "    topic_text_list = []\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of Mitochondria in Energy Production, Signaling, and Apoptosis\\n   - Mitochondria as the powerhouse of the cell\\n   - Role of mitochondria in energy production, signaling, and apoptosis\\n   - Significance of studying mitochondrial function and involvement in diseases\\n\\n2. Challenges with Traditional Methods of Mitochondrial Isolation\\n   - Limitations of traditional methods like differential centrifugation\\n   - Potential damage to mitochondrial double membrane and variable viability\\n\\n3. Innovative Techniques for Mitochondrial Isolation\\n   - Nitrogen cavitation for gentle disruption and release of intact mitochondria\\n   - Affinity purification using anti-TOM22 magnetic beads for efficient isolation\\n   - Filtration-based methods to reduce isolation time and improve viability\\n   - Differential isopycnic density gradient centrifugation for separation based on buoyant density\\n\\n4. Quality Control Measures for Validating Mitochondrial Isolation\\n   - Assessment of mitochondrial respiration, metabolic activity, protein import, and membrane fusion\\n   - High-resolution respirometry and bioluminescent measurements of ATP synthesis\\n\\n5. Importance of Continued Refinement and Standardization of Techniques\\n   - Advancing understanding of mitochondrial biology and implications in health and disease\\n   - Need for standardized protocols to facilitate comparisons and translation of research findings into clinical applications\\n\\n**Notes**: The summary provides a comprehensive overview of the importance of mitochondria, challenges with traditional methods of isolation, innovative techniques for isolation, quality control measures, and the need for continued refinement and standardization. The topics cover the main ideas and themes discussed in the summary, providing a clear and comprehensive analysis of the content.\") \n",
        "    topic_text_list.append(\"**Topic List:**\\n\\n1. Challenges in isolating intact mitochondria from plant cells\\n   - Cell walls, mitochondrial membranes, and large amounts of starting material\\n2. Comprehensive protocol for isolating intact mitochondria from plant cells\\n   - Grinding, filtering, centrifuging, and resuspending\\n3. Characterization and analysis of isolated mitochondria\\n   - Purity, integrity, and functionality assessment\\n   - Techniques: protein profiling, enzymatic activity assays, respiratory chain measurements, and oxygen consumption analysis\\n4. Storage of purified mitochondria\\n   - Long-term storage at -80°C\\n5. Adaptation of isolation process for different tissue types and plant species\\n   - Consideration of phenolic compounds and metabolite profiles\\n6. Validation and controls for quality and functionality assurance\\n7. Downstream applications of isolated mitochondria\\n   - Protein and tRNA uptake experiments, enzyme activity assays, Western blot analyses, and mass spectrometry analyses\\n\\n**Notes:**\\n- The revised summary provides a comprehensive overview of the topic, covering various aspects of isolating intact mitochondria from plant cells.\\n- The topics are specific and non-repetitive, ensuring a clear and distinct representation of the core themes.\\n- The summary is focused on the technical process and considerations involved in isolating mitochondria, as well as the analysis and applications of the isolated mitochondria.\")\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction\\n    - Justification: The summary highlights the crucial role of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction.\\n\\n2. Significance of gentle and effective mitochondrial isolation techniques\\n    - Justification: The summary emphasizes the importance of gentle and effective isolation techniques for studying mitochondrial biology and developing mitochondrial-based therapies.\\n\\n3. Overview of macroscale mitochondrial isolation techniques\\n    - Justification: The summary discusses macroscale mitochondrial isolation techniques, such as manual homogenization and differential filtration-based isolation.\\n\\n4. Advancements in microscale and nanoscale mitochondrial isolation techniques\\n    - Justification: The summary mentions microscale and nanoscale techniques, including microfluidic techniques and nanoprobe-based technologies, for mitochondrial isolation.\\n\\n5. Breakthroughs in sub-cellular isolation techniques for mitochondria\\n    - Justification: The summary highlights breakthroughs in sub-cellular isolation techniques that enable the isolation of mitochondria from subcellular compartments with minimal disruption.\\n\\n6. Challenges in mitochondrial isolation techniques\\n    - Justification: The summary mentions challenges such as the presence of whole cell contaminants in mitochondrial isolates and the time sensitivity of isolated mitochondria.\\n\\n7. Emerging therapeutic approach: Autologous mitochondrial transplants\\n    - Justification: The summary discusses the development of autologous mitochondrial transplants as an emerging therapeutic approach.\\n\\n8. Contributions of the London Centre for Nanotechnology and the McCully laboratory\\n    - Justification: The summary mentions the significant contributions of the London Centre for Nanotechnology and the McCully laboratory in optimizing differential filtration-based mitochondrial isolation for use in cellular models.\\n\\n9. Role of Stem Cell Research & Therapy in advancing mitochondrial medicine\\n    - Justification: The summary highlights the role of Stem Cell Research & Therapy in providing in-depth overviews of advancements in mitochondrial research and facilitating the development of novel therapies for mitochondrial diseases.\")\n",
        "    topic_text_list.append(\"Topics:\\n1. Genetic modifications to enhance mitochondrial autonomy\\n   - Justification: The main focus of the report is exploring genetic modifications to enhance the autonomy of mitochondria from nuclear-encoded proteins and functions.\\n2. Role of mitochondria in cellular function\\n   - Justification: The report highlights the crucial role played by mitochondria in cellular function.\\n3. Coordination between mtDNA and nuclear DNA\\n   - Justification: The report discusses the coordination required between mtDNA and nuclear DNA, as most proteins are encoded by nuclear DNA.\\n4. Therapeutic strategies for mitochondrial diseases\\n   - Justification: The report mentions that enhancing mitochondrial autonomy could lead to new therapeutic strategies for mitochondrial diseases.\\n5. Research on genome engineering, programmable nucleases, and base editors\\n   - Justification: The report mentions that recent research in genome engineering, programmable nucleases, and base editors shows promise for treating hereditary mitochondrial diseases.\\n6. Challenges in genetic manipulation of mtDNA\\n   - Justification: The report discusses challenges such as mtDNA mutations, resistance to genetic manipulation, and limitations in mtDNA recombination.\\n7. Advancements in protein-only gene editing platforms\\n   - Justification: The report mentions advancements in protein-only gene editing platforms as potential solutions to the challenges in genetic manipulation of mtDNA.\\n8. Somatic mitochondrial DNA-replaced cells\\n   - Justification: The report mentions the generation of somatic mitochondrial DNA-replaced cells as a potential solution to the challenges in genetic manipulation of mtDNA.\\n9. Mitochondrial nucleoids and their role in maintaining genetic autonomy\\n   - Justification: The report highlights the concept of mitochondrial nucleoids and their role in maintaining genetic autonomy as a key area of study.\\n10. Mitochondrial epigenomics and gene expression regulation\\n    - Justification: The report emphasizes the importance of understanding mitochondrial epigenomics and gene expression regulation in different cellular contexts, including stress conditions, for identifying genetic modifications that could enhance mitochondrial autonomy.\")\n",
        "    for topic_text in topic_text_list:\n",
        "        extracted_topics = extract_topics_with_justification(topic_text)\n",
        "        print(f'Extracted topics: {extracted_topics}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor url in urls:\\n    try:\\n        pdf =  pdfx.PDFx(url)\\n        metadata = pdf.get_metadata()\\n        print(f\\'Metadata: {metadata}\\')\\n        references_list = pdf.get_references()\\n        print(f\\'References: {references_list}\\')\\n        references_dict = pdf.get_references_as_dict()\\n        print(f\\'References dict: {references_dict}\\')\\n        papers = paperscraper.link_to_pdf(url, pdir=\\'downloaded-papers\\')\\n        print(f\\'Papers: {papers}\\')\\n    except:\\n        print(\"Error in extracting references\")\\n        continue\\n#pdf.download_pdfs(\"target-directory\")\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#!pip install pdfx\n",
        "import pdfx\n",
        "#!pip install paperscraper\n",
        "#import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def test_extract_urls():\n",
        "    # Define the reference text\n",
        "    reference_text = \"\"\"\\n\\nAmerican Institute of Physics. (2023). The powerhouse of the future: Artificial cells. Phys.org. Retrieved from https://phys.org/news/2023-03-powerhouse-future-artificial-cells.html\\n\\nNational Institutes of Health. (2023). Artificial mitochondria transfer (AMT) and transplant. PMC. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5511681/\\n\\nNature. (2023). Spatiotemporal simulations of mitochondrial dynamics. Nature.com. Retrieved from https://www.nature.com/articles/s41598-019-54159-1\\n\\nSogang University & Harbin Institute of Technology. (2023). Artificial organelles for sustainable chemical energy conversion and production: Artificial mitochondria and chloroplasts. Biophysics Reviews. Retrieved from https://publishing.aip.org/publications/latest-content/the-powerhouse-of-the-future-artificial-cells/\"\"\"\n",
        "\n",
        "    urls = extract_urls(reference_text)\n",
        "    print(f'Extracted URLs: {urls}')\n",
        "\n",
        "#pdf = pdfx.PDFx(\"filename-or-url.pdf\")\n",
        "#urls = ['/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/2308.00352.pdf']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for url in urls:\n",
        "    try:\n",
        "        pdf =  pdfx.PDFx(url)\n",
        "        metadata = pdf.get_metadata()\n",
        "        print(f'Metadata: {metadata}')\n",
        "        references_list = pdf.get_references()\n",
        "        print(f'References: {references_list}')\n",
        "        references_dict = pdf.get_references_as_dict()\n",
        "        print(f'References dict: {references_dict}')\n",
        "        papers = paperscraper.link_to_pdf(url, pdir='downloaded-papers')\n",
        "        print(f'Papers: {papers}')\n",
        "    except:\n",
        "        print(\"Error in extracting references\")\n",
        "        continue\n",
        "#pdf.download_pdfs(\"target-directory\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Questions: [('Content-Based Question', 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'), ('Analytical Question', 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'), ('Creative/Scenario-Based Question', 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'), ('Contextual/Relational Question', 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'), ('User-Interactive Question', 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Content-Based Question', 'question': 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'}, {'question_type': 'Analytical Question', 'question': 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'}, {'question_type': 'User-Interactive Question', 'question': 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?'}]\n",
            "Questions: [('Analytical Question', 'How do theoretical models help in understanding mitochondrial ATP production?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'), ('Contextual/Relational Question', 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'), ('User-Interactive Question', 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?')]\n",
            "Extracted hypothetical questions: [{'question_type': 'Analytical Question', 'question': 'How do theoretical models help in understanding mitochondrial ATP production?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'}, {'question_type': 'User-Interactive Question', 'question': 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_hypothetical_questions(hypothetical_questions_text):\n",
        "    # Regular expression pattern for identifying hypothetical questions\n",
        "    question_pattern = re.compile(r'\\d+\\.\\s+([A-Za-z\\/-]+ Question):\\n\\s+-\\s+([^\\n]+)')\n",
        "    questions = question_pattern.findall(hypothetical_questions_text)\n",
        "    print(f'Questions: {questions}')\n",
        "    if len(questions) == 0:\n",
        "        return hypothetical_questions_text\n",
        "    return [{\"question_type\": question_type, \"question\": question} for question_type, question in questions]\n",
        "def test_extract_hypothetical_questions():\n",
        "    # Example hypothetical questions text\n",
        "    hypothetical_questions_text_1 = \"1. Content-Based Question:\\n   - How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?\\n\\n2. Analytical Question:\\n   - What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?\\n\\n5. User-Interactive Question:\\n   - What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?\"\n",
        "    hypothetical_questions_text_2 = \"1. Content-Based Question: \\n   - What does this report investigate regarding mitochondrial ATP production?\\n   - How does this report contribute to our understanding of mitochondrial function?\\n   - What are the key findings regarding the replication of mitochondrial ATP production outside the cellular environment?\\n\\n2. Analytical Question:\\n   - How do theoretical models help in understanding mitochondrial ATP production?\\n   - What experimental evidence supports the concept of artificial organelles for ATP synthesis?\\n   - What are the implications of studying mitochondrial dynamics and stress responses for ex vivo methods of ATP synthesis?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?\\n   - If artificial organelles capable of ATP synthesis were successfully developed, what potential applications could they have in various industries?\\n   - How might the understanding of mitochondrial dynamics and stress responses lead to the development of innovative approaches for ATP synthesis?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?\\n   - In what ways does the replication of mitochondrial ATP production outside cells build upon previous studies in the field?\\n   - How do the findings in this report align with or challenge existing theories and models of mitochondrial function?\\n\\n5. User-Interactive Question:\\n   - How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?\\n   - Can you think of any potential limitations or ethical considerations in developing artificial organelles for ATP synthesis?\\n   - What questions or areas of research would you like to see explored further in the study of mitochondrial dynamics and stress responses?\"\n",
        "    hypothetical_questions = []\n",
        "    hypothetical_questions.append(hypothetical_questions_text_1)\n",
        "    hypothetical_questions.append(hypothetical_questions_text_2)\n",
        "    for hypothetical_questions_text in hypothetical_questions:\n",
        "        extracted_hypothetical_questions = extract_hypothetical_questions(hypothetical_questions_text)\n",
        "        print(f'Extracted hypothetical questions: {extracted_hypothetical_questions}')\n",
        "\n",
        "test_extract_hypothetical_questions()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'tailored isolation protocol'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimized damage to ensure the integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_entity_relationships(entity_relationships_text):\n",
        "    # Regular expression pattern for identifying entity relationships\n",
        "    entity_pattern = re.compile(r'\\d+\\.\\s+\\((.+?),\\s+(.+?),\\s+(.+?)\\)')\n",
        "    entity_relationships = entity_pattern.findall(entity_relationships_text)\n",
        "    return [{\"subject\": relationship[0], \"relationship\": relationship[1], \"target\": relationship[2]} for relationship in entity_relationships]\n",
        "\n",
        "# Example entity relationships text\n",
        "entity_relationships_text =  \"Entity Relationships:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n30. (mitochondria, isolated with, tailored isolation protocol)\\n31. (mitochondria, isolated with, minimized damage to ensure the integrity)\\n32. (mitochondria, isolated with, reduced contamination from other organelles)\\n33. (mitochondria, isolated with, improved methods)\\n34. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n35. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n36. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n37. (mitochondria, isolated with, minimal contamination from other organelles)\\n38. (mitochondria, isolated with, improved methods)\\n39. (mitochondria, isolated with, slight modifications)\\n40. (mitochondria, isolated with, traditional plant protoplast isolation)\\n41. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n42. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n43. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n44. (mitochondria, isolated with, minimal contamination from other organelles)\\n45. (mitochondria, isolated from, Arabidopsis thaliana)\\n46. (mitochondria, isolated using, continuous colloidal density gradients)\\n47. (mitochondria, isolated at, 4 °C)\\n48. (mitochondria, used for, protein and tRNA uptake experiments)\\n49. (mitochondria, used for, enzyme activity assays)\\n50. (mitochondria, used for, western blot analyses)\\n51. (mitochondria, used for, mass spectrometry analyses)\\n52. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n53. (mitochondria, assessed for, purity and integrity)\\n54. (mitochondria, assessed using, proteinase digestion assays)\\n55. (mitochondria, assessed using, electron microscopy)\\n56. (mitochondria, assessedThe article discusses the protocol for isolating mitochondria from plant cells. Mitochondria are double-membraned organelles responsible for energy production in eukaryotic cells. The isolation of mitochondria is crucial for various studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays.\\n\\nThe isolation process is challenging due to the presence of cell walls, vacuoles, and secondary metabolites in plant cells. The protocol must be tailored to minimize damage to the mitochondria and ensure their integrity. Specificity in isolation protocols is required as different plant species and tissue types have varying phenolic compounds and metabolite profiles. Earlier methods led to contamination with nuclei and chloroplasts, but recent advancements have improved isolation methods, reducing the need for heavy labor, expensive equipment, and large amounts of starting material.\\n\\nThe protocol for isolating intact mitochondria involves several steps. First, the preparation of grinding medium, wash buffer, and gradient solutions is necessary. The plant material is then homogenized in the grinding medium to release the mitochondria, which are then filtered and centrifuged to pellet the mitochondria. The mitochondrial pellet is resuspended in the wash buffer. Oxygen consumption measurements are crucial for determining the intactness and functional capacity of the isolated mitochondria. Evaluation of mitochondrial purity and integrity can be done through proteinase digestion assays, electron microscopy, and checks of mitochondrial membrane potential and electron transport chain activity.\\n\\nOnce purified, the isolated mitochondria can be used for various studies, including protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses. For mass spectrometry analyses, targeted multiple reaction monitoring (MRM) or quantification by dimethyl or other isotope labels can be employed.\\n\\nIn conclusion, the isolation of mitochondria from plant cells is a delicate process that requires careful consideration of the specific requirements of the plant species and tissue type. Recent advancements have made the process more effective and accessible for a range of tissue types and species, allowing for a broader application of mitochondrial studies across different plant species.\\n\\nReferences:\\n- Plant Methods. (2015). https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0099-x\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908444/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7640673/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687074/Extraction and Categorization:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, used for, protein and tRNA uptake experiments)\\n30. (mitochondria, used for, enzyme activity assays)\\n31. (mitochondria, used for, western blot analyses)\\n32. (mitochondria, used for, mass spectrometry analyses)\\n33. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n34. (mitochondria, assessed for, purity and integrity)\\n35. (mitochondria, assessed using, proteinase digestion assays)\\n36. (mitochondria, assessed using, electron microscopy)\\n37. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n38. (mitochondria, assessed using, electron transport chain activity measurement)\\n39. (mitochondria, isolated from, Arabidopsis thaliana)\\n40. (mitochondria, isolated using, continuous colloidal density gradients)\\n41. (mitochondria, isolated at, 4 °C)\"\n",
        "\n",
        "# Clean the entity relationships\n",
        "cleaned_entity_relationships = clean_entity_relationships(entity_relationships_text)\n",
        "\n",
        "# Output the cleaned entity relationships\n",
        "cleaned_entity_relationships\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pydantic 2.0.3\n",
            "Uninstalling pydantic-2.0.3:\n",
            "  Successfully uninstalled pydantic-2.0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping instructor as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (0.26.0)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
            "Requirement already satisfied: sniffio in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: certifi in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
            "Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "Using cached pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
            "Installing collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.3.0\n",
            "    Uninstalling pydantic_core-2.3.0:\n",
            "      Successfully uninstalled pydantic_core-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "paper-qa 3.13.4 requires openai<1, but you have openai 1.8.0 which is incompatible.\n",
            "paper-qa 3.13.4 requires pydantic<2, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.5.3 pydantic-core-2.14.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.venv/lib/python3.11/site-packages (3.0.1)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'instructor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'instructor'"
          ]
        }
      ],
      "source": [
        "!pip uninstall pydantic -y\n",
        "!pip uninstall instructor -y\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.7.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.5.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.14.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.7.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.mmd\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Questions: [('Content-Based Question', 'How have researchers demonstrated the conductivity of doped molecular conductors at room temperature?'), ('Analytical Question', 'What are the oxidation and reduction reactions involved in the synthesis of organic conductors?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where the crystal packing of a charge-transfer salt is modified through sublimation. How might this affect the conductivity of the material?'), ('Contextual/Relational Question', 'How does the presence of iodine atoms within the columnar cavity of the crystal structures impact the conductivity of neutral-radical charge-transfer salts?'), ('User-Interactive Question', 'Have you ever encountered any materials in your everyday life that exhibit conductivity? How do you think the crystal structure of those materials might contribute to their conductivity?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Questions: [('Content-Based Question', 'What are the different phases of [HCN2S2]2 that were prepared and characterized in the study?'), ('Analytical Question', 'How did the doped material exhibit magnetic and conductivity characteristics?'), ('Creative/Scenario-Based Question', 'If the crystal growth of the triclinic phase involved loading in air-sensitive conditions, how would this affect the efficiency of space filling?'), ('Contextual/Relational Question', 'How do the crystal structures of the triclinic and doped hexagonal phases of [HCN2S2]2 differ in terms of their atomic coordinates, bond lengths, and angles?'), ('User-Interactive Question', 'If you were to adjust the stoichiometry to a different ratio during the crystal growth process, how do you think it would affect the structural properties of the doped material?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Questions: [('Content-Based Question', 'How does the triclinic [HCN2S2]2*[N2]0a4 structure arrange the molecular tubes?'), ('Analytical Question', 'What is the significance of modeling two centers of electron density with nitrogen atoms in the crystal structure?'), ('Creative/Scenario-Based Question', 'If the size of the columnar cavity in the crystal structure were increased, how would it affect the overall symmetry of the arrangement?'), ('Contextual/Relational Question', 'How does the estimated average diameter of the columnar cavity relate to the distances of the innermost sulfur atoms of the dimer units to the I axis?'), ('User-Interactive Question', 'How might the triclinic [HCN2S2]2*[N2]0a4 structure be applied in the field of materials science or nanotechnology?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How is the columnar cavity size estimated in this study?'), ('Analytical Question', 'What is the range of distances between the innermost sulfur atoms of the three dimer units and the I axis?'), ('Creative/Scenario-Based Question', 'If the average diameter of the cavity is 7.43 A, how might this impact the properties or behavior of the molecular tubes?'), ('Contextual/Relational Question', 'How does the close-packed configuration of the molecular tubes contribute to the near-hexagonal crystal symmetry?'), ('User-Interactive Question', 'Can you think of any other ways to estimate the size of the columnar cavity in this study?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How is the columnar cavity size estimated based on the distances of the innermost sulfur atoms to the I axis?'), ('Analytical Question', 'What is the significance of the average diameter of the cavity being 7.43 A?'), ('Creative/Scenario-Based Question', 'If the columnar cavity size was smaller, how would it affect the close-packed arrangement of the molecular tubes?'), ('Contextual/Relational Question', 'How does the presence of nitrogen atoms with fractional occupancies impact the crystal symmetry of the molecular tubes?'), ('User-Interactive Question', 'How would you explain the concept of electron density centers within the tubes to someone unfamiliar with the experimental section?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How is the columnar cavity size estimated in the given context?'), ('Analytical Question', 'What is the significance of the range of distances between the outermost sulfur atoms and the I axis in estimating the columnar cavity size?'), ('Creative/Scenario-Based Question', 'Imagine if the range of distances between the outermost sulfur atoms and the I axis was much larger, how would that affect the estimation of the columnar cavity size?'), ('Contextual/Relational Question', 'How does the close-packed configuration of the molecular tubes contribute to the near-hexagonal crystal symmetry observed in the arrangement?'), ('User-Interactive Question', 'Can you think of any other methods or techniques that could be used to estimate the columnar cavity size in a similar context?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How is the columnar cavity size estimated in the given structure?'), ('Analytical Question', 'What is the significance of the molecular repeat distance in the doped structure?'), ('Creative/Scenario-Based Question', 'If the iodine atoms were evenly distributed along the cavities, how would the Raman spectra of the doped compound be affected?'), ('Contextual/Relational Question', 'How does the close-packed near-hexagonal crystal symmetry contribute to the arrangement of the molecular tubes in the structure?'), ('User-Interactive Question', 'How would you explain the observed changes in the Raman spectra of the undoped and iodine-doped compounds to someone with no background in chemistry?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'What is the concentration of spin-bearing defects in [1,4-(S2N2C)-CHHHCS2]1.3?'), ('Analytical Question', 'What is the relationship between temperature and spin susceptibility in [1,4-(S2N2C)CHHC(S2)]1?'), ('Creative/Scenario-Based Question', 'How might the internal rearrangement of columnar cavities in [HCNS2]2 when iodine is doped affect its properties?'), ('Contextual/Relational Question', 'How do the Raman spectra of triclinic [HCNS2]2 and [HCNS2]2(I)1.1 differ, and what could be the cause of the spike near 2800 cm-1 in the doped material?'), ('User-Interactive Question', 'How could the sensitivity of [HCN3S2]6[[1]]1 to oxygen and moisture impact its analysis and understanding of its low-temperature structure and transport properties?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Questions: [('Content-Based Question', 'How does the material undergo a sublimation cycle in glass tubes and what is the resulting formation?'), ('Analytical Question', 'What methods are used to establish the elemental composition and solve the crystal structures of the material?'), ('Creative/Scenario-Based Question', 'Imagine you are handling the crystals briefly in dry air for X-ray mounting. What precautions would you take to ensure their stability?'), ('Contextual/Relational Question', 'How does the inclusion of nitrogen atoms and N2 molecules in the material improve the refinement process?'), ('User-Interactive Question', 'How do you think the presence of dimer units and radical units in the material affects its crystal growth process and properties?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'What technique was used to measure the magnetic susceptibility of [HCN3]6e[I1,1]?'), ('Analytical Question', 'How did the Raman spectra recorded on the Bruker FRA 106 FT Raman module contribute to the understanding of the material?'), ('Creative/Scenario-Based Question', 'Imagine you are a researcher interested in studying the magnetic properties of [HCN3]6e[I1,1]. What other experimental techniques could you use to further investigate its behavior?'), ('Contextual/Relational Question', 'How might the financial support provided by NSERC and the National Science Foundation impact the research on [HCN3]6e[I1,1] and its magnetic susceptibility?'), ('User-Interactive Question', 'Have you ever used the Faraday technique or Raman spectroscopy in your own research? How do you think these techniques could be applied to study other materials or phenomena?')]\n",
            "Storing data for file_id: bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.json\n",
            "Successfully summarized /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-molecular-conductors-from-neutral-radical-charge-transfer-salts-preparation-and-characterization-of-an.mmd\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand.mmd\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Questions: [('Content-Based Question', 'How does H-bonding to thiolate ligands impact their donor properties in metalloproteins?'), ('Analytical Question', 'What insights have electrochemical measurements on model compounds provided regarding the effects of H-bonding on redox potentials?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where H-bonding to thiolate ligands does not weaken their donor properties. How would this affect the redox potentials in metalloproteins?'), ('Contextual/Relational Question', 'How do spectroscopic probes, such as the carbonyl ligand, help reveal electronic changes at metal centers in the context of studying the effects of H-bonding on thiolate ligands?'), ('User-Interactive Question', 'How would you design an experiment to investigate the impact of H-bonding on the donor properties of thiolate ligands in metalloproteins?')]\n",
            "Storing data for file_id: mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4161 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting dirty knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4161 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand.mmd: Error in extracting knowledge\n",
            "Error summarizing /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/mcguire-et-al-2002-discontinuum-between-a-thiolate-and-a-thiol-ligand.mmd\n",
            "Summarizing /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and.mmd\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and.json\n",
            "Questions: [('Content-Based Question', 'How does the cosublimation of benzene-bridged 1,2,3,5-dithiadiazolyl diradicals with iodine/bromine result in the formation of mixed valence salts with one-dimensional and three-dimensional structures?'), ('Analytical Question', 'What are the factors that contribute to the weakly metallic behavior observed in the mixed valence salts at room temperature?'), ('Creative/Scenario-Based Question', 'Imagine you are a chemist working on the synthesis of these mixed valence salts. How would you modify the preparation process to enhance the three-dimensionality of the crystal structures?'), ('Contextual/Relational Question', 'How do the crystal structures of the bromide and iodide salts differ in terms of their intermolecular interactions and electronic properties?'), ('User-Interactive Question', 'How do you think the charge density wave (CDW) driven metal-insulator phase transition occurring in these salts near specific temperatures could impact their potential applications in electronic devices?')]\n",
            "Storing data for file_id: bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and.json\n",
            "Questions: [('Content-Based Question', 'How does doping with iodine using sealed tube reactions contribute to the preparation of mixed valence salts?'), ('Analytical Question', 'How can the charge transfer in the structures of [3][I] and [4][I] be assessed by comparing the bond lengths with those in related compounds?'), ('Creative/Scenario-Based Question', 'Imagine a scenario where the temperature gradient is not carefully controlled during the reaction. How might this impact the formation of the compound with nested spoons and a one-dimensional electronic structure?'), ('Contextual/Relational Question', 'How does the packing arrangement in [3][I] lead to an extremely one-dimensional electronic structure, while the dovetailed packing in [4][I] affords better lateral approaches?'), ('User-Interactive Question', 'What are some potential applications for the compound [4][I] with its unique solid-state structure and one-dimensional electronic structure?')]\n",
            "Storing data for file_id: bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and\n",
            "Successfully saved data to /home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/bryan-et-al-2002-charge-transfer-salts-of-benzene-bridged-1-2-3-5-dithiadiazolyl-diradicals-preparation-structures-and.json\n",
            "Error in summarizing article: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4143 tokens (3343 in the messages, 800 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            " Occured in generate_summary function\n",
            "Error in summarizing article: argument of type 'NoneType' is not iterable\n",
            " Using last summary\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
            "Error in extracting topics: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 567\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article_info \u001b[38;5;129;01min\u001b[39;00m article_list:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 567\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43mIncrementally_Refine_Article_Summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully summarized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 479\u001b[0m, in \u001b[0;36mIncrementally_Refine_Article_Summary\u001b[0;34m(article_info)\u001b[0m\n\u001b[1;32m    476\u001b[0m knowledge_triplets \u001b[38;5;241m=\u001b[39m extract_knowledge(chunk, clean_entities\u001b[38;5;241m=\u001b[39mentities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m], dirty_entities\u001b[38;5;241m=\u001b[39mentities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirty_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Extract the topic and hypothetical questions from the refined summary\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m topic, questions \u001b[38;5;241m=\u001b[39m \u001b[43mextract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefined_sumary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Store the summary, entities, and citation\u001b[39;00m\n\u001b[1;32m    482\u001b[0m chunk_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[6], line 359\u001b[0m, in \u001b[0;36mextract_info\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_info\u001b[39m(summary):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# NLP logic to extract topic and hypothetical questions \u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         topic \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m topic:\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 324\u001b[0m, in \u001b[0;36mrequest_topics\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m    322\u001b[0m client \u001b[38;5;241m=\u001b[39m instructor\u001b[38;5;241m.\u001b[39mpatch(OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key))\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-0613\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_system_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGet Topic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     topic \u001b[38;5;241m=\u001b[39m extract_topics_with_justification(parse_response(response))\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/instructor/patch.py:392\u001b[0m, in \u001b[0;36mwrap_chatcompletion.<locals>.new_chatcompletion_sync\u001b[0;34m(response_model, validation_context, max_retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_chatcompletion_sync\u001b[39m(\n\u001b[1;32m    383\u001b[0m     response_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    388\u001b[0m ):\n\u001b[1;32m    389\u001b[0m     response_model, new_kwargs \u001b[38;5;241m=\u001b[39m handle_response_model(\n\u001b[1;32m    390\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model, kwargs\u001b[38;5;241m=\u001b[39mkwargs, mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m    391\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretry_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/instructor/patch.py:300\u001b[0m, in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retries \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Excepts ValidationError, and JSONDecodeError\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m         stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39musage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1100\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1109\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1110\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:887\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    884\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    893\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0613\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0613\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    while True:\n",
        "        topic = request_topics(summary)\n",
        "        if topic:\n",
        "            break\n",
        "    while True:\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "        if questions:\n",
        "            break\n",
        "    return topic, questions\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        extract_references_from_pdf(file_path, references_path)\n",
        "        with open(references_path) as f:\n",
        "            references = f.read()\n",
        "    else:\n",
        "        references = extract_references(file_path)  \n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        article_text = pdf_to_text(file_path)\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic, \n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\") or file_name.endswith(\".mmd\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/gpt-researcher/outputs/\" # mmd files\n",
        "article_list = get_article_list(filetype=\"mmd\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open summary.json to see the results \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ChatGPT Message Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def analyze_file(file_path, output_file_path, document_name):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().split('\\n')\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            current_message = \"\"\n",
        "            message_started = False\n",
        "            sender = \"\"\n",
        "            message_number = 0\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line.lower().startswith('user') or line.lower().startswith('chatgpt'):\n",
        "                    if message_started:  # End of a message\n",
        "                        message_number += 1\n",
        "                        word_count = count_words(current_message.strip())\n",
        "                        output_file.write(f\"{sender} Line number {i}, Message number {message_number}, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "                        current_message = \"\"\n",
        "                    message_started = True\n",
        "                    sender = \"User\" if line.lower().startswith('user') else \"ChatGPT\"\n",
        "                    continue\n",
        "                if message_started:\n",
        "                    current_message += \" \" + line\n",
        "\n",
        "            # Add the last message if it exists\n",
        "            if current_message:\n",
        "                message_number += 1\n",
        "                word_count = count_words(current_message.strip())\n",
        "                output_file.write(f\"{sender} Last message, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your text file\n",
        "# Replace 'output_messages.txt' with the path for the output file\n",
        "# Add the document name (e.g., 'ChatGPT_history.txt')\n",
        "file_path = 'ChatGPT_history.txt'\n",
        "output_file_path = 'output_messages.txt'\n",
        "document_name = 'ChatGPT_history'  # This is the document name without the extension\n",
        "analyze_file(file_path, output_file_path, document_name)\n",
        "print(\"Messages have been written to the output file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class MessageAnalysisStore:\n",
        "    def __init__(self, output_file_path):\n",
        "        self.output_file_path = output_file_path\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.output_file_path):\n",
        "            empty_data = []\n",
        "            self._save(empty_data)\n",
        "\n",
        "    def store(self, analyzed_data):\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(analyzed_data)\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.output_file_path):\n",
        "            with open(self.output_file_path, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.output_file_path, \"w\") as file:\n",
        "                json.dump(content, file, indent=4)\n",
        "            print(f\"Successfully saved data to {self.output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.output_file_path}: {e}\")\n",
        "\n",
        "# Assuming the required classes and functions from your new code are already defined and imported\n",
        "# like SummaryStore, generate_summary, get_entities, extract_knowledge, etc.\n",
        "\n",
        "def extract_messages_with_citation(lines: List[str], sender_keyword: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts messages with citation from the given lines based on the sender keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    messages_with_citation = []\n",
        "    current_message = \"\"\n",
        "    message_started = False\n",
        "    citation_info = \"\"\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(sender_keyword):\n",
        "            if message_started:\n",
        "                # End of the current message, add it to the list\n",
        "                messages_with_citation.append((current_message.strip(), citation_info))\n",
        "                current_message = \"\"\n",
        "            message_started = True\n",
        "            citation_info = line  # Capture the line with sender info as citation\n",
        "        elif message_started:\n",
        "            current_message += \" \" + line\n",
        "\n",
        "    # Add the last message if it exists\n",
        "    if current_message:\n",
        "        messages_with_citation.append((current_message.strip(), citation_info))\n",
        "\n",
        "    return messages_with_citation\n",
        "\n",
        "def analyze_conversation(message: str, citation: str, sender_keyword: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyzes a single conversation message, extracting and summarizing information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate an initial summary\n",
        "        if sender_keyword == \"ChatGPT\":\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"Entity Dense\")\n",
        "        else:\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"SPR\")\n",
        "\n",
        "        # Extract entities and knowledge\n",
        "        entities = get_entities(message)\n",
        "        knowledge = extract_knowledge(message, entities[\"clean_entities\"], entities[\"dirty_entities\"])\n",
        "\n",
        "        # Extract the topic and hypothetical questions from the summary\n",
        "        topic, questions = extract_info(initial_summary)\n",
        "\n",
        "        analyzed_data = {\n",
        "            \"id\": citation,\n",
        "            \"sender\": sender_keyword,\n",
        "            \"message\": message,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": questions,\n",
        "            \"clean_entities\": entities[\"clean_entities\"],\n",
        "            \"dirty_entities\": entities[\"dirty_entities\"],\n",
        "            \"summary\": initial_summary,\n",
        "            \"knowledge\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        return analyzed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred in analyzing conversation: {e}\")\n",
        "\n",
        "def extract_and_analyze_messages(file_path: str, output_file_path: str, sender_keyword: str, log_file_path: str):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    messages_with_citation = extract_messages_with_citation(lines, sender_keyword)\n",
        "    store = MessageAnalysisStore(output_file_path)\n",
        "    error_log = []\n",
        "\n",
        "    for message, citation in messages_with_citation:\n",
        "        try:\n",
        "            analyzed_data = analyze_conversation(message, citation, sender_keyword)\n",
        "            store.store(analyzed_data)\n",
        "            time.sleep(15)  # Delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            error_info = {\"citation\": citation, \"error\": str(e), \"timestamp\": datetime.now().isoformat()}\n",
        "            # Appending to the error log\n",
        "            error_log.append(error_info)\n",
        "            with open(log_file_path, \"a\") as log_file:\n",
        "                json.dump(error_info, log_file, indent=4)\n",
        "                log_file.write(\"\\n\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Messages analysis completed. Data saved to {output_file_path}\")\n",
        "    if error_log:\n",
        "        print(f\"Errors logged to {log_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'output_messages.txt'\n",
        "output_file_path_user = 'analyzed_user_messages.json'\n",
        "log_file_path_user = 'error_log_user.json'\n",
        "extract_and_analyze_messages(file_path, output_file_path_user, 'user', log_file_path_user)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neo4j in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (5.16.0)\n",
            "Requirement already satisfied: pytz in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from neo4j) (2023.3.post1)\n"
          ]
        }
      ],
      "source": [
        "!pip install neo4j\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MATCH (n) \n",
        "DETACH DELETE n\n",
        "\n",
        "CREATE CONSTRAINT FOR ()-[r:RELATED_TO]-() REQUIRE r.id IS UNIQUE\n",
        "\n",
        "// Shared Entities  \n",
        "MATCH (a1:ArticleID)-[:MENTIONS]->(e)<-[:MENTIONS]-(a2:ArticleID) \n",
        "WHERE e:CleanEntity OR e:DirtyEntity\n",
        "WITH DISTINCT a1, a2, collect(e.name) AS shared \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Entity'}]->(a2)\n",
        "ON CREATE SET r.entities = shared\n",
        "\n",
        "// Shared Topics\n",
        "MATCH (a1)-[:HAS_TOPIC]->(t1)<-[:HAS_TOPIC]-(a2) \n",
        "WITH a1, a2, split(t1.name, ' ') AS words1\n",
        "UNWIND words1 AS word1\n",
        "MATCH (a3)-[:HAS_TOPIC]->(t2)<-[:HAS_TOPIC]-(a4)\n",
        "WHERE word1 =~ '(?i).*?' + word1  \n",
        "WITH DISTINCT a1, a3, collect(word1) AS shared\n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Topic Word'}]->(a3)\n",
        "ON CREATE SET r.words = shared\n",
        "\n",
        "// Shared Subjects\n",
        "MATCH (a:ArticleID)-[:HAS_TRIPLET]->(s:Subject)<-[:HAS_TRIPLET]-(b:ArticleID)\n",
        "WHERE id(a) < id(b) \n",
        "WITH DISTINCT a, b, collect(s.name) AS shared \n",
        "MERGE (a)-[r:RELATED_TO {type:'Shared Subject'}]->(b)  \n",
        "ON CREATE SET r.subjects = shared\n",
        "\n",
        "// Shared URLs\n",
        "MATCH (a1)-[:HAS_REFERENCE]->(r)<-[:HAS_REFERENCE]-(a2)\n",
        "WHERE r:Reference AND id(a1) < id(a2)  \n",
        "WITH DISTINCT a1, a2, collect(r.url) AS shared  \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared URL'}]->(a2)\n",
        "ON CREATE SET r.urls = shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import dateutil.parser  # You need to install the python-dateutil package\n",
        "# 11226 13863\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp):\n",
        "    # Convert timestamp from string to Unix timestamp (seconds since epoch)\n",
        "    try:\n",
        "        timestamp_unix = dateutil.parser.parse(timestamp).timestamp()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing timestamp: {timestamp}. Error: {e}\")\n",
        "        timestamp_unix = 0  # Default to 0 or some other value in case of parsing failure\n",
        "    # Ensure ArticleID node exists (create if it doesn't)\n",
        "    session.run(f\"MERGE (articleID:ArticleID {{id: '{file_id}'}})\")\n",
        "\n",
        "    # Create or update the Article node and link it to ArticleID\n",
        "    session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) MERGE (article:Article) MERGE (articleID)-[:IDENTIFIES]->(article)\")\n",
        "\n",
        "    # Create a new Chunk node with a unique identifier, timestamp, and link it to ArticleID\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "    create_chunk_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"CREATE (chunk:Chunk {{id: '{chunk_id}', content: '{chunk_content}', timestamp: '{timestamp_unix}'}}) \"\n",
        "        f\"CREATE (articleID)-[:HAS_CHUNK]->(chunk)\"\n",
        "    )\n",
        "    session.run(create_chunk_query)\n",
        "\n",
        "    # Link this chunk to the previous chunk if it's not the first one\n",
        "    if chunk_index > 0:\n",
        "        previous_chunk_id = f\"{file_id}_{chunk_index - 1}\"\n",
        "        link_chunks_query = (\n",
        "            f\"MATCH (prevChunk:Chunk {{id: '{previous_chunk_id}'}}), (currChunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "            f\"CREATE (prevChunk)-[:NEXT]->(currChunk)\"\n",
        "        )\n",
        "        session.run(link_chunks_query)\n",
        "\n",
        "# Functions to link nodes to ArticleID\n",
        "def link_topic_to_article(session, file_id, topic_name, justification):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (topic:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TOPIC]->(topic)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_entity_to_article(session, file_id, entity_name, clean=True):\n",
        "    entity_type = \"CleanEntity\" if clean else \"DirtyEntity\"\n",
        "    # Don't add irrelevant entities: \n",
        "    if entity_name in ['Abstract Concepts:', 'References:', 'Key Phrases:', 'Keywords:', 'Entities:', 'Topics:', 'Concepts:', 'Final Output:', 'Introduction', 'Conclusion', 'Summary']:\n",
        "        return\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (entity:{entity_type} {{name: '{entity_name}'}}) \"\n",
        "        f\"MERGE (articleID)-[:MENTIONS]->(entity)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_question_to_article(session, file_id, question_content, question_type):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (hq:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_QUESTION]->(hq)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_triplet_to_article(session, file_id, subject, target, relationship):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (subj:Subject {{name: '{subject}'}}) \"\n",
        "        f\"MERGE (targ:Target {{name: '{target}'}}) \"\n",
        "        f\"MERGE (rel:Relationship {{type: '{relationship}'}}) \"\n",
        "        f\"MERGE (subj)-[:HAS_RELATIONSHIP]->(rel)-[:TARGETS]->(targ) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TRIPLET]->(subj)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_summary_to_chunk(session, file_id, chunk_index, summary_content):\n",
        "    # Unique identifier for the chunk\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Link the summary to the corresponding chunk\n",
        "    link_summary_query = (\n",
        "        f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "        f\"MERGE (summary:Summary {{content: '{summary_content}'}}) \"\n",
        "        f\"MERGE (chunk)-[:HAS_SUMMARY]->(summary)\"\n",
        "    )\n",
        "    session.run(link_summary_query)\n",
        "def link_url_references_to_article(session, file_id, urls):\n",
        "    for url in urls:\n",
        "        escaped_url = escape_string(url)\n",
        "        query = (\n",
        "            f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "            f\"MERGE (ref:Reference {{url: '{escaped_url}'}}) \"\n",
        "            f\"WITH articleID, ref \"\n",
        "            f\"MERGE (articleID)-[:HAS_REFERENCE]->(ref)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "def link_textual_references_to_article(session, file_id, textual_references):\n",
        "    for reference in textual_references.split('\\n'):  # Assuming each reference is on a new line\n",
        "        if reference.strip():\n",
        "            escaped_reference = escape_string(reference)\n",
        "            query = (\n",
        "                f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "                f\"MERGE (textRef:TextualReference {{text: '{escaped_reference}'}}) \"\n",
        "                f\"WITH articleID, textRef \"\n",
        "                f\"MERGE (articleID)-[:HAS_TEXTUAL_REFERENCE]->(textRef)\"\n",
        "            )\n",
        "            session.run(query)\n",
        "\n",
        "def process_knowledge_triplet(session, file_id, subject, target, relationship):\n",
        "    # Constructing a query to check if the specific triplet combination already exists\n",
        "    check_triplet_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TRIPLET]->\"\n",
        "        f\"(subj:Subject {{name: '{subject}'}})-[:HAS_RELATIONSHIP]->\"\n",
        "        f\"(rel:Relationship {{type: '{relationship}'}})-[:TARGETS]->\"\n",
        "        f\"(targ:Target {{name: '{target}'}}) \"\n",
        "        f\"RETURN subj, rel, targ\"\n",
        "    )\n",
        "    if not session.run(check_triplet_query).single():\n",
        "        # Linking the triplet only if it doesn't exist\n",
        "        link_triplet_to_article(session, file_id, subject, target, relationship)\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        # Group records by file_id\n",
        "        grouped_records = {}\n",
        "        for record in data:\n",
        "            file_id = record['file_id']\n",
        "            grouped_records.setdefault(file_id, []).append(record)\n",
        "\n",
        "        for file_id, records in grouped_records.items():\n",
        "            try:\n",
        "                # Check if the ArticleID already exists\n",
        "                check_query = f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) RETURN articleID\"\n",
        "                result = session.run(check_query).single()\n",
        "                if result:\n",
        "                    logging.info(f\"File ID {file_id} already exists in the database. Processing only new chunks.\")\n",
        "                else:\n",
        "                    logging.info(f\"File ID {file_id} does not exist. Processing all chunks.\")\n",
        "\n",
        "                for index, record in enumerate(records):\n",
        "                    chunk_content = escape_string(record['article'])\n",
        "                    timestamp = record.get('timestamp', '')\n",
        "                    summary_content = escape_string(record['summary']) if \"summary\" in record else \"\"\n",
        "\n",
        "                    # Process each chunk\n",
        "                    process_record_with_chunks(session, file_id, chunk_content, index, timestamp, summary_content, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "def process_record_with_chunks(session, file_id, chunk_content, chunk_index, timestamp, summary_content, record):\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Check and process each chunk and its summary\n",
        "    if not session.run(f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) RETURN chunk\").single():\n",
        "        create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp)\n",
        "        if summary_content:\n",
        "            link_summary_to_chunk(session, file_id, chunk_index, summary_content)\n",
        "\n",
        "    # Process Topics\n",
        "    for topic in record.get(\"topics\", []):\n",
        "        topic_name = escape_string(topic['topic'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TOPIC]->(topic:Topic {{name: '{topic_name}'}}) RETURN topic\").single():\n",
        "            justification = escape_string(topic.get('justification', ''))\n",
        "            link_topic_to_article(session, file_id, topic_name, justification)\n",
        "\n",
        "    # Process Entities (both clean and dirty)\n",
        "    for clean, entities in [(True, record.get(\"clean_entities\", [])), (False, record.get(\"dirty_entities\", []))]:\n",
        "        for entity in entities:\n",
        "            entity_name = escape_string(entity)\n",
        "            if entity_name.strip() and not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:MENTIONS]->(entity {{name: '{entity_name}'}}) RETURN entity\").single():\n",
        "                link_entity_to_article(session, file_id, entity_name, clean)\n",
        "\n",
        "    # Process Hypothetical Questions\n",
        "    for question in record.get(\"hypothetical_questions\", []):\n",
        "        question_content = escape_string(question['question'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_QUESTION]->(hq:HypotheticalQuestion {{content: '{question_content}'}}) RETURN hq\").single():\n",
        "            question_type = escape_string(question.get('question_type', ''))\n",
        "            link_question_to_article(session, file_id, question_content, question_type)\n",
        "\n",
        "    # Process Knowledge Triplets\n",
        "    for triplet in record.get(\"knowledge_triplets\", []):\n",
        "        subject = escape_string(triplet['subject'])\n",
        "        target = escape_string(triplet['target'])\n",
        "        relationship = escape_string(triplet['relationship'])\n",
        "        process_knowledge_triplet(session, file_id, subject, target, relationship)\n",
        "\n",
        "    # Process URL References\n",
        "    if \"references\" in record and \"urls\" in record[\"references\"]:\n",
        "        for url in record[\"references\"][\"urls\"]:\n",
        "            escaped_url = escape_string(url)\n",
        "            if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_REFERENCE]->(ref:Reference {{url: '{escaped_url}'}}) RETURN ref\").single():\n",
        "                link_url_references_to_article(session, file_id, [url])\n",
        "\n",
        "    # Process Textual References\n",
        "    if \"references\" in record and \"textual_references\" in record[\"references\"]:\n",
        "        for reference in record[\"references\"][\"textual_references\"].split('\\n'):\n",
        "            if reference.strip():\n",
        "                escaped_reference = escape_string(reference)\n",
        "                if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TEXTUAL_REFERENCE]->(textRef:TextualReference {{text: '{escaped_reference}'}}) RETURN textRef\").single():\n",
        "                    link_textual_references_to_article(session, file_id, [reference])\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                process_json_file(file_path, session)\n",
        "                print(f\"Successfully processed file: {file_path}\")\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder4 = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder1 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder2 = \"/home/epas/Programming/gpt-researcher/outputs/\"\n",
        "    output_folder3 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\"\n",
        "    for output_folder in [output_folder1, output_folder2, output_folder3, output_folder4]:\n",
        "        try:\n",
        "            add_jsons_to_neo4j(output_folder)\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding jsons to Neo4j database: {e}\")\n",
        "            continue\n",
        "    print(\"Successfully added data to Neo4j database.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for text: [-0.06638545542955399, -0.6339597702026367, 0.31231772899627686, 0.7250638008117676, -0.07424969226121902, 0.003917177673429251, 0.28195762634277344, -0.28801611065864563, 1.1876319646835327, 0.13408410549163818, -0.18229246139526367, -0.25617411732673645, -0.007907763123512268, -0.06861094385385513, -0.14425835013389587, -0.4060131013393402, -0.12490224093198776, -0.22354011237621307, -0.09113853424787521, 0.5715342164039612, -1.0153146982192993, 0.7802836298942566, -1.8314872980117798, -0.6482148766517639, -0.6345102787017822, -0.055900949984788895, -0.41331279277801514, 0.5485435128211975, 0.6850767731666565, 0.92955082654953, -0.23747427761554718, 0.36668792366981506, 0.41014957427978516, -0.450754314661026, -0.2309989184141159, -0.40863823890686035, 0.5641539096832275, -0.6355576515197754, 0.28896597027778625, -0.3447493314743042, 0.29975250363349915, -0.41504788398742676, 1.0835380554199219, -0.06404896825551987, -0.9426522254943848, -0.630429744720459, -0.18927760422229767, -1.2148202657699585, -0.28303641080856323, -0.6812933087348938, -0.06259401887655258, -0.2892148494720459, 0.4589127004146576, -0.12901252508163452, 0.3451903760433197, -0.02625919319689274, -0.20780682563781738, -0.577512264251709, 0.04585981369018555, 0.6345730423927307, 0.3943246304988861, 0.1717558354139328, 0.8116572499275208, -0.9516306519508362, -0.10030750185251236, -0.3299899101257324, -0.34290769696235657, 0.3058057725429535, 0.2981114685535431, -0.46314337849617004, -0.35688650608062744, 0.30135831236839294, 0.09041231870651245, -0.7646503448486328, -0.41093945503234863, -0.18937794864177704, -0.3114785850048065, -0.22530721127986908, -0.09003976732492447, 0.6237339973449707, -0.04287995770573616, 0.5525371432304382, 0.436759352684021, 0.10059095174074173, -1.2565385103225708, -0.8605012893676758, 0.00351608800701797, 0.1454509049654007, 0.1686619520187378, -0.08428845554590225, 0.1269635707139969, 0.7939238548278809, 0.20816051959991455, -0.06896204501390457, 0.09108457714319229, 0.9699058532714844, -0.5094056725502014, 0.643207848072052, -0.14774850010871887, 0.24192936718463898, 0.13412892818450928, 0.024555638432502747, -0.6215619444847107, 1.429786205291748, -0.49438783526420593, 0.27748608589172363, 0.34579142928123474, -0.1700395941734314, -0.8566100001335144, -0.7298824787139893, -0.38325032591819763, 0.15978123247623444, 0.5505183339118958, -0.005860110279172659, -0.3183335065841675, 0.5208038687705994, 0.2876225411891937, 0.38971617817878723, -0.41697636246681213, 0.1641843318939209, 0.11710923910140991, -0.12386607378721237, -0.1091427430510521, -0.25284436345100403, -0.37213316559791565, -0.20359039306640625, -0.24687300622463226, 1.0524193048477173, -0.5788931846618652, -0.029266877099871635, -0.44107022881507874, -0.6332050561904907, 0.9751874804496765, 0.41248998045921326, 0.048822637647390366, 0.008100882172584534, 0.1515430361032486, 0.05917438864707947, 0.4436018466949463, -0.5836031436920166, -0.04643234610557556, -0.3124428689479828, -0.29744952917099, 1.5901861190795898, 0.20506906509399414, 0.7712386250495911, -0.4186425507068634, 0.6402022242546082, -1.0219546556472778, 0.8820237517356873, -0.6917324066162109, 0.7681145668029785, 0.17052488029003143, 0.832865297794342, -0.06595902889966965, 0.11727148294448853, 0.449399471282959, -0.41103219985961914, -0.12907250225543976, 0.04458525776863098, -0.08452636003494263, -0.23169445991516113, 0.019580552354454994, 0.6798405647277832, -0.20783162117004395, 0.811784565448761, -0.2514553666114807, -0.08750750869512558, -0.032233625650405884, -0.31447353959083557, 0.1636779010295868, 0.6297440528869629, -0.48636817932128906, 0.23125624656677246, 0.47106993198394775, 0.1979428082704544, -0.03695023059844971, -0.27015233039855957, 1.1104869842529297, 0.18153004348278046, 0.23543167114257812, 0.16446010768413544, 0.6060754656791687, 0.306442528963089, 0.48012956976890564, -0.187733456492424, -0.03159903362393379, -0.283925861120224, -0.7402288317680359, -0.13971441984176636, -0.42804646492004395, 0.6315296292304993, -0.34006568789482117, 0.26508769392967224, 0.016041388735175133, 0.054897379130125046, -0.019056836143136024, -0.4168541133403778, -0.3724840581417084, -0.5660857558250427, -0.985235869884491, 0.2589219808578491, -0.8056300282478333, 0.29961198568344116, -0.4164508581161499, 0.06483674049377441, 0.19516044855117798, 1.184812307357788, -0.24729281663894653, 0.5936188697814941, 0.8480401635169983, 0.1214774027466774, -0.8035319447517395, 0.668709933757782, 0.6065922379493713, 0.030863618478178978, -0.8576899170875549, 0.6193895936012268, -0.6038681864738464, -0.20703370869159698, -0.09678538888692856, 0.30672791600227356, 0.324958860874176, 0.6550207138061523, 0.03192616254091263, -0.05398606136441231, -0.31272855401039124, 1.0229382514953613, -0.4532911777496338, -0.29337048530578613, 0.18848200142383575, 0.4965660274028778, 0.039490800350904465, 0.7370559573173523, 0.11534187942743301, 0.7714880108833313, 1.0452803373336792, 0.5631124377250671, -0.013746839947998524, 0.6105285286903381, -0.1836709827184677, 0.2926558554172516, 1.0416570901870728, 0.3400731086730957, -0.5593492388725281, 0.8016298413276672, 0.469163179397583, 0.28287872672080994, -0.421413391828537, 0.5340933799743652, 0.12613995373249054, 0.5180429816246033, 0.6776633262634277, 0.3252580463886261, -0.6583413481712341, -0.35363972187042236, 0.38808417320251465, 0.8353076577186584, -0.48595499992370605, -0.3708415925502777, 0.4266974925994873, 0.19392947852611542, 0.24914179742336273, 0.6971611380577087, 0.09736812859773636, 0.08315856009721756, 0.24164979159832, 0.44281938672065735, -0.19335849583148956, -0.8117950558662415, -0.2752039134502411, -0.9108282923698425, -1.0260432958602905, -0.2277180552482605, -1.3545184135437012, 0.13600243628025055, 0.755073070526123, -0.8345186114311218, 0.7676825523376465, -0.1165986955165863, -0.2430015206336975, -0.38342952728271484, -0.757767379283905, 0.23319195210933685, -0.06353703886270523, 0.5601463317871094, 0.2965658903121948, 0.22299152612686157, 0.46068549156188965, 0.3924051821231842, -0.6836888194084167, -0.35758134722709656, 0.09856539964675903, 0.04302426055073738, 0.43763992190361023, -0.6673410534858704, -0.071711465716362, 0.09627208858728409, -0.9746482968330383, -0.5913717150688171, 0.044341206550598145, -0.9110473990440369, -0.2968592047691345, -0.32073819637298584, -0.29785019159317017, 0.47479262948036194, 0.49641624093055725, -0.4777649939060211, 0.09802799671888351, -0.016415053978562355, -0.37234067916870117, 0.4409250020980835, 0.022506937384605408, -0.04655905440449715, -0.7836282253265381, 0.1318216472864151, 0.4863952696323395, 0.4107251465320587, 0.19514930248260498, -0.08033338189125061, 0.0072578489780426025, 0.12559781968593597, 0.05179870128631592, -0.47576943039894104, -0.5911538004875183, -0.24872498214244843, 0.9063441157341003, -1.4697455167770386, 0.6937468647956848, -0.5141360759735107, -1.1233285665512085, -0.44980037212371826, -0.13695202767848969, 0.7385030388832092, 0.189810648560524, -0.1420876830816269, -0.0640905573964119, -0.015047512017190456, 0.7612505555152893, -0.45756950974464417, 1.0618871450424194, -0.7566285729408264, 0.05909125134348869, 0.5493795275688171, -0.31802234053611755, 0.16484297811985016, -0.16317443549633026, 0.12269953638315201, -0.06981910020112991, 0.5198438167572021, -0.11138898879289627, 0.4804588854312897, 0.6731104850769043, 0.011179660446941853, 0.7287718653678894, 0.604705274105072, -0.30820295214653015, 0.30019059777259827, 0.39407026767730713, 0.22967404127120972, 0.4141809940338135, 0.2131778746843338, 0.2193482518196106, -0.42793646454811096, -0.10090348869562149, -0.8955166935920715, 0.5178670287132263, 0.13590489327907562, 0.4659028947353363, -1.1783004999160767, 0.9803188443183899, -0.188924178481102, -0.4960108697414398, 0.2756882905960083, -0.6272470951080322, -0.25801464915275574, 1.6567217111587524, 0.06843451410531998, 0.5176124572753906, -0.5459084510803223, 0.44090452790260315, -0.0984177365899086, 0.33688807487487793, 0.4876948297023773, 0.9047570824623108, 0.797149121761322, 0.5667043328285217, 0.2987910807132721, 0.13325314223766327, -0.448823481798172, -0.3127870559692383, 0.1263914555311203, 0.27376657724380493, -0.2560252845287323, -0.41905704140663147, -1.2787498235702515, 0.5410906672477722, 0.05834047123789787, 0.3948824405670166, 0.05718308687210083, 0.10291388630867004, 0.4027211666107178, 1.0199875831604004, 0.5778898596763611, -0.39016327261924744, 0.07144622504711151, -0.48245564103126526, 0.5941470265388489, -0.3601057827472687, 0.4755524694919586, -0.7759808897972107, -0.3503049612045288, -0.465859055519104, 0.3322318196296692, -0.17673908174037933, -0.1856289952993393, 0.03498268499970436, -0.003992935176938772, 0.5159812569618225, 0.28759029507637024, -0.5033491253852844, -0.18094468116760254, 0.31995442509651184, 0.3049783706665039, 0.5847995281219482, -0.15340672433376312, 0.12077363580465317, -0.5798972249031067, 0.4213906228542328, 0.7416572570800781, -0.025222016498446465, 0.0040701329708099365, -0.03810015693306923, -0.3879692554473877, -0.8786093592643738, 0.3363783657550812, -0.047530900686979294, -0.6182878017425537, 0.4059809744358063, -0.8649070858955383, 0.1043267548084259, 0.6424930095672607, 0.045388560742139816, 0.06409210711717606, 0.09279888868331909, 0.0878182053565979, 0.5107882618904114, 0.5486069917678833, -0.6142460703849792, -0.540412425994873, 0.7646309733390808, -1.1980652809143066, 0.05454501509666443, -0.541087806224823, 0.16008731722831726, 0.09805765002965927, 0.0015796819934621453, -0.40824997425079346, 0.8100306391716003, -0.3800790309906006, 0.006211094558238983, -0.4748079776763916, 0.35774528980255127, -0.5397021770477295, -0.34324145317077637, 1.207323431968689, -0.012998796999454498, -0.8011061549186707, 0.29361990094184875, 0.13184243440628052, -0.3384673297405243, 0.25980761647224426, 0.7600006461143494, 0.11468392610549927, 0.060197148472070694, -0.7232505679130554, 0.3561800420284271, -0.6426227688789368, -1.0541201829910278, 0.033711016178131104, 0.3424588441848755, 0.5035905241966248, -0.7991164326667786, -0.4546959400177002, -0.08835791796445847, -0.8551809191703796, -0.24573887884616852, -0.10700592398643494, -0.3108745813369751, -0.4379747807979584, 0.8797017931938171, 0.13462300598621368, -0.46553340554237366, -0.9718912243843079, -0.6162722110748291, -0.103373222053051, -0.1694301962852478, 0.0009796321392059326, -0.30874770879745483, 0.06876789033412933, 0.3533405363559723, -0.38423576951026917, -1.0343202352523804, 0.5094431042671204, 0.3327324092388153, -0.3432072103023529, -0.4708557426929474, -0.2439049631357193, -0.0836101844906807, 0.297299325466156, 0.38737866282463074, 0.058700498193502426, 0.06802570074796677, 0.15987829864025116, 0.816002368927002, 0.5945596098899841, 0.04704130068421364, -0.2314355969429016, -0.2679906189441681, 0.3981597423553467, 1.1157832145690918, -0.4801725447177887, -0.10405844449996948, 0.29457300901412964, -0.2651868164539337, 0.23037564754486084, 0.3997069299221039, -0.025684481486678123, -0.527834415435791, -0.9166503548622131, 0.025166137143969536, -0.47119569778442383, 0.013402449898421764, -0.9081764221191406, 0.1473964899778366, -0.12226366996765137, 0.26684272289276123, 0.572847306728363, -0.25484421849250793, -0.056756000965833664, -0.9470329880714417, -0.007386211305856705, -0.39034202694892883, 0.7142021059989929, -0.16730792820453644, -0.05724325776100159, -0.753962516784668, 1.4079971313476562, 0.066371850669384, 0.3285394608974457, -0.5209212899208069, 0.23614013195037842, 0.12437417358160019, 0.40903541445732117, -0.44456085562705994, -0.45841848850250244, 0.39416393637657166, 0.11757325381040573, 0.6880228519439697, 0.6589815020561218, -0.26849403977394104, 0.9816743731498718, -0.6999149918556213, -0.6882044672966003, 0.03499526157975197, -0.11363860219717026, -0.18556122481822968, 0.3987114131450653, 0.5132656693458557, -0.354193776845932, -0.4844399392604828, -0.5310859084129333, 0.8624822497367859, 0.3933115303516388, 0.03570426627993584, -0.7972269058227539, -0.4078560769557953, -0.6762134432792664, -0.8459882736206055, 0.24052287638187408, -0.307402640581131, -0.34671124815940857, -0.3648991286754608, 0.40335771441459656, 1.271229863166809, -0.5723927617073059, 0.100108802318573, 1.2839033603668213, -0.5041873455047607, -0.46312782168388367, -0.44607019424438477, 0.25030308961868286, -0.1197752133011818, -0.5195214152336121, -0.6138412952423096, -0.9354734420776367, -0.242648646235466, 0.18467526137828827, -0.3228505551815033, -0.4789447784423828, -0.8375265002250671, -0.42205896973609924, 1.2625914812088013, -0.4198819100856781, 0.8966360688209534, 0.27478086948394775, -0.5554385185241699, -1.058193564414978, 0.3609464168548584, 0.8704162240028381, -0.44164302945137024, -0.22349028289318085, 0.15556283295154572, 0.2045801877975464, -0.49174025654792786, -0.025738870725035667, -0.26031577587127686, -0.5060476660728455, 0.8099451661109924, -0.11837907880544662, -0.599166214466095, 0.23061233758926392, 1.0080090761184692, -0.5557587742805481, -0.22489877045154572, 0.5582465529441833, 0.3718808889389038, -0.9899237751960754, -1.0197358131408691, -0.1016678437590599, -0.6099753975868225, -0.11249510198831558, -0.30455756187438965, 0.23261205852031708, 0.181255042552948, 0.667879581451416, 0.0902390405535698, 0.43384769558906555, -0.1843879669904709, -0.4898501932621002, 0.9653282165527344, -0.2856052815914154, 0.20765244960784912, -0.49313583970069885, -0.14439956843852997, -0.11800836771726608, -0.01484382152557373, 1.116578459739685, 0.09060264378786087, -0.47711384296417236, 0.7632573246955872, -0.26354894042015076, 0.6760141849517822, -0.14299701154232025, -0.4610374867916107, 0.8951897621154785, -0.18094085156917572, -0.6798960566520691, -0.8881971836090088, 0.8634352684020996, 0.7309816479682922, -0.004656834062188864, -0.5931118726730347, 0.15379519760608673, 0.6165668368339539, -0.16627614200115204, -0.6011727452278137, -0.5931110978126526, -0.7977921366691589, -0.6446205973625183, -0.9104804992675781, -0.9279152750968933, -0.023909173905849457, 0.13860291242599487, 0.6501744389533997, -0.4159739017486572, -0.6119562983512878, -0.2849895656108856, 1.216559886932373, -0.23617906868457794, 1.3405367136001587, 0.32481470704078674, 0.6804153919219971, -0.5672169327735901, -0.858095645904541, -0.8959692120552063, -0.2020202875137329, -0.054090093821287155, -0.23644083738327026, -0.6985489726066589, -0.09624090790748596, -0.16933594644069672, 0.8257520794868469, 0.8661472797393799, 0.18160831928253174, 0.06344176083803177, -0.6123438477516174, 0.4358525276184082, 0.2135152816772461, 0.18442386388778687, 0.2680031657218933, 0.6215443015098572, -0.14046266674995422, -0.41378259658813477, 0.28180885314941406, -0.07782170921564102, -0.39394983649253845, 0.3294634521007538, 1.0568828582763672, -0.3988141119480133, 0.44669345021247864, -1.1542967557907104, 0.07319273799657822, -0.7991887927055359, -0.2836831510066986, 0.07147075235843658, 0.20036762952804565, -0.21692126989364624, -0.14760597050189972, 0.29216548800468445, 1.1862733364105225, 0.09551757574081421, 0.400997132062912, 0.5431531667709351, 0.18273551762104034, 0.06626585870981216, 0.4035435616970062, -0.38955819606781006, 0.005472339689731598, 1.0750043392181396, -0.17492659389972687, -0.5408955812454224, -0.1675584316253662, 0.08223935961723328, 0.6571308970451355, -0.2512669265270233, -0.6632903218269348, -0.7239808440208435, -0.11836905032396317, 0.3638514578342438, 0.17462287843227386, 0.38608279824256897, -0.7125549912452698, 0.24808068573474884, -0.32316744327545166, -0.5263240337371826, 0.36726197600364685, -0.5191857218742371, -0.3165324330329895, 0.9747705459594727, 0.13981477916240692, -0.389574259519577, 0.5282807350158691, -0.6088158488273621, 0.10323572903871536, -0.5390452742576599, -0.05394035577774048, -0.21262194216251373, 0.1645238846540451, 0.18304379284381866, 0.09424170106649399, -0.3949432075023651, 0.5665667653083801, -0.2114344835281372, 1.0135079622268677, -0.9787573218345642, -0.926402747631073, 0.46350133419036865, 0.34425392746925354, -0.08993339538574219, -0.06382721662521362, -0.6742064356803894, 0.0655408725142479, -0.6105044484138489, -0.3694218099117279, -0.15911631286144257, 0.2954540550708771, 0.171355202794075, -0.5848672986030579, -0.22246064245700836, -0.07082065939903259, -0.4032400846481323, 0.2106013298034668, 0.35212409496307373, -0.4841744005680084, -0.2063305377960205, 0.6622775197029114, 0.9958577752113342, -0.3212616443634033, 0.8129479289054871, -0.06769838184118271, 0.5913970470428467, 0.6048431396484375, -0.6597508788108826, -0.45576390624046326, 0.7070335745811462, 1.229397177696228, 0.2282826155424118, -0.430462509393692, 0.08540591597557068, 0.6566457152366638, 0.08053943514823914, -0.04549941048026085, 0.21215105056762695, -0.3273695409297943, 0.1320274919271469, 0.07765411585569382, -0.8421124815940857, 0.07066372036933899, 0.12279175966978073, -0.01716853678226471, 0.496768981218338, -0.8677034378051758, -0.1931166648864746, -0.20554041862487793, -0.8415277004241943, 0.2497023344039917, -0.4399840831756592, 0.022114356979727745, -0.18732531368732452, -0.4749957025051117, -0.6279932856559753, 0.6225922107696533, -0.4315533936023712, 0.5006032586097717, 0.9972548484802246, -0.039577413350343704, 0.375156968832016, 0.09166330099105835, -0.09984610229730606, 0.49016615748405457, -0.424716591835022, -0.06264066696166992, 0.43420544266700745, -0.32491782307624817, 0.004443836864084005, -0.2150387316942215, -0.4017716944217682, 0.6043490171432495, -0.13372917473316193, -0.2789261043071747, 0.5360795855522156, -0.4441373646259308, -0.5685417056083679, -0.5826858878135681, -0.06213018298149109, 0.003682141425088048, 0.20414777100086212, 0.3310263156890869, -1.3002912998199463, -0.7581307888031006, 0.43936142325401306, -0.26756715774536133, 0.9872896075248718, -0.09953702241182327, 1.3000062704086304, -0.0768013671040535, 0.9333392977714539, 0.3680470883846283, -0.9157933592796326, -0.8666912913322449, -0.02587319351732731, 0.13749076426029205, -0.6425922513008118, -0.3301352560520172, -0.7674319744110107, 0.2484148144721985, -0.22504298388957977, -0.19558513164520264, -0.258640319108963, 0.9933570027351379, -0.5583330392837524, -0.8822863101959229, -0.4689582586288452, -0.026735328137874603, -0.8398467898368835, 0.228656604886055, 0.25891590118408203, 0.6098241209983826, -0.14282257854938507, -0.16266141831874847, -0.5952775478363037, 0.1901302933692932, 0.06413334608078003, -0.4272572696208954, 0.38403937220573425, -0.6469180583953857, -0.7797099947929382, -0.18895411491394043, -0.8362982869148254, 0.10784899443387985, -0.12824374437332153, -0.19467474520206451, -1.453391194343567, -0.028316592797636986, 0.7775217890739441, 0.37588295340538025, 0.4318504333496094, -0.6231740117073059, 0.29395630955696106, 0.9552192091941833, 0.8644700646400452, 0.12181341648101807, 1.409940242767334, 0.03605063632130623, -0.30146411061286926, 0.18790914118289948, -0.8735982775688171, 1.1460014581680298, -0.0810689926147461, 0.002161345211789012, 0.10394782572984695, 0.13581682741641998, -0.3920704424381256, -0.7195512652397156, 0.5485828518867493, 1.060302495956421, 0.41728901863098145, 0.09622400999069214, -1.0899372100830078, 0.058363061398267746, -0.4030133783817291, 0.3653634488582611, -0.4330756664276123, 0.5507684946060181, -0.014515343122184277, -0.20020318031311035, 0.4366699159145355, -1.4345911741256714, 3.6233818531036377, 1.2059952020645142, 0.5430933833122253, -0.024302417412400246, -0.0954539105296135, 0.19508333504199982, 0.4662441313266754, -0.1431187242269516, -0.10351110249757767, -0.11345142126083374, 1.198728084564209, -0.2498801201581955, 0.20104289054870605, 0.35573849081993103, 0.46567806601524353, 0.7692150473594666, -0.7193975448608398, 0.10279042273759842, 0.02317568100988865, -0.5016698241233826, -0.4342128336429596, 0.4364389479160309, 0.04221033677458763, 0.7256613373756409, -0.21937094628810883, 0.03708471730351448, 0.8180193305015564, -0.20613114535808563, -0.85671067237854, -0.7071499824523926, 0.2471858710050583, -1.099500060081482, 0.298480361700058, -0.4368368685245514, -0.8060207962989807, 0.0039691864512860775, -0.29463833570480347, -0.3846270740032196, -0.20506958663463593, -0.08770888298749924, -0.7794786095619202, -0.30212023854255676, 0.8624199032783508, -0.31780484318733215, 0.21072323620319366, -0.036128997802734375, -0.023653091862797737, 0.6184148192405701, 0.23859739303588867, -1.137436866760254, 0.8934342861175537, -0.7672290802001953, 0.08798231929540634, -0.5061333179473877, -0.7468881607055664, -0.4405137002468109, 0.7708021998405457, -0.47159305214881897, 0.16095437109470367, -0.013113888911902905, 0.034753188490867615, 0.03507647290825844, 0.006333579774945974, 0.18609577417373657, -0.29494303464889526, -0.02916821278631687, 0.5620741844177246, 0.6072617173194885, -0.26255902647972107, -0.25714346766471863, -0.7547635436058044, -0.5584690570831299, 0.04365931823849678, -0.4452477693557739, -0.1442817896604538, 0.9381036758422852, -0.18441791832447052, 0.6153355240821838, 0.05690428242087364, -0.2650289237499237, -0.1284218281507492, -0.38671037554740906, -0.10802201181650162, 0.06152069568634033, 0.3521086871623993, 0.8889800906181335, -1.0216126441955566, -0.21469895541667938, -0.9618147015571594, 1.04731285572052, 1.2929061651229858, 0.22595088183879852, -0.005266748368740082, -0.1299854964017868, -0.527457058429718]\n",
            " With shape: 1024\n"
          ]
        }
      ],
      "source": [
        "embedding = embeddings.embed_query(\"text\")\n",
        "print(f\"Embedding for text: {embedding}\\n With shape: {len(embedding)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Configuration for HuggingFaceEmbeddings\n",
        "model_name = \"WhereIsAI/UAE-Large-V1\"  # or another model of your choice\n",
        "model_kwargs = {'device': 'mps'}  # use 'cuda' for GPU acceleration if available\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "def LoadEmbedding(node_type, text_properties):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Handle multiple properties (concatenation) for a single embedding\n",
        "        if isinstance(text_properties, list) and len(text_properties) > 1:\n",
        "            properties_str = \" + ' ' + \".join([f\"n.{prop}\" for prop in text_properties])\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, {properties_str} AS text\"\n",
        "        else:\n",
        "            text_property = text_properties[0]\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, n.{text_property} AS text\"\n",
        "\n",
        "        result = session.run(query)\n",
        "        count = 0\n",
        "        for record in result:\n",
        "            id = record[\"id\"]\n",
        "            text = record[\"text\"]\n",
        "\n",
        "            # Generate the embedding\n",
        "            embedding = embeddings.embed_query(text)\n",
        "            print(f\"Embedding for {node_type} with id {id}: With shape: {len(embedding)}\")\n",
        "            cypher = \"MATCH (n) WHERE id(n) = $id SET n.embedding = $embedding\"\n",
        "            session.run(cypher, embedding=embedding, id=id)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Processed {count} {node_type} nodes for property @{' and '.join(text_properties)}.\")\n",
        "    return count\n",
        "\n",
        "# Example usages\n",
        "LoadEmbedding(\"Chunk\", [\"content\"])\n",
        "LoadEmbedding(\"CleanEntity\", [\"name\"])\n",
        "LoadEmbedding(\"DirtyEntity\", [\"name\"])\n",
        "LoadEmbedding(\"Subject\", [\"name\"])\n",
        "LoadEmbedding(\"Target\", [\"name\"])\n",
        "LoadEmbedding(\"Relationship\", [\"type\"])\n",
        "LoadEmbedding(\"Topic\", [\"name\", \"justification\"])  # Concatenating name and justification for topics\n",
        "LoadEmbedding(\"HypotheticalQuestion\", [\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ClientError",
          "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.createNodeIndex`: Caused by: org.neo4j.kernel.api.exceptions.schema.EquivalentSchemaRuleAlreadyExistsException: An equivalent index already exists, 'Index( id=5, name='chunkVectorIndex', type='VECTOR', schema=(:Embedding {embedding}), indexProvider='vector-1.0' )'.}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOSINE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mcreate_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_USERNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_PASSWORD\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mcreate_indexes\u001b[0;34m(uri, user, password)\u001b[0m\n\u001b[1;32m     11\u001b[0m create_vector_index_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mCALL db.index.vector.createNodeIndex(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkVectorIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, 1024, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOSINE\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Create Full-Text Index#\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#session.run(create_full_text_index_query) # Neo.ClientError.Procedure.ProcedureNotFound\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(\"Full-text index created.\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Create Vector Index\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_vector_index_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector index created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Close the driver connection\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/work/session.py:313\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m bookmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bookmarks()\n\u001b[1;32m    312\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/work/result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pull()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/work/result.py:298\u001b[0m, in \u001b[0;36mResult._attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
            "\u001b[0;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.createNodeIndex`: Caused by: org.neo4j.kernel.api.exceptions.schema.EquivalentSchemaRuleAlreadyExistsException: An equivalent index already exists, 'Index( id=5, name='chunkVectorIndex', type='VECTOR', schema=(:Embedding {embedding}), indexProvider='vector-1.0' )'.}"
          ]
        }
      ],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "def create_indexes(uri, user, password):\n",
        "    # Establish a connection to the Neo4j database\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "    # Define Cypher queries for creating the indexes\n",
        "    create_full_text_index_query = \"\"\"\n",
        "    CALL db.index.fulltext.createNodeIndex(\"textIndex\", [\"Chunk\", \"CleanEntity\", \"DirtyEntity\", \"HypotheticalQuestion\"], [\"content\", \"name\"])\n",
        "    \"\"\"\n",
        "    create_vector_index_query = \"\"\"\n",
        "    CALL db.index.vector.createNodeIndex(\"chunkVectorIndex\", \"Embedding\", \"embedding\", 1024, \"COSINE\")\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # Create Full-Text Index#\n",
        "        #session.run(create_full_text_index_query) # Neo.ClientError.Procedure.ProcedureNotFound\n",
        "        #print(\"Full-text index created.\")\n",
        "\n",
        "        # Create Vector Index\n",
        "        session.run(create_vector_index_query)\n",
        "        print(\"Vector index created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Drop the old index\n",
        "        session.run(f\"DROP INDEX {old_index_name}\")\n",
        "\n",
        "        # Create the new vector index\n",
        "        create_vector_index_query = f\"\"\"\n",
        "        CALL db.index.vector.createNodeIndex(\"{new_index_name}\", \"{label}\", \"{property}\", {dimensions}, \"{similarity}\")\n",
        "        \"\"\"\n",
        "        session.run(create_vector_index_query)\n",
        "        print(f\"Vector index {new_index_name} created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "# Usage\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "old_index_name = \"chunkVectorIndex\"  # Replace with the actual name of the index to drop\n",
        "new_index_name = \"chunkVectorIndex\"  # Replace with your new index name\n",
        "label = \"Embedding\"  # Replace with the appropriate label\n",
        "property = \"embedding\"  # Replace with the correct property\n",
        "dimensions = 1024\n",
        "similarity = \"COSINE\"\n",
        "\n",
        "#modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity)\n",
        "\n",
        "\n",
        "create_indexes(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "No sentence-transformers model found with name /Users/tomriddle1/.cache/torch/sentence_transformers/WhereIsAI_UAE-Large-V1. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query vector: [0.29388344287872314, -0.35879090428352356, 0.5651235580444336, -0.37260448932647705, 0.25242531299591064, 0.6161744594573975, -0.3828858435153961, -0.2459946870803833, -0.05286187678575516, 0.28351396322250366, 0.2228599190711975, -0.5512051582336426, 0.5548465251922607, -0.8177943229675293, -0.08566708862781525, 0.7465562224388123, -0.05828264355659485, -0.6179527640342712, -0.12545719742774963, 0.787604570388794, 0.05956621095538139, -0.006148473359644413, -0.8567864894866943, -0.15741442143917084, 0.3303377032279968, 0.4624573588371277, -0.2774983048439026, 0.15916335582733154, 1.0408663749694824, 1.3058102130889893, -0.5331791043281555, -0.8361573815345764, 0.9316243529319763, -0.7023252248764038, -0.450112521648407, -0.31314411759376526, 0.6898304224014282, -0.1967000514268875, -0.019613545387983322, -0.23931077122688293, 0.0587921179831028, -0.2660609483718872, 0.35565662384033203, -0.4439776837825775, -0.27494555711746216, -0.5333778858184814, -0.3146158456802368, -0.5817266702651978, -0.7210743427276611, -0.4262408912181854, 0.21039460599422455, 0.07711237668991089, 0.501015305519104, 0.07081033289432526, -0.42930448055267334, -0.2108708918094635, -0.15223415195941925, -0.3971090316772461, -0.8720787763595581, 0.588686466217041, 0.7451245188713074, -0.2223462164402008, -0.019853776320815086, -0.7805471420288086, 1.181835651397705, 0.8083630204200745, -0.5606272220611572, -0.4079452455043793, 0.54485684633255, -0.19363126158714294, -0.49213093519210815, -0.12089493870735168, 0.4363189935684204, 0.23348723351955414, -0.8051375150680542, -0.14844298362731934, 0.6139037609100342, 0.14112761616706848, -0.4814354181289673, 0.49171167612075806, 0.13458049297332764, -0.1460256576538086, 0.4117664098739624, 0.5149809122085571, -0.7508350610733032, 0.30781108140945435, -0.23239612579345703, 0.49779191613197327, 0.19415313005447388, 0.24661244451999664, 0.05378587916493416, 1.024033546447754, 0.02417340874671936, -0.22793546319007874, 0.4766760468482971, 0.6102744936943054, -0.23309528827667236, 0.788063645362854, 0.14492876827716827, 0.0858331248164177, 0.09239139407873154, 0.7418078780174255, -0.4257752597332001, 0.5675856471061707, -0.5200546979904175, -0.3876552879810333, 0.22263483703136444, -0.5053369998931885, 0.33414649963378906, -0.9958095550537109, 1.1752935647964478, -0.772883415222168, 0.24170517921447754, 0.2516779601573944, 0.2640334963798523, 0.811765193939209, -0.3791109323501587, 0.5260554552078247, -0.46464210748672485, -0.44188109040260315, 0.03599759191274643, 0.4736934006214142, 0.5245427489280701, 0.45085597038269043, 0.3489503860473633, -0.03220093995332718, -0.39613819122314453, 0.09851228445768356, -0.41480883955955505, -0.31186729669570923, 0.13216035068035126, -0.3023957908153534, -0.4669831395149231, 0.9841843843460083, 0.18347029387950897, 0.08075155317783356, 0.1550871729850769, 0.7442914843559265, 0.47551047801971436, -0.041375502943992615, 0.8355462551116943, -0.02587740309536457, 0.42307567596435547, 1.6256520748138428, 0.12922196090221405, -0.0011898186057806015, -0.5795078277587891, -0.3169836401939392, -0.4353521466255188, 0.6827307343482971, -0.7974215149879456, -0.5323978662490845, -0.2452692985534668, 0.21184727549552917, 0.34528690576553345, 0.10386931896209717, 0.04195832461118698, 0.3239721953868866, 0.010173201560974121, 0.39364001154899597, 0.354941725730896, -0.016431409865617752, 0.36607247591018677, -0.1856684386730194, -0.4630187451839447, 0.1559402048587799, -0.9471725225448608, -0.17879363894462585, 0.44279974699020386, 0.209328755736351, -0.037160687148571014, -0.30560845136642456, 0.006499521434307098, 0.8973590135574341, 0.47543007135391235, 0.42386648058891296, 0.6841030120849609, -0.2663462460041046, 0.49500560760498047, 0.7889537215232849, -0.4204888939857483, -0.6314558982849121, 0.6583665609359741, 0.9974465370178223, 0.3393726944923401, -0.506708025932312, -0.6252639293670654, 0.233114093542099, 0.2952396273612976, -0.9758496284484863, 0.11011147499084473, 1.0472509860992432, -0.6855746507644653, 0.4025486409664154, -0.2638072967529297, -0.09799013286828995, -1.248619794845581, -0.12218537926673889, -0.36000126600265503, -1.2571370601654053, -0.35089486837387085, 0.6730668544769287, -0.22006410360336304, 0.0064910463988780975, -1.2706732749938965, -0.1460943967103958, 0.8756027817726135, 1.3262507915496826, -0.7049561738967896, 1.0175530910491943, 0.7812536954879761, -0.24386905133724213, -0.14097855985164642, 0.3981967568397522, 0.34198740124702454, 0.36146000027656555, -0.23500674962997437, 0.4347660541534424, 0.2001001089811325, 0.04178916662931442, 0.48047012090682983, -0.04920150339603424, 0.3480778932571411, 0.750029444694519, -0.16610828042030334, 0.7058171033859253, -0.19276723265647888, 0.8983331322669983, -0.16750776767730713, -0.028891166672110558, -0.36734646558761597, 0.45154809951782227, 0.4285624623298645, 0.5730916857719421, 0.39719533920288086, 0.8667734861373901, 1.271382451057434, 0.5662455558776855, 0.3758891224861145, 0.1227932870388031, 0.16219787299633026, -0.21116846799850464, 0.2357184886932373, 0.32102566957473755, 0.7320619821548462, 0.042112890630960464, -0.3178469240665436, 0.18157470226287842, -0.7347812652587891, 0.15818041563034058, -0.6386052966117859, 1.013785481452942, 0.6580269932746887, 0.025684427469968796, -0.11112256348133087, -0.394084632396698, 0.5501108169555664, 0.7568966746330261, -0.4180670380592346, -0.09043136984109879, -0.1601751148700714, 0.4719054102897644, -0.15863382816314697, 0.5985733270645142, 0.28873157501220703, 0.16544018685817719, -0.3161812722682953, 0.46858522295951843, -0.5585310459136963, -1.078670620918274, -1.7697060108184814, -0.9801582098007202, -0.5655382871627808, -0.22610753774642944, 0.5241080522537231, -0.41337060928344727, 0.5540205240249634, -0.6737605333328247, 1.2192392349243164, 0.6482394337654114, 0.02693464234471321, 0.0006430037319660187, -0.4760204553604126, 0.8101285696029663, 0.6985973119735718, 0.9602712392807007, -0.7291946411132812, 0.6044136881828308, -0.3297960162162781, 0.5294744968414307, -0.7871962189674377, -0.25306403636932373, 0.5571657419204712, -0.32677483558654785, 0.5620258450508118, -0.7295641899108887, -0.624241292476654, -0.23938611149787903, -0.0706651583313942, -0.26057761907577515, -0.4345185160636902, 0.03519195318222046, -0.7335594892501831, -0.27331244945526123, -0.0965203195810318, 0.9908745288848877, 0.46381497383117676, 0.2357052117586136, 0.39238274097442627, 0.3795775771141052, 0.1834140121936798, 0.2536625266075134, 1.0146101713180542, 0.750290036201477, -0.4267362356185913, 0.8410326838493347, 1.31061851978302, 0.254864364862442, 0.26759010553359985, -0.14191746711730957, -0.5721368789672852, 0.16438861191272736, 0.05530460551381111, -0.8317646384239197, -0.18795400857925415, 0.8519876599311829, 0.34301623702049255, -1.377739667892456, 0.3194981813430786, -0.5678665637969971, -1.1083626747131348, 0.024361692368984222, -0.1289055347442627, 0.5134078860282898, -0.3298775553703308, 0.40978750586509705, -0.1392839401960373, 0.39327389001846313, 0.3557335138320923, -0.5739942789077759, 0.8242794871330261, -0.05016423761844635, 0.771539032459259, 0.17891967296600342, -0.4512251615524292, 0.44903677701950073, -0.19523698091506958, -0.42995089292526245, -0.2275276482105255, 0.06848010420799255, -0.1291317343711853, -0.27098000049591064, 0.18365423381328583, 1.0765732526779175, 0.9231761693954468, 0.6144561767578125, -0.7171891927719116, -0.42413806915283203, 0.07883117347955704, -0.14347410202026367, 0.3496209979057312, -0.04421567544341087, -0.2285906970500946, -0.05725804716348648, -0.7589443922042847, -0.7893853187561035, -0.054150037467479706, 0.2183038890361786, 0.3135594129562378, -0.8590074777603149, 1.137594223022461, 0.0027266910765320063, -0.8003937005996704, 0.5956491231918335, -0.24381791055202484, -0.08608673512935638, 1.2608873844146729, -0.11990788578987122, 1.295440435409546, -0.7794967293739319, -0.12182565033435822, 0.49815797805786133, 0.3584464192390442, 0.4949324131011963, -0.007394921500235796, 0.7785978317260742, 0.0726497620344162, 0.2417425513267517, -0.4626719355583191, -0.6556935906410217, -0.5317908525466919, 0.1661922186613083, 0.1900763213634491, -0.7323914766311646, -1.3220080137252808, -0.49599921703338623, 0.6515412926673889, 0.5562508702278137, 0.776862621307373, -0.5904340744018555, -0.02008948102593422, 0.6450164318084717, 0.31886908411979675, 0.7093924880027771, -0.26396089792251587, 0.426341712474823, -0.18063262104988098, 0.49962303042411804, 0.13240382075309753, -0.1872197389602661, -0.9751712083816528, -0.2568284869194031, -0.322378933429718, 0.5386783480644226, 0.25135403871536255, 0.08033561706542969, -0.2619621753692627, 0.4428524374961853, -0.9423723220825195, -0.14528582990169525, -1.3075742721557617, -0.19003957509994507, 0.08696044981479645, 0.338207870721817, -0.1547393500804901, -0.4431758522987366, 0.0363694429397583, -0.821168839931488, -0.0499931275844574, 0.5694551467895508, -0.07369082421064377, -0.6742696762084961, 0.018019789829850197, -1.0646916627883911, -0.7758514285087585, 0.27765846252441406, -0.3326595425605774, 0.7203325033187866, 0.18629230558872223, 0.03797127306461334, 0.3976270258426666, -0.6115100383758545, -0.4812832474708557, -0.31939807534217834, -0.38320595026016235, 0.3536425828933716, 0.624993085861206, 1.013897180557251, 0.6531287431716919, -0.6903669238090515, -0.19045349955558777, -1.1692326068878174, 0.1831018328666687, -0.1728041172027588, -0.40019798278808594, -0.4831302762031555, 0.3478613495826721, -0.5038111209869385, 0.3918679356575012, -0.5269481539726257, 0.15616047382354736, -0.029039785265922546, 0.3506777286529541, -1.045020580291748, -0.7802438139915466, 0.8530474901199341, -0.12107164412736893, -0.27637171745300293, 0.4646507203578949, 0.25081801414489746, -0.3450397849082947, -0.2997812032699585, 0.36254504323005676, -0.23298922181129456, 0.6969640851020813, -0.36489200592041016, 0.08626629412174225, 0.1002025157213211, -1.1778303384780884, 0.2550981342792511, 0.04585854336619377, -0.282209575176239, -0.4023433029651642, -0.532151460647583, -0.782488226890564, -0.3916264772415161, 0.2010345458984375, -0.0949845165014267, -0.9290891289710999, 0.036939337849617004, 0.4807334542274475, -0.43222975730895996, 0.4582868218421936, -0.7309173941612244, -0.40146252512931824, -0.0790913850069046, 0.30008578300476074, -0.07962419837713242, -0.04181439429521561, 0.4643430709838867, -0.09318945556879044, 0.832696259021759, -0.23716489970684052, -0.2720105051994324, -0.31960874795913696, -0.2165318876504898, -0.07779037207365036, 0.24225905537605286, -0.3350408971309662, 0.45936018228530884, -0.6764371991157532, -0.282071053981781, -0.608694314956665, 0.18740040063858032, 0.712000846862793, 0.8103376030921936, 0.3949812948703766, 0.8604879975318909, -0.564978301525116, 0.4670211672782898, 0.45632117986679077, -1.0991795063018799, -0.21925455331802368, 0.46034038066864014, -0.29301440715789795, 0.2505779266357422, 0.023825401440262794, -0.5534631013870239, -0.607282280921936, -0.17398160696029663, 0.12944185733795166, -1.1007130146026611, -0.5205414295196533, -0.8484166860580444, -0.938091516494751, 0.11535482108592987, 0.7026045322418213, 0.5915770530700684, -0.44985246658325195, -0.31338298320770264, -0.890227198600769, -0.18022745847702026, -0.07905983179807663, -0.7299576997756958, -0.00041062384843826294, -0.011586174368858337, -0.7742472887039185, 1.1205072402954102, -0.12125957757234573, 0.5731230974197388, -0.3070484399795532, 0.7811100482940674, 1.107053518295288, 0.2392907291650772, -0.3894748091697693, -0.5749133229255676, 0.031106166541576385, 1.0010945796966553, -0.4663481116294861, 0.7215761542320251, -0.49835333228111267, 0.566546618938446, -0.793113112449646, 0.7524406909942627, 0.2167627364397049, 0.066452756524086, -0.34261077642440796, -0.15108773112297058, 0.5353180170059204, -0.2549436688423157, -0.13877278566360474, 0.09399250894784927, 0.5330569744110107, 0.3929685354232788, 0.5049965977668762, -0.32496827840805054, -0.3851473927497864, -0.2305808812379837, -0.9190705418586731, 0.13571560382843018, -0.3116469085216522, -0.48424822092056274, -0.5013325214385986, 0.0808580219745636, 0.948278546333313, -0.747215211391449, 0.9086653590202332, 0.4982105493545532, -0.2026059329509735, 0.2630583941936493, 0.6418015956878662, -0.016781244426965714, 0.18258672952651978, -0.14662423729896545, -0.5752875804901123, -0.49860456585884094, -0.7870765924453735, 0.5718309879302979, -0.6927717924118042, -0.8439255952835083, -0.5977202653884888, -0.07253439724445343, 0.520668625831604, -0.2925039827823639, 1.1931458711624146, 0.06308768689632416, -0.5832185745239258, 0.05279581621289253, 0.16163136065006256, -0.3273921608924866, 0.4212898015975952, 0.6772359013557434, -0.4389362335205078, -0.6123511791229248, 0.20199038088321686, -0.022663820534944534, -0.3465842604637146, -0.23163020610809326, 1.1717801094055176, 0.06450772285461426, -0.7135047912597656, 0.18292467296123505, 0.7428951263427734, -0.44597309827804565, -0.24376869201660156, 0.21708160638809204, 0.30248355865478516, -0.07220010459423065, -0.5465090274810791, 0.5557529926300049, -0.6094444990158081, -0.6719328165054321, -0.1851312518119812, 0.5331997871398926, -1.0006279945373535, 0.5394922494888306, 0.5033101439476013, -0.2890356183052063, -0.26312851905822754, -0.050377413630485535, 1.1433863639831543, -0.7427992224693298, -0.9410377740859985, -0.36436837911605835, -0.8119039535522461, -0.04507310688495636, -0.5293519496917725, -0.010219756513834, -0.6902176141738892, 0.824103057384491, 0.2507655620574951, 0.47482991218566895, 0.3007909059524536, 0.7051570415496826, -0.12130837142467499, 0.25702300667762756, 0.044735491275787354, -1.2264553308486938, -0.2418382167816162, 0.14543119072914124, 0.3439885377883911, 0.5145031809806824, -0.5686829090118408, -0.008920334279537201, 0.8904365301132202, -0.026955226436257362, -0.2530285120010376, -0.3220033049583435, -0.17091773450374603, -0.3942142724990845, -0.8789323568344116, -0.5907538533210754, -0.7903696298599243, 0.02642427757382393, -0.34940963983535767, -0.05582001805305481, -0.7886791229248047, 1.9008293747901917e-05, 0.1454518586397171, -1.0198216438293457, 0.2729780375957489, 0.10160268843173981, 0.4682956337928772, -0.7266994118690491, -1.4008302688598633, 0.24187009036540985, -0.5726985335350037, -0.10253255069255829, -0.06425291299819946, -0.4159528613090515, 0.17469526827335358, -0.48641157150268555, 0.7990923523902893, -0.37576547265052795, 0.29366108775138855, -0.2018369883298874, -0.9042074680328369, -0.4343176484107971, 0.6579655408859253, -0.9096442461013794, 0.8204749822616577, 0.44443100690841675, -0.3369848132133484, 0.20857568085193634, 0.3377029597759247, -0.901660680770874, 0.37010708451271057, 0.5255993604660034, 0.14651209115982056, 0.13778844475746155, -0.4173051714897156, -0.32092422246932983, -0.3823261857032776, -1.369105339050293, 0.15646791458129883, 0.2510871887207031, -0.23500168323516846, -0.041627734899520874, -0.947081983089447, -0.2933226227760315, 0.7579513192176819, 0.21256527304649353, 0.20915445685386658, 0.40620988607406616, -0.38913846015930176, -0.5057394504547119, 0.485402911901474, 0.25746527314186096, 0.016514882445335388, 0.11830329895019531, 0.2035330832004547, -0.1752530038356781, 0.6671249866485596, -0.4558435082435608, 0.02384588122367859, 0.08479823172092438, -0.24073994159698486, 0.09356479346752167, 0.24976937472820282, -0.23806294798851013, 0.3103982210159302, 0.05959126725792885, -0.6063659191131592, 0.1882980614900589, -0.9256213903427124, -1.0813487768173218, 0.5680775046348572, -1.386497974395752, -0.16907832026481628, -0.1977272480726242, -1.0372562408447266, -0.16283681988716125, 0.2652449607849121, -1.2247447967529297, -0.23476022481918335, -0.30983594059944153, 0.11855743080377579, -0.4384845495223999, 0.5833897590637207, 0.546777606010437, 0.46761971712112427, -0.15224885940551758, 0.058540377765893936, 0.019733507186174393, 0.6188865900039673, -0.5778715014457703, -0.8395630121231079, 0.0713733434677124, -0.009587977081537247, 0.2930872440338135, -0.1285131573677063, -0.4994657337665558, -0.2649732828140259, 0.28046631813049316, 0.44925612211227417, -0.38579925894737244, 0.10664767026901245, -0.22057639062404633, 0.5995914936065674, -0.3352211117744446, -0.03844631463289261, -0.7425252795219421, 0.6083755493164062, 0.3739386796951294, -0.2802036702632904, -0.9246300458908081, 0.4775601625442505, 0.30584824085235596, -0.08470184355974197, 0.7377387285232544, 0.3296443521976471, 0.6568723917007446, 0.10784713923931122, -0.7100614309310913, 0.20865768194198608, 0.22442027926445007, 0.728993833065033, 0.6116448640823364, 0.30152833461761475, 0.7637788653373718, 1.1306451559066772, 0.6179630160331726, 0.4346948266029358, 0.05303596705198288, 0.04686913639307022, 0.06395553797483444, -0.013298459351062775, 0.06662416458129883, -0.1935720145702362, 1.4372972249984741, 0.3895786702632904, -0.022972775623202324, -0.6266477108001709, -0.5511376857757568, 0.36679860949516296, 0.029733283445239067, 0.8777799010276794, -1.367600679397583, 0.0480775386095047, -0.1790233850479126, -0.09037619829177856, 0.38097769021987915, 0.6935775279998779, -0.3571638762950897, -0.16992536187171936, 0.4520416855812073, 0.84665846824646, -0.47115957736968994, 0.875930905342102, 0.4434860348701477, -0.25087642669677734, -0.4265540540218353, 0.2707561254501343, 0.18895350396633148, 1.054748773574829, -0.5047569274902344, -1.3315894603729248, -1.2465893030166626, 0.62895268201828, -0.6213496327400208, -0.6459817290306091, 0.5583102107048035, -0.06259670108556747, -0.5487907528877258, -0.5394873023033142, 0.6594152450561523, -0.37108394503593445, -0.026911862194538116, 0.5028120279312134, 0.41523677110671997, -0.573609471321106, 1.2422900199890137, -0.09511430561542511, 0.6719448566436768, 0.18465498089790344, 0.8376073241233826, -0.26280519366264343, 1.3012267351150513, 0.010203979909420013, -0.39027056097984314, -0.7484408617019653, 0.5484521389007568, -0.47740742564201355, -0.9487504959106445, -0.46460217237472534, -0.8174793124198914, -0.005963332951068878, -0.08030138164758682, -0.43547236919403076, 0.192084401845932, 0.7987304329872131, -0.509638786315918, -0.2689579427242279, -0.3555351495742798, 0.3539787530899048, -0.3194643259048462, -0.42423954606056213, -0.0411318838596344, 1.0003646612167358, 0.79655921459198, -0.3618696928024292, -0.3644617199897766, -0.2942426800727844, -0.2159576117992401, -0.8360722064971924, 0.32322484254837036, 0.3387587070465088, 0.4362374544143677, -0.7436996698379517, 0.08553863316774368, -0.30165430903434753, -0.06622327864170074, -0.6364331245422363, -1.0269265174865723, 0.6463419795036316, 0.4667032063007355, 0.39596661925315857, 0.22143816947937012, -1.059319019317627, -0.014219586737453938, 0.09969253838062286, 1.0488382577896118, -0.367811918258667, 1.234555959701538, 0.3809306025505066, 0.38337892293930054, -0.10893461108207703, -0.3347031772136688, 0.338556706905365, -0.46364259719848633, -0.16498418152332306, 0.15955519676208496, 0.06083419546484947, 0.18044748902320862, -0.5265988111495972, 0.6595878005027771, 0.785259485244751, -0.7792114615440369, -0.41269686818122864, -0.35175439715385437, 0.5266921520233154, -0.7481588125228882, 0.599908709526062, -0.5176360607147217, 0.45408138632774353, -0.02062057889997959, 0.21765360236167908, -0.20932917296886444, -1.0060402154922485, 3.467928886413574, 0.9940065741539001, 0.5919326543807983, 0.01004735752940178, 0.2598934471607208, 0.47396397590637207, -0.4152914881706238, -0.8487117290496826, 0.04073500633239746, -0.5835176706314087, 0.284261554479599, 0.20028692483901978, 0.854951024055481, -0.3734530806541443, -0.23072761297225952, 1.0264363288879395, -1.2543106079101562, -0.6871843338012695, 0.10525736212730408, -0.8460168838500977, -0.6578510999679565, 0.7060235142707825, -0.16834594309329987, -0.14080484211444855, -0.021568123251199722, 0.029749073088169098, 0.819553017616272, -0.13415668904781342, -0.8406407833099365, -0.5303366184234619, -0.389392614364624, -0.3018178939819336, 0.5428423285484314, -0.422498494386673, -1.390771746635437, 1.0100593566894531, -0.2126038670539856, -1.1701884269714355, 0.18447118997573853, 1.1039845943450928, -1.300994634628296, -0.1956050544977188, 0.6804056167602539, -0.0567752867937088, -0.5707401037216187, 0.9353824257850647, -0.8834109306335449, 0.2061392366886139, -0.14898259937763214, -0.7246346473693848, 0.510789692401886, -0.859046459197998, -0.29239174723625183, -0.8050265312194824, -1.1165438890457153, -0.02530476078391075, 0.4867628216743469, -0.5333938598632812, -0.08445237576961517, 0.1315089464187622, 0.5128399729728699, 0.09563455730676651, -0.24653111398220062, -0.14261025190353394, -0.48356106877326965, 0.5017324686050415, 0.32435059547424316, 0.9219766855239868, -0.8286813497543335, 0.007915866561233997, 0.0019398331642150879, -0.43667876720428467, 0.02966877818107605, 0.5187044739723206, -0.01019168272614479, 0.6737887859344482, -0.5807812809944153, 0.43405985832214355, 0.15734486281871796, -0.6133139133453369, 0.15891176462173462, -0.43093764781951904, -0.5093957185745239, 0.5772749185562134, 0.8348362445831299, 0.5752806663513184, -0.2335086166858673, -1.4270620346069336, -0.5845642685890198, 0.8998867273330688, 0.5464873313903809, 0.29744836688041687, 0.6627489328384399, 0.21241745352745056, -0.27771857380867004]\n",
            " With shape: 1024\n"
          ]
        },
        {
          "ename": "ClientError",
          "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 1024 dimensions, but indexed vectors have 500.}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m password \u001b[38;5;241m=\u001b[39m NEO4J_PASSWORD\n\u001b[1;32m     34\u001b[0m vector_index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkVectorIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your actual vector index name\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m similar_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mquery_similar_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node, score \u001b[38;5;129;01min\u001b[39;00m similar_nodes:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(node, score)\n",
            "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mquery_similar_nodes\u001b[0;34m(uri, user, password, vector_index_name, query_vector, top_k)\u001b[0m\n\u001b[1;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mCALL db.index.vector.queryNodes(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_index_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, $queryVector)\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mYIELD node, score\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mRETURN node, score\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(query, queryVector\u001b[38;5;241m=\u001b[39mquery_vector)\n\u001b[0;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/work/result.py:270\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
            "\u001b[0;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 1024 dimensions, but indexed vectors have 500.}"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def generate_query_vector(text, model_name=\"WhereIsAI/UAE-Large-V1\", model_kwargs={'device': 'mps'}):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "    return embeddings.embed_query(text)\n",
        "\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def query_similar_nodes(uri, user, password, vector_index_name, query_vector, top_k=10):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    similar_nodes = []\n",
        "    with driver.session() as session:\n",
        "        query = f\"\"\"\n",
        "        CALL db.index.vector.queryNodes('{vector_index_name}', {top_k}, $queryVector)\n",
        "        YIELD node, score\n",
        "        RETURN node, score\n",
        "        \"\"\"\n",
        "        result = session.run(query, queryVector=query_vector)\n",
        "        for record in result:\n",
        "            node = record[\"node\"]\n",
        "            score = record[\"score\"]\n",
        "            similar_nodes.append((node, score))\n",
        "    driver.close()\n",
        "    return similar_nodes\n",
        "\n",
        "\n",
        "query_text = \"X-ray crystallography\"\n",
        "query_vector = generate_query_vector(query_text)\n",
        "print(f\"Query vector: {query_vector}\\n With shape: {len(query_vector)}\")\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "vector_index_name = \"chunkVectorIndex\"  # Replace with your actual vector index name\n",
        "\n",
        "similar_nodes = query_similar_nodes(uri, user, password, vector_index_name, query_vector)\n",
        "for node, score in similar_nodes:\n",
        "    print(node, score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import OllamaEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from typing import List, Any\n",
        "#from utils import BaseLogger\n",
        "from langchain.chains import GraphCypherQAChain \n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" \n",
        "def load_embedding_model(embedding_model_name: str, config={}):\n",
        "    if embedding_model_name == \"ollama\":\n",
        "        embeddings = OllamaEmbeddings(\n",
        "            base_url=config[\"ollama_base_url\"], model=\"llama2\"\n",
        "        )\n",
        "        dimension = 4096\n",
        "        #logger.info(\"Embedding: Using Ollama\")\n",
        "    elif embedding_model_name == \"openai\":\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        dimension = 1536\n",
        "        #logger.info(\"Embedding: Using OpenAI\")\n",
        "    elif embedding_model_name == \"WhereIsAI/UAE-Large-V1\":\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"WhereIsAI/UAE-Large-V1\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 1024\n",
        "        #logger.info(\"Embedding: Using HuggingFace\")\n",
        "    else:\n",
        "        embeddings = SentenceTransformerEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 384\n",
        "        #logger.info(\"Embedding: Using SentenceTransformer\")\n",
        "    return embeddings, dimension\n",
        "\n",
        "\n",
        "def load_llm(llm_name: str, config={}):\n",
        "    if llm_name == \"gpt-4\":\n",
        "        logger.info(\"LLM: Using GPT-4\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True)\n",
        "    elif llm_name == \"gpt-3.5\":\n",
        "        logger.info(\"LLM: Using GPT-3.5\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "    elif len(llm_name):\n",
        "        logger.info(f\"LLM: Using Ollama: {llm_name}\")\n",
        "        return ChatOllama(\n",
        "            temperature=0,\n",
        "            base_url=config[\"ollama_base_url\"],\n",
        "            model=llm_name,\n",
        "            streaming=True,\n",
        "            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.\n",
        "            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.\n",
        "            num_ctx=3072,  # Sets the size of the context window used to generate the next token.\n",
        "        )\n",
        "    logger.info(\"LLM: Using GPT-3.5\")\n",
        "    return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "\n",
        "\n",
        "def configure_llm_only_chain(llm):\n",
        "    # LLM only response\n",
        "    template = \"\"\"\n",
        "    You are a helpful assistant that helps with answering general questions.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    \"\"\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "    human_template = \"{text}\"\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [system_message_prompt, human_message_prompt]\n",
        "    )\n",
        "\n",
        "    def generate_llm_output(\n",
        "        user_input: str, callbacks: List[Any], prompt=chat_prompt\n",
        "    ) -> str:\n",
        "        answer = llm(\n",
        "            prompt.format_prompt(\n",
        "                text=user_input,\n",
        "            ).to_messages(),\n",
        "            callbacks=callbacks,\n",
        "        ).content\n",
        "        return {\"answer\": answer}\n",
        "\n",
        "    return generate_llm_output\n",
        "\n",
        "\n",
        "def configure_qa_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response\n",
        "    general_system_template = \"\"\"\n",
        "    Use the following context to answer the question at the end.\n",
        "    The context contains article summaries, related topics, and hypothetical questions.\n",
        "    Make sure to rely on accurate information from the summaries and topics.\n",
        "    If a particular article or topic in the context is useful, refer to it in your response.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Your response should be concise and based on the given context.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS questionEmb, score\n",
        "        MATCH (questionEmb) <-[:HAS_EMBEDDING]- (article:Article)\n",
        "        OPTIONAL MATCH (article)-[:HAS_SUMMARY]->(summary:Summary)\n",
        "        OPTIONAL MATCH (article)-[:HAS_TOPIC]->(topic:Topic)\n",
        "        RETURN '##Article ID: ' + article.id + '\\n' \n",
        "            + '##Summary: ' + coalesce(summary.content, 'No summary available') + '\\n'\n",
        "            + '##Related Topics: ' + coalesce(topic.name, 'No topics available') AS text, \n",
        "            score AS similarity\n",
        "        ORDER BY similarity DESC LIMIT 5\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=3375,\n",
        "    )\n",
        "    return kg_qa\n",
        "\n",
        "# ADDED\n",
        "# >>>> Extended to support vector search over strucutured chunking\n",
        "\n",
        "def configure_qa_structure_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response based on vector search and retrieval of structured chunks\n",
        "    \n",
        "    sample_query = \"\"\"\n",
        "    // 0 - prepare question and its embedding \n",
        "        MATCH (ch:Chunk) -[:HAS_EMBEDDING]-> (chemb) \n",
        "        WHERE ch.block_idx = 19\n",
        "        WITH ch.sentences AS question, chemb.value AS qemb\n",
        "        // 1 - search chunk vectors\n",
        "        CALL db.index.vector.queryNodes($index_name, $k, qemb) YIELD node, score\n",
        "        // 2 - retrieve connectd chunks, sections and documents\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (answer) -[:HAS_PARENT*]-> (s:Section)\n",
        "        WITH s, score LIMIT 1\n",
        "        MATCH (d:Document) <-[*]- (s) <-[:HAS_PARENT*]- (chunk:Chunk)\n",
        "        WITH d, s, chunk, score ORDER BY chunk.block_idx ASC\n",
        "        // 3 - prepare results\n",
        "        WITH d, collect(chunk) AS chunks, score\n",
        "        RETURN {source: d.url, page: chunks[0].page_idx} AS metadata, \n",
        "            reduce(text = \"\", x IN chunks | text + x.sentences + '.') AS text, score;   \n",
        "    \"\"\"\n",
        "\n",
        "    general_system_template = \"\"\"\n",
        "    You are an assistant providing detailed answers based on specific chunks of articles.\n",
        "    Use the context provided to answer the question at the end.\n",
        "    Ensure that the context is not altered and that your responses are based on the content of the chunks.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Each answer should include reference to the relevant document and page, as indicated in the context metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        node_label=\"Embedding\",  # embedding node label\n",
        "        embedding_node_property=\"embedding\",  # embedding value property\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (chunk:Chunk) -[:HAS_CHUNK]-> (article:Article)\n",
        "        WITH article, chunk, score\n",
        "        ORDER BY score DESC LIMIT 10\n",
        "        MATCH (chunk)-[:NEXT]->(nextChunk:Chunk)\n",
        "        WITH article, chunk, nextChunk, score\n",
        "        RETURN '##Article ID: ' + article.id + '\\n'\n",
        "            + 'Relevant Chunk: ' + chunk.content + '\\n'\n",
        "            + 'Next Chunk: ' + nextChunk.content AS text, \n",
        "            score AS similarity\n",
        "        LIMIT 3\n",
        "\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 25}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=7000,      # gpt-4\n",
        "    )\n",
        "    return kg_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name Literature_Review/cache/WhereIsAI_UAE-Large-V1. Creating a new one with MEAN pooling.\n",
            "2024-01-22 13:05:03.756 LLM: Using GPT-4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#!pip install streamlit\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "from streamlit.logger import get_logger\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from dotenv import load_dotenv\n",
        "#from utils import (\n",
        "#    extract_title_and_question,\n",
        "#    create_vector_index,\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "# >>>> initialise - environemnt <<<< \n",
        "\n",
        "#load_dotenv(\".env\")\n",
        "\"\"\"\n",
        "url = os.getenv(\"NEO4J_URI\")\n",
        "username = os.getenv(\"NEO4J_USERNAME\")\n",
        "password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "database = os.getenv(\"NEO4J_DATABASE\")\n",
        "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\n",
        "embedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n",
        "llm_name = os.getenv(\"LLM\")\n",
        "\"\"\"\n",
        "url = NEO4J_URI\n",
        "username = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "database = \"neo4j\"\n",
        "#ollama_base_url = \"http://localhost:8000\"\n",
        "embedding_model_name = \"WhereIsAI/UAE-Large-V1\"\n",
        "llm_name = \"gpt-4\"\n",
        "\n",
        "\n",
        "# Remapping for Langchain Neo4j integration\n",
        "# os.environ[\"NEO4J_URL\"] = url\n",
        "\n",
        "\n",
        "# >>>> initialise - services <<<< \n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "neo4j_graph = Neo4jGraph(url=url, username=username, password=password, database=database)\n",
        "\n",
        "embeddings, dimension = load_embedding_model(\n",
        "    embedding_model_name)\n",
        "\n",
        "llm = load_llm(llm_name)\n",
        "\n",
        "\n",
        "# llm_chain: LLM only response\n",
        "llm_chain = configure_llm_only_chain(llm)\n",
        "\n",
        "# rag_chain: KG augmented response\n",
        "rag_chain = configure_qa_structure_rag_chain(\n",
        "    llm, embeddings, embeddings_store_url=url, username=username, password=password\n",
        ")\n",
        "\n",
        "# SKIPPED: create_vector_index(neo4j_graph, dimension)\n",
        "\n",
        "# >>>> Class definition - StreamHander <<<< \n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# >>>> Streamlit UI <<<<\n",
        "\n",
        "styl = f\"\"\"\n",
        "<style>\n",
        "    /* not great support for :has yet (hello FireFox), but using it for now */\n",
        "    .element-container:has([aria-label=\"Select RAG mode\"]) {{\n",
        "      position: fixed;\n",
        "      bottom: 33px;\n",
        "      background: white;\n",
        "      z-index: 101;\n",
        "    }}\n",
        "    .stChatFloatingInputContainer {{\n",
        "        bottom: 20px;\n",
        "    }}\n",
        "\n",
        "    /* Generate question text area */\n",
        "    textarea[aria-label=\"Description\"] {{\n",
        "        height: 200px;\n",
        "    }}\n",
        "</style>\n",
        "\"\"\"\n",
        "st.markdown(styl, unsafe_allow_html=True)\n",
        "st.image(\"qna-logo.png\", width=160) \n",
        "\n",
        "# >>>> UI interations <<<<\n",
        "\n",
        "def chat_input():\n",
        "    user_input = st.chat_input(\"What service questions can I help you resolve today?\")\n",
        "\n",
        "    if user_input:\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_input)\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.caption(f\"RAG: {name}\")\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "\n",
        "            # Call chain to generate answers\n",
        "            result = output_function(\n",
        "                {\"question\": user_input, \"chat_history\": []}, callbacks=[stream_handler]\n",
        "            )[\"answer\"]\n",
        "\n",
        "            output = result\n",
        "\n",
        "            st.session_state[f\"user_input\"].append(user_input)\n",
        "            st.session_state[f\"generated\"].append(output)\n",
        "            st.session_state[f\"rag_mode\"].append(name)\n",
        "\n",
        "\n",
        "def display_chat():\n",
        "    # Initialize session state keys if they do not exist\n",
        "    #if \"generated\" not in st.session_state:\n",
        "    #    st.session_state[\"generated\"] = []\n",
        "    if \"user_input\" not in st.session_state:\n",
        "        st.session_state[\"user_input\"] = []\n",
        "    if \"rag_mode\" not in st.session_state:\n",
        "        st.session_state[\"rag_mode\"] = []\n",
        "\n",
        "    # Now you can safely access st.session_state[\"generated\"] and other keys\n",
        "\n",
        "\n",
        "\n",
        "def mode_select() -> str:\n",
        "    options = [\"Disabled\", \"Enabled\"]\n",
        "    return st.radio(\"Select RAG mode\", options, horizontal=True)\n",
        "\n",
        "# >>>>> switch on/off RAG mode\n",
        "\n",
        "name = mode_select()\n",
        "if name == \"LLM only\" or name == \"Disabled\":\n",
        "    output_function = llm_chain\n",
        "elif name == \"Vector + Graph\" or name == \"Enabled\":\n",
        "    output_function = rag_chain\n",
        "\n",
        "\"\"\"\n",
        "def generate_ticket():\n",
        "    # Get high ranked questions\n",
        "    records = neo4j_graph.query(\n",
        "        \"MATCH (q:Question) RETURN q.title AS title, q.body AS body ORDER BY q.score DESC LIMIT 3\"\n",
        "    )\n",
        "    questions = []\n",
        "    for i, question in enumerate(records, start=1):\n",
        "        questions.append((question[\"title\"], question[\"body\"]))\n",
        "    # Ask LLM to generate new question in the same style\n",
        "    questions_prompt = \"\"\n",
        "    for i, question in enumerate(questions, start=1):\n",
        "        questions_prompt += f\"{i}. {question[0]}\\n\"\n",
        "        questions_prompt += f\"{question[1]}\\n\\n\"\n",
        "        questions_prompt += \"----\\n\\n\"\n",
        "\n",
        "    gen_system_template = f\\\"\"\"\n",
        "    You're an expert in formulating high quality questions. \n",
        "    Can you formulate a question in the same style, detail and tone as the following example questions?\n",
        "    {questions_prompt}\n",
        "    ---\n",
        "\n",
        "    Don't make anything up, only use information in the following question.\n",
        "    Return a title for the question, and the question post itself.\n",
        "\n",
        "    Return example:\n",
        "    ---\n",
        "    Title: How do I use the Neo4j Python driver?\n",
        "    Question: I'm trying to connect to Neo4j using the Python driver, but I'm getting an error.\n",
        "    ---\n",
        "    \\\"\"\"\n",
        "    # we need jinja2 since the questions themselves contain curly braces\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "        gen_system_template, template_format=\"jinja2\"\n",
        "    )\n",
        "    q_prompt = st.session_state[f\"user_input\"][-1]\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            system_prompt,\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \\\"\"\"\n",
        "                Respond in the following format or you will be unplugged.\n",
        "                ---\n",
        "                Title: New title\n",
        "                Question: New question\n",
        "                ---\n",
        "                \\\"\"\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "        ]\n",
        "    )\n",
        "    llm_response = llm_chain(\n",
        "        f\"Here's the question to rewrite in the expected format: ```{q_prompt}```\",\n",
        "        [],\n",
        "        chat_prompt,\n",
        "    )\n",
        "    new_title, new_question = extract_title_and_question(llm_response[\"answer\"])\n",
        "    return (new_title, new_question)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def open_sidebar():\n",
        "    st.session_state.open_sidebar = True\n",
        "\n",
        "\n",
        "def close_sidebar():\n",
        "    st.session_state.open_sidebar = False\n",
        "\n",
        "\n",
        "if not \"open_sidebar\" in st.session_state:\n",
        "    st.session_state.open_sidebar = False\n",
        "\"\"\"\n",
        "if st.session_state.open_sidebar:\n",
        "    new_title, new_question = generate_ticket()\n",
        "    with st.sidebar:\n",
        "        st.title(\"Ticket draft\")\n",
        "        st.write(\"Auto generated draft ticket\")\n",
        "        st.text_input(\"Title\", new_title)\n",
        "        st.text_area(\"Description\", new_question)\n",
        "        st.button(\n",
        "            \"Submit to support team\",\n",
        "            type=\"primary\",\n",
        "            key=\"submit_ticket\",\n",
        "            on_click=close_sidebar,\n",
        "        )\n",
        "\"\"\"\n",
        "\n",
        "# >>>> UI: show chat <<<<\n",
        "display_chat()\n",
        "chat_input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### *****************************************************************\n",
        "# Neo4j configuration\n",
        "#\n",
        "# For more details and a complete list of settings, please see\n",
        "# https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/\n",
        "#*****************************************************************\n",
        "\n",
        "# Paths of directories in the installation.\n",
        "#server.directories.data=data\n",
        "#server.directories.plugins=plugins\n",
        "#server.directories.logs=logs\n",
        "#server.directories.lib=lib\n",
        "#server.directories.run=run\n",
        "#server.directories.licenses=licenses\n",
        "#server.directories.metrics=metrics\n",
        "#server.directories.dumps.root=data/dumps\n",
        "#server.directories.transaction.logs.root=data/transactions\n",
        "\n",
        "# This setting constrains all `LOAD CSV` import files to be under the `import` directory. Remove or comment it out to\n",
        "# allow files to be loaded from anywhere in the filesystem; this introduces possible security problems. See the\n",
        "# `LOAD CSV` section of the manual for details.\n",
        "server.directories.import=import\n",
        "\n",
        "# Whether requests to Neo4j are authenticated.\n",
        "# To disable authentication, uncomment this line\n",
        "dbms.security.auth_enabled=true\n",
        "\n",
        "# Number of databases in Neo4j is limited.\n",
        "# To change this limit please uncomment and adapt following setting:\n",
        "# dbms.max_databases=100\n",
        "\n",
        "# Enable online backups to be taken from this database.\n",
        "#server.backup.enabled=true\n",
        "\n",
        "# By default the backup service will only listen on localhost.\n",
        "# To enable remote backups you will have to bind to an external\n",
        "# network interface (e.g. 0.0.0.0 for all interfaces).\n",
        "# The protocol running varies depending on deployment. In a cluster this is the\n",
        "# same protocol that runs on server.cluster.listen_address.\n",
        "#server.backup.listen_address=0.0.0.0:6362\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial DBMS Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial DBMS settings are picked up from the config file once, when a cluster first starts, and then transferred into\n",
        "# the running DBMS. This means later changes to the values will not be seen. There are procedures to change the values\n",
        "# after the initial start\n",
        "\n",
        "# Name of the default database (aliases are not supported). Can be changed with the 'dbms.setDefaultDatabase' procedure.\n",
        "#initial.dbms.default_database=neo4j\n",
        "\n",
        "# Initial default number of primary and secondary instances of user databases. If the user does not specify the number\n",
        "# of primaries and secondaries in 'CREATE DATABASE', these values will be used, unless they are overwritten with the\n",
        "# 'dbms.setDefaultAllocationNumbers' procedure.\n",
        "#initial.dbms.default_primaries_count=1\n",
        "#initial.dbms.default_secondaries_count=0\n",
        "\n",
        "#********************************************************************\n",
        "# Memory Settings\n",
        "#********************************************************************\n",
        "#\n",
        "# Memory settings are specified kibibytes with the 'k' suffix, mebibytes with\n",
        "# 'm' and gibibytes with 'g'.\n",
        "# If Neo4j is running on a dedicated server, then it is generally recommended\n",
        "# to leave about 2-4 gigabytes for the operating system, give the JVM enough\n",
        "# heap to hold all your transaction state and query context, and then leave the\n",
        "# rest for the page cache.\n",
        "\n",
        "# Java Heap Size: by default the Java heap size is dynamically calculated based\n",
        "# on available system resources. Uncomment these lines to set specific initial\n",
        "# and maximum heap size.\n",
        "server.memory.heap.initial_size=512m\n",
        "server.memory.heap.max_size=2g\n",
        "\n",
        "# The amount of memory to use for mapping the store files.\n",
        "# The default page cache memory assumes the machine is dedicated to running\n",
        "# Neo4j, and is heuristically set to 50% of RAM minus the Java heap size.\n",
        "#server.memory.pagecache.size=10g\n",
        "\n",
        "# Limit the amount of memory that all of the running transaction can consume.\n",
        "# The default value is 70% of the heap size limit.\n",
        "#dbms.memory.transaction.total.max=256m\n",
        "\n",
        "# Limit the amount of memory that a single transaction can consume.\n",
        "# By default there is no limit.\n",
        "#db.memory.transaction.max=16m\n",
        "\n",
        "#*****************************************************************\n",
        "# Network connector configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# With default configuration Neo4j only accepts local connections.\n",
        "# Use 0.0.0.0 to bind to all network interfaces on the machine. If you want to only use a specific interface\n",
        "# (such as a private IP address on AWS, for example) then use that IP address instead.\n",
        "server.default_listen_address=0.0.0.0\n",
        "\n",
        "# You can also choose a specific network interface, and configure a non-default\n",
        "# port for each connector, by setting their individual listen_address.\n",
        "\n",
        "# The address at which this server can be reached by its clients. This may be the server's IP address or DNS name, or\n",
        "# it may be the address of a reverse proxy which sits in front of the server. This setting may be overridden for\n",
        "# individual connectors below.\n",
        "#server.default_advertised_address=localhost\n",
        "\n",
        "# You can also choose a specific advertised hostname or IP address, and\n",
        "# configure an advertised port for each connector, by setting their\n",
        "# individual advertised_address.\n",
        "\n",
        "# By default, encryption is turned off.\n",
        "# To turn on encryption, an ssl policy for the connector needs to be configured\n",
        "# Read more in SSL policy section in this file for how to define a SSL policy.\n",
        "\n",
        "# Bolt connector\n",
        "server.bolt.enabled=true\n",
        "#server.bolt.tls_level=DISABLED\n",
        "server.bolt.listen_address=:7687\n",
        "#server.bolt.advertised_address=:7687\n",
        "\n",
        "# HTTP Connector. There can be zero or one HTTP connectors.\n",
        "server.http.enabled=true\n",
        "server.http.listen_address=:7474\n",
        "#server.http.advertised_address=:7474\n",
        "\n",
        "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
        "server.https.enabled=false\n",
        "#server.https.listen_address=:7473\n",
        "#server.https.advertised_address=:7473\n",
        "\n",
        "# Number of Neo4j worker threads.\n",
        "#server.threads.worker_count\n",
        "\n",
        "#*****************************************************************\n",
        "# SSL policy configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Each policy is configured under a separate namespace, e.g.\n",
        "#    dbms.ssl.policy.<scope>.*\n",
        "#    <scope> can be any of 'bolt', 'https', 'cluster' or 'backup'\n",
        "#\n",
        "# The scope is the name of the component where the policy will be used\n",
        "# Each component where the use of an ssl policy is desired needs to declare at least one setting of the policy.\n",
        "# Allowable values are 'bolt', 'https', 'cluster' or 'backup'.\n",
        "\n",
        "# E.g if bolt and https connectors should use the same policy, the following could be declared\n",
        "#   dbms.ssl.policy.bolt.base_directory=certificates/default\n",
        "#   dbms.ssl.policy.https.base_directory=certificates/default\n",
        "# However, it's strongly encouraged to not use the same key pair for multiple scopes.\n",
        "#\n",
        "# N.B: Note that a connector must be configured to support/require\n",
        "#      SSL/TLS for the policy to actually be utilized.\n",
        "#\n",
        "# see: dbms.connector.*.tls_level\n",
        "\n",
        "# SSL settings (dbms.ssl.policy.<scope>.*)\n",
        "#  .base_directory       Base directory for SSL policies paths. All relative paths within the\n",
        "#                        SSL configuration will be resolved from the base dir.\n",
        "#\n",
        "#  .private_key          A path to the key file relative to the '.base_directory'.\n",
        "#\n",
        "#  .private_key_password The password for the private key.\n",
        "#\n",
        "#  .public_certificate   A path to the public certificate file relative to the '.base_directory'.\n",
        "#\n",
        "#  .trusted_dir          A path to a directory containing trusted certificates.\n",
        "#\n",
        "#  .revoked_dir          Path to the directory with Certificate Revocation Lists (CRLs).\n",
        "#\n",
        "#  .verify_hostname      If true, the server will verify the hostname that the client uses to connect with. In order\n",
        "#                        for this to work, the server public certificate must have a valid CN and/or matching\n",
        "#                        Subject Alternative Names.\n",
        "#\n",
        "#  .client_auth          How the client should be authorized. Possible values are: 'none', 'optional', 'require'.\n",
        "#\n",
        "#  .tls_versions         A comma-separated list of allowed TLS versions. By default only TLSv1.2 is allowed.\n",
        "#\n",
        "#  .trust_all            Setting this to 'true' will ignore the trust truststore, trusting all clients and servers.\n",
        "#                        Use of this mode is discouraged. It would offer encryption but no security.\n",
        "#\n",
        "#  .ciphers              A comma-separated list of allowed ciphers. The default ciphers are the defaults of\n",
        "#                        the JVM platform.\n",
        "\n",
        "# Bolt SSL configuration\n",
        "#dbms.ssl.policy.bolt.enabled=true\n",
        "#dbms.ssl.policy.bolt.base_directory=certificates/bolt\n",
        "#dbms.ssl.policy.bolt.private_key=private.key\n",
        "#dbms.ssl.policy.bolt.public_certificate=public.crt\n",
        "#dbms.ssl.policy.bolt.client_auth=NONE\n",
        "\n",
        "# Https SSL configuration\n",
        "#dbms.ssl.policy.https.enabled=true\n",
        "#dbms.ssl.policy.https.base_directory=certificates/https\n",
        "#dbms.ssl.policy.https.private_key=private.key\n",
        "#dbms.ssl.policy.https.public_certificate=public.crt\n",
        "#dbms.ssl.policy.https.client_auth=NONE\n",
        "\n",
        "# Cluster SSL configuration\n",
        "#dbms.ssl.policy.cluster.enabled=true\n",
        "#dbms.ssl.policy.cluster.base_directory=certificates/cluster\n",
        "#dbms.ssl.policy.cluster.private_key=private.key\n",
        "#dbms.ssl.policy.cluster.public_certificate=public.crt\n",
        "\n",
        "# Backup SSL configuration\n",
        "#dbms.ssl.policy.backup.enabled=true\n",
        "#dbms.ssl.policy.backup.base_directory=certificates/backup\n",
        "#dbms.ssl.policy.backup.private_key=private.key\n",
        "#dbms.ssl.policy.backup.public_certificate=public.crt\n",
        "\n",
        "#*****************************************************************\n",
        "# Logging configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# To enable HTTP logging, uncomment this line\n",
        "#dbms.logs.http.enabled=true\n",
        "\n",
        "# To enable GC Logging, uncomment this line\n",
        "#server.logs.gc.enabled=true\n",
        "\n",
        "# GC Logging Options\n",
        "# see https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-BE93ABDC-999C-4CB5-A88B-1994AAAC74D5\n",
        "#server.logs.gc.options=-Xlog:gc*,safepoint,age*=trace\n",
        "\n",
        "# Number of GC logs to keep.\n",
        "#server.logs.gc.rotation.keep_number=5\n",
        "\n",
        "# Size of each GC log that is kept.\n",
        "#server.logs.gc.rotation.size=20m\n",
        "\n",
        "# Log executed queries. One of OFF, INFO and VERBOSE. INFO logs queries longer than a given threshold, VERBOSE logs start and end of all queries.\n",
        "#db.logs.query.enabled=VERBOSE\n",
        "\n",
        "# If the execution of query takes more time than this threshold, the query is logged. If set to zero then all queries\n",
        "# are logged. Only used if `db.logs.query.enabled` is set to INFO\n",
        "#db.logs.query.threshold=0\n",
        "\n",
        "# Include parameters for the executed queries being logged (this is enabled by default).\n",
        "#db.logs.query.parameter_logging_enabled=true\n",
        "\n",
        "# The security log is always enabled when `dbms.security.auth_enabled=true`, for addition\n",
        "# configuration, look at $NEO4J_HOME/conf/server-logs.xml\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Uncomment and specify these lines for running Neo4j in a cluster.\n",
        "# See the cluster documentation at https://neo4j.com/docs/ for details.\n",
        "\n",
        "# A comma-separated list of endpoints which a server should contact in order to discover other cluster members. It must\n",
        "# be in the host:port format. For each machine in the cluster, the address will usually be the public ip address of\n",
        "# that machine. The port will be the value used in the setting \"server.discovery.advertised_address\" of that server.\n",
        "#dbms.cluster.discovery.endpoints=localhost:5000,localhost:5001,localhost:5002\n",
        "\n",
        "# Host and port to bind the cluster member discovery management communication.\n",
        "# This is the setting to add to the collection of addresses in dbms.cluster.discovery.endpoints.\n",
        "server.discovery.listen_address=:5002\n",
        "#server.discovery.advertised_address=:5000\n",
        "\n",
        "# Network interface and port for the transaction shipping server to listen on.\n",
        "# Please note that it is also possible to run the backup client against this port so always limit access to it via the\n",
        "# firewall and configure an ssl policy.\n",
        "server.cluster.listen_address=:6001\n",
        "#server.cluster.advertised_address=:6000\n",
        "\n",
        "# Network interface and port for the RAFT server to listen on.\n",
        "server.cluster.raft.listen_address=:7002\n",
        "#server.cluster.raft.advertised_address=:7000\n",
        "\n",
        "# Network interface and port for server-side routing within the cluster. This allows requests to be forwarded\n",
        "# from one cluster member to another, if the requests can't be satisfied by the first member (e.g. write requests\n",
        "# received by a non-leader).\n",
        "server.routing.listen_address=:7688\n",
        "#server.routing.advertised_address=:7688\n",
        "\n",
        "# List a set of names for groups to which this server should belong. This\n",
        "# is a comma-separated list and names should only use alphanumericals\n",
        "# and underscore. This can be used to identify groups of servers in the\n",
        "# configuration for load balancing and replication policies.\n",
        "#\n",
        "# The main intention for this is to group servers, but it is possible to specify\n",
        "# a unique identifier here as well which might be useful for troubleshooting\n",
        "# or other special purposes.\n",
        "#server.groups\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial Server Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial server settings are used as the default values when enabling a server, but can be overridden by specifying\n",
        "# options when calling ENABLE (relevant for servers in a cluster *after* those that form the initial cluster).\n",
        "\n",
        "# Restrict the modes of database that can be hosted on this server\n",
        "# Allowed values:\n",
        "# PRIMARY - Host standalone databases, and members of the consensus quorum for a multi-primary database.\n",
        "# SECONDARY - Only host read replicas, eventually-consistent read-only instances of databases.\n",
        "# NONE - Can host any mode of database\n",
        "#initial.server.mode_constraint=NONE\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Load Balancing\n",
        "#*****************************************************************\n",
        "\n",
        "# N.B: Read the online documentation for a thorough explanation!\n",
        "\n",
        "# Selects the load balancing plugin that shall be enabled.\n",
        "#dbms.routing.load_balancing.plugin=server_policies\n",
        "\n",
        "####### Examples for \"server_policies\" plugin #######\n",
        "\n",
        "# Will select all available servers as the default policy, which is the\n",
        "# policy used when the client does not specify a policy preference. The\n",
        "# default configuration for the default policy is all().\n",
        "#dbms.routing.load_balancing.config.server_policies.default=all()\n",
        "\n",
        "# Will select servers in groups 'group1' or 'group2' under the default policy.\n",
        "#dbms.routing.load_balancing.config.server_policies.default=groups(group1,group2)\n",
        "\n",
        "# Slightly more advanced example:\n",
        "# Will select servers in 'group1', 'group2' or 'group3', but only if there are at least 2.\n",
        "# This policy will be exposed under the name of 'mypolicy'.\n",
        "#dbms.routing.load_balancing.config.server_policies.mypolicy=groups(group1,group2,group3) -> min(2)\n",
        "\n",
        "# Below will create an even more advanced policy named 'regionA' consisting of several rules\n",
        "# yielding the following behaviour:\n",
        "#\n",
        "#            select servers in regionA, if at least 2 are available\n",
        "# otherwise: select servers in regionA and regionB, if at least 2 are available\n",
        "# otherwise: select all servers\n",
        "#\n",
        "# The intention is to create a policy for a particular region which prefers\n",
        "# a certain set of local servers, but which will fallback to other regions\n",
        "# or all available servers as required.\n",
        "#\n",
        "# N.B: The following configuration uses the line-continuation character \\\n",
        "#      which allows you to construct an easily readable rule set spanning\n",
        "#      several lines.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.policyA=\\\n",
        "#groups(regionA) -> min(2);\\\n",
        "#groups(regionA,regionB) -> min(2);\n",
        "\n",
        "# Note that implicitly the last fallback is to always consider all() servers,\n",
        "# but this can be prevented by specifying a halt() as the last rule.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.regionA_only=\\\n",
        "#groups(regionA);\\\n",
        "#halt();\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Additional Configuration Options\n",
        "#*****************************************************************\n",
        "# The following settings are used less frequently.\n",
        "# If you don't know what these are, you don't need to change these from their default values.\n",
        "\n",
        "# Cluster Routing Connector. Disable the opening of an additional port to allow\n",
        "# for internal communication using the same security configuration as CLUSTER\n",
        "#dbms.routing.enabled=false\n",
        "\n",
        "# The time window within which the loss of the leader is detected and the first re-election attempt is held.\n",
        "# The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.leader_failure_detection_window=20s-23s\n",
        "\n",
        "# The rate at which leader elections happen. Note that due to election conflicts it might take several attempts to\n",
        "# find a leader. The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.election_failure_detection_window=3s-6s\n",
        "\n",
        "# The time limit allowed for a new member to attempt to update its data to match the rest of the cluster.\n",
        "#dbms.cluster.raft.membership.join_timeout=10m\n",
        "\n",
        "# Maximum amount of lag accepted for a new follower to join the Raft group.\n",
        "#dbms.cluster.raft.membership.join_max_lag=10s\n",
        "\n",
        "# Raft log pruning frequency.\n",
        "#dbms.cluster.raft.log.pruning_frequency=10m\n",
        "\n",
        "# The size to allow the raft log to grow before rotating.\n",
        "#dbms.cluster.raft.log.rotation_size=250M\n",
        "\n",
        "# The name of a server_group whose members should be prioritized as leaders for the given database.\n",
        "# This does not guarantee that members of this group will be leader at all times, but the cluster\n",
        "# will attempt to transfer leadership to such a member when possible.\n",
        "# N.B. the final portion of this config key is dynamic and refers to the name of the database being configured.\n",
        "# You may specify multiple `db.cluster.raft.leader_transfer.priority_group.<database-name>=<server-group>` pairs:\n",
        "#db.cluster.raft.leader_transfer.priority_group.foo\n",
        "#db.cluster.raft.leader_transfer.priority_group.neo4j\n",
        "\n",
        "# Which strategy to use when transferring database leaderships around a cluster.\n",
        "# This can be one of `equal_balancing` or `no_balancing`.\n",
        "# `equal_balancing` automatically ensures that each Core server holds the leader role for an equal number of databases.\n",
        "# `no_balancing` prevents any automatic balancing of the leader role.\n",
        "# Note that if a `leadership_priority_group` is specified for a given database,\n",
        "# the value of this setting will be ignored for that database.\n",
        "#dbms.cluster.raft.leader_transfer.balancing_strategy=equal_balancing\n",
        "\n",
        "# The following setting controls how frequently a server hosting a secondary for a given database attempts to\n",
        "# fetch an update from a server hosting a primary for that database\n",
        "#db.cluster.catchup.pull_interval=1s\n",
        "\n",
        "#********************************************************************\n",
        "# Security Configuration\n",
        "#********************************************************************\n",
        "\n",
        "# The authentication and authorization providers that contains both users and roles.\n",
        "# This can be one of the built-in `native` or `ldap` auth providers,\n",
        "# or it can be an externally provided plugin, with a custom name prefixed by `plugin`,\n",
        "# i.e. `plugin-<AUTH_PROVIDER_NAME>`.\n",
        "dbms.security.authentication_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "dbms.security.authorization_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "\n",
        "# The time to live (TTL) for cached authentication and authorization info when using\n",
        "# external auth providers (LDAP or plugin). Setting the TTL to 0 will\n",
        "# disable auth caching.\n",
        "#dbms.security.auth_cache_ttl=10m\n",
        "\n",
        "# The maximum capacity for authentication and authorization caches (respectively).\n",
        "#dbms.security.auth_cache_max_capacity=10000\n",
        "\n",
        "# Set to log successful authentication events to the security log.\n",
        "# If this is set to `false` only failed authentication events will be logged, which\n",
        "# could be useful if you find that the successful events spam the logs too much,\n",
        "# and you do not require full auditing capability.\n",
        "#dbms.security.log_successful_authentication=true\n",
        "\n",
        "#================================================\n",
        "# LDAP Auth Provider Configuration\n",
        "#================================================\n",
        "\n",
        "# URL of LDAP server to use for authentication and authorization.\n",
        "# The format of the setting is `<protocol>://<hostname>:<port>`, where hostname is the only required field.\n",
        "# The supported values for protocol are `ldap` (default) and `ldaps`.\n",
        "# The default port for `ldap` is 389 and for `ldaps` 636.\n",
        "# For example: `ldaps://ldap.example.com:10389`.\n",
        "#\n",
        "# NOTE: You may want to consider using STARTTLS (`dbms.security.ldap.use_starttls`) instead of LDAPS\n",
        "# for secure connections, in which case the correct protocol is `ldap`.\n",
        "#dbms.security.ldap.host=localhost\n",
        "\n",
        "# Use secure communication with the LDAP server using opportunistic TLS.\n",
        "# First an initial insecure connection will be made with the LDAP server, and then a STARTTLS command\n",
        "# will be issued to negotiate an upgrade of the connection to TLS before initiating authentication.\n",
        "#dbms.security.ldap.use_starttls=false\n",
        "\n",
        "# The LDAP referral behavior when creating a connection. This is one of `follow`, `ignore` or `throw`.\n",
        "# `follow` automatically follows any referrals\n",
        "# `ignore` ignores any referrals\n",
        "# `throw` throws an exception, which will lead to authentication failure\n",
        "#dbms.security.ldap.referral=follow\n",
        "\n",
        "# The timeout for establishing an LDAP connection. If a connection with the LDAP server cannot be\n",
        "# established within the given time the attempt is aborted.\n",
        "# A value of 0 means to use the network protocol's (i.e., TCP's) timeout value.\n",
        "#dbms.security.ldap.connection_timeout=30s\n",
        "\n",
        "# The timeout for an LDAP read request (i.e. search). If the LDAP server does not respond within\n",
        "# the given time the request will be aborted. A value of 0 means wait for a response indefinitely.\n",
        "#dbms.security.ldap.read_timeout=30s\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authentication Configuration\n",
        "#----------------------------------\n",
        "\n",
        "# LDAP authentication mechanism. This is one of `simple` or a SASL mechanism supported by JNDI,\n",
        "# for example `DIGEST-MD5`. `simple` is basic username\n",
        "# and password authentication and SASL is used for more advanced mechanisms. See RFC 2251 LDAPv3\n",
        "# documentation for more details.\n",
        "#dbms.security.ldap.authentication.mechanism=simple\n",
        "\n",
        "# LDAP user DN template. An LDAP object is referenced by its distinguished name (DN), and a user DN is\n",
        "# an LDAP fully-qualified unique user identifier. This setting is used to generate an LDAP DN that\n",
        "# conforms with the LDAP directory's schema from the user principal that is submitted with the\n",
        "# authentication token when logging in.\n",
        "# The special token {0} is a placeholder where the user principal will be substituted into the DN string.\n",
        "#dbms.security.ldap.authentication.user_dn_template=uid={0},ou=users,dc=example,dc=com\n",
        "\n",
        "# Determines if the result of authentication via the LDAP server should be cached or not.\n",
        "# Caching is used to limit the number of LDAP requests that have to be made over the network\n",
        "# for users that have already been authenticated successfully. A user can be authenticated against\n",
        "# an existing cache entry (instead of via an LDAP server) as long as it is alive\n",
        "# (see `dbms.security.auth_cache_ttl`).\n",
        "# An important consequence of setting this to `true` is that\n",
        "# Neo4j then needs to cache a hashed version of the credentials in order to perform credentials\n",
        "# matching. This hashing is done using a cryptographic hash function together with a random salt.\n",
        "# Preferably a conscious decision should be made if this method is considered acceptable by\n",
        "# the security standards of the organization in which this Neo4j instance is deployed.\n",
        "#dbms.security.ldap.authentication.cache_enabled=true\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authorization Configuration\n",
        "#----------------------------------\n",
        "# Authorization is performed by searching the directory for the groups that\n",
        "# the user is a member of, and then map those groups to Neo4j roles.\n",
        "\n",
        "# Perform LDAP search for authorization info using a system account instead of the user's own account.\n",
        "#\n",
        "# If this is set to `false` (default), the search for group membership will be performed\n",
        "# directly after authentication using the LDAP context bound with the user's own account.\n",
        "# The mapped roles will be cached for the duration of `dbms.security.auth_cache_ttl`,\n",
        "# and then expire, requiring re-authentication. To avoid frequently having to re-authenticate\n",
        "# sessions you may want to set a relatively long auth cache expiration time together with this option.\n",
        "# NOTE: This option will only work if the users are permitted to search for their\n",
        "# own group membership attributes in the directory.\n",
        "#\n",
        "# If this is set to `true`, the search will be performed using a special system account user\n",
        "# with read access to all the users in the directory.\n",
        "# You need to specify the username and password using the settings\n",
        "# `dbms.security.ldap.authorization.system_username` and\n",
        "# `dbms.security.ldap.authorization.system_password` with this option.\n",
        "# Note that this account only needs read access to the relevant parts of the LDAP directory\n",
        "# and does not need to have access rights to Neo4j, or any other systems.\n",
        "#dbms.security.ldap.authorization.use_system_account=false\n",
        "\n",
        "# An LDAP system account username to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "# Note that the `dbms.security.ldap.authentication.user_dn_template` will not be applied to this username,\n",
        "# so you may have to specify a full DN.\n",
        "#dbms.security.ldap.authorization.system_username\n",
        "\n",
        "# An LDAP system account password to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "#dbms.security.ldap.authorization.system_password\n",
        "\n",
        "# The name of the base object or named context to search for user objects when LDAP authorization is enabled.\n",
        "# A common case is that this matches the last part of `dbms.security.ldap.authentication.user_dn_template`.\n",
        "#dbms.security.ldap.authorization.user_search_base=ou=users,dc=example,dc=com\n",
        "\n",
        "# The LDAP search filter to search for a user principal when LDAP authorization is\n",
        "# enabled. The filter should contain the placeholder token {0} which will be substituted for the\n",
        "# user principal.\n",
        "#dbms.security.ldap.authorization.user_search_filter=(&(objectClass=*)(uid={0}))\n",
        "\n",
        "# A list of attribute names on a user object that contains groups to be used for mapping to roles\n",
        "# when LDAP authorization is enabled. This setting is ignored when `dbms.ldap_authorization_nested_groups_enabled` is `true`.\n",
        "#dbms.security.ldap.authorization.group_membership_attributes=memberOf\n",
        "\n",
        "# This setting determines whether multiple LDAP search results will be processed (as is required for the lookup of nested groups).\n",
        "# If set to `true` then instead of using attributes on the user object to determine group membership (as specified by\n",
        "# `dbms.security.ldap.authorization.group_membership_attributes`), the `user` object will only be used to determine the user's\n",
        "# Distinguished Name, which will subsequently be used with  `dbms.security.ldap.authorization.user_search_filter`\n",
        "# in order to perform a nested group search. The Distinguished Names of the resultant group search results will be used to determine roles.\n",
        "#dbms.security.ldap.authorization.nested_groups_enabled=false\n",
        "\n",
        "# The search template which will be used to find the nested groups which the user is a member of.\n",
        "# The filter should contain the placeholder token `{0}` which will be substituted with the user's\n",
        "# Distinguished Name (which is found for the specified user principle using `dbms.security.ldap.authorization.user_search_filter`).\n",
        "# The default value specifies Active Directory's LDAP_MATCHING_RULE_IN_CHAIN (aka 1.2.840.113556.1.4.1941) implementation\n",
        "# which will walk the ancestry of group membership for the specified user.\n",
        "#dbms.security.ldap.authorization.nested_groups_search_filter=(&(objectclass=group)(member:1.2.840.113556.1.4.1941:={0}))\n",
        "\n",
        "# An authorization mapping from LDAP group names to Neo4j role names.\n",
        "# The map should be formatted as a semicolon separated list of key-value pairs, where the\n",
        "# key is the LDAP group name and the value is a comma separated list of corresponding role names.\n",
        "# For example: group1=role1;group2=role2;group3=role3,role4,role5\n",
        "#\n",
        "# You could also use whitespaces and quotes around group names to make this mapping more readable,\n",
        "# for example: dbms.security.ldap.authorization.group_to_role_mapping=\\\n",
        "#          \"cn=Neo4j Read Only,cn=users,dc=example,dc=com\"      = reader;    \\\n",
        "#          \"cn=Neo4j Read-Write,cn=users,dc=example,dc=com\"     = publisher; \\\n",
        "#          \"cn=Neo4j Schema Manager,cn=users,dc=example,dc=com\" = architect; \\\n",
        "#          \"cn=Neo4j Administrator,cn=users,dc=example,dc=com\"  = admin\n",
        "#dbms.security.ldap.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# OpenID Connect configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# The display name for the provider. This will be displayed in clients such as Neo4j Browser and Bloom.\n",
        "#dbms.security.oidc.<provider>.display_name\n",
        "\n",
        "# The OIDC auth_flow for clients such as Neo4j Browser and Bloom to use. Supported values are 'pkce' and 'implicit'\n",
        "#dbms.security.oidc.<provider>.auth_flow=pkce\n",
        "\n",
        "# The OpenID Connect Discovery URL for the provider\n",
        "#dbms.security.oidc.<provider>.well_known_discovery_uri\n",
        "\n",
        "# URL of the provider's Authorization Endpoint\n",
        "#dbms.security.oidc.<provider>.auth_endpoint\n",
        "\n",
        "# Parameters to use with the Authorization Endpoint.\n",
        "#dbms.security.oidc.<provider>.auth_params\n",
        "\n",
        "# URL of the provider's OAuth 2.0 Token Endpoint\n",
        "#dbms.security.oidc.<provider>.token_endpoint\n",
        "\n",
        "# Parameters to use with the Token Endpoint.\n",
        "#dbms.security.oidc.<provider>.token_params\n",
        "\n",
        "# URL of the provider's JSON Web Key Set\n",
        "#dbms.security.oidc.<provider>.jwks_uri\n",
        "\n",
        "# URL of the provider's UserInfo Endpoint\n",
        "#dbms.security.oidc.<provider>.user_info_uri\n",
        "\n",
        "# URL that the provider asserts as its issuer identifier. This will be checked against the iss claim in the token\n",
        "#dbms.security.oidc.<provider>.issuer\n",
        "\n",
        "# The expected value for the `aud` claim\n",
        "#dbms.security.oidc.<provider>.audience\n",
        "\n",
        "# The client_id of this client as issued by the provider.\n",
        "#dbms.security.oidc.<provider>.client_id\n",
        "\n",
        "# Whether to fetch the groups claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_groups_from_user_info=false\n",
        "\n",
        "# Whether to fetch the username claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_username_from_user_info=false\n",
        "\n",
        "# The claim to use for the database username.\n",
        "#dbms.security.oidc.<provider>.claims.username=sub\n",
        "\n",
        "# The claim to use for the database roles.\n",
        "#dbms.security.oidc.<provider>.claims.groups\n",
        "\n",
        "# General parameters to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.params\n",
        "\n",
        "# General config to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.config\n",
        "\n",
        "# An authorization mapping from identity provider group names to Neo4j role names. See dbms.security.ldap.authorization.group_to_role_mapping above\n",
        "# for the format.\n",
        "#dbms.security.oidc.<provider>.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# Miscellaneous configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Compresses the metric archive files.\n",
        "server.metrics.csv.rotation.compression=zip\n",
        "\n",
        "# Determines if Cypher will allow using file URLs when loading data using\n",
        "# `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV`\n",
        "# clauses that load data from the file system.\n",
        "#dbms.security.allow_csv_import_from_file_urls=true\n",
        "\n",
        "\n",
        "# Value of the Access-Control-Allow-Origin header sent over any HTTP or HTTPS\n",
        "# connector. This defaults to '*', which allows broadest compatibility. Note\n",
        "# that any URI provided here limits HTTP/HTTPS access to that URI only.\n",
        "#dbms.security.http_access_control_allow_origin=*\n",
        "\n",
        "# Value of the HTTP Strict-Transport-Security (HSTS) response header. This header\n",
        "# tells browsers that a webpage should only be accessed using HTTPS instead of HTTP.\n",
        "# It is attached to every HTTPS response. Setting is not set by default so\n",
        "# 'Strict-Transport-Security' header is not sent. Value is expected to contain\n",
        "# directives like 'max-age', 'includeSubDomains' and 'preload'.\n",
        "#dbms.security.http_strict_transport_security\n",
        "\n",
        "# Retention policy for transaction logs needed to perform recovery and backups.\n",
        "#db.tx_log.rotation.retention_policy=2 days\n",
        "\n",
        "# Limit the number of IOs the background checkpoint process will consume per second.\n",
        "# This setting is advisory, is ignored in Neo4j Community Edition, and is followed to\n",
        "# best effort in Enterprise Edition.\n",
        "# An IO is in this case a 8 KiB (mostly sequential) write. Limiting the write IO in\n",
        "# this way will leave more bandwidth in the IO subsystem to service random-read IOs,\n",
        "# which is important for the response time of queries when the database cannot fit\n",
        "# entirely in memory. The only drawback of this setting is that longer checkpoint times\n",
        "# may lead to slightly longer recovery times in case of a database or system crash.\n",
        "# A lower number means lower IO pressure, and consequently longer checkpoint times.\n",
        "# Set this to -1 to disable the IOPS limit and remove the limitation entirely,\n",
        "# this will let the checkpointer flush data as fast as the hardware will go.\n",
        "# Removing the setting, or commenting it out, will set the default value of 600.\n",
        "# db.checkpoint.iops.limit=600\n",
        "\n",
        "# Whether or not any database on this instance are read_only by default.\n",
        "# If false, individual databases may be marked as read_only using dbms.database.read_only.\n",
        "# If true, individual databases may be marked as writable using dbms.databases.writable.\n",
        "#dbms.databases.default_to_read_only=false\n",
        "\n",
        "# Comma separated list of JAX-RS packages containing JAX-RS resources, one\n",
        "# package name for each mountpoint. The listed package names will be loaded\n",
        "# under the mountpoints specified. Uncomment this line to mount the\n",
        "# org.neo4j.examples.server.unmanaged.HelloWorldResource.java from\n",
        "# neo4j-server-examples under /examples/unmanaged, resulting in a final URL of\n",
        "# http://localhost:7474/examples/unmanaged/helloworld/{nodeId}\n",
        "#server.unmanaged_extension_classes=org.neo4j.examples.server.unmanaged=/examples/unmanaged\n",
        "\n",
        "# A comma separated list of procedures and user defined functions that are allowed\n",
        "# full access to the database through unsupported/insecure internal APIs.\n",
        "dbms.security.procedures.unrestricted=jwt.security.*\n",
        "\n",
        "# A comma separated list of procedures to be loaded by default.\n",
        "# Leaving this unconfigured will load all procedures found.\n",
        "#dbms.security.procedures.allowlist=apoc.coll.*,apoc.load.*,gds.*\n",
        "\n",
        "# For how long should drivers cache the discovery data from\n",
        "# the dbms.routing.getRoutingTable() procedure. Defaults to 300s.\n",
        "#dbms.routing_ttl=300s\n",
        "\n",
        "#********************************************************************\n",
        "# JVM Parameters\n",
        "#********************************************************************\n",
        "\n",
        "# G1GC generally strikes a good balance between throughput and tail\n",
        "# latency, without too much tuning.\n",
        "server.jvm.additional=-XX:+UseG1GC\n",
        "\n",
        "# Have common exceptions keep producing stack traces, so they can be\n",
        "# debugged regardless of how often logs are rotated.\n",
        "server.jvm.additional=-XX:-OmitStackTraceInFastThrow\n",
        "\n",
        "# Make sure that `initmemory` is not only allocated, but committed to\n",
        "# the process, before starting the database. This reduces memory\n",
        "# fragmentation, increasing the effectiveness of transparent huge\n",
        "# pages. It also reduces the possibility of seeing performance drop\n",
        "# due to heap-growing GC events, where a decrease in available page\n",
        "# cache leads to an increase in mean IO response time.\n",
        "# Try reducing the heap memory, if this flag degrades performance.\n",
        "server.jvm.additional=-XX:+AlwaysPreTouch\n",
        "\n",
        "# Trust that non-static final fields are really final.\n",
        "# This allows more optimizations and improves overall performance.\n",
        "# NOTE: Disable this if you use embedded mode, or have extensions or dependencies that may use reflection or\n",
        "# serialization to change the value of final fields!\n",
        "server.jvm.additional=-XX:+UnlockExperimentalVMOptions\n",
        "server.jvm.additional=-XX:+TrustFinalNonStaticFields\n",
        "\n",
        "# Disable explicit garbage collection, which is occasionally invoked by the JDK itself.\n",
        "server.jvm.additional=-XX:+DisableExplicitGC\n",
        "\n",
        "# Allow Neo4j to use @Contended annotation\n",
        "server.jvm.additional=-XX:-RestrictContended\n",
        "\n",
        "# Restrict size of cached JDK buffers to 1 KB\n",
        "server.jvm.additional=-Djdk.nio.maxCachedBufferSize=1024\n",
        "\n",
        "# More efficient buffer allocation in Netty by allowing direct no cleaner buffers.\n",
        "server.jvm.additional=-Dio.netty.tryReflectionSetAccessible=true\n",
        "\n",
        "# Exits JVM on the first occurrence of an out-of-memory error. Its preferable to restart VM in case of out of memory errors.\n",
        "# server.jvm.additional=-XX:+ExitOnOutOfMemoryError\n",
        "\n",
        "# Expand Diffie Hellman (DH) key size from default 1024 to 2048 for DH-RSA cipher suites used in server TLS handshakes.\n",
        "# This is to protect the server from any potential passive eavesdropping.\n",
        "server.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048\n",
        "\n",
        "# This mitigates a DDoS vector.\n",
        "server.jvm.additional=-Djdk.tls.rejectClientInitiatedRenegotiation=true\n",
        "\n",
        "# Enable remote debugging\n",
        "#server.jvm.additional=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\n",
        "\n",
        "# This filter prevents deserialization of arbitrary objects via java object serialization, addressing potential vulnerabilities.\n",
        "# By default this filter whitelists all neo4j classes, as well as classes from the hazelcast library and the java standard library.\n",
        "# These defaults should only be modified by expert users!\n",
        "# For more details (including filter syntax) see: https://openjdk.java.net/jeps/290\n",
        "#server.jvm.additional=-Djdk.serialFilter=java.**;org.neo4j.**;com.neo4j.**;com.hazelcast.**;net.sf.ehcache.Element;com.sun.proxy.*;org.openjdk.jmh.**;!*\n",
        "\n",
        "# Increase the default flight recorder stack sampling depth from 64 to 256, to avoid truncating frames when profiling.\n",
        "server.jvm.additional=-XX:FlightRecorderOptions=stackdepth=256\n",
        "\n",
        "# Allow profilers to sample between safepoints. Without this, sampling profilers may produce less accurate results.\n",
        "server.jvm.additional=-XX:+UnlockDiagnosticVMOptions\n",
        "server.jvm.additional=-XX:+DebugNonSafepoints\n",
        "\n",
        "# Open modules for neo4j to allow internal access\n",
        "server.jvm.additional=--add-opens=java.base/java.nio=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/java.io=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n",
        "\n",
        "# Disable logging JMX endpoint.\n",
        "server.jvm.additional=-Dlog4j2.disable.jmx=true\n",
        "\n",
        "# Limit JVM metaspace and code cache to allow garbage collection. Used by cypher for code generation and may grow indefinitely unless constrained.\n",
        "# Useful for memory constrained environments\n",
        "#server.jvm.additional=-XX:MaxMetaspaceSize=1024m\n",
        "#server.jvm.additional=-XX:ReservedCodeCacheSize=512m\n",
        "\n",
        "# Allow big methods to be JIT compiled.\n",
        "# Useful for big queries and big expressions where cypher code generation can create large methods.\n",
        "#server.jvm.additional=-XX:-DontCompileHugeMethods\n",
        "\n",
        "#********************************************************************\n",
        "# Wrapper Windows NT/2000/XP Service Properties\n",
        "#********************************************************************\n",
        "# WARNING - Do not modify any of these properties when an application\n",
        "#  using this configuration file has been installed as a service.\n",
        "#  Please uninstall the service before modifying this section.  The\n",
        "#  service can then be reinstalled.\n",
        "\n",
        "# Name of the service\n",
        "server.windows_service_name=neo4j\n",
        "\n",
        "#********************************************************************\n",
        "# Other Neo4j system properties\n",
        "#********************************************************************\n",
        "\n",
        "dbms.memory.heap.initial_size=512m\n",
        "dbms.memory.heap.max_size=1G\n",
        "dbms.memory.pagecache.size=512m\n",
        "dbms.backup.enabled=false\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true -Xss1G\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: TTS in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.22.0)\n",
            "Requirement already satisfied: cython>=0.29.30 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (3.0.8)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (1.11.4)\n",
            "Requirement already satisfied: torch>=2.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (2.1.2)\n",
            "Requirement already satisfied: torchaudio in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (2.1.2)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (1.3.2)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (4.66.1)\n",
            "Requirement already satisfied: anyascii>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.3.2)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (3.9.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (23.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (3.0.1)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.3.4)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.5.5)\n",
            "Requirement already satisfied: pandas<2.0,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (3.8.2)\n",
            "Requirement already satisfied: trainer>=0.0.32 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.0.36)\n",
            "Requirement already satisfied: coqpit>=0.0.16 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.0.17)\n",
            "Requirement already satisfied: jieba in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: pypinyin in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.50.0)\n",
            "Requirement already satisfied: hangul-romanize in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.1.0)\n",
            "Requirement already satisfied: gruut==2.2.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.2.3)\n",
            "Requirement already satisfied: jamo in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.4.1)\n",
            "Requirement already satisfied: nltk in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (3.8.1)\n",
            "Requirement already satisfied: g2pkk>=0.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.1.2)\n",
            "Requirement already satisfied: bangla in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: bnnumerizer in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: bnunicodenormalizer in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.1.6)\n",
            "Requirement already satisfied: einops>=0.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.7.0)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (4.36.2)\n",
            "Requirement already satisfied: encodec>=0.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.1.1)\n",
            "Requirement already satisfied: unidecode>=1.3.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (1.3.8)\n",
            "Requirement already satisfied: num2words in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.5.13)\n",
            "Requirement already satisfied: spacy>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy[ja]>=3->TTS) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (1.26.3)\n",
            "Requirement already satisfied: numba>=0.57.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from TTS) (0.58.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.14.0)\n",
            "Requirement already satisfied: dateparser~=1.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.1.8)\n",
            "Requirement already satisfied: gruut-ipa<1.0,>=0.12.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (0.13.0)\n",
            "Requirement already satisfied: gruut-lang-en~=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.0.0)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.2.0)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.8.8)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (0.9.10)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-es~=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.0)\n",
            "Requirement already satisfied: gruut-lang-fr~=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp>=3.8.1->TTS) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp>=3.8.1->TTS) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp>=3.8.1->TTS) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp>=3.8.1->TTS) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from flask>=2.0.1->TTS) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from flask>=2.0.1->TTS) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from flask>=2.0.1->TTS) (2.1.2)\n",
            "Requirement already satisfied: click>=8.1.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from flask>=2.0.1->TTS) (1.7.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from inflect>=5.6.0->TTS) (2.5.3)\n",
            "Requirement already satisfied: typing-extensions in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from inflect>=5.6.0->TTS) (4.8.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (5.1.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from librosa>=0.10.0->TTS) (1.0.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from num2words->TTS) (0.6.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from numba>=0.57.0->TTS) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pandas<2.0,>=1.4->TTS) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from scikit-learn>=1.3.0->TTS) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from soundfile>=0.12.0->TTS) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (68.2.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.3.0)\n",
            "Requirement already satisfied: sudachipy!=0.6.1,>=0.5.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy[ja]>=3->TTS) (0.6.8)\n",
            "Requirement already satisfied: sudachidict-core>=20211220 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from spacy[ja]>=3->TTS) (20240109)\n",
            "Requirement already satisfied: filelock in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=2.1->TTS) (3.13.1)\n",
            "Requirement already satisfied: sympy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=2.1->TTS) (1.12)\n",
            "Requirement already satisfied: psutil in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from trainer>=0.0.32->TTS) (5.9.0)\n",
            "Requirement already satisfied: tensorboard in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from trainer>=0.0.32->TTS) (2.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers>=4.33.0->TTS) (0.20.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers>=4.33.0->TTS) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers>=4.33.0->TTS) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers>=4.33.0->TTS) (0.4.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from umap-learn>=0.5.1->TTS) (0.5.11)\n",
            "Requirement already satisfied: pycparser in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.21)\n",
            "Requirement already satisfied: tzlocal in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (2.1.3)\n",
            "Requirement already satisfied: six in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic>=1.9.1->inflect>=5.6.0->TTS) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic>=1.9.1->inflect>=5.6.0->TTS) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3->spacy[ja]>=3->TTS) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sympy->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (2.26.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (4.23.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->trainer>=0.0.32->TTS) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->trainer>=0.0.32->TTS) (3.2.2)\n",
            "Collecting speake3\n",
            "  Downloading speake3-0.3.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: speake3\n",
            "  Building wheel for speake3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for speake3: filename=speake3-0.3-py3-none-any.whl size=3561 sha256=be0c1319d9435aa5f7a874dcee4c6832e57d30065ccb3c3dfad4147f7515ad08\n",
            "  Stored in directory: /Users/tomriddle1/Library/Caches/pip/wheels/3b/b8/49/02cc64ea4db625b4be325b8f24a9ec2d1e813d63e047f6a2e7\n",
            "Successfully built speake3\n",
            "Installing collected packages: speake3\n",
            "Successfully installed speake3-0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install TTS\n",
        "!pip install speake3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Name format: type/language/dataset/model\n",
            " 1: tts_models/multilingual/multi-dataset/xtts_v2\n",
            " 2: tts_models/multilingual/multi-dataset/xtts_v1.1\n",
            " 3: tts_models/multilingual/multi-dataset/your_tts\n",
            " 4: tts_models/multilingual/multi-dataset/bark\n",
            " 5: tts_models/bg/cv/vits\n",
            " 6: tts_models/cs/cv/vits\n",
            " 7: tts_models/da/cv/vits\n",
            " 8: tts_models/et/cv/vits\n",
            " 9: tts_models/ga/cv/vits\n",
            " 10: tts_models/en/ek1/tacotron2\n",
            " 11: tts_models/en/ljspeech/tacotron2-DDC\n",
            " 12: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
            " 13: tts_models/en/ljspeech/glow-tts\n",
            " 14: tts_models/en/ljspeech/speedy-speech\n",
            " 15: tts_models/en/ljspeech/tacotron2-DCA\n",
            " 16: tts_models/en/ljspeech/vits [already downloaded]\n",
            " 17: tts_models/en/ljspeech/vits--neon\n",
            " 18: tts_models/en/ljspeech/fast_pitch\n",
            " 19: tts_models/en/ljspeech/overflow\n",
            " 20: tts_models/en/ljspeech/neural_hmm\n",
            " 21: tts_models/en/vctk/vits\n",
            " 22: tts_models/en/vctk/fast_pitch\n",
            " 23: tts_models/en/sam/tacotron-DDC\n",
            " 24: tts_models/en/blizzard2013/capacitron-t2-c50\n",
            " 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
            " 26: tts_models/en/multi-dataset/tortoise-v2\n",
            " 27: tts_models/en/jenny/jenny\n",
            " 28: tts_models/es/mai/tacotron2-DDC\n",
            " 29: tts_models/es/css10/vits\n",
            " 30: tts_models/fr/mai/tacotron2-DDC\n",
            " 31: tts_models/fr/css10/vits\n",
            " 32: tts_models/uk/mai/glow-tts\n",
            " 33: tts_models/uk/mai/vits\n",
            " 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
            " 35: tts_models/nl/mai/tacotron2-DDC\n",
            " 36: tts_models/nl/css10/vits\n",
            " 37: tts_models/de/thorsten/tacotron2-DCA\n",
            " 38: tts_models/de/thorsten/vits\n",
            " 39: tts_models/de/thorsten/tacotron2-DDC\n",
            " 40: tts_models/de/css10/vits-neon\n",
            " 41: tts_models/ja/kokoro/tacotron2-DDC\n",
            " 42: tts_models/tr/common-voice/glow-tts\n",
            " 43: tts_models/it/mai_female/glow-tts\n",
            " 44: tts_models/it/mai_female/vits\n",
            " 45: tts_models/it/mai_male/glow-tts\n",
            " 46: tts_models/it/mai_male/vits\n",
            " 47: tts_models/ewe/openbible/vits\n",
            " 48: tts_models/hau/openbible/vits\n",
            " 49: tts_models/lin/openbible/vits\n",
            " 50: tts_models/tw_akuapem/openbible/vits\n",
            " 51: tts_models/tw_asante/openbible/vits\n",
            " 52: tts_models/yor/openbible/vits\n",
            " 53: tts_models/hu/css10/vits\n",
            " 54: tts_models/el/cv/vits\n",
            " 55: tts_models/fi/css10/vits\n",
            " 56: tts_models/hr/cv/vits\n",
            " 57: tts_models/lt/cv/vits\n",
            " 58: tts_models/lv/cv/vits\n",
            " 59: tts_models/mt/cv/vits\n",
            " 60: tts_models/pl/mai_female/vits\n",
            " 61: tts_models/pt/cv/vits\n",
            " 62: tts_models/ro/cv/vits\n",
            " 63: tts_models/sk/cv/vits\n",
            " 64: tts_models/sl/cv/vits\n",
            " 65: tts_models/sv/cv/vits\n",
            " 66: tts_models/ca/custom/vits [already downloaded]\n",
            " 67: tts_models/fa/custom/glow-tts\n",
            " 68: tts_models/bn/custom/vits-male\n",
            " 69: tts_models/bn/custom/vits-female\n",
            " 70: tts_models/be/common-voice/glow-tts\n",
            "\n",
            " Name format: type/language/dataset/model\n",
            " 1: vocoder_models/universal/libri-tts/wavegrad\n",
            " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
            " 3: vocoder_models/en/ek1/wavegrad\n",
            " 4: vocoder_models/en/ljspeech/multiband-melgan\n",
            " 5: vocoder_models/en/ljspeech/hifigan_v2\n",
            " 6: vocoder_models/en/ljspeech/univnet\n",
            " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
            " 8: vocoder_models/en/vctk/hifigan_v2\n",
            " 9: vocoder_models/en/sam/hifigan_v2\n",
            " 10: vocoder_models/nl/mai/parallel-wavegan\n",
            " 11: vocoder_models/de/thorsten/wavegrad\n",
            " 12: vocoder_models/de/thorsten/fullband-melgan\n",
            " 13: vocoder_models/de/thorsten/hifigan_v1\n",
            " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
            " 15: vocoder_models/uk/mai/multiband-melgan\n",
            " 16: vocoder_models/tr/common-voice/hifigan\n",
            " 17: vocoder_models/be/common-voice/hifigan\n",
            "\n",
            " Name format: type/language/dataset/model\n",
            " 1: voice_conversion_models/multilingual/vctk/freevc24\n",
            "\u001b[0m > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: Hello, how are you?\n",
            " > Text splitted to sentences.\n",
            "['Hello, how are you?']\n",
            " > Processing time: 0.4613659381866455\n",
            " > Real-time factor: 0.2899315702523807\n",
            " > Saving output to output.wav\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!tts --list_models\n",
        "!tts --text \"Hello, how are you?\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from calendar import c\n",
        "from librosa import ex\n",
        "from regex import E\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "#!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    chunk_idx =0\n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text = page.extract_text()\n",
        "        if chunk_idx <=9:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "        else:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "        \n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, shell=True)\n",
        "            chunk_idx += 1\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            # try again but drop the last 5 sentences\n",
        "            text = text.rsplit(\".\", 5)[0]\n",
        "            if chunk_idx <=9:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "            else:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "            try:\n",
        "                subprocess.run(cmd, check=True, shell=True)\n",
        "                chunk_idx += 1\n",
        "            except Exception as e:\n",
        "                # skip this chunk\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                chunk_idx += 1\n",
        "                continue\n",
        "\n",
        "        \n",
        "\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "pdf = \"/Users/tomriddle1/Documents/Dr_Bryan_Chemisty.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Get device\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# List available 🐸TTS models\n",
        "#print(TTS().list_models())\n",
        "\n",
        "# Init TTS\n",
        "#tts = TTS(\"tts_models/en/ljspeech/vits\").to(device)\n",
        "\n",
        "# Run TTS\n",
        "# Text to speech to a file\n",
        "#tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n",
        "# Init TTS with the target model name\n",
        "#tts = TTS(model_name=\"tts_models/en/ljspeech/vits\", progress_bar=True).to(device)\n",
        "\n",
        "# Run TTS\n",
        "#tts.tts_to_file(text=text, file_path=\"output.wav\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.0.1)\n",
            "1320609\n",
            "10224\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: fRecent Advances in X-ray\n",
            "Crystallography Improving\n",
            "Resolution and Reliability\n",
            "Introduction\n",
            "X-ray crystallography is a pivotal technique in structural biology, allowing\n",
            "scientists to determine the atomic structure of molecules\n",
            " > Text splitted to sentences.\n",
            "['fRecent Advances in X-ray', 'Crystallography Improving', 'Resolution and Reliability', 'Introduction', 'X-ray crystallography is a pivotal technique in structural biology, allowing', 'scientists to determine the atomic structure of molecules']\n",
            " > Processing time: 4.885490894317627\n",
            " > Real-time factor: 0.251018460171929\n",
            " > Saving output to output.wav\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "pdf = \"/Users/tomriddle1/Documents/Dr_Bryan_Chemisty.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Break text into chunks of 5000 words\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "text_chunks = split_to_sentences(text)\n",
        "print(len(text_chunks))\n",
        "!tts --text f\"{text_chunks[0]}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will remove 20 (25.2 MB) package(s).\n"
          ]
        }
      ],
      "source": [
        "!conda clean --packages -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
