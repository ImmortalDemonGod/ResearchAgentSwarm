{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PaperQA - A Question Answering Dataset for Academic Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "!pip install paper-qa\n",
        "#!pip install openai==1.7.2\n",
        "!pip install openai==0.28\n",
        "!pip install langchain==0.1.1\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "!pip install sentence-transformers\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "# Configuration\n",
        "#output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "output_folder_json = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\" \n",
        "output_folder_pdf = \"/home/epas/Documents/MitoMAVEN/full_texts/\" \n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, embeddings=embeddings)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    process_json_files(output_folder_json)\n",
        "    process_research_papers(output_folder_pdf)\n",
        "    questions = read_questions(questions_file_path)\n",
        "    responses = [docs.query(question) for question in questions]\n",
        "    write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135742.80s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
            "Requirement already satisfied: paper-qa in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.13.3)\n",
            "Requirement already satisfied: pypdf in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.17.4)\n",
            "Collecting pydantic<2 (from paper-qa)\n",
            "  Downloading pydantic-1.10.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=0.0.303 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.1.1)\n",
            "Collecting openai<1 (from paper-qa)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: faiss-cpu in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (1.7.4)\n",
            "Requirement already satisfied: PyCryptodome in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.19.0)\n",
            "Requirement already satisfied: html2text in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (2020.1.16)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.1.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.0.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.26.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<1->paper-qa) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<2->paper-qa) (4.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tiktoken>=0.4.0->paper-qa) (2023.10.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->paper-qa) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (2023.7.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain>=0.0.303->paper-qa) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (1.0.0)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.15-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic, openai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.0\n",
            "    Uninstalling pydantic-2.6.0:\n",
            "      Successfully uninstalled pydantic-2.6.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.10.0\n",
            "    Uninstalling openai-1.10.0:\n",
            "      Successfully uninstalled openai-1.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 4.12.0 requires pydantic>=2.0, but you have pydantic 1.10.15 which is incompatible.\n",
            "agency-swarm 0.1.0 requires instructor==0.3.4, but you have instructor 0.4.8 which is incompatible.\n",
            "agency-swarm 0.1.0 requires openai==1.3.0, but you have openai 0.28.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires certifi==2023.11.17, but you have certifi 2023.7.22 which is incompatible.\n",
            "aider-chat 0.19.1 requires charset-normalizer==3.3.2, but you have charset-normalizer 3.2.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires idna==3.6, but you have idna 3.4 which is incompatible.\n",
            "aider-chat 0.19.1 requires networkx==3.2.1, but you have networkx 2.8.8 which is incompatible.\n",
            "aider-chat 0.19.1 requires numpy==1.26.2, but you have numpy 1.26.3 which is incompatible.\n",
            "aider-chat 0.19.1 requires openai==1.3.7, but you have openai 0.28.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic==2.5.2, but you have pydantic 1.10.15 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic-core==2.14.5, but you have pydantic-core 2.16.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires tiktoken==0.5.2, but you have tiktoken 0.5.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires urllib3==2.1.0, but you have urllib3 1.26.16 which is incompatible.\n",
            "instructor 0.4.8 requires openai<2.0.0,>=1.1.0, but you have openai 0.28.1 which is incompatible.\n",
            "instructor 0.4.8 requires pydantic<3.0.0,>=2.0.2, but you have pydantic 1.10.15 which is incompatible.\n",
            "fleet-context 1.1.9 requires openai>1.1.0, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1 pydantic-1.10.15\n"
          ]
        },
        {
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaperqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Docs\n\u001b[1;32m     13\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-OWZcQX5sKQZGw4CKQqdAT3BlbkFJBDSnkR3m7JultVNAHYAZ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperqa/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, Docs, PromptCollection, Doc, Text, Context\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperqa/docs.py:42\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Answer, CallbackFactory, Context, Doc, DocKey, PromptCollection, Text\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     gather_with_concurrency,\n\u001b[1;32m     31\u001b[0m     get_llm_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     strip_citations,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDocs\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marbitrary_types_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmart_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"A collection of documents to be used for answering questions.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         is_untouched(value)\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_type \u001b[38;5;241m!=\u001b[39m PyObject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     fields[ann_name] \u001b[38;5;241m=\u001b[39m \u001b[43mModelField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39munderscore_attrs_are_private:\n\u001b[1;32m    205\u001b[0m     private_attributes[ann_name] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/fields.py:504\u001b[0m, in \u001b[0;36mModelField.infer\u001b[0;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[1;32m    501\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    502\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/fields.py:434\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[0;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/fields.py:544\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/fields.py:568\u001b[0m, in \u001b[0;36mModelField._set_default_and_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m errors_\u001b[38;5;241m.\u001b[39mConfigError(\n\u001b[1;32m    564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou need to set the type of field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m when using `default_factory`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    565\u001b[0m         )\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m default_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m default_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/fields.py:437\u001b[0m, in \u001b[0;36mModelField.get_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msmart_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory()\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pydantic/v1/utils.py:693\u001b[0m, in \u001b[0;36msmart_deepcopy\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m):\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# do we really dare to catch ALL errors? Seems a bit risky\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install paper-qa\n",
        "\n",
        "api_key = \"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\"\n",
        "import os\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import json\n",
        "from paperqa import Docs\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "# Configuration\n",
        "#output_folder = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/json_summaries/\"\n",
        "output_folder_json = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\" \n",
        "output_folder_pdf = \"/Volumes/Backup Plus/REMARKABLE/Small_Useful_PDFs/\" \n",
        "questions_file_path = \"questions_file.txt\"\n",
        "responses_file_path = \"responses_file.txt\"\n",
        "\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, embeddings=embeddings)\n",
        "\n",
        "\n",
        "def process_json_files(folder):\n",
        "    json_files = os.listdir(folder)\n",
        "    json_files = [file for file in json_files if file.endswith('.json')]\n",
        "\n",
        "    for filename in json_files:\n",
        "        with open(os.path.join(folder, filename), 'r') as file_obj:\n",
        "            data = json.load(file_obj)\n",
        "            \n",
        "            # Check if the JSON data is not empty\n",
        "            if data:\n",
        "                citation = \"\"\n",
        "                for entry in data:\n",
        "                    file_id = str(entry[\"file_id\"])\n",
        "                    citation = str(entry[\"references\"])\n",
        "                \n",
        "                # Check if file_id and citation are not empty\n",
        "                if file_id:\n",
        "                    docs.add(path=os.path.join(folder, filename), dockey=file_id)\n",
        "            else:\n",
        "                print(f\"Skipped empty or invalid JSON file: {filename}\")\n",
        "\n",
        "def process_research_papers(folder):\n",
        "    research_papers = os.listdir(folder)\n",
        "    research_papers = [file for file in research_papers if file.endswith('.pdf')]\n",
        "    for filename in research_papers:\n",
        "        docs.add(path=os.path.join(folder, filename))\n",
        "            \n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def write_responses(responses, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response.formatted_answer + \"\\n\\n\")\n",
        "\n",
        "\n",
        "# Rest of your main function...\n",
        "\n",
        "def main():\n",
        "    #process_json_files(output_folder_json)\n",
        "    process_research_papers(output_folder_pdf)\n",
        "    #questions = read_questions(questions_file_path)\n",
        "    #responses = [docs.query(question) for question in questions]\n",
        "    #write_responses(responses, responses_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall openai -y\n",
        "!pip uninstall paperqa -y\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "def parse_qa_responses(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    responses = []\n",
        "    response = {}\n",
        "    references_lines = []\n",
        "    capturing_references = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('Question:'):\n",
        "            if response:  # Add the previous response with its references to the list\n",
        "                response['references'] = ''.join(references_lines).strip()\n",
        "                responses.append(response)\n",
        "                references_lines = []\n",
        "            # Handling split operation\n",
        "            split_line = line.split('    ')\n",
        "            if len(split_line) >= 2:\n",
        "                response = {'question': split_line[1].strip(), 'answer': '', 'references': ''}\n",
        "            else:\n",
        "                response = {'question': '', 'answer': '', 'references': ''}\n",
        "            capturing_references = False\n",
        "        elif 'I cannot answer' in line.strip() or 'The provided context does not contain' in line.strip():\n",
        "            response['answer'] = line.strip()\n",
        "        elif line.strip().startswith('References'):\n",
        "            capturing_references = True\n",
        "        elif capturing_references:\n",
        "            references_lines.append(line)\n",
        "    \n",
        "    if response:  # Add the last response with its references to the list\n",
        "        response['references'] = ''.join(references_lines).strip()\n",
        "        responses.append(response)\n",
        "\n",
        "    # Add timestamp and extract URLs from references\n",
        "    for response in responses:\n",
        "        response[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        if response['references']:\n",
        "            response['references_urls'] = extract_urls(response['references'])\n",
        "    return responses\n",
        "\n",
        "\n",
        "# You would then continue with your original code for saving the JSON.\n",
        "\n",
        "\n",
        "def save_json_append(responses, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    combined_data = existing_data + responses\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "file_path =\"responses_file.txt\"\n",
        "output_json_file = 'structured_responses.json'\n",
        "\n",
        "responses = parse_qa_responses(file_path)\n",
        "save_json_append(responses, output_json_file)\n",
        "\n",
        "print(f\"Processed responses are saved in JSON format to {output_json_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "laptop= \"/home/epas/Documents/docs.pickle\"\n",
        "desktop = \"/Volumes/Backup Plus/REMARKABLE/Small_Useful_PDFs/docs.pickle\"\n",
        "if os.path.exists(laptop):\n",
        "    # save\n",
        "    with open(desktop, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    # load\n",
        "    with open(laptop, \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "    print(\"Saved and loaded successfully\")\n",
        "\n",
        "else:\n",
        "    # save\n",
        "    with open(laptop, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    # load\n",
        "    with open(desktop, \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "    print(\"Saved and loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Generate Search Queries\":\n",
        "        with open(\"generate_search_queries.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pydantic==2.0.3\n",
        "!pip install instructor\n",
        "#\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"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\"\n",
        "import os\n",
        "!pip install openai==0.28\n",
        "# Set up the environment and PaperQA\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import json\n",
        "#import instructor\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "import openai\n",
        "\n",
        "\n",
        "def generate_search_queries(question: str):\n",
        "    prompt = build_system_prompt(\"Generate Search Queries\")\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "\n",
        "    # Initialize the OpenAI client without explicitly passing the API key\n",
        "    #client = OpenAI(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        response =  openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0125\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": \"Search Queries:\"},\n",
        "            ]\n",
        "        )\n",
        "        return parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_unanswered_questions(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    unanswered_questions = []\n",
        "\n",
        "    for entry in data:\n",
        "        # Checking for phrases that indicate an unanswered question\n",
        "        if \"cannot answer\" in entry[\"answer\"] or \"does not contain\" in entry[\"answer\"] or \"no answer\" in entry[\"answer\"] or \"no results\" in entry[\"answer\"] or \"no information\" in entry[\"answer\"]:\n",
        "            unanswered = True\n",
        "        else:\n",
        "            unanswered = False\n",
        "\n",
        "        # Building the result entry\n",
        "        result_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"answerable\": not unanswered,\n",
        "            \"timestamp\": entry.get(\"timestamp\", \"Unknown timestamp\")\n",
        "        }\n",
        "\n",
        "        if entry.get(\"references\"):\n",
        "            result_entry[\"references\"] = entry[\"references\"]\n",
        "\n",
        "        if entry.get(\"references_urls\"):  # Using .get to avoid KeyError\n",
        "            result_entry[\"references_urls\"] = entry[\"references_urls\"]\n",
        "\n",
        "        unanswered_questions.append(result_entry)\n",
        "\n",
        "    return unanswered_questions\n",
        "\n",
        "json_file = 'structured_responses.json'\n",
        "unanswered_questions = check_unanswered_questions(json_file)\n",
        "\n",
        "# Display the results\n",
        "for item in unanswered_questions:\n",
        "    print(f\"Question: {item['question']}\\nAnswerable: {item['answerable']}\\n\")\n",
        "    if item.get(\"references\") and item.get(\"references_urls\"):\n",
        "        print(f\"Url(s): {item['references_urls']}\\n\")\n",
        "        #print(f\"Reference(s): {item['references']}\\n\")\n",
        "    # Generate search queries for unanswered questions\n",
        "    if item['answerable'] == False:\n",
        "        if item.get(\"references\"):\n",
        "            search_queries = generate_search_queries(f\"{item['question']}\\n{item['references']}\")\n",
        "        else:\n",
        "            search_queries = generate_search_queries(item[\"question\"])\n",
        "        # save search queries to json file\n",
        "        with open(\"search_queries.json\", \"a\") as outfile:\n",
        "            json.dump(search_queries, outfile)\n",
        "        print(f\"Search Queries: {search_queries}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "client = OpenAI(api_key= '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')\n",
        "\n",
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": \"You are a kind helpfull assistant\"}\n",
        "]\n",
        "\n",
        "while True:\n",
        "    message = input(\"user:\")\n",
        "    if message:\n",
        "        messages.append(\n",
        "            {\"role\":\"user\", \"content\": message},\n",
        "        )\n",
        "        completion = client.chat.completions.create(\n",
        "            messages = messages,\n",
        "            model = \"gpt-3.5-turbo\"\n",
        "        )\n",
        "\n",
        "    reply = completion.choices[0].message.content\n",
        "    print(f\"ChatGPT: {reply}\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install paper-qa\n",
        "#!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('/home/epas/Programming/ResearchAgentSwarm/Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    #docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "    print(paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=1,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Research Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "\n",
        "# Function to clean entities based on new lines and remove leading hyphens\n",
        "def clean_and_separate_entities(entities_list):\n",
        "    entities_str = '\\n'.join(entities_list)\n",
        "    cleaned_entities = []\n",
        "    dirty_entities = []\n",
        "\n",
        "    for line in entities_str.split('\\n'):\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line.startswith('-'):\n",
        "            # Remove the leading hyphen and any extra space after it\n",
        "            cleaned_entities.append(stripped_line.lstrip('-').strip())\n",
        "        else:\n",
        "            dirty_entities.append(stripped_line)\n",
        "\n",
        "    return cleaned_entities, dirty_entities\n",
        "def test_clean_and_separate_entities():\n",
        "    \n",
        "    # Define the summary JSON file path\n",
        "    SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "    # Read the summaries.json file\n",
        "    with open(SUMMARY_JSON, \"r\") as file:\n",
        "        summaries_json = json.load(file)\n",
        "\n",
        "    # Extract the first entities entry\n",
        "    first_entities_list = summaries_json[0][\"entities\"][0]\n",
        "\n",
        "    # Clean the entities and separate the uncleaned ones\n",
        "    cleaned_entities, dirty_entities = clean_and_separate_entities(first_entities_list)\n",
        "\n",
        "    # Save the results to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp_file:\n",
        "        json.dump({\n",
        "            \"cleaned_entities\": cleaned_entities,\n",
        "            \"dirty_entities\": dirty_entities\n",
        "        }, temp_file, indent=4)\n",
        "\n",
        "    print(\"Results saved in:\", temp_file.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_topics_with_justification(topic_text):\n",
        "    # Regular expression pattern for identifying topics with their justifications\n",
        "    topic_pattern = re.compile(r'(\\d+)\\.\\s+([^\\n]+)(\\n\\s+-[^\\n]+)*')\n",
        "    topics = topic_pattern.findall(topic_text)\n",
        "    \n",
        "    extracted_topics = []\n",
        "    for match in topics:\n",
        "        topic = match[1].strip()\n",
        "        justification = ' '.join(match[2].split('\\n')).strip()\n",
        "        # Remove \"Justification:\" if it starts with it\n",
        "        if justification.lower().startswith('- justification:'):\n",
        "            justification = justification[len('- justification:'):].strip()\n",
        "        # Remove the - if it starts with it\n",
        "        if justification.startswith('-'):\n",
        "            justification = justification[1:].strip()\n",
        "        extracted_topics.append({\"topic\": topic, \"justification\": justification})\n",
        "\n",
        "    return extracted_topics\n",
        "\n",
        "\n",
        "\n",
        "def test_extract_topics_with_justification():\n",
        "    # Adjusted topic text\n",
        "    topic_text_list = []\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of Mitochondria in Energy Production, Signaling, and Apoptosis\\n   - Mitochondria as the powerhouse of the cell\\n   - Role of mitochondria in energy production, signaling, and apoptosis\\n   - Significance of studying mitochondrial function and involvement in diseases\\n\\n2. Challenges with Traditional Methods of Mitochondrial Isolation\\n   - Limitations of traditional methods like differential centrifugation\\n   - Potential damage to mitochondrial double membrane and variable viability\\n\\n3. Innovative Techniques for Mitochondrial Isolation\\n   - Nitrogen cavitation for gentle disruption and release of intact mitochondria\\n   - Affinity purification using anti-TOM22 magnetic beads for efficient isolation\\n   - Filtration-based methods to reduce isolation time and improve viability\\n   - Differential isopycnic density gradient centrifugation for separation based on buoyant density\\n\\n4. Quality Control Measures for Validating Mitochondrial Isolation\\n   - Assessment of mitochondrial respiration, metabolic activity, protein import, and membrane fusion\\n   - High-resolution respirometry and bioluminescent measurements of ATP synthesis\\n\\n5. Importance of Continued Refinement and Standardization of Techniques\\n   - Advancing understanding of mitochondrial biology and implications in health and disease\\n   - Need for standardized protocols to facilitate comparisons and translation of research findings into clinical applications\\n\\n**Notes**: The summary provides a comprehensive overview of the importance of mitochondria, challenges with traditional methods of isolation, innovative techniques for isolation, quality control measures, and the need for continued refinement and standardization. The topics cover the main ideas and themes discussed in the summary, providing a clear and comprehensive analysis of the content.\") \n",
        "    topic_text_list.append(\"**Topic List:**\\n\\n1. Challenges in isolating intact mitochondria from plant cells\\n   - Cell walls, mitochondrial membranes, and large amounts of starting material\\n2. Comprehensive protocol for isolating intact mitochondria from plant cells\\n   - Grinding, filtering, centrifuging, and resuspending\\n3. Characterization and analysis of isolated mitochondria\\n   - Purity, integrity, and functionality assessment\\n   - Techniques: protein profiling, enzymatic activity assays, respiratory chain measurements, and oxygen consumption analysis\\n4. Storage of purified mitochondria\\n   - Long-term storage at -80°C\\n5. Adaptation of isolation process for different tissue types and plant species\\n   - Consideration of phenolic compounds and metabolite profiles\\n6. Validation and controls for quality and functionality assurance\\n7. Downstream applications of isolated mitochondria\\n   - Protein and tRNA uptake experiments, enzyme activity assays, Western blot analyses, and mass spectrometry analyses\\n\\n**Notes:**\\n- The revised summary provides a comprehensive overview of the topic, covering various aspects of isolating intact mitochondria from plant cells.\\n- The topics are specific and non-repetitive, ensuring a clear and distinct representation of the core themes.\\n- The summary is focused on the technical process and considerations involved in isolating mitochondria, as well as the analysis and applications of the isolated mitochondria.\")\n",
        "    topic_text_list.append(\"**Topics Identified:**\\n\\n1. Importance of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction\\n    - Justification: The summary highlights the crucial role of mitochondrial research in understanding cellular biology and addressing diseases related to mitochondrial dysfunction.\\n\\n2. Significance of gentle and effective mitochondrial isolation techniques\\n    - Justification: The summary emphasizes the importance of gentle and effective isolation techniques for studying mitochondrial biology and developing mitochondrial-based therapies.\\n\\n3. Overview of macroscale mitochondrial isolation techniques\\n    - Justification: The summary discusses macroscale mitochondrial isolation techniques, such as manual homogenization and differential filtration-based isolation.\\n\\n4. Advancements in microscale and nanoscale mitochondrial isolation techniques\\n    - Justification: The summary mentions microscale and nanoscale techniques, including microfluidic techniques and nanoprobe-based technologies, for mitochondrial isolation.\\n\\n5. Breakthroughs in sub-cellular isolation techniques for mitochondria\\n    - Justification: The summary highlights breakthroughs in sub-cellular isolation techniques that enable the isolation of mitochondria from subcellular compartments with minimal disruption.\\n\\n6. Challenges in mitochondrial isolation techniques\\n    - Justification: The summary mentions challenges such as the presence of whole cell contaminants in mitochondrial isolates and the time sensitivity of isolated mitochondria.\\n\\n7. Emerging therapeutic approach: Autologous mitochondrial transplants\\n    - Justification: The summary discusses the development of autologous mitochondrial transplants as an emerging therapeutic approach.\\n\\n8. Contributions of the London Centre for Nanotechnology and the McCully laboratory\\n    - Justification: The summary mentions the significant contributions of the London Centre for Nanotechnology and the McCully laboratory in optimizing differential filtration-based mitochondrial isolation for use in cellular models.\\n\\n9. Role of Stem Cell Research & Therapy in advancing mitochondrial medicine\\n    - Justification: The summary highlights the role of Stem Cell Research & Therapy in providing in-depth overviews of advancements in mitochondrial research and facilitating the development of novel therapies for mitochondrial diseases.\")\n",
        "    topic_text_list.append(\"Topics:\\n1. Genetic modifications to enhance mitochondrial autonomy\\n   - Justification: The main focus of the report is exploring genetic modifications to enhance the autonomy of mitochondria from nuclear-encoded proteins and functions.\\n2. Role of mitochondria in cellular function\\n   - Justification: The report highlights the crucial role played by mitochondria in cellular function.\\n3. Coordination between mtDNA and nuclear DNA\\n   - Justification: The report discusses the coordination required between mtDNA and nuclear DNA, as most proteins are encoded by nuclear DNA.\\n4. Therapeutic strategies for mitochondrial diseases\\n   - Justification: The report mentions that enhancing mitochondrial autonomy could lead to new therapeutic strategies for mitochondrial diseases.\\n5. Research on genome engineering, programmable nucleases, and base editors\\n   - Justification: The report mentions that recent research in genome engineering, programmable nucleases, and base editors shows promise for treating hereditary mitochondrial diseases.\\n6. Challenges in genetic manipulation of mtDNA\\n   - Justification: The report discusses challenges such as mtDNA mutations, resistance to genetic manipulation, and limitations in mtDNA recombination.\\n7. Advancements in protein-only gene editing platforms\\n   - Justification: The report mentions advancements in protein-only gene editing platforms as potential solutions to the challenges in genetic manipulation of mtDNA.\\n8. Somatic mitochondrial DNA-replaced cells\\n   - Justification: The report mentions the generation of somatic mitochondrial DNA-replaced cells as a potential solution to the challenges in genetic manipulation of mtDNA.\\n9. Mitochondrial nucleoids and their role in maintaining genetic autonomy\\n   - Justification: The report highlights the concept of mitochondrial nucleoids and their role in maintaining genetic autonomy as a key area of study.\\n10. Mitochondrial epigenomics and gene expression regulation\\n    - Justification: The report emphasizes the importance of understanding mitochondrial epigenomics and gene expression regulation in different cellular contexts, including stress conditions, for identifying genetic modifications that could enhance mitochondrial autonomy.\")\n",
        "    for topic_text in topic_text_list:\n",
        "        extracted_topics = extract_topics_with_justification(topic_text)\n",
        "        print(f'Extracted topics: {extracted_topics}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor url in urls:\\n    try:\\n        pdf =  pdfx.PDFx(url)\\n        metadata = pdf.get_metadata()\\n        print(f\\'Metadata: {metadata}\\')\\n        references_list = pdf.get_references()\\n        print(f\\'References: {references_list}\\')\\n        references_dict = pdf.get_references_as_dict()\\n        print(f\\'References dict: {references_dict}\\')\\n        papers = paperscraper.link_to_pdf(url, pdir=\\'downloaded-papers\\')\\n        print(f\\'Papers: {papers}\\')\\n    except:\\n        print(\"Error in extracting references\")\\n        continue\\n#pdf.download_pdfs(\"target-directory\")\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#!pip install pdfx\n",
        "import pdfx\n",
        "#!pip install paperscraper\n",
        "#import paperscraper\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def test_extract_urls():\n",
        "    # Define the reference text\n",
        "    reference_text = \"\"\"\\n\\nAmerican Institute of Physics. (2023). The powerhouse of the future: Artificial cells. Phys.org. Retrieved from https://phys.org/news/2023-03-powerhouse-future-artificial-cells.html\\n\\nNational Institutes of Health. (2023). Artificial mitochondria transfer (AMT) and transplant. PMC. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5511681/\\n\\nNature. (2023). Spatiotemporal simulations of mitochondrial dynamics. Nature.com. Retrieved from https://www.nature.com/articles/s41598-019-54159-1\\n\\nSogang University & Harbin Institute of Technology. (2023). Artificial organelles for sustainable chemical energy conversion and production: Artificial mitochondria and chloroplasts. Biophysics Reviews. Retrieved from https://publishing.aip.org/publications/latest-content/the-powerhouse-of-the-future-artificial-cells/\"\"\"\n",
        "\n",
        "    urls = extract_urls(reference_text)\n",
        "    print(f'Extracted URLs: {urls}')\n",
        "\n",
        "#pdf = pdfx.PDFx(\"filename-or-url.pdf\")\n",
        "#urls = ['/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/2308.00352.pdf']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for url in urls:\n",
        "    try:\n",
        "        pdf =  pdfx.PDFx(url)\n",
        "        metadata = pdf.get_metadata()\n",
        "        print(f'Metadata: {metadata}')\n",
        "        references_list = pdf.get_references()\n",
        "        print(f'References: {references_list}')\n",
        "        references_dict = pdf.get_references_as_dict()\n",
        "        print(f'References dict: {references_dict}')\n",
        "        papers = paperscraper.link_to_pdf(url, pdir='downloaded-papers')\n",
        "        print(f'Papers: {papers}')\n",
        "    except:\n",
        "        print(\"Error in extracting references\")\n",
        "        continue\n",
        "#pdf.download_pdfs(\"target-directory\")\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted hypothetical questions: [{'question_type': 'Content-Based Question', 'question': 'How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?'}, {'question_type': 'Analytical Question', 'question': 'What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?'}, {'question_type': 'User-Interactive Question', 'question': 'What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?'}]\n",
            "Extracted hypothetical questions: [{'question_type': 'Analytical Question', 'question': 'How do theoretical models help in understanding mitochondrial ATP production?'}, {'question_type': 'Creative/Scenario-Based Question', 'question': 'Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?'}, {'question_type': 'Contextual/Relational Question', 'question': 'How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?'}, {'question_type': 'User-Interactive Question', 'question': 'How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_hypothetical_questions(hypothetical_questions_text):\n",
        "    # Regular expression pattern for identifying hypothetical questions\n",
        "    question_pattern = re.compile(r'\\d+\\.\\s+([A-Za-z\\/-]+ Question):\\n\\s+-\\s+([^\\n]+)')\n",
        "    questions = question_pattern.findall(hypothetical_questions_text)\n",
        "    #print(f'Questions: {questions}')\n",
        "    if len(questions) == 0:\n",
        "        return hypothetical_questions_text\n",
        "    return [{\"question_type\": question_type, \"question\": question} for question_type, question in questions]\n",
        "def test_extract_hypothetical_questions():\n",
        "    # Example hypothetical questions text\n",
        "    hypothetical_questions_text_1 = \"1. Content-Based Question:\\n   - How do genetic modifications contribute to increasing mitochondrial autonomy from nuclear-encoded proteins and functions?\\n\\n2. Analytical Question:\\n   - What are the key tools and methods used in modifying the mitochondrial genome to study the interplay between nuclear and mitochondrial genomes?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a future where mitochondrial autonomy from nuclear-encoded proteins and functions is fully achieved. How might this impact our understanding of cellular functions and the development of new treatments for mitochondrial diseases?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on modifying the mitochondrial genome relate to other areas of genetic engineering and its potential for future advancements in the field?\\n\\n5. User-Interactive Question:\\n   - What are your thoughts on the ethical considerations surrounding genetic modifications in mitochondrial genome engineering? How do you think society should approach this research?\"\n",
        "    hypothetical_questions_text_2 = \"1. Content-Based Question: \\n   - What does this report investigate regarding mitochondrial ATP production?\\n   - How does this report contribute to our understanding of mitochondrial function?\\n   - What are the key findings regarding the replication of mitochondrial ATP production outside the cellular environment?\\n\\n2. Analytical Question:\\n   - How do theoretical models help in understanding mitochondrial ATP production?\\n   - What experimental evidence supports the concept of artificial organelles for ATP synthesis?\\n   - What are the implications of studying mitochondrial dynamics and stress responses for ex vivo methods of ATP synthesis?\\n\\n3. Creative/Scenario-Based Question:\\n   - Imagine a scenario where mitochondrial ATP production could be replicated outside the cellular environment. How could this impact medical research and treatments?\\n   - If artificial organelles capable of ATP synthesis were successfully developed, what potential applications could they have in various industries?\\n   - How might the understanding of mitochondrial dynamics and stress responses lead to the development of innovative approaches for ATP synthesis?\\n\\n4. Contextual/Relational Question:\\n   - How does the research on mitochondrial ATP production relate to the broader field of cellular bioenergetics?\\n   - In what ways does the replication of mitochondrial ATP production outside cells build upon previous studies in the field?\\n   - How do the findings in this report align with or challenge existing theories and models of mitochondrial function?\\n\\n5. User-Interactive Question:\\n   - How would you approach studying the replication of mitochondrial ATP production outside the cellular environment?\\n   - Can you think of any potential limitations or ethical considerations in developing artificial organelles for ATP synthesis?\\n   - What questions or areas of research would you like to see explored further in the study of mitochondrial dynamics and stress responses?\"\n",
        "    hypothetical_questions = []\n",
        "    hypothetical_questions.append(hypothetical_questions_text_1)\n",
        "    hypothetical_questions.append(hypothetical_questions_text_2)\n",
        "    for hypothetical_questions_text in hypothetical_questions:\n",
        "        extracted_hypothetical_questions = extract_hypothetical_questions(hypothetical_questions_text)\n",
        "        print(f'Extracted hypothetical questions: {extracted_hypothetical_questions}')\n",
        "\n",
        "test_extract_hypothetical_questions()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'tailored isolation protocol'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimized damage to ensure the integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'minimal contamination from other organelles'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'responsible for',\n",
              "  'target': 'energy production'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'plant cells'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated for',\n",
              "  'target': 'studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'improved methods'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'slight modifications'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'traditional plant protoplast isolation'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'mammalian mitochondria extraction protocols'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'adjustments in isolation medium compositions'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated with',\n",
              "  'target': 'reduced need for heavy labor, expensive equipment, and large amounts of starting material'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'respiratory chain measurements, western blot analyses, and mass spectrometry'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'confirm the intactness and functional capacity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial purity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'evaluate the mitochondrial integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the mitochondrial membrane potential'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed to',\n",
              "  'target': 'measure the electron transport chain activity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed at',\n",
              "  'target': 'the DNA and protein levels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'protein and tRNA uptake experiments'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'enzyme activity assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'western blot analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'mass spectrometry analyses'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'used for',\n",
              "  'target': 'targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed for',\n",
              "  'target': 'purity and integrity'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'proteinase digestion assays'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron microscopy'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'mitochondrial membrane potential measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'assessed using',\n",
              "  'target': 'electron transport chain activity measurement'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated from',\n",
              "  'target': 'Arabidopsis thaliana'},\n",
              " {'subject': 'mitochondria',\n",
              "  'relationship': 'isolated using',\n",
              "  'target': 'continuous colloidal density gradients'},\n",
              " {'subject': 'mitochondria', 'relationship': 'isolated at', 'target': '4 °C'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_entity_relationships(entity_relationships_text):\n",
        "    # Regular expression pattern for identifying entity relationships\n",
        "    entity_pattern = re.compile(r'\\d+\\.\\s+\\((.+?),\\s+(.+?),\\s+(.+?)\\)')\n",
        "    entity_relationships = entity_pattern.findall(entity_relationships_text)\n",
        "    return [{\"subject\": relationship[0], \"relationship\": relationship[1], \"target\": relationship[2]} for relationship in entity_relationships]\n",
        "\n",
        "# Example entity relationships text\n",
        "entity_relationships_text =  \"Entity Relationships:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n30. (mitochondria, isolated with, tailored isolation protocol)\\n31. (mitochondria, isolated with, minimized damage to ensure the integrity)\\n32. (mitochondria, isolated with, reduced contamination from other organelles)\\n33. (mitochondria, isolated with, improved methods)\\n34. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n35. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n36. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n37. (mitochondria, isolated with, minimal contamination from other organelles)\\n38. (mitochondria, isolated with, improved methods)\\n39. (mitochondria, isolated with, slight modifications)\\n40. (mitochondria, isolated with, traditional plant protoplast isolation)\\n41. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n42. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n43. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n44. (mitochondria, isolated with, minimal contamination from other organelles)\\n45. (mitochondria, isolated from, Arabidopsis thaliana)\\n46. (mitochondria, isolated using, continuous colloidal density gradients)\\n47. (mitochondria, isolated at, 4 °C)\\n48. (mitochondria, used for, protein and tRNA uptake experiments)\\n49. (mitochondria, used for, enzyme activity assays)\\n50. (mitochondria, used for, western blot analyses)\\n51. (mitochondria, used for, mass spectrometry analyses)\\n52. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n53. (mitochondria, assessed for, purity and integrity)\\n54. (mitochondria, assessed using, proteinase digestion assays)\\n55. (mitochondria, assessed using, electron microscopy)\\n56. (mitochondria, assessedThe article discusses the protocol for isolating mitochondria from plant cells. Mitochondria are double-membraned organelles responsible for energy production in eukaryotic cells. The isolation of mitochondria is crucial for various studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays.\\n\\nThe isolation process is challenging due to the presence of cell walls, vacuoles, and secondary metabolites in plant cells. The protocol must be tailored to minimize damage to the mitochondria and ensure their integrity. Specificity in isolation protocols is required as different plant species and tissue types have varying phenolic compounds and metabolite profiles. Earlier methods led to contamination with nuclei and chloroplasts, but recent advancements have improved isolation methods, reducing the need for heavy labor, expensive equipment, and large amounts of starting material.\\n\\nThe protocol for isolating intact mitochondria involves several steps. First, the preparation of grinding medium, wash buffer, and gradient solutions is necessary. The plant material is then homogenized in the grinding medium to release the mitochondria, which are then filtered and centrifuged to pellet the mitochondria. The mitochondrial pellet is resuspended in the wash buffer. Oxygen consumption measurements are crucial for determining the intactness and functional capacity of the isolated mitochondria. Evaluation of mitochondrial purity and integrity can be done through proteinase digestion assays, electron microscopy, and checks of mitochondrial membrane potential and electron transport chain activity.\\n\\nOnce purified, the isolated mitochondria can be used for various studies, including protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses. For mass spectrometry analyses, targeted multiple reaction monitoring (MRM) or quantification by dimethyl or other isotope labels can be employed.\\n\\nIn conclusion, the isolation of mitochondria from plant cells is a delicate process that requires careful consideration of the specific requirements of the plant species and tissue type. Recent advancements have made the process more effective and accessible for a range of tissue types and species, allowing for a broader application of mitochondrial studies across different plant species.\\n\\nReferences:\\n- Plant Methods. (2015). https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0099-x\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908444/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7640673/\\n- NCBI. (2018). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687074/Extraction and Categorization:\\n\\n1. (mitochondria, responsible for, energy production)\\n2. (mitochondria, isolated from, plant cells)\\n3. (mitochondria, isolated for, studies involving mitochondrial DNA, protein profiling, and enzymatic activity assays)\\n4. (mitochondria, isolated using, continuous colloidal density gradients)\\n5. (mitochondria, isolated with, improved methods)\\n6. (mitochondria, isolated with, slight modifications)\\n7. (mitochondria, isolated with, traditional plant protoplast isolation)\\n8. (mitochondria, isolated with, mammalian mitochondria extraction protocols)\\n9. (mitochondria, isolated with, adjustments in isolation medium compositions)\\n10. (mitochondria, isolated with, reduced need for heavy labor, expensive equipment, and large amounts of starting material)\\n11. (mitochondria, used for, respiratory chain measurements, western blot analyses, and mass spectrometry)\\n12. (mitochondria, used for, protein and tRNA uptake experiments, enzyme activity assays, and western blot analyses)\\n13. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n14. (mitochondria, assessed for, purity and integrity)\\n15. (mitochondria, assessed using, proteinase digestion assays, electron microscopy, mitochondrial membrane potential measurement, and electron transport chain activity measurement)\\n16. (mitochondria, assessed to, confirm the intactness and functional capacity)\\n17. (mitochondria, assessed to, evaluate the mitochondrial purity)\\n18. (mitochondria, assessed to, evaluate the mitochondrial integrity)\\n19. (mitochondria, assessed to, measure the mitochondrial membrane potential)\\n20. (mitochondria, assessed to, measure the electron transport chain activity)\\n21. (mitochondria, assessed at, the DNA and protein levels)\\n22. (mitochondria, assessed using, electron microscopy)\\n23. (mitochondria, assessed using, proteinase digestion assays)\\n24. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n25. (mitochondria, assessed using, electron transport chain activity measurement)\\n26. (mitochondria, isolated from, Arabidopsis thaliana)\\n27. (mitochondria, isolated using, continuous colloidal density gradients)\\n28. (mitochondria, isolated at, 4 °C)\\n29. (mitochondria, used for, protein and tRNA uptake experiments)\\n30. (mitochondria, used for, enzyme activity assays)\\n31. (mitochondria, used for, western blot analyses)\\n32. (mitochondria, used for, mass spectrometry analyses)\\n33. (mitochondria, used for, targeted multiple reaction monitoring or quantification by dimethyl or other isotope labels)\\n34. (mitochondria, assessed for, purity and integrity)\\n35. (mitochondria, assessed using, proteinase digestion assays)\\n36. (mitochondria, assessed using, electron microscopy)\\n37. (mitochondria, assessed using, mitochondrial membrane potential measurement)\\n38. (mitochondria, assessed using, electron transport chain activity measurement)\\n39. (mitochondria, isolated from, Arabidopsis thaliana)\\n40. (mitochondria, isolated using, continuous colloidal density gradients)\\n41. (mitochondria, isolated at, 4 °C)\"\n",
        "\n",
        "# Clean the entity relationships\n",
        "cleaned_entity_relationships = clean_entity_relationships(entity_relationships_text)\n",
        "\n",
        "# Output the cleaned entity relationships\n",
        "cleaned_entity_relationships\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pydantic 2.6.1\n",
            "Uninstalling pydantic-2.6.1:\n",
            "  Successfully uninstalled pydantic-2.6.1\n",
            "Found existing installation: instructor 0.5.2\n",
            "Uninstalling instructor-0.5.2:\n",
            "  Successfully uninstalled instructor-0.5.2\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Using cached pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
            "Installing collected packages: pydantic\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "agency-swarm 0.1.0 requires instructor==0.3.4, which is not installed.\n",
            "paper-qa 3.13.4 requires openai<1, but you have openai 1.8.0 which is incompatible.\n",
            "paper-qa 3.13.4 requires pydantic<2, but you have pydantic 2.6.1 which is incompatible.\n",
            "agency-swarm 0.1.0 requires openai==1.3.0, but you have openai 1.8.0 which is incompatible.\n",
            "llama-index 0.8.62 requires openai<1, but you have openai 1.8.0 which is incompatible.\n",
            "pymemgpt 0.1.18 requires openai<0.29.0,>=0.28.1, but you have openai 1.8.0 which is incompatible.\n",
            "pyautogen 0.1.14 requires openai<1, but you have openai 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.6.1\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'instructor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minstructor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'instructor'"
          ]
        }
      ],
      "source": [
        "!pip uninstall pydantic -y\n",
        "!pip uninstall instructor -y\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (2.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
            "Requirement already satisfied: instructor in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (0.5.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (1.8.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (2.6.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (8.2.3)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: PyPDF2 in /home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages (3.0.1)\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.txt\n",
            "Protocol: **Article:** \"Mitochondria isolated from lipid droplets of white adipose tissue reveal functional differences based on lipid droplet size\" by Alexandra J Brownstein et al.\n",
            "\n",
            "**Steps:**\n",
            "1. PDM can be isolated from BAT by differential centrifugation and salt washes.\n",
            "2. A method was developed to isolate PDM from WAT with high yield and purity by an optimized proteolytic treatment.\n",
            "3. The respiratory function of mitochondria is preserved during the isolation process.\n",
            "4. WAT PDM have lower respiratory and ATP synthesis capacities compared with WAT cytoplasmic mitochondria (CM).\n",
            "5. PDM can be isolated from LDs of different sizes.\n",
            "6. There is a negative correlation between LD size and the respiratory capacity of their PDM in WAT.\n",
            "\n",
            "**Incompleteness/Notes:**\n",
            "- The specific steps for differential centrifugation and salt washes for PDM isolation from BAT are not provided.\n",
            "- The details of the optimized proteolytic treatment for PDM isolation from WAT are not mentioned.\n",
            "- The protocol for preserving respiratory function during isolation is not described.\n",
            "- The specific method for isolating PDM from LDs of different sizes is not explained.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides some steps for isolating PDM from white adipose tissue (WAT) and describes their functional differences based on lipid droplet (LD) size. However, the protocol lacks specific details for differential centrifugation, salt washes, proteolytic treatment, and isolation of PDM from LDs of different sizes. The article highlights the need for an optimized protocol to isolate PDM with high yield and purity.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: Steps for isolating PDM from lipid droplets:\n",
            "\n",
            "1. \"We describe a new approach to isolate PDM that combines protease treatment and centrifugation.\"\n",
            "2. \"The combined protease and centrifugation method successfully detaches mitochondria from WAT lipid droplets.\"\n",
            "3. \"Using our new approach, we detached PDM from small and large lipid droplets, uncovering the functional diversity of PDM segregated by their lipid droplet size.\"\n",
            "4. \"Mechanical separation effectively removed the majority of, but not all, PDM from lipid droplets in BAT.\"\n",
            "5. \"Application of the same mechanical protocol to WAT did not result in any significant removal of PDM from lipid droplets.\"\n",
            "6. \"We treated the fat layers of WAT with Proteinase K (Prot K) to digest the protein-mediated tethers anchoring mitochondria to LDs and potentially strip mitochondria that are resistant to stripping by centrifugation.\"\n",
            "7. \"To confine degradation to the protein tethers in the LD and outer mitochondrial membrane, we inactivated Prot K with PMSF right before separating PDM from the LDs by centrifugation.\"\n",
            "8. \"Prot K treatment significantly increased the amount of protein in the PDM fraction of WAT, demonstrating an improvement in the yield of PDM isolation.\"\n",
            "\n",
            "**Incompleteness/uncertainties:**\n",
            "- The article does not provide a detailed protocol for the protease treatment and centrifugation method.\n",
            "- The specific concentrations, incubation times, and temperatures for the protease treatment and inactivation are not mentioned.\n",
            "- The specific centrifugation parameters (speed, time, temperature) are not provided.\n",
            "- The article does not mention the specific types of proteases used or their sources.\n",
            "\n",
            "**Conclusion:**\n",
            "The article describes a new approach to isolate PDM from lipid droplets using protease treatment and centrifugation. The method successfully detaches mitochondria from WAT lipid droplets and reveals functional diversity based on lipid droplet size. However, the article lacks specific details of the protocol, including concentrations, incubation times, temperatures, and centrifugation parameters. The specific proteases used and their sources are also not mentioned.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: Steps for mitochondrial isolation from white adipose tissue (WAT) and brown adipose tissue (BAT) using proteinase K (Prot K) treatment:\n",
            "\n",
            "1. Centrifuge fat homogenates at low speed to separate the fat layer containing peridroplet mitochondria (PDM) from the supernatant containing cytoplasmic mitochondria (CM).\n",
            "2. Add Prot K to both the lipid droplet (LD) fraction and supernatant and incubate for 15 minutes.\n",
            "3. Perform high-speed centrifugation to separate PDM from Prot K-treated lipid droplets and CM from the supernatant.\n",
            "4. Quantify the amount of PDM attached to LDs by determining the percentage of LD surface covered by mitochondria.\n",
            "5. Analyze images of LD fractions from BAT to quantify the percentage of LD surface covered by mitochondria.\n",
            "6. Compare the protein recovered in PDM and CM fractions in the presence or absence of Prot K treatment from WAT and BAT.\n",
            "7. Perform Western blot analysis of PDM and CM isolated from WAT and BAT using regular PDM protocol, PDM isolation + Prot K, PDM isolation with Prot K + PMSF, and PMSF alone.\n",
            "8. Quantify PDM in WAT and BAT using the fluorescence plate reader assay by measuring the ratio of mitochondrial fluorescence to LD fluorescence.\n",
            "\n",
            "**Notes:**\n",
            "- The article does not provide a complete step-by-step protocol for mitochondrial isolation using Prot K treatment. It only mentions specific steps and their outcomes.\n",
            "- The article mentions the use of Prot K to disrupt protein-mediated interactions and increase the yield of isolated mitochondria.\n",
            "- The article also mentions the use of PMSF to prevent protein degradation during freeze-thaw cycles.\n",
            "- The article provides some details about the quantification methods used to assess the amount of PDM and the purity of mitochondrial fractions.\n",
            "\n",
            "**Conclusion:**\n",
            "The article describes the use of Prot K treatment to isolate peridroplet mitochondria (PDM) from white adipose tissue (WAT) and increase the yield of PDM isolated from brown adipose tissue (BAT). However, the article does not provide a complete and detailed protocol for the mitochondrial isolation process. Additional information is needed to fully understand the steps involved in the protocol.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: 1. Quantiﬁcation of VDAC and OXPHOS proteins by Western blot revealed no differences in their content per microgram of protein.\n",
            "2. Prot K treatment increased the yield of mitochondrial and contaminant proteins in PDM fractions.\n",
            "3. PMSF treatment confirmed that the decrease in VDAC and OXPHOS content was caused by the persistence of Prot K activity in PDM and CM fractions even after washes and freeze-thawing.\n",
            "4. PMSF treatment alone did not result in any significant differences in VDAC and OXPHOS protein content.\n",
            "5. Prot K treatment facilitated the separation of mitochondria from LDs, as confirmed by a plate reader-based assay.\n",
            "6. LD fractions were stained with BODIPY493/503 (BD493) and Mito-Tracker DeepRed (MTDR) to visualize lipid droplets and mitochondria, respectively.\n",
            "7. MTDR was chosen because its membrane potential dependency is minimal and does not bias mitochondrial mass.\n",
            "8. The reduction in the ratio of MTDR/BD493 fluorescence after separating the mitochondria by centrifugation was used to quantify PDM content.\n",
            "9. Centrifugation alone was unable to strip a significant amount of PDM from WAT, but Prot K treatment allowed for PDM isolation.\n",
            "10. In BAT, centrifugation alone reduced the ratio of MTDR/BD493, and Prot K treatment further reduced it, although not significantly.\n",
            "11. Prot K treatment enhances PDM isolation from WAT and can increase the yield of PDM isolated from BAT.\n",
            "12. Prot K treatment does not inhibit mitochondrial respiratory function.\n",
            "13. PMSF and Prot K treatments were reported to impact oxygen consumption measured in intact mitochondria, so the respiratory function of freshly isolated mitochondria was tested.\n",
            "14. PMSF treatment at 2 mM did not affect mitochondrial respiratory function by itself.\n",
            "15. Respiratory function of isolated PDM and CM using Prot K was determined by quantifying oxygen consumption rates (OCR) using the XF96 flux analyzer.\n",
            "16. State 3 respiration, reflecting coupled respiration associated with maximal ATP synthesis, was measured.\n",
            "17. Maximal complex IV (CoxIV) activity was measured separately by providing TMPD/Ascorbate directly to isolated mitochondria.\n",
            "18. Prot K + PMSF-treated CM and PDM respiration data were normalized to the respiration of untreated mitochondria from the same tissue sample.\n",
            "19. WAT CM isolated using Prot K showed a significant increase in state 3 OCR, which was prevented by PMSF treatment.\n",
            "20. PMSF alone decreased respiration rates to the same levels as mitochondria treated with Prot K + PMSF, indicating an autonomous effect of PMSF on decreasing state 3 respiration.\n",
            "21. CoxIV-driven respiration was not increased in mitochondria isolated using Prot K, supporting the idea that increased state 3 respiration reflected a specific increase in ATP-synthesizing activity.\n",
            "22. ATP synthesis rates in isolated mitochondria were measured using firefly luciferase luminescence, and Prot K treatment increased ATP synthesis rates in WAT CM.\n",
            "23. The effects of Prot K on WAT PDM respiratory function showed some similarities to WAT CM.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for isolating mitochondria from white adipose tissue (WAT) using Proteinase K (Prot K):\n",
            "\n",
            "1. \"Isolating PDM using Prot K increased both state 3 respiration and ATP synthesis rates.\"\n",
            "2. \"The major difference induced by Prot K treatment in PDM was an increase in CoxIV activity.\"\n",
            "3. \"In all, isolating either CM or PDM using Prot K yielded mitochondria with higher respiratory capacity.\"\n",
            "4. \"Because we observed that Prot K treatment selectively increased ATP synthesizing respiration in some mitochondrial preparations, it was still possible that Prot K induced mild damage to the outer membrane, to cause mild cytochrome C leakage.\"\n",
            "5. \"If mild leakage was present, we should see a larger increase in respiration induced by cytochrome C supplementation in mitochondria isolated with Prot K, when compared with cytochrome c supplementation in mitochondria isolated without Prot K.\"\n",
            "6. \"It is important to note that cytochrome c supplementation can increase oxygen consumption independently of complex IV activity as well.\"\n",
            "7. \"We found that cytochrome c supplementation did not induce a larger increase in respiration in Prot K-treated mitochondria, with this effect being even of lower magnitude than the effect in control mitochondria.\"\n",
            "8. \"This latter result shows that cytochrome c-induced increase in respiration is not explained by an increase in the availability of cytochrome c for mitochondrial respiration.\"\n",
            "9. \"These new data show that Prot K treatment did not induce mild damage in the outer membrane to cause a small leak in cytochrome c.\"\n",
            "10. \"Our protocol allowed us to compare the respiratory capacity between CM and PDM in WAT, which revealed remarkable differences between PDM and CM in BAT.\"\n",
            "11. \"PDM showed lower respiratory capacity than CM in WAT, as demonstrated by decreased state 3 and CoxIV-driven respiration.\"\n",
            "12. \"Confirming the decrease in state 3 respiration fueled by pyruvate, WAT PDM also showed lower ATP-synthesis rates than CM.\"\n",
            "13. \"We next determined whether Prot K treatment altered the function of PDM isolated from BAT, and preserved the previously published distinct bioenergetics and proteomic makeup of BAT PDM.\"\n",
            "14. \"First, we found that, as in WAT, using Prot K to isolate CM from BAT did not decrease state 3 CoxIV-driven respiration or ATP synthesis rates.\"\n",
            "15. \"Similarly, Prot K treatment did not decrease the respiratory function of PDM fractions from BAT.\"\n",
            "16. \"Indeed, Prot K treatment even increased ATP synthesis activity in BAT PDM.\"\n",
            "\n",
            "**Conclusion:** The article provides steps for isolating mitochondria from white adipose tissue (WAT) and brown adipose tissue (BAT) using Proteinase K (Prot K). The steps mention the increase in state 3 respiration and ATP synthesis rates with Prot K treatment, as well as the increase in CoxIV activity in PDM. The protocol also compares the respiratory capacity between CM and PDM in WAT and BAT, highlighting differences in respiratory capacity and ATP synthesis rates. However, the article does not provide a complete protocol for mitochondrial isolation using Prot K, and some steps are described ambiguously or partially.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, it is difficult to extract a complete mitochondrial isolation protocol. The text mentions various experiments and measurements related to mitochondrial function in white adipose tissue (WAT) and brown adipose tissue (BAT), but specific isolation steps are not clearly described. It is mentioned that WAT peridroplet mitochondria were isolated using Prot K, and PMSF treatment was used to analyze the mitochondrial proteome. However, the exact details of the isolation procedure, such as buffer composition, centrifugation steps, and purification methods, are not provided.\n",
            "\n",
            "In conclusion, the text does not provide a complete mitochondrial isolation protocol. It only mentions some experimental measurements and treatments related to mitochondrial function in WAT and BAT.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Separate lipid droplets by size from mouse BAT and WAT.\n",
            "2. Analyze diverse protein profiles between the subpopulations of lipid droplets.\n",
            "3. Observe heterogeneity in PDM function and composition in intact primary brown adipocytes.\n",
            "4. Compare ATP-synthesizing respiration and CoxIV activity between PDM from WAT and BAT.\n",
            "5. Test the hypothesis that LD size determines the heterogeneity of PDM function.\n",
            "6. Strip PDM from SM-LD and LG-LD fractions and analyze them separately by respirometry.\n",
            "7. Measure state 3 OCR, CoxIV-driven respiration, and ATP-synthesizing respiration in WAT PDM isolated from SM-LD and LG-LD fractions.\n",
            "8. Observe disappearance of differences in mitochondrial respiration between WAT PDM isolated from LG versus SM-LD when using palmitoyl carnitine as the oxidative fuel.\n",
            "9. Measure state 3 respiration, proton leak-driven respiration, and ATP-synthesizing respiration in BAT PDM isolated from LG-LD and SM-LD fractions under pyruvate malate.\n",
            "10. Observe similar CoxIV-driven respiration in BAT PDM from LG-LD and SM-LD fractions.\n",
            "11. Preserve increased state 3, leak, and ATP-synthesizing respiration, as well as increased CoxIV, in BAT PDM from LG-LD fractions when palmitoyl-carnitine is provided as a fuel.\n",
            "12. Validate bioenergetic differences by measuring the content of mitochondrial proteins (VDAC and OXPHOS subunits) per total protein of PDM fraction by Western blot.\n",
            "13. Note higher content of VDAC and increased levels of respiratory complexes in WAT PDM isolated from SM-LD fractions.\n",
            "14. Normalize respirometry data for WAT by VDAC as a marker for total mitochondrial content.\n",
            "15. Observe no differences in VDAC or OXPHOS subunit protein content in BAT PDM isolated from large and small LDs, and no need to re-normalize respirometry data for BAT. \n",
            "\n",
            "**Conclusion:** The article discusses the separation of lipid droplets by size from mouse BAT and WAT and the analysis of diverse protein profiles between the subpopulations of lipid droplets. It also explores the heterogeneity in PDM function and composition in intact primary brown adipocytes. The article compares ATP-synthesizing respiration and CoxIV activity between PDM from WAT and BAT and tests the hypothesis that LD size determines the heterogeneity of PDM function. Respirometry experiments are performed on PDM isolated from SM-LD and LG-LD fractions, and differences in mitochondrial respiration are observed. The article validates the bioenergetic differences by measuring the content of mitochondrial proteins. However, the article does not provide a complete step-by-step protocol for mitochondrial isolation.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: Steps for the isolation of peridroplet mitochondria (PDM) from white adipose tissue (WAT) using proteinase K treatment:\n",
            "\n",
            "1. Homogenize the tissue.\n",
            "2. Centrifuge the homogenate at 500 g to create a fat layer containing large lipid droplets (LG-LDs).\n",
            "3. Remove the first fat layer.\n",
            "4. Centrifuge the remaining homogenate at 2,000 g to create a second fat layer containing small lipid droplets (SM-LDs).\n",
            "5. Treat the lipid droplet fractions (containing PDM) with proteinase K.\n",
            "6. Use Prot K, a serine endopeptidase, which remains active in the presence of different detergents and temperature ranges.\n",
            "7. Digest a wide variety of proteins using Prot K.\n",
            "8. Use Prot K to isolate sufficient WAT PDM.\n",
            "9. Add PMSF to inactivate Prot K and prevent degradation in freeze-thawed PDM.\n",
            "\n",
            "Incompleteness: The article does not provide details on the specific concentrations or volumes of reagents used, the duration of each step, or the specific centrifugation conditions.\n",
            "\n",
            "Conclusion: The article describes a method for isolating peridroplet mitochondria (PDM) from white adipose tissue (WAT) using proteinase K treatment. The steps mentioned include homogenization, centrifugation to create fat layers containing large and small lipid droplets, treatment of lipid droplet fractions with proteinase K, and inactivation of proteinase K with PMSF. However, the article lacks specific details on reagent concentrations, volumes, duration of steps, and centrifugation conditions, which are important for replicating the protocol accurately.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4159 tokens (3359 in the messages, 800 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            " Occured in generate_summary function\n",
            "Error in summarizing article: argument of type 'NoneType' is not iterable\n",
            " Using last summary\n",
            "Protocol: 1. All mitochondria were isolated from WAT and BAT using the Prot K protocol.\n",
            "2. Quantiﬁcation of state 3, mitochondrial proton leak, ATP-linked, and CoxIV oxygen consumption rate using pyruvate malate as substrate or palmitoyl-carnitine + malate as substrate in PDM from LG- and SM-LDs in WAT.\n",
            "3. Assessing mitochondrial function in PDM isolated from LG- and SM-LDs in BAT.\n",
            "4. Quantiﬁcation of state 3, mitochondrial proton leak, ATP-linked, and CoxIV oxygen consumption rate using pyruvate + malate as substrate or palmitoyl-carnitine + malate as substrate from four to six individual isolations.\n",
            "5. Representative Western blots of mitochondrial proteins from mitochondria isolated from LG- and SM-LDs from WAT and quantiﬁcation of the protein data from n = 3–5 individual isolations.\n",
            "6. Representative Western blots of mitochondrial proteins from mitochondria isolated from LG- and SM-LDs from BAT and quantiﬁcation of the protein data from n = 3–5 individual isolations.\n",
            "\n",
            "**Incompleteness:** The article does not provide a detailed description of the Prot K protocol for mitochondrial isolation. The specific steps and reagents used in the protocol are not mentioned.\n",
            "\n",
            "**Conclusion:** The article mentions the isolation of mitochondria from WAT and BAT using the Prot K protocol. However, the specific steps and reagents used in the protocol are not provided, making it difficult to replicate the isolation procedure accurately. Further information is needed to complete the mitochondrial isolation protocol.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"To isolate PDM from WAT, Prot K treatment is needed regardless of the size of the LD that is being used for the preparation.\"\n",
            "2. \"As the yield of PDM isolated from SM-LD and LG-LD by centrifugation is similar, we can conclude that the strength of PDM attachment to different size LDs is similar in BAT.\"\n",
            "3. \"We do, however, observe more PDM attached to smaller LDs in WAT, and the opposite in BAT.\"\n",
            "\n",
            "**Incompleteness:**\n",
            "\n",
            "The article does not provide a complete protocol for mitochondrial isolation from white adipose tissue (WAT) or brown adipose tissue (BAT). It only mentions that proteinase K (Prot K) treatment is necessary for isolating PDM from WAT, and that centrifugation is used to isolate PDM from different-sized lipid droplets in BAT. However, the specific details of the isolation protocol, such as buffer composition, centrifugation conditions, and additional steps, are not provided.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The article provides some information about the isolation of PDM from WAT and BAT, but the protocol details are incomplete. The steps mentioned highlight the use of proteinase K treatment for WAT isolation and centrifugation for BAT isolation. However, the lack of specific protocol details limits the ability to replicate the isolation procedure accurately. Further information or additional sources would be needed to obtain a complete mitochondrial isolation protocol.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for mitochondrial isolation from white adipose fat pads and brown adipose tissue:\n",
            "\n",
            "1. Mice were euthanized by isoﬂurane.\n",
            "2. BAT and WAT were isolated from the interscapular BAT depot and both perigonadal fat pads, respectively.\n",
            "3. BAT was homogenized using a glass-teflon dounce homogenizer, and WAT was homogenized using a glass-glass dounce homogenizer in Sucrose-HEPES-EGTA buffer supplemented with BSA (SHE + BSA; 250 mM sucrose, 5 mM HEPES, 2 mM EGTA, 1% fatty acid-free BSA, pH 7.2).\n",
            "4. The homogenate was split into four equal parts for protocol optimizations.\n",
            "5. Homogenates were centrifuged at 1,000 g for 10 min at 4°C.\n",
            "6. The supernatant was transferred to a new tube, and the fat layer was scraped into a second tube and resuspended in SHE + BSA buffer.\n",
            "7. Fat layers and supernatant were either left untreated or treated with Prot K at 2 μg/ml for 15 min.\n",
            "8. All fat layers were incubated for 15 min at 4°C under constant rotation.\n",
            "9. For protocols including PMSF, 2 mM PMSF was added for an additional 20 min of incubation on ice while inverting the samples every 2 min.\n",
            "10. All samples were centrifuged again at 10,000 g for 10 min at 4°C.\n",
            "11. The pellets were resuspended in SHE + BSA and centrifuged with the same settings once more.\n",
            "12. The pellets were then resuspended in SHE without BSA and again centrifuged with the same settings.\n",
            "13. Final pellets were resuspended in SHE without BSA, and protein concentration was determined by BCA.\n",
            "\n",
            "Isolation of peridroplet mitochondria from large and small lipid droplets:\n",
            "\n",
            "14. Fat tissue homogenates were sieved through a 630-μm mesh and transferred into ice-cold tubes.\n",
            "15. Homogenates were centrifuged at 500 g for 3 min at 4°C, creating a fat layer of larger lipid droplets (LG-LD).\n",
            "16. The supernatant was poured into a new tube and centrifuged at 2,000 g for 10 min at 4°C, creating a second fat layer of smaller lipid droplets (SM-LD).\n",
            "17. The supernatant, containing cytoplasmic mitochondria, was transferred into a new tube.\n",
            "18. LG-LDs and SM-LDs were resuspended in SHE + BSA.\n",
            "19. LG-LDs were centrifuged again at 500 g for 5 min at 4°C, and SM-LDs were centrifuged at 2,000 g for 10 min at 4°C.\n",
            "20. Both large and small LDs were resuspended in 1 ml SHE + BSA and incubated with Prot K as described above.\n",
            "21. All fractions, including the supernatant containing the cytoplasmic mitochondria, were centrifuged at 10,000 g for 10 min at 4°C to pellet the mitochondria.\n",
            "22. Mitochondrial pellets were resuspended in SHE without BSA and spun one more time with the same conditions.\n",
            "23. Mitochondrial pellets were finally resuspended in SHE without BSA, and protein concentrations were determined by BCA.\n",
            "\n",
            "**Incompleteness:**\n",
            "- The article does not provide details on the specific modifications made to the PDM isolation protocol.\n",
            "- The article does not specify the exact steps for the sieving process or the volumes used for resuspension.\n",
            "\n",
            "**Conclusion:**\n",
            "The steps for mitochondrial isolation from white adipose fat pads and brown adipose\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: Steps for mitochondrial isolation and respirometry:\n",
            "\n",
            "1. Isolated mitochondria were resuspended in a mitochondrial assay buffer (MAS) containing 100 mM KCl, 10 mM KH2PO4, 2 mM MgCl2, 5 mM HEPES, 1 mM EGTA, 0.1% BSA, and 1 mM GDP, pH 7.2.\n",
            "2. Two micrograms of isolated mitochondria were loaded into a Seahorse XF96 microplate in a 20 μl volume containing substrates.\n",
            "3. The loaded plate was centrifuged at 2,000 g for 5 min at 4°C.\n",
            "4. An additional 130 μl of MAS buffer + substrate was added to each well.\n",
            "5. Substrate concentrations in the well were as follows:\n",
            "   - For pyruvate plus malate-dependent respiration: 5 mM pyruvate + 5 mM malate + 4 mM ADP.\n",
            "   - For palmitoyl-carnitine dependent respiration: 2 mM malate + 4 mM ADP + 40 μM palmitoyl-carnitine.\n",
            "6. For pyruvate plus malate-dependent respiration:\n",
            "   - Oligomycin was injected at port A (3.5 μM).\n",
            "   - TMPD + ascorbic acid (0.5 + 1 mM) was injected at port B.\n",
            "   - Azide (50 mM) was injected at port C.\n",
            "7. For palmitoyl-carnitine dependent respiration:\n",
            "   - 5 mM malate + 4 mM ADP + 40 μM palmitoyl-carnitine was injected at port A.\n",
            "   - Oligomycin was injected at port B (3.5 μM).\n",
            "   - TMPD + ascorbic acid (0.5 + 1 mM) was injected at port C.\n",
            "   - Azide (50 mM) was injected at port D.\n",
            "8. Mix and measure times were 0.5 and 4 min, respectively.\n",
            "9. A 2-min wait time was included for oligomycin-resistant respiration measurements.\n",
            "10. Various mitochondrial states were measured, including state 3 (coupled mitochondria producing ATP in the presence of substrates and ADP), oligomycin-resistant leak, ATP-linked respiration, Complex IV-driven respiration, and non-mitochondrial oxygen consumption.\n",
            "11. Fold changes in respiration were calculated from raw OCR values obtained in each experimental group, normalizing to untreated mitochondria.\n",
            "12. The effects of Prot K on OCR were calculated by comparing the raw OCR values of treated mitochondria with untreated mitochondria.\n",
            "13. Fold changes over untreated mitochondria were averaged and represented in bar graphs.\n",
            "14. For quantification of peridroplet mitochondria, the fat layer and stripped fat layer were incubated in MAS buffer containing MTDR and BODIPY493/503 dyes.\n",
            "15. Fluorescence measurements were taken using a plate reader.\n",
            "16. ATP synthesis assay was performed by resuspending 10 μg of isolated mitochondria in MAS buffer containing 5 mM pyruvate + 5 mM malate + 3.5 mM ADP and measuring luminescent counts over time.\n",
            "17. Protein gel electrophoresis and immunoblotting were performed using 5-15 mg of isolated mitochondrial protein or fat layer.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a detailed protocol for mitochondrial isolation and respirometry. The steps are described clearly, and all mentioned details are included. However, there may be additional steps or details that are not mentioned in the article, and the protocol may not be complete.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: 1. Proteins were transferred to methanol-activated Immuno-Blot PVDF Membrane (Bio-Rad) in 30 V constant voltage for 1 h at 4°C.\n",
            "2. Blots were incubated overnight with a primary antibody diluted in PBST (phosphate buffered saline with 1 ml/liter Tween-20/PBS) + 5% BSA (Thermo Fisher Scientific) at 4°C.\n",
            "3. The next day, blots were washed in PBST and incubated with fluorescent or HRP secondary antibodies, diluted in PBST+ 5% BSA for 1 h at room temperature.\n",
            "4. Proteins were detected using the following antibodies:\n",
            "   - MTCO1 antibody (1D6E1A8) (ab14705)\n",
            "   - Total Rodent OXPHOS Cocktail (ab110413)\n",
            "   - VDAC (ab15895)\n",
            "   - ATP5A1 Monoclonal Antibody (15H4C4) (43-9800; Thermo Fisher Scientific)\n",
            "   - NDUFFB8 monoclonal antibody (20E9DH10C12) (459210; Thermo Fisher Scientific)\n",
            "   - SDHA Monoclonal Antibody (2E3GC12FB2AE2) (459200; Thermo Fisher Scientific)\n",
            "   - UQCRC2 Rabbit Polyclonal Antibody (14742-1-AP) from Proteintech\n",
            "   - TOMM20 monoclonal antibody clone 4F3 (WH0009804M1-100UG) Sigma-Aldrich\n",
            "   - VDAC (D73D12) Rabbit mAb Cell signaling Technology 4661T\n",
            "5. Secondary antibodies used were:\n",
            "   - goat anti-Mouse IgG secondary antibody, Alexa Fluor 660 conjugate (Thermo Fisher Scientific)\n",
            "   - goat anti-rabbit IgG secondary antibody DyLight 800 (Thermo Fisher Scientific)\n",
            "   - donkey anti-mouse IgG (H + L) Highly Cross Adsorbed Secondary Antibody, Alexa Fluor 488 conjugate (Thermo Fisher Scientific)\n",
            "   - anti-rabbit IgG, HRP-linked antibody (7074; Cell Signaling Technology)\n",
            "   - anti-mouse IgG, HRP-linked antibody (7076; Cell Signaling Technology)\n",
            "6. Blots were imaged on the ChemiDoc MP imaging system (Bio-Rad Laboratories).\n",
            "7. Band densitometry was quantified using FIJI (ImageJ, NIH).\n",
            "8. Fluorescence microscopy was performed using a confocal microscope.\n",
            "9. Super-resolution imaging was performed on Zeiss LSM880 with a 63x and 40x Apochromat oil-immersion lens and Airyscan super-resolution detector.\n",
            "10. Fluorophores were excited on separate tracks to avoid artifacts due to bleed-through emission.\n",
            "11. BODIPY 493/503 was excited with a 488-nm 25 mW Argon-ion laser and its emission captured through a 500-550 nm band-pass filter.\n",
            "12. MTDR was excited using a 633-nm 5 mW helium-neon laser and its emission was captured through a 645 nm long-pass filter.\n",
            "13. Fractions of large and small lipid droplets were stained with BODIPY 493/503 (BD493, 1 μM, final) for 10 min at 37°C.\n",
            "14. 10-20 μl of each sample was mixed in a 1:1 ratio with Matrigel and pipetted into a single compartment of a cellview glass bottom four-compartment cell culture dish (#627975; Griener Bio-One).\n",
            "15. Imaging was performed using a 63x Apochromat oil-immersion lens for lipid droplet layers.\n",
            "16. PDM imaging from fat layers:\n",
            "   - Pre-stripped\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Error in summarizing article: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4145 tokens (3345 in the messages, 800 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            " Occured in generate_summary function\n",
            "Error in summarizing article: argument of type 'NoneType' is not iterable\n",
            " Using last summary\n",
            "Protocol: 1. \"Recruitment and remodeling of peridroplet mitochondria in human adipose tissue\" by Acín-Pérez et al. (2021) mentions mitochondrial isolation in the context of studying peridroplet mitochondria in human adipose tissue.\n",
            "2. The article does not provide specific steps or a detailed protocol for mitochondrial isolation.\n",
            "3. Therefore, it is not possible to extract the mitochondrial isolation steps from this article.\n",
            "4. Conclusion: The article does not provide a complete protocol for mitochondrial isolation.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: **Article:** Mirza AH, Cui L, Zhang S, Liu P (2021) Comparative proteomics reveals that lipid droplet-anchored mitochondria are more sensitive to cold in brown adipocytes. Biochim Biophys Acta Mol Cell Biol Lipids 1866: 158992.\n",
            "\n",
            "**1. Read Article:**\n",
            "\n",
            "The article focuses on comparative proteomics analysis to investigate the sensitivity of lipid droplet-anchored mitochondria to cold in brown adipocytes.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "\n",
            "- \"Brown adipocytes were isolated from interscapular BAT depots of 8-week-old male C57BL/6J mice.\"\n",
            "- \"Mitochondria were isolated from BAT using a standard differential centrifugation method.\"\n",
            "- \"The mitochondrial pellet was resuspended in MSHE buffer (210 mM mannitol, 70 mM sucrose, 5 mM HEPES, and 1 mM EGTA, pH 7.2) and centrifuged at 600 × g for 10 min at 4°C to remove unbroken cells and nuclei.\"\n",
            "- \"The supernatant was then centrifuged at 12,000 × g for 15 min at 4°C to obtain the mitochondrial pellet.\"\n",
            "- \"The mitochondrial pellet was resuspended in MSHE buffer and stored at -80°C until further analysis.\"\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "\n",
            "The article does not provide detailed information about the specific centrifugation speeds and durations used in the differential centrifugation method. It also does not mention the steps involved in resuspending the mitochondrial pellet in MSHE buffer.\n",
            "\n",
            "**4. Format Steps:**\n",
            "\n",
            "1. Isolate brown adipocytes from interscapular BAT depots of 8-week-old male C57BL/6J mice.\n",
            "2. Isolate mitochondria from BAT using a standard differential centrifugation method.\n",
            "3. Resuspend the mitochondrial pellet in MSHE buffer.\n",
            "4. Centrifuge the resuspended mitochondrial pellet at 600 × g for 10 min at 4°C to remove unbroken cells and nuclei.\n",
            "5. Centrifuge the supernatant at 12,000 × g for 15 min at 4°C to obtain the mitochondrial pellet.\n",
            "6. Resuspend the mitochondrial pellet in MSHE buffer.\n",
            "7. Store the resuspended mitochondrial pellet at -80°C until further analysis.\n",
            "\n",
            "**5. Indicate Uncertainties:**\n",
            "\n",
            "- The specific centrifugation speeds and durations used in the differential centrifugation method are not mentioned.\n",
            "- The steps involved in resuspending the mitochondrial pellet in MSHE buffer are not described.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The article provides a general overview of the mitochondrial isolation steps, including the use of a standard differential centrifugation method. However, specific details about centrifugation speeds, durations, and resuspension steps are missing. The protocol gaps make it difficult to replicate the exact procedure without additional information.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: **Article:** Ngo J, Benador IY, Brownstein AJ, Vergnes L, Veliova M, Shum M, Ac ´ın-P´erez R, Reue K, Shirihai OS, Liesa M (2021) Isolation and functional analysis of peridroplet mitochondria from murine brown adipose tissue STAR Protoc 2: 100243 doi: 10.1016/j.xpro.2020.100243\n",
            "\n",
            "**1. Read Article:**\n",
            "I have read the article and identified the relevant sections for mitochondrial isolation.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "The article provides the following steps for isolating peridroplet mitochondria from murine brown adipose tissue:\n",
            "\n",
            "1. \"Euthanize a male C57BL/6J mouse by cervical dislocation and spray the abdomen with 70% ethanol.\"\n",
            "2. \"Remove the interscapular brown adipose tissue (BAT) depot using scissors and forceps.\"\n",
            "3. \"Immediately place the BAT in ice-cold isolation buffer (IB) containing 250 mM sucrose, 10 mM HEPES, 2 mM EGTA, 1 mM DTT, and 0.2% (w/v) BSA, pH 7.4.\"\n",
            "4. \"Mince the BAT into small pieces using scissors and transfer them to a pre-chilled glass homogenizer.\"\n",
            "5. \"Homogenize the tissue using a motor-driven Teflon pestle at a speed of 1,000 rpm for 30 strokes.\"\n",
            "6. \"Transfer the homogenate to a pre-chilled 15 mL conical tube and centrifuge at 600 x g for 5 min at 4°C to pellet the nuclei and unbroken cells.\"\n",
            "7. \"Transfer the supernatant to a fresh pre-chilled 15 mL conical tube and centrifuge at 10,000 x g for 10 min at 4°C to pellet the mitochondria.\"\n",
            "8. \"Discard the supernatant and resuspend the mitochondrial pellet in 1 mL of ice-cold IB.\"\n",
            "9. \"Centrifuge the resuspended mitochondria at 10,000 x g for 10 min at 4°C to obtain a purified mitochondrial pellet.\"\n",
            "10. \"Discard the supernatant and resuspend the mitochondrial pellet in 100 μL of ice-cold IB for further analysis.\"\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "The article does not mention the specific speed and duration of the centrifugation steps. It also does not provide information on the concentration of BSA in the isolation buffer or any additional steps that may be required for further analysis of the isolated mitochondria.\n",
            "\n",
            "**4. Format Steps:**\n",
            "1. Euthanize a male C57BL/6J mouse by cervical dislocation and spray the abdomen with 70% ethanol.\n",
            "2. Remove the interscapular brown adipose tissue (BAT) depot using scissors and forceps.\n",
            "3. Immediately place the BAT in ice-cold isolation buffer (IB) containing 250 mM sucrose, 10 mM HEPES, 2 mM EGTA, 1 mM DTT, and 0.2% (w/v) BSA, pH 7.4.\n",
            "4. Mince the BAT into small pieces using scissors and transfer them to a pre-chilled glass homogenizer.\n",
            "5. Homogenize the tissue using a motor-driven Teflon pestle at a speed of 1,000 rpm for 30 strokes.\n",
            "6. Transfer the homogenate to a pre-chilled 15 mL conical tube and centrifuge at 600 x g for 5 min at 4°C to pellet the nuclei and unbroken cells.\n",
            "7. Transfer the supernatant to a fresh pre-chilled 15 mL conical tube and centrifuge at 10,000 x g for 10\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Protocol: Based on the provided information, I will need access to the articles mentioned in order to accurately extract the mitochondrial isolation steps. Please provide the full text of the articles or specify which article I should focus on.\n",
            "Storing data for file_id: Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.txt\n",
            "Protocol: **Article:** Rapid isolation of respiring skeletal muscle mitochondria using nitrogen cavitation\n",
            "\n",
            "**Relevant Sections:** Introduction, Methods\n",
            "\n",
            "**Steps:**\n",
            "1. \"Here, we describe a method for the recovery of intact, respiring mitochondria from murine skeletal muscle tissue and cell lines using nitrogen cavitation.\"\n",
            "2. \"This protocol results in high-yield, pure and respiring mitochondria without the need for purification gradients or ultracentrifugation.\"\n",
            "3. \"The protocol takes under an hour and requires limited specialized equipment.\"\n",
            "4. \"Our methodology is successful in extracting mitochondria of both cell extracts and skeletal muscle tissue.\"\n",
            "5. \"Western blotting and electron microscopy demonstrate the enrichment of mitochondria with their ultrastructure well-preserved and an absence of contamination from cytoplasmic or nuclear fractions.\"\n",
            "6. \"Using respirometry analysis we show that mitochondria extracted from murine skeletal muscle cell lines (C2C12) and tibialis anterior tissue have an appropriate respiratory control ratio.\"\n",
            "7. \"These measures are indicative of healthy coupled mitochondria.\"\n",
            "8. \"Our method successfully demonstrates the rapid isolation of functional mitochondria and will benefit researchers studying mitochondrial bioenergetics as well as providing greater throughput and application for time-sensitive assays.\"\n",
            "\n",
            "**Conclusion:**\n",
            "The article describes a method for isolating intact, respiring mitochondria from murine skeletal muscle tissue and cell lines using nitrogen cavitation. The protocol is rapid, taking under an hour, and does not require purification gradients or ultracentrifugation. The methodology is successful in extracting mitochondria from both cell extracts and skeletal muscle tissue. The article provides evidence of the enrichment of mitochondria and the preservation of their ultrastructure without contamination from cytoplasmic or nuclear fractions. Respirometry analysis shows that the isolated mitochondria have an appropriate respiratory control ratio, indicating healthy coupled mitochondria. Overall, the method demonstrates the rapid isolation of functional mitochondria and is suitable for researchers studying mitochondrial bioenergetics and time-sensitive assays. However, the article does not provide a detailed step-by-step protocol, and there may be additional steps or details not mentioned.\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Animals: Mice (males) were purchased from Charles River, United Kingdom (B6j). They were group housed in a standard temperature (22 °C) and humidity-controlled environment with a 12 h light 12 h dark cycle. Mice had ad libitum access to normal chow and water. Mice were sacrificed at 8 weeks of age using cervical dislocation.\n",
            "2. Skeletal muscle (tibialis anterior) isolation: The muscle was removed and placed in ice-cold sucrose buffer (10 mM HEPES, pH 7.5, 70 mM sucrose, 200 mM mannitol, 1 mM EGTA, 1% protease inhibitors, and 1% phosphatase inhibitors) for nitrogen cavitation and mitochondrial respirometry.\n",
            "3. Cell culture and tissue collection: C2C12 murine myoblasts (ATCC) in a T175 flask were maintained in high glucose-Dulbecco's Minimal Eagle's Medium (DMEM) supplemented with 10% (v/v) fetal bovine serum, 1% L-glutamine (v/v), and 1% penicillin-streptomycin (v/v) in a humidified atmosphere with 5% CO2 at 37°C. Once at 80% confluence, cells were incubated in DMEM supplemented with 2% (v/v) heat-inactivated horse serum for 5 days to facilitate myocyte differentiation, with media being replaced every 48 h. Cells were used between passages 18-28.\n",
            "4. Mitochondrial isolation from cells:\n",
            "   a. Cell pellets were collected by washing the monolayer twice with Dulbecco's phosphate-buffered saline without Ca2+ and Mg2+ (DPBS-) to remove traces of serum.\n",
            "   b. Cells were then incubated with trypsin for approximately 1 min at 37°C. Trypsin was quenched with 10 mL of complete growth media, and the suspension was transferred to a 50 mL centrifuge tube.\n",
            "   c. Cells were pelleted by centrifugation at 300 g for 5 min. The supernatant was discarded, and the pellet was washed twice with PBS.\n",
            "   d. The washed pellet was resuspended in 10 mL sucrose extraction buffer (10 mM HEPES, pH 7.5, 70 mM sucrose, 200 mM mannitol, 1 mM EGTA, 1% protease inhibitors, and 1% phosphatase inhibitors).\n",
            "5. Mitochondrial isolation from tissue:\n",
            "   a. Skeletal muscle tissue (tibialis anterior) was freshly dissected from C57/BL6J mice and placed in ice-cold sucrose extraction buffer.\n",
            "   b. The tissue was placed in a 1.5 mL centrifuge tube in 1 mL of sucrose extraction buffer and chopped into small (approx 2 mm) chunks.\n",
            "   c. The tissue suspension was transferred into a pre-cooled Dounce homogenizer and homogenized with 10 strokes of a tight pestle.\n",
            "   d. The suspension was strained sequentially through 70 µm and 40 μm cell strainers.\n",
            "   e. The strained solution was made up to 10 mL with sucrose buffer.\n",
            "6. Mitochondrial isolation procedure (for both cells and tissue):\n",
            "   a. The cell or tissue suspension was placed in a nitrogen cavitation chamber with a magnetic stirrer and pressurized to 500 psi for 10 min while stirring on ice.\n",
            "   b. The suspension was released dropwise through the outlet valve into a pre-chilled 15 mL tube.\n",
            "   c. The cell or tissue suspension was centrifuged at 800 g for 10 min to pellet nuclei and cellular debris. The supernatant was transferred to a pre-chilled tube, and the pellet\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Protocol: Mitochondrial Isolation Protocol:\n",
            "\n",
            "1. The supernatant was transferred into a fresh tube and centrifuged at 10,500 g for 10 min.\n",
            "2. The resultant pellet was resuspended in 500 µL of isolation buffer 2 (3 mM EGTA, 215 mM D-mannitol, 600 mM sucrose, 160 mM HEPES, 0.8% w/v BSA, pH 7.4).\n",
            "3. The supernatant was collected as the 'cyto' fraction for immunoblotting.\n",
            "4. The suspension was centrifuged again at 10,500 g for 10 min.\n",
            "5. The resultant pellet was the mitochondrial fraction and resuspended in 100 µL of isolation buffer 2.\n",
            "6. The protein was quantified.\n",
            "\n",
            "Note: The protocol mentions the use of a mitochondrial isolation kit from Thermo Fisher, but the specific steps for using the kit are not provided.\n",
            "\n",
            "Conclusion:\n",
            "The provided steps outline the mitochondrial isolation protocol from the article. However, there are some gaps in the protocol, such as the lack of detailed instructions for using the Thermo Fisher kit and the absence of information on the specific centrifugation speed and time for resuspending the pellet.\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. The isolation of mitochondria through nitrogen cavitation takes approximately 45 min.\n",
            "2. Total protein yield from a 330 mg cell pellet was 830 µg of mitochondrial protein, which is 0.28% of the input protein.\n",
            "3. From tissue, the method was successful in isolating 358 µg of mitochondria from an average tissue weight of 107 mg, representing 0.34% of mitochondrial recovery from total tissue input.\n",
            "4. Isolated mitochondria from both tissue and cells showed no contamination from cytosolic or nuclear proteins, indicating a pure fraction without contamination from other major organelles.\n",
            "5. Western blot analysis of cytochrome C and COX IV showed enrichment in the mitochondrial fraction, indicating intact mitochondria with no cytochrome C leakage into the cytosol.\n",
            "6. The nitrogen cavitation-based method produced lower variability and high reproducibility compared to a mechanical homogenization-based method and a commercially available kit for cells.\n",
            "7. Mitochondria isolated using the mechanical homogenization-based method were contaminated with nuclear (histone H3) and cytosolic (GAPDH) proteins.\n",
            "8. Isolated mitochondria responded to complex I, III, and V inhibitors, indicating preserved mitochondrial respiratory activity.\n",
            "9. The respiratory control ratio of isolated mitochondria from C2C12 myoblast cells was 4.9 ± 0.8, and for skeletal muscle tissue was 4.6 ± 0.2.\n",
            "10. State III respiration was measured at an oxygen consumption rate (OCR) of 273.8 ± 69.20 pmol/min for mitochondria isolated from cells and 480.27 ± 174.87 pmol/min for mitochondria isolated from skeletal muscle tissue.\n",
            "11. State IV respiration was measured at an average OCR of 71.4 pmol/min ± 26.70 for mitochondria isolated from cells and 107.72 ± 39.78 pmol/min for mitochondria isolated from skeletal muscle tissue.\n",
            "12. The mechanical homogenization-based method also measured states of respiration using a Seahorse XFe24 analyzer and responded to complex I, III, and V inhibitors.\n",
            "13. The respiratory control ratio for the mechanical homogenization-based method was lower at 2.89 ± 0.21 for tissue and 1.59 ± 0.7 for cells.\n",
            "14. The mechanical homogenization-based method resulted in a lower total yield of mitochondria, with only 104 µg recovered from an initial tissue weight of 166 mg, representing a recovery of just under 0.1% compared to 0.34% from nitrogen cavitation.\n",
            "\n",
            "**Conclusion:** The article provides detailed information on the isolation of mitochondria through nitrogen cavitation, including protein yield, absence of contamination, and preserved respiratory activity. However, the article does not provide a complete protocol for the isolation method, and some steps are unclear.\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Protocol: Based on the given text, it is difficult to extract the complete mitochondrial isolation protocol. However, some information about the protocol can be identified. Here are the steps that can be extracted:\n",
            "\n",
            "1. The selection of a mitochondrial extraction protocol is done to best suit the demands of the upstream experimental workflow and downstream endpoints.\n",
            "2. The authors provide an additional protocol for the rapid and robust isolation of mitochondria, which would be ideal for studies of a larger scale or those requiring higher throughput respirometry analysis.\n",
            "3. Nitrogen cavitation is successful in isolating intact, contaminant-free, respiring mitochondria.\n",
            "4. Mechanical homogenization of muscle cells and tissue is another method used for isolating mitochondria.\n",
            "5. The rates of respiration in State 3 and State 4 are measured and expressed as pmol O2/min.\n",
            "6. The authors' method has the benefits of consistent cellular lysis followed by brief differential centrifugation to isolate intact, respiring mitochondria.\n",
            "7. The method may have ER contamination, but there are no nuclear, cytoplasmic, or whole-cell contaminants.\n",
            "8. The authors' method may hold significant advantages for mitochondrial proteomic analysis.\n",
            "9. The authors present a valuable alternative to existing mitochondrial isolation protocols.\n",
            "10. The protocol is applicable to tissue and cells of skeletal muscle origin.\n",
            "\n",
            "It is important to note that the given text does not provide a complete and detailed protocol for mitochondrial isolation. The steps mentioned above are all that can be extracted from the text. There may be additional steps or details missing from the protocol.\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Protocol: 1. Read Article: Identified relevant sections for mitochondrial isolation.\n",
            "2. Extract Steps:\n",
            "   - \"Isolation of intact mitochondria from skeletal muscle by differential centrifugation for high-resolution respirometry measurements\" (Djafarzadeh and Jakob, 2017)\n",
            "   - \"Efﬁcient isolation of pure and functional mitochondria from mouse tissues using automated tissue disruption and enrichment with anti-TOM22 magnetic beads\" (Franko et al., 2013)\n",
            "   - \"Mitochondrial isolation from skeletal muscle\" (Garcia-Cazarin et al., 2011)\n",
            "   - \"Isolation of functional pure mitochondria by superparamagnetic microbeads\" (Hornig-Do et al., 2009)\n",
            "   - \"Isolation of mitochondria from cells and tissues\" (Liao et al., 2020)\n",
            "   - \"Isolation of mitochondria from rat brain using Percoll density gradient centrifugation\" (Sims and Anderson, 2008)\n",
            "3. Note Incompleteness: None mentioned.\n",
            "4. Format Steps:\n",
            "   a. Isolation of intact mitochondria from skeletal muscle by differential centrifugation for high-resolution respirometry measurements (Djafarzadeh and Jakob, 2017)\n",
            "   b. Efficient isolation of pure and functional mitochondria from mouse tissues using automated tissue disruption and enrichment with anti-TOM22 magnetic beads (Franko et al., 2013)\n",
            "   c. Mitochondrial isolation from skeletal muscle (Garcia-Cazarin et al., 2011)\n",
            "   d. Isolation of functional pure mitochondria by superparamagnetic microbeads (Hornig-Do et al., 2009)\n",
            "   e. Isolation of mitochondria from cells and tissues (Liao et al., 2020)\n",
            "   f. Isolation of mitochondria from rat brain using Percoll density gradient centrifugation (Sims and Anderson, 2008)\n",
            "5. Indicate Uncertainties: None mentioned.\n",
            "6. Conclusion: The article mentions several methods for isolating mitochondria, including differential centrifugation, automated tissue disruption with magnetic beads, and Percoll density gradient centrifugation. However, the specific steps for each method are not provided in this section of the article. Further details would be required to obtain a complete mitochondrial isolation protocol.\n",
            "Storing data for file_id: Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/epas/miniconda3/envs/autogen/bin/pdfx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/cli.py\", line 158, in main\n",
            "    pdf = pdfx.PDFx(args.pdf)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/__init__.py\", line 128, in __init__\n",
            "    self.reader = PDFMinerBackend(self.stream)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/backends.py\", line 207, in __init__\n",
            "    self.metadata.update(xmp_to_dict(metadata))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 93, in xmp_to_dict\n",
            "    return XmpParser(xmp).meta\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 43, in __init__\n",
            "    self.tree = ET.XML(xmp)\n",
            "                ^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/xml/etree/ElementTree.py\", line 1338, in XML\n",
            "    parser.feed(text)\n",
            "xml.etree.ElementTree.ParseError: unbound prefix: line 5, column 220\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Command 'pdfx -v '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.pdf' -o '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.txt'' returned non-zero exit status 1.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.txt'\n",
            "Protocol: **Article:** Wang, X., Karlsson, J. O., Zhu, C., Bahr, B. A., Hagberg, H., & Blomgren, K. (2011). Caspersen et al. 2008) The procedure previously reported for neonatal mice produced mitochondria with well-preserved respiratory properties. Journal of Neurochemistry, 119(6), 1253-1261.\n",
            "\n",
            "**Steps:**\n",
            "1. \"Using evaluation of respiratory activity, marker enzymes, western blotting and electron microscopy, we have compared a previously published procedure for isolating mitochondria from neonatal mouse brain (method A) with procedures adapted from those for adult rats (method B) and neonatal rats (method C)\"\n",
            "2. \"All three procedures use Percoll density gradient centrifugation as a key step in the isolation but differ in many aspects of the fractionation procedure and the solutions used during fractionation\"\n",
            "3. \"Methods A and B both produced highly enriched fractions of well-coupled mitochondria with high rates of respiratory activity\"\n",
            "4. \"The fraction from method C exhibited less preservation of respiratory properties and was more contaminated with other subcellular components\"\n",
            "5. \"Method A offers the advantage of being more rapid and producing larger mitochondrial yields making it useful for routine applications\"\n",
            "6. \"However, method B produced mitochondria that were less contaminated with synaptosomes and associated cytosolic components that suits studies that have a requirement for higher mitochondrial purification\"\n",
            "\n",
            "**Conclusion:**\n",
            "The article compares three different methods (A, B, and C) for isolating mitochondria from neonatal mouse brain. All three methods use Percoll density gradient centrifugation, but they differ in the fractionation procedure and solutions used. Method A produces highly enriched fractions of well-coupled mitochondria with high respiratory activity, while method B yields mitochondria with less contamination from synaptosomes and cytosolic components. Method C exhibits less preservation of respiratory properties and more contamination. Method A is more rapid and produces larger mitochondrial yields, making it suitable for routine applications. However, the article does not provide a complete protocol for any of the methods, and additional information on enzyme markers and contamination levels is lacking.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: Mitochondrial isolation was performed using a combination of two successive differential centrifugations and an isopycnic centrifugation using a two-phase Percoll gradient. The isolation steps are described as follows:\n",
            "\n",
            "Method A:\n",
            "1. Dissect one single postnatal day 9 (PND9) mouse hemisphere weighing 107 ± 8 mg.\n",
            "2. Homogenize the hemisphere in 1.9 mL ice-cold isolation buffer (IB) containing 225 mM mannitol, 75 mM sucrose, 1 mM EGTA, 5 mM HEPES-KOH (pH 7.2), and 1 mg/mL fatty-acid-free bovine serum albumin (BSA).\n",
            "3. Centrifuge the homogenate at 1100 g for 2 min at 4°C.\n",
            "4. Divide the supernatant into two Eppendorf tubes, each with a final volume of 700 µL.\n",
            "5. Mix each tube with 75 µL of freshly made 80% Percoll.\n",
            "6. Layer the mixture on top of freshly made 10% Percoll in a 2 mL Eppendorf tube.\n",
            "7. Centrifuge at 18,500 g for 10 min at 4°C.\n",
            "8. Remove the cloudy myelin-containing top fraction, leaving the mitochondria-enriched pellet at the bottom of the Eppendorf tube.\n",
            "9. Add 1 µL of sucrose washing buffer to the pellet, mix, and centrifuge at 10,000 g for 5 min at 4°C.\n",
            "10. Retain the final mitochondrial pellet on ice until further analysis.\n",
            "\n",
            "Method B:\n",
            "1. Dissect one single PND9 mouse hemisphere weighing 107 ± 8 mg.\n",
            "2. Homogenize the hemisphere in 0.95 mL 12% Percoll.\n",
            "3. Prepare Percoll gradients (40% and 19%) in 1 mL volumes.\n",
            "4. Carefully add the homogenate to the Percoll gradients.\n",
            "5. Centrifuge at 30,700 g for 7 min at 4°C.\n",
            "6. Remove bands 1 and 2, and collect band 3 (mitochondria-enriched fraction).\n",
            "7. Dilute band 3 with 1 mL IB (320 mM sucrose, 1 mM EDTA, 10 mM Trizma base, pH 7.4).\n",
            "8. Perform a washing step at 16,700 g for 12 min at 4°C.\n",
            "9. Transfer the mitochondrial pellet to an Eppendorf tube containing 400 µL IB and 100 µL BSA (1 mg/mL), and mix carefully.\n",
            "10. Perform a final washing step using IB at 7300 g for 6 min at 4°C.\n",
            "11. Retain the final mitochondrial pellet on ice until further analysis.\n",
            "\n",
            "Method C: (Incomplete description, missing details)\n",
            "\n",
            "In conclusion, the article provides detailed steps for mitochondrial isolation using Method A and Method B. Method C is mentioned but lacks complete description and specific steps. Further information is required to fully understand and replicate Method C.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: Steps for mitochondrial isolation from a PND9 mouse brain:\n",
            "\n",
            "1. Dissect out one whole brain (two hemispheres) weighing 214 ± 16 mg from a PND9 mouse.\n",
            "2. Homogenize the brain in 3.8 mL ice-cold IB buffer with polyvinylpyrrolidone (PVP) (320 mM sucrose, 1 mM EDTA, 10 mM Trizma base, 2 mg/mL PVP, pH 7.4) using a 15 mL Dounce homogenizer with glass pestles.\n",
            "3. Centrifuge the homogenate at 4°C, 760 g for 10 min.\n",
            "4. Save the supernatant (around 3.25 mL) and transfer it to new tubes.\n",
            "5. Centrifuge the supernatant at 10,000 g for 10 min at 4°C using a Hettich Zentrifugen Universal 16 centrifuge.\n",
            "6. Resuspend the pellet (enriched in mitochondria) in 4 mL 12% Percoll in IB.\n",
            "7. Carefully layer the resuspended pellet onto 4 mL 26% Percoll to form a sharp interface between the two layers in a Sarstedt 15 mL centrifuge tube.\n",
            "8. Centrifuge the gradient at 10,000 g for 10 min at 4°C.\n",
            "9. Carefully remove the lower band, which contains most of the mitochondria, and transfer it to a new tube containing 4 mL IB without PVP.\n",
            "10. Centrifuge the tube at 10,000 g for 10 min at 4°C and wash the pellet two times.\n",
            "11. Retain the final mitochondrial pellet on ice prior to further analysis.\n",
            "\n",
            "Note: The protocol does not provide complete information for all steps. Some steps have missing details or are unclear.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"Recovery was calculated by converting the measured respiration rate in the sample to a total activity for the whole isolated mitochondrial fraction (by multiplying the measured flux by the total volume in the fraction/the volume that was assayed) and expressing this as a percentage of the total activity in initial homogenates measured under the same conditions.\"\n",
            "\n",
            "2. \"To measure the activity of mitochondria in the isolated mitochondrial fractions, as well as for detecting cytosolic contaminations, the following mitochondrial/cytosolic marker enzymes were used:\"\n",
            "\n",
            "   a. \"Cytosolic enzyme lactate dehydrogenase (LDH; Roche, Boehringer Mannheim, Germany; Cat No 1644793) was used to measure contamination with synaptosomes or other structures such as gliosomes.\"\n",
            "   \n",
            "   b. \"Citrate Synthase Assay (Citrate Synthase Assay Kit Sigma, Gothenburg, Sweden; Cat No CS0720) was used as an exclusive marker of the mitochondrial matrix. It was also used for testing the intactness of the mitochondrial inner membrane because the citrate synthase activity is not readily measurable in intact mitochondria owing to the impermeability of the inner membrane to the substrates.\"\n",
            "   \n",
            "   c. \"Cytochrome c oxidase activity (Assay kit Cat No KC310100 from BioChain Institute, Inc., Hayward, USA) was measured as a further marker of mitochondrial activity and of the integrity of the mitochondrial outer membrane. The integrity of the outer membrane was assessed by measuring cytochrome c oxidase activity in mitochondrial membrane in the presence and absence of the detergent, n-dodecyl b-D-maltoside. Cytochrome c oxidase locates in the inner membrane of the mitochondria. Cytochrome c cannot access the cytochrome c oxidase when the outer membrane is intact. So the ratio between activity without and with n-dodecyl b-D-maltoside presence is a measure of the integrity of the mitochondrial outer membrane.\"\n",
            "\n",
            "3. \"Electron microscopy (EM) was performed as previously described. In brief, after the isolation, brain mitochondria from PND9 mice (n=3/method) were fixed with a mixture of 2% paraformaldehyde, 2.5% glutaraldehyde, and 0.02% sodium azide in 0.05 M sodium cacodylate, embedded in epoxy resin (Agar 100). The preparations were sectioned and stained with lead citrate and uranyl acetate and examined with a Zeiss 912AB electron microscope equipped with a MegaView III camera (Soft Imaging Systems, Mu ¨nster, Germany) for digital image capture. The proportion of free mitochondria was determined by examining 250 particles and organelles in low magnification EM pictures.\"\n",
            "\n",
            "**Protocol Completeness and Gaps:**\n",
            "\n",
            "The article provides some steps for mitochondrial isolation, including the calculation of recovery, the use of marker enzymes to measure mitochondrial activity and contamination, and the assessment of mitochondrial integrity using cytochrome c oxidase activity. However, the article does not provide a complete protocol for mitochondrial isolation, as it does not specify the methods used for the actual isolation process, such as tissue homogenization, centrifugation, and fractionation.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, the steps for mitochondrial isolation are not explicitly mentioned. The text mainly discusses the results and properties of the isolated mitochondria. Therefore, it is not possible to accurately extract the mitochondrial isolation steps from this article.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: Based on the provided text, the steps for mitochondrial isolation are not explicitly mentioned. The article discusses the performance and outcomes of three different methods (A, B, and C) for isolating mitochondria from the PND9 mouse forebrain. The article mentions the measurement of citrate synthase activity, the degree of mitochondrial integrity, electron microscopic evaluation, and the presence of subcellular contaminations such as synaptosomes, nuclei, and myelin. However, the specific steps for mitochondrial isolation are not described in detail. Therefore, it is not possible to extract the isolation steps accurately from this article.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: 1. The article discusses the importance of having a good mitochondrial isolation method specifically for the immature mouse brain.\n",
            "2. Isolation procedures for brain mitochondria for adult mice/rats have been developed, but few methods have been used for the immature brain, and the relative merits of these techniques are unknown.\n",
            "3. An ideal isolation method for brain mitochondria would generate fractions containing mitochondria with intact morphology, high respiratory activity, preserved integrity of both the mitochondrial inner and outer membrane, and no contamination with other cell fractions such as synaptosomes, myelin, and nuclei.\n",
            "4. The article presents three different methods for isolating neonatal mouse brain mitochondria, referred to as Method A, Method B, and Method C.\n",
            "5. Method A is a relatively simple and fast isolation procedure that offers a slightly higher yield of mitochondria compared to Method B. Both methods are easy to perform and require relatively short times for the whole preparation process.\n",
            "6. Method B requires high-speed centrifugation that might not be available for all laboratories, and the isolation process is more complex than Methods A and C.\n",
            "7. Method C includes an initial slow-speed centrifugation step and a medium-speed centrifugation step to generate a crude mitochondrial fraction prior to further fractionation.\n",
            "8. All three methods make use of discontinuous Percoll gradient centrifugation as the key step for isolating an enriched mitochondrial fraction but differ in the concentration profile of this gradient.\n",
            "9. The procedures also differ in other details, including the amount of starting material that was fractionated and the composition of solutions used during mitochondrial isolation.\n",
            "10. The article mentions that all three methods resulted in enriched mitochondria with preserved morphology compared to previous results in adult and neonatal brain mitochondria.\n",
            "11. Method C suffered from the most pronounced contamination, while Method B had the least contamination.\n",
            "\n",
            "**Conclusion:** The article provides information on three different methods (A, B, and C) for isolating neonatal mouse brain mitochondria. These methods have been shown to generate fractions containing mitochondria with intact morphology and high respiratory activity. However, the relative merits of these techniques are unknown, and there are differences in terms of complexity, availability of equipment, and contamination levels. The article also mentions that the composition of solutions and the concentration profile of the Percoll gradient differ between the methods. Overall, while the article provides some information on the isolation procedures, there are still gaps in understanding the complete protocols for isolating neonatal mouse brain mitochondria.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, it is not possible to extract the specific steps for mitochondrial isolation. The article mentions three different methods (A, B, and C) for isolating mitochondria from neonatal brain tissue, but it does not provide a detailed protocol or step-by-step instructions for each method. The article only provides a comparison of the methods based on the RCR ratio, mitochondrial activity, integrity, and contamination levels.\n",
            "\n",
            "Therefore, the protocol for mitochondrial isolation from neonatal brain tissue cannot be accurately extracted from this article.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Protocol: **Article:** Neonatal mouse brain mitochondrial isolation\n",
            "\n",
            "**Steps:**\n",
            "1. \"Neonatal mouse brain mitochondrial isolation was performed as previously described\" (no further details provided).\n",
            "\n",
            "**Conclusion:**\n",
            "The article mentions the isolation of neonatal mouse brain mitochondria but does not provide any specific steps or protocol details. Thus, the protocol is incomplete and lacks sufficient information to replicate the isolation procedure accurately.\n",
            "Storing data for file_id: Isolation_of_brain_mitochondria_from_neonatal_mice\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.txt\n",
            "Protocol: **Article:** Isolation of Mitochondria From Fresh Mice Lung Tissue\n",
            "\n",
            "**Steps:**\n",
            "1. Direct analysis of isolated mitochondria enables a better understanding of lung dysfunction.\n",
            "2. A protocol for the isolation of mitochondria from lung tissue is described.\n",
            "3. The protocol aims to enable functional analyses of mitochondrial O2 consumption, transmembrane potential, reactive oxygen species (ROS) formation, ATP production, and swelling.\n",
            "4. The protocol is compared to the well-established protocol used for heart mitochondrial function.\n",
            "5. The authors achieved similar results to the heart protocol.\n",
            "6. The main objective of the article is to provide a practical step-by-step user protocol for isolating pulmonary mitochondria.\n",
            "7. The isolation protocol is based on the differentiated centrifugation method of mice lung homogenate.\n",
            "8. Innovative steps are introduced to obtain better and more functional isolated mitochondria.\n",
            "9. The characteristics of reagents and equipment used in the protocol are not fully described in detail in this article.\n",
            "10. The authors describe in detail the mitochondrial function pertaining to several respiratory complexes.\n",
            "\n",
            "**Materials and Equipment:**\n",
            "1. Teﬂon beaker (BRANDR beaker, PTFE, low form, catalog number: Z322660; Merck, Darmstadt, Germany)\n",
            "2. 50 ml Falcon tubes (FALCONR Brand, 50 ml polypropylene conical tube 30 mm × 115 mm style, catalog number: 352070; Coring Science Mexico, Col del, Mexico)\n",
            "3. 14 ml round-bottom tubes (Thermo ScientiﬁcTM, NuncTM 14 ml round-bottom tube, catalog number: 150268; Thermo Fisher ScientiﬁcTM, Waltham, MA, United States)\n",
            "4. 1.5 and 2 ml microfuge tubes (Eppendorf Safe-Lock Tubes, 1.5 and 2 ml Eppendorf QualityTM, catalog numbers: 0030120086 and 0030120094, respectively; Eppendorf, Hamburgo, Germany)\n",
            "5. Syringe ﬁlter (CorningR syringe ﬁlters, nylon membrane, diameter 25 mm, pore size 0.2 mm, catalog number: CLS431224; Merck, Darmstadt, Germany)\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a protocol for the isolation of mitochondria from fresh mice lung tissue. The protocol is based on the differentiated centrifugation method of mice lung homogenate. While the article describes the protocol in detail, there are some missing details regarding the characteristics of reagents and equipment. The protocol aims to enable functional analyses of various mitochondrial parameters, and the authors achieved similar results to the well-established heart mitochondrial function protocol.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4110 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: Based on the provided information, I was unable to find the specific steps for mitochondrial isolation from lung tissue. The information provided includes a list of equipment and reagents used in the isolation process, as well as recipes for various buffers used. However, the actual steps for the isolation process are not mentioned in the text. Therefore, I cannot provide a numbered list of steps for mitochondrial isolation from lung tissue based on the given information.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4399 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: 1. Add 6.25 ml of 1 mol/l KCl, 1 ml of 500 mmol/l MOPS, 0.1 ml of 1 mol/l MgCl2, 0.25 ml of 1 mol/l KH2PO4, 0.1 ml of 100 mmol/l EGTA, and 1 ml of 250/250 mol/l pyruvate/malate. Adjust pH to 7.4 using 500 mmol/l Tris, and bring the solution to 50 ml using ultrapure water and filtrate it. Store at 4°C.\n",
            "2. Prepare incubation buffer for respiration succinate (IBRS) in mmol/l: 125 KCl, 10 MOPS, 5 MgCl2, 5 KH2PO4, 0.02 EGTA, 5 succinate, pH 7.4. Add 6.25 ml of 1 mol/l KCl, 1 ml of 500 mmol/l MOPS, 0.1 ml of 1 mol/l MgCl2, 0.25 ml of 1 mol/l KH2PO4, 0.1 ml of 100 mmol/l EGTA, and 1 ml of 250 mmol/l succinate. Adjust pH to 7.4 using 0.5 mol/l Tris, and bring the solution to 50 ml using ultrapure water, and filtrate it. Store at 4°C.\n",
            "3. Prepare 100 mmol/l ADP: Dissolve 427 mg of ADP in 10 ml of ultrapure water. Prepare 100 ml aliquots and store at -20°C.\n",
            "4. Prepare 500 mmol/l ascorbate: Dissolve 880.65 mg of ascorbic acid in 10 ml of ultrapure water. Prepare 100 ml aliquots and store at -20°C.\n",
            "5. Prepare 10 mmol/l calcium chloride: Dissolve 55.49 mg of CaCl2 in 50 ml of ultrapure water and store at -20°C.\n",
            "6. Prepare 10 mmol/l cyclosporin A: Dissolve 12 mg of cyclosporin A in 1 ml of absolute ethanol and store at -20°C.\n",
            "7. Prepare 0.1 mol/l EGTA stock solution: Dissolve 1.9 g of EGTA in 30 ml of ultrapure water. Adjust pH to 7.4 using 0.5 mol/l Tris and dilute to 50 ml. Store at 4°C.\n",
            "8. Prepare 10 mmol/l FCCP stock solution: Dissolve 2.5 mg of FCCP in 1 ml of absolute ethanol. Store at -20°C. Dilute the stock solution to 5 mM by adding 5 ml of 10 mmol/l FCCP in 10 ml of absolute ethanol. Prepare 20 ml aliquots and store at -20°C.\n",
            "9. Prepare 0.25 mol/l Pyruvate/0.25 mol/l malate stock solution: Dissolve 1.38 g of pyruvate and 1.68 g of malate in 30 ml of ultrapure water and adjust pH to 7.4 with 2 mol/l Tris. Dilute to 50 ml and store at 4°C.\n",
            "10. Prepare 1 mol/l KCl stock solution: Dissolve 18.64 g of KCl in 250 ml of ultrapure water and store at 4°C.\n",
            "11. Prepare 1 mol/l KH2PO4 stock solution: Dissolve 6.8 g of KH2PO4 in 30 ml of ultrapure water. Adjust pH to 7.4 using 0.5 mol/l Tris\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Protocol: Steps for mitochondrial isolation from lung tissue:\n",
            "\n",
            "1. \"The mice were euthanized and underwent a bilateral thoracotomy. The lungs were carefully removed en bloc and immediately placed in a tube containing an ice isolation buffer at 4°C to remove excess blood.\"\n",
            "2. \"Remove the adipose tissue and all large vessels using scissors.\"\n",
            "3. \"Mince the tissue into 1–2 g fragments and transfer each one into a Teflon beaker with 10 ml BSA isolation buffer on ice.\"\n",
            "4. \"Remove all remaining fat. The tissue must be thoroughly minced, since the size of the sample directly affects the subsequent homogenization step and eventually the yield of mitochondria.\"\n",
            "5. \"Split the minced tissue from one Teflon beaker into two 14-ml round-bottom tubes. The tube should not contain more than 2 ml tissue volume. Whenever necessary, use more 14-ml round-bottom tubes.\"\n",
            "6. \"Wash the minced tissue samples: fill the 14-ml round-bottom tubes with 10 ml BSA isolation buffer, let the tissue sink, remove the buffer, and repeat tissue washing until the buffer is clear. Usually, four or five washings are enough to obtain a clean BSA isolation buffer. Hence, fill the 14-ml round-bottom tubes up to 6 ml with isolation buffer. The optimal tissue/buffer ratio is 1:3 or less. After mincing the tissue, part of it precipitates and some pieces float because of the air in the air spaces. Be careful in removing the blood during washing to avoid tissue loss.\"\n",
            "7. \"Homogenize the samples with the tissue homogenizer (Ultra-Turrax) using two 10-s treatments at a shaft rotation rate of 6,500 g each. Perform the homogenization on ice with slight movements of the centrifuge tube. Wait for 10 s between the homogenization steps to avoid heating of the homogenizer and the samples, and to avoid foaming.\"\n",
            "8. \"Collect the samples and transfer them to a tissue glass Potter-Elvehjem homogenizer. Homogenize the samples, and stroke the suspension about 30–40 times. This procedure can compromise mitochondrial integrity if not done carefully. It is recommended to precool the glassware in an ice bath 5–10 min before starting the procedure.\"\n",
            "9. \"Centrifuge the homogenate at 700 g for 10 min at 4°C.\"\n",
            "10. \"Collect the supernatant in 2-ml microfuge tubes and discard the pellets. Centrifuge the supernatant at 12,300 g for 10 min at 4°C.\"\n",
            "11. \"Discard the supernatant and resuspend the pellet in 0.5 ml of ice-cold isolation buffer by gentle pipetting, and collect the mitochondrial suspensions in 2-ml microfuge tubes. Avoid the formation of foam during the resuspension process.\"\n",
            "12. \"Centrifuge the supernatant in ice-cold isolation buffer at 10,300 g for 10 min at 4°C.\"\n",
            "13. \"Pool all the mitochondrial suspensions in one 2-ml microfuge tube and repeat the previous step.\"\n",
            "14. \"Resuspend the resulting pellet in 100–200 ml isolation buffer and store it on ice. Resuspend the pellets carefully by gentle pipetting to obtain a uniform suspension without any visible clump.\"\n",
            "15. \"Measure mitochondrial concentration using the Lowry or BSA method.\"\n",
            "\n",
            "**Note:** The article does not provide information on the specific details of the isolation buffer or the concentrations used. Additionally, the article does not provide information on the specific equipment used for the homogenization steps.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Mitochondrial Isolation Steps:\n",
            "\n",
            "1. Lungs were removed from euthanized mice.\n",
            "2. Lung tissue was minced and washed with BSA isolation buffer to remove blood.\n",
            "3. The sample was homogenized using a high-speed tissue homogenizer and a tissue glass Potter-Elvehjem homogenizer to disrupt cells without compromising mitochondrial integrity.\n",
            "4. The homogenate was centrifuged at 700 g for 10 min at 4°C.\n",
            "5. The pellet was discarded, and the supernatant was collected for further analysis.\n",
            "6. The resultant pellet containing mitochondria was ready for functional analyses.\n",
            "\n",
            "Mitochondrial Oxygen Consumption Steps:\n",
            "\n",
            "1. Use 200 mg of protein per ml for good acquisition data.\n",
            "2. Add 0.5 ml of IBRP/M buffer to two chambers of a respirometer.\n",
            "3. Optionally, add pyruvate/malate or glutamate/malate to measure the state 1 respiration of complex I and add rotenone to measure complex II respiration.\n",
            "4. Ensure constant movement of the magnetic stirrer.\n",
            "5. Equilibrate the temperature and oxygen tension of the buffer and close the chamber.\n",
            "6. Start recording the oxygen concentration in the chamber, waiting for a stable baseline.\n",
            "7. Add 200 mg of mitochondrial protein and record for 3 minutes.\n",
            "8. Optionally, add pyruvate/malate and record for 3 minutes.\n",
            "9. Add 4 ml of 100 mmol/l ADP and record oxygen concentration for 3 minutes.\n",
            "10. The decrease in oxygen concentration should speed up, indicating ADP-stimulated respiration.\n",
            "\n",
            "Conclusion:\n",
            "The article provides a detailed protocol for mitochondrial isolation from lung tissue. The steps for isolation are complete and well-described. However, the protocol for mitochondrial oxygen consumption measurement has some gaps. The article mentions the addition of substrates and inhibitors but does not provide specific details on the concentrations or types of substrates and inhibitors to be used. Additionally, the article does not provide instructions for the measurement of state 3 respiration or the addition of ascorbate, TMPD, and FCCP. These gaps make it difficult to fully replicate the oxygen consumption measurements.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for Mitochondrial Isolation:\n",
            "\n",
            "1. \"Add simultaneously 2 ml of 150 mmol/l TMPD and 6 ml of 500 mmol/l ascorbate to obtain final concentrations of 300 and 3 mmol/l, respectively.\"\n",
            "2. \"Record oxygen concentration for 1 min. The oxygen concentration will decrease faster than with ADP stimulation.\"\n",
            "3. \"Add 6 ml of 5 mmol/l FCCP to obtain a final concentration of 30 nmol/l.\"\n",
            "4. \"Record oxygen concentration for 1 min. The oxygen concentration will decrease further.\"\n",
            "5. \"Stop recording.\"\n",
            "6. \"Calculate mitochondrial oxygen consumption using the software Analysis (Strathkelvin 782 2-channel Oxygen System version 1.0; Strathkelvin, North Lanarkshire, Scotland) or similar.\"\n",
            "7. \"Calculate baseline oxygen consumption 75 s after the addition of mitochondria.\"\n",
            "8. \"Calculate state 2 complex I oxygen consumption 75 s after the addition of pyruvate/malate.\"\n",
            "9. \"Determine ADP-stimulated respiration 30 s after the addition of ADP.\"\n",
            "10. \"Determine complex VI respiration 30 s after the addition of ascorbate/TMPD.\"\n",
            "11. \"Calculate maximal uncoupled respiration rate 30 s after the addition of FCCP.\"\n",
            "\n",
            "Conclusion: The steps provided outline the process of mitochondrial isolation from lung tissue. However, there are some protocol gaps and missing details. The article does not provide information on the initial steps of tissue homogenization or centrifugation to obtain the mitochondria. Additionally, it does not specify the concentration or volume of mitochondria used in the subsequent steps. Further information is also needed regarding the incubation conditions and duration for each step.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4174 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: Based on the provided text, it is difficult to extract the complete mitochondrial isolation protocol. The text primarily discusses the differences between the present protocol and previous protocols, as well as the differences in mitochondrial oxygen consumption between lung and heart mitochondria. However, specific steps for mitochondrial isolation are not clearly mentioned. Therefore, it is not possible to accurately extract the mitochondrial isolation steps from this text.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Protocol: Based on the provided text, it is difficult to extract the complete mitochondrial isolation protocol. However, some information can be gathered:\n",
            "\n",
            "**Step 1:** \"We used herein a new protocol with specific and detailed steps aiming to improve mitochondrial isolation from lung tissue.\"\n",
            "\n",
            "**Step 2:** \"It was abridgedly published (Caldeira et al., 2021) but not tested against a well-documented and broadly used one (Gedik et al., 2017).\"\n",
            "\n",
            "**Step 3:** \"This new protocol improves the acquisition of a robust and preserved sample of isolated mitochondria, allowing a range of analyses with the same sample, increasing mitochondria viability and experimental reproducibility.\"\n",
            "\n",
            "**Step 4:** \"Here, we describe step-by-step the instructions for lung mitochondria isolation and warn for critical steps (steps of the procedure in which the researcher must be extremely careful, or attentive, with the procedure for the perfect execution of the isolation).\"\n",
            "\n",
            "**Step 5:** \"Before our improved method, there was no consensus concerning protocols for mitochondrial isolation from the lung tissue (Zhang et al., 2018).\"\n",
            "\n",
            "From this information, it can be concluded that the article discusses a new protocol for isolating mitochondria from lung tissue. The protocol is described as specific, detailed, and aimed at improving the acquisition of robust and preserved samples. However, the exact steps of the protocol are not provided in the text. The article mentions that the protocol has not been tested against a well-documented and broadly used one, and that there was no consensus on protocols for mitochondrial isolation from lung tissue before the new method was introduced.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for isolating mitochondria from lung tissue:\n",
            "\n",
            "1. Prepare an isolation buffer containing a high amount of fat-free BSA to bind to free fatty acids.\n",
            "2. Mince the lung tissue carefully, taking into account the difficulties caused by its air-filled nature and the need to remove residual blood.\n",
            "3. Remove the blood, including hemoglobin, from the tissue to avoid oxygen sequestration during O2-consumption assays.\n",
            "4. Compare mitochondrial functional characteristics using three experimental groups: control group (heart mitochondria), lung mitochondria with the same protein loading as heart mitochondria, and lung mitochondria with a higher protein loading.\n",
            "5. Measure mitochondrial respiration using a Clark-type electrode at 37°C during magnetic stirring.\n",
            "6. Note that the concentration of lung-isolated mitochondria can affect the results.\n",
            "7. Use mitochondrial complex IV activation as a loading control.\n",
            "8. Observe that lung mitochondria loaded with 50 mg of protein show less oxygen consumption in complex IV compared to heart mitochondria.\n",
            "9. Consider the possibility that lung tissue yields fewer mitochondria and may require greater loading to generate comparable data to heart tissue mitochondria.\n",
            "10. Note that FCCP-induced uncoupled respiration does not differ between all groups, indicating similar viability and behavior of the mitochondria.\n",
            "11. Use 200 mg of protein per experiment as an acceptable amount for isolated mitochondria.\n",
            "12. Perform other techniques to analyze mitochondrial function, such as ATP production and ROS formation.\n",
            "13. Note that mitochondrial swelling, mitochondrial transmembrane potential, electron leakage, and ROS/ATP ratio do not show significant differences among the three groups.\n",
            "14. Acknowledge the limitations of the method, as it does not analyze all 40 subtypes of cells found in the lung.\n",
            "\n",
            "**Conclusion:** The article provides a detailed protocol for isolating mitochondria from lung tissue. However, some protocol steps are missing, such as the specific composition of the isolation buffer and the exact procedure for mincing the tissue and removing residual blood. Additionally, there is no mention of the centrifugation or homogenization steps typically involved in mitochondrial isolation protocols. Therefore, while the article provides valuable insights into the isolation of lung mitochondria, there are significant protocol gaps that need to be addressed.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, it is difficult to extract the specific steps for mitochondrial isolation from lung tissue. The article mentions the development of an optimized technique for mitochondrial isolation from lung tissue, but it does not provide a detailed description of the protocol or the specific steps involved. The article refers to a method briefly described by Caldeira et al. (2021), but this description is not provided in the current text.\n",
            "\n",
            "Therefore, it is not possible to accurately extract the steps for mitochondrial isolation from lung tissue based on the information provided. The article does not provide a complete protocol or specific details about the isolation procedure.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Protocol: **Article:** Picard, M., Taivassalo, T., Gouspillou, G., and Hepple, R T (2011) Mitochondria: isolation, structure and function J Physiol 589, 4413–4421 doi: 10.1113/jphysiol.2011.212712\n",
            "\n",
            "**1. Read Article:**\n",
            "I have read the article and identified the relevant sections for mitochondrial isolation.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "- \"Mitochondria were isolated from rat skeletal muscle by differential centrifugation according to standard protocols.\" (p. 4413)\n",
            "- \"The tissue was homogenized in a glass homogenizer in ice-cold isolation buffer (250 mM sucrose, 10 mM HEPES, 1 mM EGTA, pH 7.4).\" (p. 4413)\n",
            "- \"The homogenate was centrifuged at 600 × g for 10 min at 4°C to remove nuclei and unbroken cells.\" (p. 4413)\n",
            "- \"The supernatant was then centrifuged at 7000 × g for 10 min at 4°C to pellet the mitochondria.\" (p. 4413)\n",
            "- \"The mitochondrial pellet was resuspended in isolation buffer and washed by centrifugation at 7000 × g for 10 min at 4°C.\" (p. 4413)\n",
            "- \"The final mitochondrial pellet was resuspended in a small volume of isolation buffer for further experiments.\" (p. 4413)\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "There are no missing protocol parts or unclear steps in the extracted information.\n",
            "\n",
            "**4. Format Steps:**\n",
            "1. Homogenize the tissue in ice-cold isolation buffer (250 mM sucrose, 10 mM HEPES, 1 mM EGTA, pH 7.4).\n",
            "2. Centrifuge the homogenate at 600 × g for 10 min at 4°C to remove nuclei and unbroken cells.\n",
            "3. Centrifuge the supernatant at 7000 × g for 10 min at 4°C to pellet the mitochondria.\n",
            "4. Resuspend the mitochondrial pellet in isolation buffer and wash by centrifugation at 7000 × g for 10 min at 4°C.\n",
            "5. Resuspend the final mitochondrial pellet in a small volume of isolation buffer for further experiments.\n",
            "\n",
            "**5. Indicate Uncertainties:**\n",
            "There are no ambiguous or partially described steps in the extracted information.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol for mitochondrial isolation from rat skeletal muscle is described in the article with all necessary steps provided. There are no missing protocol parts or unclear steps.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.txt\n",
            "Protocol: I'm sorry, but I'm unable to access the full-text version of the article you requested. As a language model AI, I can only provide assistance with the information that is available to me. Is there anything else I can help you with?\n",
            "Storing data for file_id: Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.txt\n",
            "Protocol: **Article:** Organelle isolation: functional mitochondria from mouse liver, muscle and cultured fibroblasts\n",
            "\n",
            "**Steps:**\n",
            "1. Mitochondria participate in key metabolic reactions of the cell and regulate crucial signaling pathways including apoptosis.\n",
            "2. Several approaches are available to study mitochondrial function in situ, but investigating functional mitochondria that have been isolated from different tissues and from cultured cells offers unmatched advantages.\n",
            "3. This protocol provides a step-by-step procedure to obtain functional mitochondria with high yield from cells grown in culture, liver, and muscle.\n",
            "4. The isolation procedures described here require 1-2 hours, depending on the source of the organelles.\n",
            "5. The polarographic analysis can be completed in 1 hour.\n",
            "\n",
            "**Introduction:**\n",
            "- Mitochondria are central organelles controlling the life and death of the cell. They participate in key metabolic reactions, synthesize most of the ATP, and regulate signaling cascades, including apoptosis.\n",
            "- George Palade and coworkers developed a protocol in the late 1940s to isolate mitochondria based on differential centrifugation. This allowed the isolation of pure organelles with high yields.\n",
            "- The discovery of mitochondrial DNA, mitochondrial precursor proteins, mitochondrial ultrastructure, and inner mitochondrial membrane channels followed the availability of isolated mitochondria.\n",
            "- In the 1990s, mitochondria regained attention due to their role in apoptosis and their involvement in various pathophysiological processes.\n",
            "- Isolation protocols for mitochondria from different sources may vary, and specific protocols for brain, brown adipose tissue, and heart should be referred to.\n",
            "- The protocols described in this article are for liver, skeletal muscle, and cultured cells, but modifications can be made by individual researchers.\n",
            "- The protocol for fibroblasts can be adopted without modification for isolating mitochondria from other cell lines such as HeLa and the prostate cancer cell line LnCaP.\n",
            "- Protocols for isolating mitochondria from organs other than muscle and liver differ from the ones described in this article.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides an introduction to the importance of mitochondria and the history of mitochondrial isolation. It mentions that the protocol described in the article is specific for liver, skeletal muscle, and cultured cells, but other protocols should be referred to for isolating mitochondria from other organs. The article does not provide the actual step-by-step procedures for isolating mitochondria from the mentioned sources, so the protocol gaps need to be addressed by referring to published protocols specific to those sources.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: **Mitochondrial Isolation Protocol:**\n",
            "\n",
            "1. Obtain the following materials and reagents: \n",
            "   - Cell line of interest or liver or muscle isolated from mice.\n",
            "   - Mice of the desired genetic background.\n",
            "   - Dulbecco’s phosphate-buffered saline without Ca2+ and Mg2+ (PBS).\n",
            "   - Sucrose.\n",
            "   - Potassium phosphate monobasic (Pi).\n",
            "   - Tris.\n",
            "   - 4-Morpholinepropanesulfonic acid (MOPS).\n",
            "   - Disodium ethylenediaminetetraacetate dihydrate (EDTA).\n",
            "   - Ethylene-bis(oxyethylenenitrilo)tetraacetic acid (EGTA).\n",
            "   - Potassium chloride (KCl).\n",
            "   - Magnesium chloride hexahydrate (MgCl2).\n",
            "   - Bovine serum albumin (BSA).\n",
            "   - Dulbecco’s modified Eagle’s medium.\n",
            "   - L-glutamine.\n",
            "   - Fetal bovine serum.\n",
            "   - Penicillin/streptomycin.\n",
            "   - Minimal essential medium nonessential amino-acid solution.\n",
            "   - Trypsin-EDTA solution.\n",
            "   - Adenosine 5'-diphosphate sodium salt (ADP).\n",
            "   - Carbonyl cyanide 4-(trifluoromethoxy)phenylhydrazone (FCCP).\n",
            "   - Glutamic acid.\n",
            "   - Malic acid.\n",
            "   - Succinic acid.\n",
            "   - Rotenone.\n",
            "   - L-Ascorbic acid.\n",
            "   - N,N,N,N-Tetramethyl-p-phenylenediamine dihydrochloride (TMPD).\n",
            "   - Antimycin A.\n",
            "   - 500 cm2 dishes for cell culture.\n",
            "   - 18-cm cell scrapers.\n",
            "   - Motor-driven tightly fitting glass/Teflon Potter Elvehjem homogenizer.\n",
            "   - Clark-type oxygen electrode.\n",
            "   - 50 ml polypropylene Falcon tubes.\n",
            "   - 14 ml polypropylene Falcon tubes.\n",
            "   - 1.5 ml microfuge test tube.\n",
            "   - 30 ml round-bottomed glass centrifuge tube.\n",
            "   - Rubber adapter sleeve for centrifuge tube.\n",
            "   - Refrigerated centrifuge for 50 ml Falcon tubes and glass centrifuge tube.\n",
            "   - Hamilton syringe: 10 ml and 50 ml.\n",
            "\n",
            "2. Set up the following reagents:\n",
            "   - Cell culture medium: Use the medium recommended for your cell line.\n",
            "   - 1 M sucrose: Dissolve 342.3 g of sucrose in 1 liter of distilled water and prepare 20 ml aliquots.\n",
            "   - 0.1 M Tris/MOPS: Dissolve 12.1 g of Tris in 500 ml of distilled water, adjust pH to 7.4 using MOPS powder, bring the solution to 1 liter, and store at 4°C.\n",
            "   - 1 M Tris/HCl: Dissolve 121.14 g of Tris in 500 ml of distilled water, adjust pH to 7.4 using HCl, bring the solution to 1 liter, and store at room temperature.\n",
            "   - 0.1 M EGTA/Tris: Dissolve 38.1 g of EGTA in 500 ml of distilled water, adjust pH to 7.4 using Tris powder, bring the solution to 1 liter, and store at 4°C.\n",
            "   - 0.5 M MgCl2: Dissolve 101.7 g of MgCl2 in 1 liter of distilled water and store at 4°C.\n",
            "   - 1 M KCl: Dissolve 74.6 g of KCl in 1 liter of distilled water and store at 4°C.\n",
            "\n",
            "Note: The article does not provide a complete protocol for mitochondrial isolation. It mentions the\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Dissolve 372.2 g of EDTA in 500 ml of distilled water, adjust pH to 7.4 using Tris powder, bring the solution to 1 liter and store at 4°C.\n",
            "2. Dissolve 10 g of BSA in 100 ml of distilled water and store at 20°C.\n",
            "3. Dissolve 136.1 g of KH2PO4 in 500 ml of distilled water, adjust pH to 7.4 using Tris powder, bring the solution to 1 liter and store at 4°C.\n",
            "4. Dissolve 4.7 mg of ADP in 1 ml of distilled water, adjust pH to 7.4, prepare 100 ml aliquots and store in the dark at -20°C for up to 6 months.\n",
            "5. Dissolve 5.1 mg of FCCP in 1 ml of absolute ethanol. Store at -20°C. Dilute the stock solution to 100 mM by adding 10 ml of 20 mM FCCP in 2 ml of absolute ethanol, just prior to use.\n",
            "6. Use a 5 ml Glass/Teﬂon Potter Elvehjem homogenizer for isolation of mitochondria from cells. Use a 30 ml homogenizer for isolation from tissues.\n",
            "7. Dissolve 9.2 g of glutamic acid and 4.2 g of malic acid in 100 ml of distilled water. Adjust pH to 7.4 with Tris base. Add water to bring the volume to 250 ml. Prepare 10 ml aliquots and store at -20°C for up to 6 months.\n",
            "8. Dissolve 3.0 g of succinic acid in 30 ml of distilled water. Adjust pH with Tris base. Add water to make up the volume to 50 ml. Prepare 10 ml aliquots and store at -20°C for up to 6 months.\n",
            "9. Dissolve 4.7 mg of rotenone in 6 ml of absolute ethanol. Mix well for complete dissolution. (Note: Protect the stock solution from direct light using aluminum foil. Rotenone is highly toxic: avoid skin contact and inhalation.)\n",
            "10. Dissolve 5.2 g of ascorbic acid in 50 ml of distilled water, adjust pH to 7.4 and store at -20°C for up to 6 months.\n",
            "11. Dissolve 0.36 g of TMPD in 50 ml of distilled water, adjust pH to 7.4, and store at -20°C for up to 6 months.\n",
            "12. Dissolve 50 mg of antimycin A in 2 ml of absolute ethanol. Dilute the stock solution to 25 mg/ml by adding 2 ml of 25 mg/ml Antimycin A in 2 ml of absolute ethanol, just prior to use. (Note: Antimycin A is highly toxic: avoid skin contact and inhalation.)\n",
            "13. Prepare 100 ml of IB buffer by adding 10 ml of 0.1 M Tris-MOPS and 1 ml of EGTA/Tris to 20 ml of 1 M sucrose. Adjust pH to 7.4.\n",
            "14. Prepare 100 ml of IB m1 buffer by mixing 6.7 ml of 1 M sucrose, 5 ml of 1 M Tris/HCl, 5 ml of 1 M KCl, 1 ml of 1 M EDTA, and 2 ml of 10% BSA. Adjust pH to 7.4.\n",
            "15. Prepare 100 ml of IB m2 buffer by mixing 25 ml of 1 M sucrose, 3 ml of 0.1 M EGTA/Tris, and 1 ml of 1 M Tris/HCl. Adjust pH to 7.4\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Wash all glassware three times with bidistilled water to avoid Ca2+ contamination.\n",
            "2. Prepare all the buffers the same day of the experiment to avoid bacterial/yeast growth in stored buffers.\n",
            "3. Measure the pH of all solutions at 25°C since pH depends on temperature.\n",
            "4. Mitochondria can be isolated from a variety of cells or tissues. Option A describes isolation of mitochondria from mouse embryonic fibroblasts (MEFs), option B describes isolation of mitochondria from mouse liver, and option C describes isolation of mitochondria from mouse skeletal muscle.\n",
            "5. Isolation of mitochondria from MEFs:\n",
            "   - Remove the medium from the cells and wash the cells once with PBS.\n",
            "   - Remove PBS and detach the cells using a cell scraper.\n",
            "   - Transfer the cell suspension to a 50 ml polypropylene Falcon tube.\n",
            "   - Wash the plate once with PBS and scrape the dish to detach the remaining cells.\n",
            "   - Transfer the cells to the same polypropylene Falcon tube.\n",
            "   - Centrifuge cells at 600 g at 4°C for 10 min.\n",
            "   - Discard the supernatant and resuspend cells in 3 ml of ice-cold IB.\n",
            "6. Homogenize the cells using a Teflon pestle operated at 1,600 r.p.m.\n",
            "7. Centrifuge the homogenate at 600 g for 10 min at 4°C.\n",
            "8. Collect the supernatant, transfer it to a glass centrifuge tube, and centrifuge it at 7,000 g for 10 min at 4°C.\n",
            "9. Discard the supernatant and wash the pellet with 200 ml of ice-cold IB.\n",
            "10. Resuspend the pellet in 200 ml of ice-cold IB and transfer the suspension to a 1.5 ml microfuge tube.\n",
            "11. Centrifuge the homogenate at 7,000 g for 10 min at 4°C.\n",
            "12. Discard the supernatant and resuspend the pellet containing mitochondria. Use a glass rod to loosen the pellet paste.\n",
            "13. Transfer the mitochondrial suspension to a microfuge and store it on ice.\n",
            "14. Measure mitochondria concentration using the Biuret method.\n",
            "15. Mitochondria are now ready to be used in experiments within 1-3 hours for better functional responses.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol for isolating mitochondria from mouse embryonic fibroblasts (MEFs) is described in detail, with all the necessary steps mentioned. However, there are some protocol gaps in terms of specific volumes and concentrations of buffers. Additionally, the protocol does not provide details for the homogenization step and the subsequent centrifugation steps. Further clarification is needed for these steps. The protocol for isolating mitochondria from mouse liver and skeletal muscle is not provided in detail, with only the homogenization step mentioned. Therefore, additional information is required to complete the protocol for these options.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: Steps for Isolation of Mitochondria from Mouse Liver:\n",
            "\n",
            "1. Centrifuge at 600 g, 10 min, 4 °C.\n",
            "2. Centrifuge at 7,000 g, 10 min, 4 °C.\n",
            "3. Concentration measurement by Biuret.\n",
            "4. Collect supernatant.\n",
            "5. Discard supernatant and wash pellet.\n",
            "6. Discard supernatant and resuspend mitochondria in pellet.\n",
            "7. Homogenization with glass-Teflon potter at 1,600 r.p.m.\n",
            "8. Resuspend pellet in IBm1.\n",
            "9. Collect supernatant.\n",
            "10. Discard supernatant and wash pellet.\n",
            "11. Discard supernatant and resuspend mitochondria in pellet.\n",
            "12. Concentration measurement by Biuret.\n",
            "\n",
            "Steps for Isolation of Mitochondria from Mouse Skeletal Muscle:\n",
            "\n",
            "1. Kill the mouse by cervical dislocation.\n",
            "2. Mince muscle and trim fat and collagen.\n",
            "3. Incubation with PBD + trypsin for 30 min.\n",
            "4. Centrifuge at 200 g, 10 min, 4 °C.\n",
            "5. Centrifuge at 700 g, 10 min, 4 °C.\n",
            "6. Centrifuge at 8,000 g, 10 min, 4 °C for mitochondria pelletting.\n",
            "7. Centrifuge at 8,000 g, 10 min, 4 °C.\n",
            "8. Concentration measurement by Biuret.\n",
            "9. Homogenization with glass-Teflon potter at 1,600 r.p.m.\n",
            "10. Resuspend pellet in IBm1.\n",
            "11. Collect supernatant.\n",
            "12. Discard supernatant and wash pellet.\n",
            "13. Discard supernatant and resuspend mitochondria in pellet.\n",
            "14. Concentration measurement by Biuret.\n",
            "\n",
            "**Incompleteness:** The article does not provide a complete protocol for mitochondrial isolation from either mouse liver or skeletal muscle. Some steps are missing or not clearly described. For example, the article does not specify the composition or preparation of the isolation buffer (IB), the duration of the incubation with PBD + trypsin, or the specific steps for resuspending the pellet. Additionally, the article does not provide information on the volume or concentration of the final mitochondrial suspension.\n",
            "\n",
            "**Conclusion:** The article provides some steps for the isolation of mitochondria from mouse liver and skeletal muscle, but it is incomplete and lacks important details. Further information is needed to fully replicate the protocol.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: 1. Using a scalpel, rapidly remove the skeletal muscles of interest and immerse them in a small beaker containing 5 ml of ice-cold PBS supplemented with 10 mM EDTA.\n",
            "2. Mince the muscles into small pieces using scissors and trim visible fat, ligaments, and connective tissue.\n",
            "3. Wash the minced muscles twice or thrice with ice-cold PBS supplemented with 10 mM EDTA.\n",
            "4. Resuspend the minced muscles in 5 ml of ice-cold PBS supplemented with 10 mM EDTA and 0.05% trypsin for 30 min.\n",
            "5. Centrifuge at 200 g for 5 min and discard the supernatant.\n",
            "6. Resuspend the pellet in isolation buffer (IB).\n",
            "7. Homogenize the muscles using a Teflon pestle operated at 1,600 r.p.m.; stroke the minced muscle ten times.\n",
            "8. Transfer the homogenate to a 50 ml polypropylene Falcon tube and centrifuge at 700 g for 10 min at 4°C.\n",
            "9. Transfer the supernatant to glass centrifuge tubes and centrifuge at 8,000 g for 10 min at 4°C.\n",
            "10. Discard the supernatant and resuspend the pellet in 5 ml of ice-cold IB.\n",
            "11. Centrifuge at 8,000 g for 10 min at 4°C.\n",
            "12. Discard the supernatant and resuspend the pellet containing mitochondria. Use a glass rod to loosen the pellet paste.\n",
            "13. Transfer mitochondrial suspension into a 14 ml Falcon tube and keep it on ice.\n",
            "14. Measure mitochondrial concentration using the Biuret method.\n",
            "15. Calibrate the Clarke-type oxygen electrode.\n",
            "16. Equilibrate temperature and oxygen tension of EBc or EBm by placing open beakers containing the buffers in the water bath connected to the oxygraph.\n",
            "17. Add an appropriate volume of EB to the oxygraph chamber.\n",
            "18. Start the recording of the oxygen consumption.\n",
            "19. Wait for 2 min to obtain a stable baseline.\n",
            "20. Using an appropriate Hamilton microsyringe, add mitochondria to obtain a final concentration of 1 mg ml^-1.\n",
            "21. Record oxygen consumption till it stops.\n",
            "\n",
            "**Incompleteness:**\n",
            "- The article does not provide the complete protocol for mitochondrial isolation. It does not mention the composition or preparation of the isolation buffer (IB) used in steps 6, 10, and 12.\n",
            "- The article lacks information on the specific substrates and inhibitors of the respiratory chain mentioned in Table 1.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol for mitochondrial isolation provided in the article is incomplete. It includes steps for tissue preparation, homogenization, centrifugation, and resuspension of the mitochondrial pellet. However, crucial details such as the composition of the isolation buffer and the substrates and inhibitors of the respiratory chain are missing. Further information is required to fully replicate the protocol.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: Steps for mitochondrial isolation:\n",
            "\n",
            "1. \"Using Hamilton microsyringes, add the appropriate concentrations of respiratory substrates and inhibitors for the complexes of the respiratory chain you wish to study.\"\n",
            "2. \"The mitochondrial suspension will now start consuming oxygen as a consequence of the basal activity of the respiratory chain in counteracting the inner mitochondrial membrane proton leak. This represents the so-called 'state 2' respiration.\"\n",
            "3. \"Record for 5 min.\"\n",
            "4. \"Add ADP to obtain a final concentration of 100-150 mM.\"\n",
            "5. \"Record the faster consumption of oxygen caused by proton back-diffusion through the stalk portion of the ATPase, which has been compensated by faster electron flow through the respiratory chain to the terminal electron acceptor, O2. This is classically referred to as 'state 3' respiration.\"\n",
            "6. \"The rate of oxygen consumption should now be faster than the rate observed with substrates alone, indicating that we have obtained well-coupled mitochondria.\"\n",
            "7. \"Wait until the respiration slows down and returns to a rate comparable to that before the addition of ADP. This is caused by the consumption of the added ADP. The respiration, which follows ADP exhaustion, is classically referred to as 'state 4' respiration.\"\n",
            "8. \"Wait for 3 min.\"\n",
            "9. \"Add the uncoupler FCCP to obtain a final concentration of 60-100 nM.\"\n",
            "10. \"The respiration will speed up and reach values slightly higher than those observed during the recording of state-3 respiration.\"\n",
            "11. \"Record for a further 5 min and then stop recording.\"\n",
            "\n",
            "Incompleteness: The article does not provide specific details on the concentrations of substrates and inhibitors for each complex of the respiratory chain.\n",
            "\n",
            "Conclusion: The article provides a general outline of the steps for mitochondrial isolation, including the addition of substrates and inhibitors, recording oxygen consumption at different states of respiration, and the use of an uncoupler. However, specific concentrations of substrates and inhibitors for each complex are not mentioned, which would be necessary for a complete protocol.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"The loose part of the pellet probably contains a high proportion of damaged (uncoupled) mitochondria and can be lost without affecting the overall quality of the mitochondrial preparation.\" (Step 1Ax)\n",
            "2. \"The white foamy material near the top of the tube consists of lipids. Mixing of lipids with the mitochondria suspension will cause some degree of uncoupling. Therefore, avoid contact with mitochondria: remove the foamy material by wiping the inside of the tube with a Kimwipe.\" (Step 1Bvii)\n",
            "3. \"Verify if your buffers are contaminated by bacterial or yeast by repeating the recording with bidistilled water in the oxygraph chamber.\" (Step 5)\n",
            "4. \"Re-calibrate the instrument.\" (Step 5)\n",
            "5. \"Check response of the oxygraph by transiently stopping stirring: due to the immediate drop in the local oxygen concentration, the recording should immediately fall and return to the original baseline only when the stirrer is restarted. If this maneuver does not give the expected results, inspect and if necessary substitute the membrane of the electrode.\" (Step 5)\n",
            "6. \"Try to double mitochondrial concentration in the chamber.\" (Step 9)\n",
            "7. \"Substitute 0.2 M sucrose with 0.3 M mannitol in the isolation buffer.\" (Step 9)\n",
            "8. \"In steps 1Axi, 1Bix, or 1Cx, wash the mitochondrial pellet with twice the amount of isolation buffer.\" (Step 11)\n",
            "9. \"Add Pi and check the respiration.\" (Step 11)\n",
            "10. \"Add exogenous cytochrome c and check the respiration; if respiration starts, the outer membrane is leaky.\" (Step 11)\n",
            "11. \"Follow carefully all the indicated critical steps to avoid the indicated contaminations; try washing glassware with isolation buffer, supplemented with EGTA.\" (Step 11)\n",
            "12. \"Include 0.1% fatty acid-free albumin in the EB; if this procedure works, increase FCCP concentration since albumin binds reversibly to FCCP.\" (Step 11)\n",
            "13. \"Since at high doses FCCP is also an inhibitor of the respiratory chain, you can overcome this problem by titrating down the concentration of FCCP used.\" (Step 15)\n",
            "\n",
            "**Incompleteness or Uncertainties:**\n",
            "- The article does not provide a complete protocol for mitochondrial isolation. It only mentions troubleshooting steps for various issues that may arise during the isolation process.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides troubleshooting steps for various issues that may occur during mitochondrial isolation but does not provide a complete protocol. It is important to consult a comprehensive protocol for mitochondrial isolation to ensure accurate and complete steps.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Protocol: **Article:** Wang, X. et al. \"Induction of apoptotic program in cell-free extracts: requirement for dATP and cytochrome c.\" Cell 86, 147–157 (1996).\n",
            "\n",
            "**1. Mitochondrial Isolation Steps:**\n",
            "- \"Mitochondria were isolated from rat liver as described previously.\"\n",
            "- \"Briefly, livers were homogenized in a buffer containing 250 mM sucrose, 10 mM HEPES (pH 7.4), 10 mM KCl, 1.5 mM MgCl2, 1 mM EDTA, 1 mM EGTA, and 0.5 mM DTT.\"\n",
            "- \"The homogenate was centrifuged at 600 × g for 10 min to pellet nuclei and unbroken cells.\"\n",
            "- \"The supernatant was then centrifuged at 7,000 × g for 10 min to pellet mitochondria.\"\n",
            "- \"The mitochondrial pellet was washed once with the same buffer and resuspended in a buffer containing 250 mM sucrose, 10 mM HEPES (pH 7.4), 10 mM KCl, 1.5 mM MgCl2, 1 mM EDTA, 1 mM EGTA, and 0.5 mM DTT.\"\n",
            "\n",
            "**2. Incompleteness:**\n",
            "- The article does not provide details about the specific centrifugation speeds or times for the initial homogenate and the mitochondrial pellet.\n",
            "- The article does not mention any steps for disruption of liver tissue before homogenization.\n",
            "\n",
            "**3. Conclusion:**\n",
            "The article provides a clear set of steps for the isolation of mitochondria from rat liver, including the composition of the isolation buffer and the centrifugation steps. However, it lacks information about the specific centrifugation speeds and times for some steps and does not mention tissue disruption before homogenization.\n",
            "Storing data for file_id: Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/epas/miniconda3/envs/autogen/bin/pdfx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/cli.py\", line 158, in main\n",
            "    pdf = pdfx.PDFx(args.pdf)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/__init__.py\", line 128, in __init__\n",
            "    self.reader = PDFMinerBackend(self.stream)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/backends.py\", line 204, in __init__\n",
            "    metadata = resolve1(doc.catalog[\"Metadata\"]).get_data()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'get_data'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Command 'pdfx -v '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.pdf' -o '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.txt'' returned non-zero exit status 1.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.txt'\n",
            "Protocol: **Steps:**\n",
            "\n",
            "1. Combined Percoll gradient centrifugation and anti-TOM22 magnetic bead extraction is used to isolate mitochondria.\n",
            "2. The mitochondria obtained are neuronal, obtained from synaptosome disruption.\n",
            "3. The mitochondria obtained have minimal cytoplasmic contaminants, including plasma membrane, peroxisomes, lysosomes, synaptosomes, and endoplasmic reticulum.\n",
            "4. The mitochondria are functionally active based on measurements of respiration and protein import.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The article describes a method for isolating functionally active and highly purified neuronal mitochondria from human cortex. The method involves using Percoll gradient centrifugation and anti-TOM22 magnetic bead extraction. The obtained mitochondria are neuronal, have minimal cytoplasmic contaminants, and are functionally active. However, the article does not provide a detailed step-by-step protocol for the isolation procedure.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Protocol: 1. The most robust protocols to isolate synaptosomal mitochondria include a Percoll/Ficoll discontinuous gradient centrifugation step followed by synaptosomal disruption with either digitonin or nitrogen cavitation pressurization.\n",
            "2. Whole synaptosomes are still found in the final mitochondrial fraction using these methods.\n",
            "3. To mitigate this obstacle, specific anti-mitochondrial outer membrane protein TOM22 antibodies are used to purify synaptosomal mitochondria without loss of function. These antibodies are conjugated with magnetic microbeads for use in magnetic activated cell sorting (MACS).\n",
            "4. Using a magnetic field, the microbeads selectively retain mitochondria as they bind to the TOM22 antibodies, washing out any non-mitochondrial fraction of the sample.\n",
            "5. The exclusive use of the MACS technique was shown to be sufficient to obtain brain mitochondria from a mixture of cell types, but not pure neuronal mitochondria because it requires disruption of the synaptosomes before application of the antibodies.\n",
            "6. The method described in this report yields highly purified and functional neuronal mitochondria.\n",
            "7. Surgical extracted fresh brain tissue is used for the isolation of mitochondria.\n",
            "8. The tissue is placed in ice-cold Isolation Buffer 1 (IB-1) and rinsed to remove blood.\n",
            "9. The washed tissue is transferred to a beaker with fresh IB-1.\n",
            "10. The tissue is minced into small pieces using scissors to eliminate trapped blood.\n",
            "11. The minced tissue is transferred to a glass potter.\n",
            "12. The tissue is homogenized using 20 slow up and down strokes with a Teflon pestle operated at 400 rpm. The volume of added IB-1 should be 5 to 10 times the estimated volume of the tissue sample.\n",
            "\n",
            "**Incompleteness or Uncertainties:**\n",
            "- The article does not provide the complete protocol for isolating synaptosomal mitochondria using Percoll/Ficoll discontinuous gradient centrifugation.\n",
            "- The article does not provide details on the synaptosomal disruption step with digitonin or nitrogen cavitation pressurization.\n",
            "- The article does not provide information on the specific concentrations or volumes of the reagents used in the isolation buffer or the Percoll solutions.\n",
            "- The article does not provide information on the specific temperature or duration of the homogenization step.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a general overview of the protocol for isolating synaptosomal mitochondria using Percoll/Ficoll discontinuous gradient centrifugation and synaptosomal disruption. However, it lacks specific details on the centrifugation and disruption steps, as well as the concentrations and volumes of reagents used. The protocol also requires the use of TOM22 antibodies and magnetic activated cell sorting (MACS) to purify synaptosomal mitochondria. Overall, the protocol is incomplete and further information is needed to replicate it accurately.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Mitochondrial Isolation Protocol:\n",
            "\n",
            "1. Slowly homogenize the brain specimen, ensuring a low pressure is induced during the upward movement of the pestle to avoid disruption of the mitochondria.\n",
            "2. If the volume of the brain specimen is too large, perform multiple homogenizations.\n",
            "3. Perform all isolation steps at 4 °C.\n",
            "4. Transfer the homogenate to a 16 mL or 50 mL centrifuge tube, depending on the final volume.\n",
            "5. Centrifuge the tube at 1,300 x g for 3 min at 4 °C.\n",
            "6. Carefully decant the supernatant into an empty polycarbonate centrifuge tube and place it on ice.\n",
            "7. The pellet should be loose and easily disrupted. Re-suspend the pellet in 5-10 mL of IB-1.\n",
            "8. Centrifuge the suspension at 1,300 x g for 3 min at 4 °C again.\n",
            "9. Repeat steps 4-8 as many times as necessary based on the total volume of the brain specimen.\n",
            "10. Centrifuge the pooled supernatant at 21,700 x g for 10 min at 4 °C. Ensure no loose pellet is transferred to avoid compromising mitochondrial activity.\n",
            "11. Prepare a Percoll gradient by creating a discontinuous layer of 1.5 mL of 40% Percoll at the bottom of a tube, with 4 mL of 24% Percoll layered on top.\n",
            "12. Discard the supernatant and re-suspend the pellet in 15% Percoll.\n",
            "13. Gently transfer 3.5 mL of the re-suspended pellet in the 15% Percoll on top of the 24% Percoll to create a third discontinuous layer.\n",
            "14. Centrifuge at 30,700 x g for 8 min at 4 °C using slow acceleration/deceleration regimes.\n",
            "15. Collect the synaptosomal fraction (SN) between the 15% and 24% layers of Percoll.\n",
            "16. Collect the non-synaptosomal fraction (NS) of mitochondria between the 24% and 40% layers of Percoll.\n",
            "17. Dilute the SN fraction at a 1:1 ratio with IB-1 and incubate in the cell disruption vessel at 1,100 psi for 15 min at 4 °C using a magnetic stirrer. Do not exceed 1,100 psi to maintain synaptosomal mitochondria amount/activity.\n",
            "18. Adjust the volume of the NS fraction to 10-12 mL and centrifuge at 16,700 x g for 10 min at 4 °C.\n",
            "19. Carefully aspirate the supernatant and re-suspend the loose pellet in 10 mL of IB-1.\n",
            "20. Centrifuge at 6,900 x g for 10 min at 4 °C.\n",
            "21. Decant the supernatant and re-suspend the pellet in 1 mL of IB-2. Centrifuge at 8,000 x g for 10 min at 4 °C.\n",
            "22. Decant the supernatant and re-suspend the pellet. The NS mitochondria in the collected suspension are ready for further experiments.\n",
            "23. Collect the SN fraction from the cell disruption vessel and centrifuge at 16,700 x g for 10 min at 4 °C.\n",
            "24. Carefully aspirate the supernatant and re-suspend the loose pellet in 10 mL of IB-1.\n",
            "25. Centrifuge at 6,900 x g for 10 min at 4 °C.\n",
            "26. Decant the supernatant and re-suspend the pellet in 1 mL of IB-2. Centrifuge at 8,000 x g for 10 min at 4 °C.\n",
            "27. Decant the supernat\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Protocol: 1. Repeat steps E and F until the flushed liquid out of the column becomes clear.\n",
            "2. Wash the collected fraction of pure SN mitochondria twice using IB-2 by centrifuging at 13,000 x g for 4 min at 4 °C to remove the separation buffer.\n",
            "3. Re-suspend the mitochondria in IB-2 using 20 μL of IB-2 buffer to maintain high concentration suspension.\n",
            "4. Transfer the mitochondrial suspension into a 1 mL test tube and store on ice until used for functional tests, no longer than 4 h. Alternatively, store the sample at -80 °C or in liquid nitrogen.\n",
            "\n",
            "**Incompleteness:** The article does not provide the complete protocol for mitochondrial isolation. It mentions steps E and F without providing the details of these steps.\n",
            "\n",
            "**Conclusion:** The article provides some steps for mitochondrial isolation, including washing the collected fraction and resuspending the mitochondria. However, it is incomplete as it does not provide the details of steps E and F.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. \"The mitochondria were kept on ice at 4°C overnight before RCR was tested again at 15 hours and showed retention of functionality of the isolated mitochondria.\"\n",
            "2. \"Mitochondrial matrix protein import requires both presence of the mitochondrial membrane potential and functional activity of the mitochondrial enzymes involved into the protein uptake and processing.\"\n",
            "3. \"A pre-ornithine transcarbamoylase (pOTC) cDNA in pGEM-3Zf(+)-pOTC plasmid was transcribed and translated in vitro using the TNT® T7 Coupled Transcription/Translation- system in the presence of L-[35S]-methionine.\"\n",
            "4. \"Following translation, [35S]-methionine-labeled pOTC was incubated with isolated mitochondria at 25 °C for different time points as shown.\"\n",
            "5. \"Mitochondria containing imported OTC were collected by centrifugation (9000 g for 10 min at 4 °C) and separated using SDS-PAGE.\"\n",
            "6. \"The radioactively labeled protein were visualized by exposing dried gel to a phospho screen and scanned on PMITM Personal Molecular Imager system.\"\n",
            "7. \"Cleaved mature OTC (mOTC), which represents the completion of import into the mitochondrial matrix, and uncleaved OTC (pOTC), were quantified using the provided software.\"\n",
            "8. \"Cleaved mOTC (35.2 kDa) has a smaller molecular weight than pOTC (39.9 kDa) and therefore migrates faster on the gel.\"\n",
            "9. \"The lower bands represent cleaved mOTC and show conclusively that the isolated mitochondria are able to import proteins into the matrix.\"\n",
            "10. \"Synaptosomal mitochondria isolated using the traditional Percoll method or our new method show decreased import activity compared to non-synaptosomal mitochondria.\"\n",
            "11. \"In comparing synaptosomal mitochondria isolated using both aforementioned methods, there is a mild non-significant decrease in import activity when the mitochondria are isolated using our method.\"\n",
            "12. \"With this method, we demonstrate the ability to isolate highly enriched and purified functional neuronal mitochondria from human brain.\"\n",
            "13. \"This method represents a significant advance over existing methods that result in highly enriched and purified mitochondria not specific to neurons, or neuronal mitochondrial fractions contaminated with synaptosomal vesicles, endoplasmic reticulum, lysosomes, peroxisomes or other cellular debris.\"\n",
            "14. \"The use of our method allows investigators to probe both structural and functional properties of neuronal mitochondria without interactions from other cellular contaminants.\"\n",
            "15. \"It is important to note that our method will eliminate any mitochondria-endoplasmic reticulum domains and therefore makes it unsuitable for any experiments studying those microdomains.\"\n",
            "\n",
            "**Conclusion:** The article provides a detailed description of the mitochondrial isolation steps, including the incubation of translated protein with isolated mitochondria, separation using SDS-PAGE, and quantification of imported proteins. However, the article does not provide a complete protocol for mitochondrial isolation.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Protocol: **Article:** Cyclophilin D-sensitive mitochondrial permeability transition in adult human brain and liver mitochondria\n",
            "\n",
            "**Section:** Materials and Methods\n",
            "\n",
            "**1. Extract Steps:**\n",
            "1. Brain tissue samples were collected from adult human brains.\n",
            "2. Mitochondria were isolated from brain tissue using a Percoll density gradient centrifugation method.\n",
            "3. Mitochondrial protein concentration was determined using the Bradford assay.\n",
            "4. Mitochondrial purity was assessed using immunoblotting.\n",
            "5. Respiration of isolated non-synaptosomal (NS) and synaptosomal (Syn) mitochondria was measured.\n",
            "6. Mitochondrial functional stability was tested by keeping isolated mitochondria on ice and measuring respiration after different time periods.\n",
            "7. Mitochondria isolated using the traditional Percoll method and the new method (Syn MACS) were compared for functional assays.\n",
            "8. Mitochondria isolated from extracted human cortex were used for pOTC import assays.\n",
            "\n",
            "**2. Note Incompleteness:**\n",
            "- The specific details of the Percoll density gradient centrifugation method are not provided.\n",
            "- The specific details of the immunoblotting procedure are not described.\n",
            "- The specific details of the respiration measurement and functional assays are not mentioned.\n",
            "\n",
            "**3. Format Steps:**\n",
            "1. Collect brain tissue samples from adult human brains.\n",
            "2. Isolate mitochondria from brain tissue using a Percoll density gradient centrifugation method.\n",
            "3. Determine mitochondrial protein concentration using the Bradford assay.\n",
            "4. Assess mitochondrial purity using immunoblotting.\n",
            "5. Measure respiration of isolated non-synaptosomal (NS) and synaptosomal (Syn) mitochondria.\n",
            "6. Test mitochondrial functional stability by keeping isolated mitochondria on ice and measuring respiration after different time periods.\n",
            "7. Compare mitochondria isolated using the traditional Percoll method and the new method (Syn MACS) for functional assays.\n",
            "8. Use mitochondria isolated from extracted human cortex for pOTC import assays.\n",
            "\n",
            "**4. Indicate Uncertainties:**\n",
            "- Unclear steps: Percoll density gradient centrifugation method, immunoblotting procedure, respiration measurement, and functional assays.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides steps for mitochondrial isolation from adult human brain tissue using a Percoll density gradient centrifugation method. However, specific details of the method, as well as the immunoblotting procedure, respiration measurement, and functional assays, are not described. Therefore, there are gaps in the protocol that need to be addressed for a complete understanding of the mitochondrial isolation process.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Centrifugation at 16,700 g for 10 min to remove brain tissue and blood.\n",
            "2. Homogenization with a glass Teflon potter at 400 rpm for 20 strokes.\n",
            "3. Centrifugation at 1,300 g for 3 min, repeated twice, to obtain the supernatant.\n",
            "4. Centrifugation at 21,000 g for 10 min to obtain the pellet of crude brain mitochondria.\n",
            "5. Percoll step-gradient centrifugation at 30,700 g for 8 min to separate nonsynaptosomal mitochondria (band between 24 and 40% Percoll) and synaptosomes (band in the 24% Percoll).\n",
            "6. Cell disruption using a vessel at 1100 psi for 15 min to obtain crude synaptosomal mitochondria.\n",
            "7. Immuno-purification using anti-TOM22 antibody conjugated to super-magnetic microbeads.\n",
            "8. Centrifugation at 8000 g for 10 min to obtain the pellet of nonsynaptosomal or synaptosomal mitochondria.\n",
            "9. Re-suspend and incubate the pellet with anti-TOM22 microbeads for 60 min at a mitochondria concentration of 0.3 mg/mL.\n",
            "10. Load the suspension with immune-labeled mitochondria onto a pre-equilibrated MACS Column mounted on the MACS Separator.\n",
            "11. Wash the MACS Column mounted on the MACS Separator three times.\n",
            "12. Elute the mitochondria from the MACS column to obtain a suspension of pure immuno-labeled mitochondria.\n",
            "13. Centrifugation at 13,000 g for 4 min to obtain the pellet of purified synaptosomal mitochondria.\n",
            "14. Resuspend the pellet in Isolation Buffer II.\n",
            "15. Percoll step-gradient centrifugation at 30,700 g for 10 min to obtain the pellet.\n",
            "16. Centrifugation at 6900 g for 10 min to obtain the final suspension of purified synaptosomal mitochondria.\n",
            "\n",
            "**Incompleteness:** The article does not provide details on the isolation buffer composition or the specific volumes used in each step. The time and temperature for each centrifugation step are also not mentioned.\n",
            "\n",
            "**Conclusion:** The article provides a detailed protocol for isolating synaptosomal mitochondria using a combination of centrifugation, Percoll step-gradient centrifugation, cell disruption, and immuno-purification. However, there are gaps in the protocol, such as missing buffer composition and volumes, as well as time and temperature details for centrifugation steps.\n",
            "Storing data for file_id: Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.txt\n",
            "Protocol: **Steps:**\n",
            "\n",
            "1. Mitochondria are isolated by homogenization of the tissue using a variety of approaches.\n",
            "2. The homogenized tissue is subjected to a series of differential centrifugation steps.\n",
            "3. This produces a crude mitochondrial pellet.\n",
            "4. The crude mitochondrial pellet is further purified using continuous colloidal density gradient centrifugation.\n",
            "5. The colloidal density material is subsequently removed by multiple centrifugation steps.\n",
            "6. Starting from 100 g of fresh leaf tissue, 2 - 3 mg of mitochondria can be routinely obtained.\n",
            "7. Respiratory experiments can be performed on these mitochondria.\n",
            "8. Typical rates of 100 - 250 nmol O2 min-1 mg total mitochondrial protein-1 (NADH-dependent rate) are observed.\n",
            "9. Various substrates and inhibitors can be used to determine which substrates are being oxidized and the capacity of the alternative and cytochrome terminal oxidases.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a clear protocol for isolating mitochondria from Arabidopsis thaliana leaves using homogenization, differential centrifugation, and continuous colloidal density gradient centrifugation. The protocol mentions the starting material and the expected yield of mitochondria. However, the article does not provide detailed information on the specific approaches used for homogenization and differential centrifugation. Further information on these steps would be necessary to fully replicate the protocol.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Protocol: Steps for the isolation of intact mitochondria from Arabidopsis thaliana organs grown on soil using continuous colloidal density gradients:\n",
            "\n",
            "1. Preparation of Grinding Medium, Wash Buffer, and Gradient Solutions:\n",
            "   - Prepare 300 mL of grinding medium (minus ascorbate and cysteine) and 200 mL of 2x wash buffer one day prior to the isolation, keeping them cold at 4 °C.\n",
            "   - On the morning of the isolation, add sodium ascorbate and L-cysteine to a final concentration of 17.84 mM and 20.36 mM, respectively, to the grinding medium.\n",
            "   - If the mitochondria are to be used for western blot or blue native-polyacrylamide gel electrophoresis (BN-PAGE) analyses, make up 2x wash buffer without BSA.\n",
            "\n",
            "2. Prepare gradients on the morning of mitochondrial isolation:\n",
            "   - Make the heavy and light gradient solutions in two beakers; 35 mL of each is sufficient for two gradient tubes.\n",
            "   - Wash the chambers of the gradient pourer and PVC peristaltic tubing with deionized water and ensure even flow through the tubing from all tubing.\n",
            "   - Place 2 centrifuge tubes (50 mL) at a slight angle on ice with the PVC peristaltic tubing outlets taped to the inside of the tubes.\n",
            "   - Close the connection between the inner and outer chambers and place the gradient pourer on a magnetic stirrer with a small stir bar in the inner chamber.\n",
            "   - Pour the 35 mL heavy gradient solution into the inner chamber and dispense the heavy gradient solution into the two centrifuge tubes placed on ice until half is remaining with the peristaltic pump rate set to fast (300 mL/h).\n",
            "   - Pour the 35 mL light gradient solution into the outer chamber, open the connection between the chambers halfway, and allow solutions to mix gently using the magnetic stirrer.\n",
            "   - Allow the solutions to run slowly (60 mL/h) until all of the gradient mix has been dispensed from the chambers resulting in two 0 - 4.4% (w/v) polyvinylpyrrolidone (PVP) gradient-filled tubes. Keep the gradients on ice until ready to use.\n",
            "\n",
            "3. Homogenization and Mitochondrial Isolation:\n",
            "   - Cut whole rosette tissue from 4-week old Arabidopsis plants grown on soil with scissors. At least 80 plants will be required for 1 prep.\n",
            "   - Place half of the plant material that has been cut into small pieces in a pre-cooled 4 °C large mortar and pestle with 75 mL of grinding medium and grind extensively until no big pieces of tissue are left.\n",
            "   - Pre-wet 4 layers of filtration material (22 - 25 µm pore size) with grinding medium and filter the homogenate through the 4 layers of filtration material into a 500 mL conical flask using a plastic funnel.\n",
            "   - Grind the remaining half of the tissue with another 75 mL of grinding medium.\n",
            "   - Combine the homogenates in the filtration materials and filter as much as possible homogenate through.\n",
            "   - Grind the remaining tissue remaining on the filtration material again with the remaining 150 mL of grinding medium and filter again into the 500 mL conical flask.\n",
            "   - Centrifuge the filtered homogenate in 8 pre-chilled 50 mL plastic centrifuge tubes at 2,500 x g at 4 °C in a fixed angle rotor for 5 min.\n",
            "   - Pour the supernatant into clean centrifuge tubes (50 mL) and centrifuge at 17,500 x g at 4 °C for 20 min in a fixed angle rotor.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol provides detailed steps for the isolation of intact mitochondria from Arabidopsis thaliana organs grown on soil using continuous\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Protocol: 1. Discard all but <3 mL of the supernatant by aspiration (or gently pour off without disturbing the pellet).\n",
            "2. Gently resuspend the pellet in the residual supernatant using a small fine paintbrush.\n",
            "3. Add 10 mL of 1x wash buffer to each tube and combine 4 tubes into 1 clean centrifuge tube (i.e., resulting in 2 tubes).\n",
            "4. Fill these tubes with 1x wash buffer to 50 mL and repeat steps 2.7 - 2.9.\n",
            "5. Pool crude mitochondrial pellets into 1 tube using a dropper and disperse evenly with the fine paintbrush (a small amount of wash buffer may be used but the final volume needs to be less than 3 mL to fit on top of the gradient).\n",
            "6. Gently layer the mitochondrial suspension with a dropper onto the 2 continuous 0 - 4.4 % (w/v) PVP gradients.\n",
            "7. Balance tubes by weight (using 1x wash buffer) and centrifuge at 40,000 x g at 4 °C for 40 min. Ensure the break is turned off. Mitochondria will migrate to a band near the bottom of the tube, or may be present in the heavy solution at the bottom of the tube (identified by a cloudy, yellowish band).\n",
            "8. Carefully remove the upper 5 cm of solution above the mitochondrial band by aspiration.\n",
            "9. Distribute the remaining solution containing the mitochondria into 2 clean tubes and fill with 1x wash buffer. Evenly distribute the colloidal density gradient and mitochondria by covering the mouth of the tube with a plastic paraffin film and inverting several times. Centrifuge for 15 min at 31,000 x g at 4 °C with slow braking.\n",
            "10. Remove the supernatant by aspiration, fill the tube with 1x wash buffer up to 50 mL, and invert again to ensure mixing. Centrifuge for 15 min at 31,000 x g at 4 °C with slow braking.\n",
            "11. Aspirate the supernatant and collect the mitochondrial pellets in as small a volume (~500 µL) as possible using a pipette with modified 200 µL tips (cut the tip a little above its end to increase its opening). Place the mitochondria into a 1.5 mL tube and keep on ice for immediate use or store at -80 °C.\n",
            "12. Determine the protein concentration of mitochondria using the Bradford method.\n",
            "\n",
            "**Notes:**\n",
            "- The protocol does not provide steps 2.7 - 2.9, so it is unclear what these steps entail.\n",
            "- The protocol mentions \"Table 2\" for preparing substrates, inhibitors, and effectors used for measuring mitochondrial respiration, but this table is not included in the provided text.\n",
            "- The protocol provides steps for software installation and calibration of the oxygen electrode, but these steps are not directly related to mitochondrial isolation.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol provides a detailed series of steps for mitochondrial isolation, but there are a few gaps and uncertainties. Steps 2.7 - 2.9 are missing, and the text does not include the necessary information from \"Table 2\" for preparing substrates, inhibitors, and effectors. Additionally, there are unrelated steps included for software installation and calibration of the oxygen electrode.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Protocol: 1. Select the channel to calibrate and enter variables for chamber temperature (25 °C) and atmospheric pressure (101.32 kPa).\n",
            "2. Set up the stirrer speed to 65 rpm.\n",
            "3. Wait for the signal to reach a plateau and click \"OK\" to continue.\n",
            "4. Establish zero oxygen in the chamber by adding 5 mg sodium dithionite and click \"OK\" to continue.\n",
            "5. Wait for the signal to reach a plateau and click \"OK\" to continue.\n",
            "6. Click \"Save Calibration\" to save the calibration.\n",
            "7. Clean the measuring chamber by rinsing it with water 5-10 times.\n",
            "8. Place 1 mL of respiration medium into the measuring chamber and allow the membrane to equilibrate for a few minutes until the oxygen consumption trace is linear.\n",
            "9. Adapt the scale of the oxygen consumption trace by setting \"Oxygen\" to 0-300 and \"Time Axis\" to 0-45 min.\n",
            "10. With the plunger open, add 970 µL air-saturated respiration medium to the reaction chamber.\n",
            "11. Add 30 µL isolated mitochondria or 150 µg of total mitochondrial protein to the reaction chamber.\n",
            "12. Stir the mitochondrial suspension continuously at 65 rpm and 25 °C to air-saturate the solution.\n",
            "13. Seal the chamber with the plunger and click \"Start Recording\" to record oxygen consumption. Allow the oxygen consumption trace to stabilize for 1-2 min.\n",
            "14. Determine the rate of oxygen consumption manually after adding 10 mM ascorbate and 25 µM cytochrome c for 5 min.\n",
            "15. Determine the rate of oxygen consumption manually for 3 min after adding 0.05% (v/v) detergent.\n",
            "16. Click the \"Rates of Change Table\" button and set the rate interval using cursors.\n",
            "17. Enter the rate into the table and calculate mitochondrial integrity by dividing the \"before detergent\" rate by the \"after detergent\" rate, expressed as a percentage, and subtracting it from 100.\n",
            "18. With the plunger open, add 970 µL of air-saturated respiration medium to the reaction chamber.\n",
            "19. Add 500 µM ATP and 30 µL isolated mitochondria or 150 µg of total mitochondrial protein to the reaction chamber.\n",
            "20. Stir the mitochondrial suspension continuously at 65 rpm and 25 °C.\n",
            "21. Close the plunger and click \"Start Recording\" to record oxygen consumption. Wait for 1-2 min for the oxygen consumption rate to stabilize.\n",
            "22. Add 5 mM succinate and record the rate of oxygen consumption for about 2 min.\n",
            "23. Add 1 mM adenosine diphosphate (ADP) and continue recording the rate of oxygen consumption for 2 min.\n",
            "24. Add 1 mM NADH and record the rate of oxygen consumption for 4-8 min.\n",
            "\n",
            "**Conclusion:** The protocol for mitochondrial isolation is incomplete. There are missing steps for the preparation of the mitochondrial suspension, including details on buffer composition and centrifugation steps. Additionally, the protocol does not specify the source of the isolated mitochondria or the method used for isolation.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"Add 1 mM potassium cyanide (KCN) (or 2.5 µM myxothiazol or 5 µM antimycin A) to inhibit the cytochrome c pathway, and continue to record for about 2 min to observe oxygen consumption via the alternative oxidase.\"\n",
            "2. \"Add 5 mM dithiothreitol (DTT) and 10 mM pyruvate to fully activate the alternative oxidase. Continue to record for 5 - 7 min: This is the capacity of the alternative oxidase.\"\n",
            "3. \"Add 500 µM n-propyl gallate (nPG), and record for 2 min. This assesses residual oxygen consumption rate that cannot be chemically inhibited. Subtract this rate from the other rates recorded, as it is not likely to be mitochondrial in origin.\"\n",
            "4. \"Click 'Stop Recording.'\"\n",
            "5. \"Click the 'Rates of Change Table' button and enter the rate into the table with a right click on one of the two cursors, and select 'Add Rate to Table.' Click the 'Add' button to confirm to display the rate on the main rate table.\"\n",
            "\n",
            "**Notes:**\n",
            "- There is no mention of the initial steps of mitochondrial isolation, such as tissue homogenization or differential centrifugation.\n",
            "- The article does not provide a complete protocol for mitochondrial isolation, focusing instead on the steps related to measuring oxygen consumption via the cytochrome c pathway and alternative oxidase.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides steps for measuring oxygen consumption via the cytochrome c pathway and alternative oxidase in isolated mitochondria, but it does not provide a complete protocol for mitochondrial isolation. The protocol assumes that the mitochondria have already been isolated using other methods.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Protocol: Steps for mitochondrial isolation from Arabidopsis thaliana:\n",
            "\n",
            "1. Prepare the grinding medium with the following components:\n",
            "   - Sucrose: 0.3 M\n",
            "   - Tetrasodium pyrophosphate (Na4P2O7 · 10 H2O): 25 mM\n",
            "   - EDTA disodium salt: 2 mM\n",
            "   - Potassium phosphate monobasic (KH2PO4): 10 mM\n",
            "   - Polyvinylpyrrolidone (PVP40): 1% (w/v)\n",
            "   - Bovine serum albumin (BSA): 1% (w/v)\n",
            "   - Deionized water\n",
            "   - Adjust pH to 7.5 using HCl\n",
            "   - Add sodium ascorbate and cysteine before use, adjusting pH if necessary\n",
            "\n",
            "2. Prepare the 2X wash buffer with the following components:\n",
            "   - Sucrose: 0.6 M\n",
            "   - TES: 20 mM\n",
            "   - Bovine serum albumin (BSA): 0.2% (w/v)\n",
            "   - Deionized water\n",
            "   - Adjust pH to 7.5 using NaOH\n",
            "\n",
            "3. Prepare the gradients:\n",
            "   - Heavy Gradient solution:\n",
            "     - PVP-40: 4.4% (w/v)\n",
            "     - 2X wash buffer: 17.5 mL\n",
            "     - Colloidal density gradient: 9.8 mL\n",
            "     - PVP-40 (20% (w/v)): 7.7 mL\n",
            "   - Light Gradient Solution:\n",
            "     - 2X wash buffer: 17.5 mL\n",
            "     - Colloidal density gradient: 9.8 mL\n",
            "     - Deionized water: 7.7 mL\n",
            "\n",
            "4. Isolate mitochondria from Arabidopsis leaves by grinding in a mortar and pestle. The grinding method is critical for obtaining a good yield, and grinding in a mortar and pestle is recommended.\n",
            "\n",
            "5. Slice or cut the tissue with a knife or scissors before grinding in the mortar and pestle.\n",
            "\n",
            "6. Perform all steps at 4 °C.\n",
            "\n",
            "7. Carry out the entire procedure from the end of grinding to obtaining a washed pellet of purified mitochondria in approximately 4 hours.\n",
            "\n",
            "8. Wash all components used in the procedure without detergent and do not use them in other procedures to avoid reducing yield.\n",
            "\n",
            "9. Adjust the mixing of the light and heavy solutions in the gradient to ensure sufficient separation of the mitochondria from other fractions. Adjust the pouring and mixing timing accordingly.\n",
            "\n",
            "10. The method outlined here can also be used for isolation of mitochondria from Arabidopsis flower and root tissue.\n",
            "\n",
            "**Note:** The article provides a detailed description of the grinding medium, 2X wash buffer, and gradient solutions used for mitochondrial isolation. It also emphasizes the importance of the grinding method and temperature control during the procedure. However, the article does not provide specific steps for the isolation process itself, such as centrifugation or purification steps. Therefore, the protocol gaps exist in terms of the actual isolation steps.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. For roots, it is convenient to grow in hydroponic culture and need 100 g of fresh weight to obtain 2 mg amounts of mitochondria.\n",
            "2. For grinding roots, they should be cut into small pieces prior to grinding in a mortar and pestle. Increase yields can be obtained by re-grinding the root tissue, either in a mortar and pestle or in a blender.\n",
            "3. For floral tissue, grinding with a mortar and pestle works well, but the limited amount of material means that a large amount of plants needs to be grown to harvest tissue.\n",
            "4. Mitochondria have been isolated from a variety of Arabidopsis organs using similar methods or slight modifications and from rice (Oryza sativa).\n",
            "5. The limitations of the method described above are:\n",
            "   - The large amounts of seeds required for mitochondrial isolation.\n",
            "   - Only specific tissues from Arabidopsis and rice applied in this isolation method.\n",
            "   - Small quantities of various contaminants (such as peroxisomal proteins) still existed in the purified mitochondria.\n",
            "6. Gradient purified mitochondria will still contain small quantities of various contaminants, such as peroxisomal proteins.\n",
            "7. A comparison with defined organelle lists can be used to determine changes in mitochondrial proteins, as long as the amount of contamination by non-mitochondrial proteins is small (<10%) and similar samples are compared, so the type and degree of non-mitochondrial proteins is similar.\n",
            "8. Analyses of protein abundance by SDS-PAGE or other gel-based approaches can be carried out with relatively small amounts of mitochondria (2 - 20 µg), using varying amounts of mitochondria to check the linearity of detection by antibodies.\n",
            "9. While porin (voltage dependent anion channel (VDAC)) is often used as a loading control for quantification, in our experience the relatively large abundance of this protein can mean that the response is often not linear if large amounts of protein are loaded on the gel, so linearity of antibody response should always be checked when using such loading controls.\n",
            "10. The significance of this approach of isolating mitochondria is that unlike gradients that use sucrose as the material to form the density gradient, colloidal density gradients do not require osmotic re-adjustment of the purified mitochondria as is required with sucrose. This means that there is less chance of rupturing the purified mitochondria and after washing to remove the colloidal density material, they can be directly used in a variety of assays or applications.\n",
            "11. Tissue blots, where mitochondrial proteins are detected from whole leaf or tissue extracts, are an attractive approach to measure the amount of mitochondrial proteins in that tissue. Given the generally low volume of the mitochondria compared to other organelles, the detection of mitochondrial protein on whole tissue extracts needs to be interpreted with some caution, as mitochondrial protein may be beyond detection in such approaches. Careful controls, where purified mitochondria are electrophoresed along with tissue extracts to ensure identical migration and linearity of detection, need to be carried out to have confidence in such approaches.\n",
            "12. With the help of a Clark-type oxygen electrode, changes in the oxygen partial pressure of a solution can be measured. The actual electrode consists of a platinum cathode and a silver anode, which are connected by a KCl bridge and covered by an electrolyte-moistened paper (cigarette paper) and an oxygen-permeable membrane (polytetrafluoroethylene membrane). A voltage of 600 - 700 mV leads to the reduction of oxygen, giving a linear relationship between oxygen concentration and voltage. Oxygen electrodes are available commercially from different companies. Each company will have its own instructions regarding the assembly and setup of the oxygen electrode. However, in general, the silver anode and platinum cathode of the electrode disk need to be cleaned with the help of an electrode cleaning kit or an eraser pen before assembly\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for mitochondrial isolation:\n",
            "\n",
            "1. Prior to the assay, warm the respiration medium to the same temperature as the assay chamber (usually 25 °C).\n",
            "2. Allow the membrane to equilibrate in respiration buffer for a few minutes.\n",
            "3. Close the plunger for oxygen consumption assays.\n",
            "4. Add effector molecules using either non-disposable microliter syringes or disposable gel loading pipette tips.\n",
            "5. If using micro syringes, thoroughly rinse the syringe with 100% ethanol and water between additions to avoid contaminating stock solutions.\n",
            "6. Wash the chamber between measurements, rinsing with water approximately five times.\n",
            "7. Most reagents used in oxygen consumption assays are soluble in water and can be removed by multiple rinsing with water.\n",
            "8. For chemicals soluble in organic solvents (e.g., antimycin A, myxothiazol, and nPG), rinse the chamber with 50% (v/v) ethanol between assays to remove residual traces, and then rinse with water approximately five times to deplete residues.\n",
            "\n",
            "**Incompleteness:** The article does not provide a complete protocol for mitochondrial isolation. It mentions specific steps related to the assay and the use of effector molecules, but it does not provide details on the actual isolation process.\n",
            "\n",
            "**Conclusion:** The article provides some steps related to the mitochondrial isolation process, specifically focusing on the assay and the use of effector molecules. However, it does not provide a complete protocol for mitochondrial isolation. Additional information is needed to fully understand the isolation process.\n",
            "Storing data for file_id: Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.txt\n",
            "Protocol: Steps for isolating intact, functional mitochondria from Arabidopsis thaliana:\n",
            "\n",
            "1. \"We present two methods for isolating mitochondria from Arabidopsis, either from heterotrophic cell suspension cultures or from hydroponic seedling cultures.\" (Introduction)\n",
            "\n",
            "**Method 1: Heterotrophic Cell Suspension Culture**\n",
            "2. Prepare culture medium: \"Culture medium (1 L): MS salts including vitamins, 3% (w/v) sucrose, 0.5 mg napthalene acetic acid, 50 Rg kinetin. Adjust pH to 5.8 with KOH and autoclave.\" (Materials)\n",
            "\n",
            "**Method 2: Hydroponic Seedling Culture**\n",
            "3. Prepare glass culture vessels: \"Glass culture vessels (100 mL) with a polypropylene Magenta B-cap. Sterilize by autoclaving.\" (Materials)\n",
            "4. Prepare culture medium: \"Culture medium (500 mL): MS salts including vitamins, 2% (w/v) sucrose, 0.2 g MES. Adjust pH to 5.8 with KOH. Transfer 60-mL aliquots to sterile glass culture vessels; add 0.06 g agar and autoclave.\" (Materials)\n",
            "5. Prepare sterilizing solutions: \"Sterilizing solution 1: 70% (v/v) ethanol. Sterilizing solution 2: 5% (v/v) sodium hypochlorite, 0.1% (v/v) Tween-20. Sterile distilled water.\" (Materials)\n",
            "\n",
            "6. Prepare density gradients: \"Gradient buffer 1: 0.6 M mannitol, 20 mM MTES-KOH, pH 7.5, 0.2% (w/v) bovine serum albumin (BSA).\" (Preparation of Density Gradients)\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides two methods for isolating mitochondria from Arabidopsis thaliana: one using heterotrophic cell suspension cultures and the other using hydroponic seedling cultures. The steps provided are complete and specific, including details on culture medium preparation, sterilization, and density gradient preparation. No protocol gaps or uncertainties are noted.\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Protocol: Steps for mitochondrial isolation from Arabidopsis tissue:\n",
            "\n",
            "1. Gradient buffer 2: 0.6 M sucrose, 20 mM MTES-KOH, pH 7.5, 0.2% (w/v) BSA, 3 Percoll.\n",
            "2. For Heterotrophic Arabidopsis Cell Suspension Cultures:\n",
            "   a. Miracloth, muslin, Buchner funnel, Waring blender, vacuum pump.\n",
            "   b. Cell extraction medium: 0.45 M mannitol, 50 mM sodium pyrophosphate (Na4P2O7), 0.5% (w/v) BSA, 0.5% (w/v) PVP-40, 2 mM EGTA. Adjust pH to 8.0 with phosphoric acid. Add cysteine to 20 mM on the day of use.\n",
            "   c. 250-mL Centrifuge tubes.\n",
            "   d. Centrifuge with fixed-angle rotor (Beckman JA-14 or equivalent).\n",
            "   e. Cell mitochondria wash medium: 0.3 M mannitol, 10 mM MTES-KOH, pH 7.5.\n",
            "   f. Paintbrush, 5-mL Pipet.\n",
            "   g. 40/23/18% Percoll step gradient.\n",
            "3. For Hydroponic Arabidopsis Seedling Cultures:\n",
            "   a. Buchner funnel, sterile distilled water, Perspex vessel (45 × 60 × 200 mm).\n",
            "   b. Seedling extraction medium: 0.3 M sucrose, 25 mM Na4P2O7, 2 mM EDTA, 10 mM KH2PO4, 1% (w/v) PVP-40, 1% (w/v) BSA. Adjust to pH 7.5 with NaOH. Add ascorbic acid to 20 mM on the day of use.\n",
            "   c. Polytron homogenizer equipped with a 20-cm long × 2-cm diameter dispersing head.\n",
            "   d. Miracloth, muslin, centrifuge and centrifuge tubes as in Heterotrophic Arabidopsis Cell Suspension Cultures.\n",
            "   e. 50-mL Centrifuge tubes, paintbrush.\n",
            "   f. 28% Percoll, 0–4.4% PVP-40 gradients.\n",
            "4. Leaf mitochondria wash medium: 0.3 M sucrose, 10 mM MTES-KOH, pH 7.5.\n",
            "5. 5-mL Pipet.\n",
            "\n",
            "Notes:\n",
            "- The article does not provide complete protocols for mitochondrial isolation, but rather mentions the required materials and buffers.\n",
            "- The specific steps for mitochondrial isolation are not mentioned, creating protocol gaps.\n",
            "\n",
            "Conclusion:\n",
            "The article provides a list of materials and buffers required for mitochondrial isolation from Arabidopsis tissue. However, it does not provide complete protocols or specific steps for the isolation process. The protocol gaps make it difficult to replicate the mitochondrial isolation procedure accurately. Further information or additional references are needed to obtain a complete protocol.\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. \"Appropriate concentrations of Percoll are prepared in the appropriate gradient buffer. Gradients should be prepared in advance and stored on ice until required.\"\n",
            "2. \"For isolation of mitochondria from heterotrophic cell suspension cultures, two different gradients are used. The first is a step gradient consisting of 5 mL 40% (v/v) Percoll overlaid with 20 mL 23% (v/v) Percoll and then 10 mL 18% (v/v) Percoll, all made up using gradient buffer 1. The layers can be conveniently poured over one another by allowing the solution to run through a 19-gauge hypodermic needle placed against the inside of the centrifuge tube held at a 45° angle. The second gradient consists of 30 mL 28% (v/v) Percoll made up in gradient buffer 2. A sigmoidal gradient self-forms during centrifugation because of the sedimentation of the Percoll polydispersed silica colloid.\"\n",
            "3. \"For isolation of mitochondria from Arabidopsis hydroponic seedling cultures, a single linear PVP-40 gradient in 28% (v/v) Percoll is used. Preparation of this gradient requires a gradient maker, which consists of a Perspex block with two cylindrical chambers connected by a small pipe at their base. One of the chambers has an outflow. Both the connecting pipe and outflow are metered by taps. Two solutions are prepared: a 'heavy' solution, in this case 28% (v/v) Percoll and 4.4% (v/v) PVP-40 from a 20% (w/v) stock made up in gradient buffer 2, and a 'light' solution (28% v/v Percoll made up in gradient buffer 2). The light solution (15 mL) is placed in the chamber with no outflow. The tap connecting the two chambers should be briefly opened to displace air in the connecting pipe and ensure a flow. The heavy solution (15 mL) is placed into the chamber with the outflow. A magnetic stir bar is placed in the chamber with the outflow, and the solution rapidly stirred. The outflow pipe should be secured against the inside of a 40-mL centrifuge tube held at a 45° angle (above the final level to which the gradient solution will reach in the tube). It is important that the end of the outflow tube is held below the level of the gradient maker so that solution will flow by gravity. The taps on both the connecting pipe and the outflow are then opened simultaneously. As the gradient pours, the level of solution in both chambers should drop at the same rate.\"\n",
            "4. \"To preserve the integrity of isolated mitochondria, it is essential that all plasticware and glassware be free from traces of detergent. All procedures should be done at 4°C, solutions should be prechilled, and the work should be done in a cold room. Centrifugation should be done using a refrigerated centrifuge.\"\n",
            "5. \"Separate cells from culture medium by filtration through a single layer of Miracloth using a Buchner funnel and a vacuum pump. Weigh cells.\"\n",
            "6. \"Transfer up to 150 g FW of cells to a 500-mL Waring blender vessel. Add at least 2 volumes of cell extraction medium per weight of cells and blend for 15 s at high speed and twice for 15 s at low speed. Leave 30-s intervals between bursts of blending to prevent excessive foaming and heating.\"\n",
            "7. \"Filter extract by wringing through one layer of Miracloth and two layers of muslin to remove starch, cell debris, and unbroken cells.\"\n",
            "8. \"Transfer filtrate to 250-mL centrifuge tubes and centrifuge at 1500 g for 10 min. Gently pour resulting supernat\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Protocol: 1. Resuspend the final pellet in 1 mL cell mitochondria wash medium using a paint-brush as before.\n",
            "2. You now have an organelle suspension.\n",
            "3. It is important to complete steps 1-9 and move on to the density gradient centrifugation as quickly as possible.\n",
            "4. Carefully pipet the organelle suspension onto the surface of the 40/23/18% step gradient.\n",
            "5. The organelle suspension from up to 10 cell cultures can be loaded onto a single gradient.\n",
            "6. Centrifuge at 40,000 g for 30 min.\n",
            "7. It is important that the centrifuge brake be disengaged at the end of the run because rapid deceleration can disturb the contents of the gradient.\n",
            "8. Mitochondria appear as a diffuse white/pale brown band at the interface of the 40/23% Percoll layers.\n",
            "9. Peroxisomes are just beneath but are not usually visible.\n",
            "10. Plastid membranes appear as a bright yellow band at the 23/18% Percoll interface.\n",
            "11. Aspirate the upper part of the gradient to waste.\n",
            "12. Using a 5-mL pipet, carefully transfer the mitochondria band to a fresh 50-mL centrifuge tube.\n",
            "13. Generally, 7-10 mL are sufficient to transfer all of the mitochondria.\n",
            "14. Add 20 mL cell mitochondria wash medium to the mitochondria and centrifuge at 18,000 g for 10 min.\n",
            "15. Remove and discard the supernatant by aspiration. Take care not to disturb the loose pellet.\n",
            "16. It is not necessary to remove all the supernatant; leave 3-4 mL in the bottom of the tube.\n",
            "17. Resuspend the mitochondrial pellet by gentle swirling and carefully pipet onto the 28% Percoll gradient.\n",
            "18. Centrifuge at 40,000 g for 30 min with the brake off as before.\n",
            "19. Mitochondria will form a white/pale brown band in the upper part of the gradient, and peroxisomes will band toward the bottom of the gradient.\n",
            "20. Carefully transfer the mitochondria to a fresh tube using a 5-mL pipet.\n",
            "21. Add 20 mL cell mitochondria wash medium to the mitochondria and centrifuge at 18,000 g for 10 min.\n",
            "22. Remove and discard the supernatant by aspiration.\n",
            "23. Repeat step 22.\n",
            "24. Pour off and discard the supernatant. The pellet is firm at this stage.\n",
            "25. Gently resuspend the mitochondria in 0.5 mL cell mitochondria wash medium using a small paintbrush.\n",
            "26. To preserve the integrity of isolated mitochondria, ensure that all plasticware and glassware are free from traces of detergent.\n",
            "27. All procedures should be done at 4°C, solutions should be prechilled, and the work should be done in a cold room.\n",
            "28. Centrifugation should be done using a refrigerated centrifuge.\n",
            "29. Remove seedlings from six to eight culture vessels (~60 g), place in a Buchner funnel, and wash with sterile distilled water.\n",
            "30. Place washed seedlings in a Perspex vessel and cover with 300 mL seedling extraction medium.\n",
            "31. Homogenize tissue with a Polytron homogenizer equipped with a 20-cm long ×2-cm diameter dispersing head using three or four 2-s bursts.\n",
            "32. Filter extract by wringing through one layer of Miracloth and two layers of muslin to remove starch, cell debris, and unbroken cells.\n",
            "33. Transfer filtrate to 250-mL centrifuge tubes and centrifuge at 1100 g for 5 min.\n",
            "34. Gently pour resulting supernatant into fresh centrifuge tubes. Take care not to transfer any of the loose\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"Repeat step 10 and pour off the resultant supernatant.\"\n",
            "2. \"Add 0.5 mL leaf mitochondria wash medium and resuspend pellet gently with a small paintbrush.\"\n",
            "3. \"Mitochondrial integrity can be assessed by measuring the latency of the assay of COX to added cytochrome-c.\"\n",
            "4. \"COX is measured as the cytochrome-c-dependent consumption of oxygen, detected by using a Clark-type oxygen electrode.\"\n",
            "5. \"Set up the electrode according to the manufacturer’s instructions using 50% saturated KCl as an electrolyte and calibrate between air-saturated water (253 nmol O2/mL at 25°C) and zero (established by adding a few crystals of sodium hydrosulfite to the water).\"\n",
            "6. \"The assay should be conducted at 25°C.\"\n",
            "7. \"To 1 mL mitochondrial reaction medium, add 5–20 RL mitochondrial suspension and the following in sequence, waiting for a linear rate of oxygen consumption to be established each time: 20 RL 500 mM Na-ascorbate, 10 RL 5 mM cytochrome-c, and 5 RL 10% (v/v) Triton X-100.\"\n",
            "8. \"COX activity is given by [rate (c) - rate (a)]. Mitochondrial outer membrane integrity is given by [rate (c) / rate (a)].\"\n",
            "9. \"Respiration of a variety of substrates can be measured as oxygen consumption using an oxygen electrode.\"\n",
            "10. \"Add 1 mL mitochondrial reaction medium and 5–20 RL of mitochondrial suspension to the reaction chamber of the oxygen electrode.\"\n",
            "11. \"Determine the background rate of oxygen consumption [rate (a)].\"\n",
            "12. \"Add 10 RL of each of the respiratory substrates listed and determine the rate of oxygen consumption [rate (b)].\"\n",
            "13. \"Add 10 RL 10 mM ADP. The rate of oxygen consumption should increase: measure the initial linear rate [rate (c)].\"\n",
            "14. \"After a few minutes, the ADP will be depleted, and the rate of oxygen consumption will be reduced to [rate (d)].\"\n",
            "15. \"The respiratory control ratio is given by [rate (b) - rate (a)] / [rate (c) - rate (a)].\"\n",
            "\n",
            "**Incompleteness or Uncertainties:**\n",
            "\n",
            "- Step 10 mentions \"the respiratory substrates listed,\" but the specific substrates are not provided.\n",
            "- Step 14 does not mention how long to wait before the ADP is depleted and the rate of oxygen consumption is reduced.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The article provides a series of steps for mitochondrial isolation, including details on resuspending the pellet, assessing mitochondrial integrity, and measuring COX activity and respiratory function. However, there are some missing details, such as the specific respiratory substrates to be used and the duration of waiting for ADP depletion.\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, it is not possible to accurately extract the steps for mitochondrial isolation. The text does not contain any specific details or instructions regarding the isolation procedure. It only mentions the importance of not adding too much mitochondrial suspension and the need to test different concentrations. Additionally, the text discusses the oxidation of Na-ascorbate solution and the need to make it fresh on the day of use. However, these details are not sufficient to construct a complete protocol for mitochondrial isolation.\n",
            "Storing data for file_id: Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.txt\n",
            "Protocol: **Extracted Steps:**\n",
            "\n",
            "1. Generate protoplasts by enzymatic disruption of Ustilago maydis cell wall.\n",
            "2. Collect the protoplasts by centrifugation at 1,000 x g for 10 minutes.\n",
            "3. Resuspend the protoplast pellet in 1 mL of isolation buffer (0.6 M sorbitol, 20 mM HEPES-KOH, pH 7.4).\n",
            "4. Homogenize the protoplast suspension using a sterile toothpick.\n",
            "5. Transfer the homogenized suspension to a 2 mL microcentrifuge tube.\n",
            "6. Centrifuge the tube at 1,000 x g for 10 minutes to pellet the mitochondria.\n",
            "7. Discard the supernatant and resuspend the mitochondrial pellet in 1 mL of isolation buffer.\n",
            "8. Centrifuge the tube at 1,000 x g for 10 minutes to pellet the mitochondria again.\n",
            "9. Discard the supernatant and resuspend the mitochondrial pellet in 1 mL of isolation buffer.\n",
            "10. Centrifuge the tube at 1,000 x g for 10 minutes to pellet the mitochondria for the final time.\n",
            "11. Discard the supernatant and resuspend the mitochondrial pellet in 50 µL of isolation buffer.\n",
            "12. Transfer the mitochondrial suspension to a new microcentrifuge tube.\n",
            "13. Measure the protein concentration of the mitochondrial suspension using a spectrophotometer.\n",
            "14. Use the isolated mitochondria for further experiments.\n",
            "\n",
            "**Incompleteness:**\n",
            "\n",
            "- The article does not provide specific details on the enzymes used for the enzymatic disruption of the cell wall.\n",
            "- The article does not mention the temperature and duration of the homogenization step.\n",
            "- The article does not provide information on the speed and duration of the centrifugation steps.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The protocol provides a clear procedure for the isolation of mitochondria from Ustilago maydis protoplasts. However, there are some gaps in the protocol, such as the lack of information on the specific enzymes used for cell wall disruption, the temperature and duration of the homogenization step, and the speed and duration of the centrifugation steps. These missing details may limit the reproducibility of the protocol.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4514 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: **Procedure:**\n",
            "\n",
            "A. Protoplast preparation\n",
            "\n",
            "1. Streak the U maydis strain from a 25% glycerol stock on solid YPD agar and incubate at 28°C for 1-2 days, then store the plate at 4°C.\n",
            "2. Select a colony with a sterile toothpick.\n",
            "3. Culture cells in 100 mL of YPD medium at 28°C and 180 rpm for 18-24 hours.\n",
            "4. Inoculate cells in 1 L of YPD liquid medium at an initial optical density at 600 nm of 0.04 (9 × 105 cells/mL) and incubate for 24 hours at 28°C and 180 rpm.\n",
            "5. Collect the cells in a pre-weighed 500 mL centrifuge bottle. Centrifuge at 3,800 × g and 4°C for 5 minutes using the FiberliteTM F12-6 × 500 LEX rotor. Remove the supernatant, suspend the cells in distilled water, and repeat the centrifugation step. Decant the supernatant.\n",
            "6. Measure the wet weight of the pellet and add 12.5 mL of solution A per gram of wet weight. Suspend the cells using a glass or Teflon rod. Cells can be vortexed to disperse cell aggregates. Transfer the cell suspension to a glass beaker. Add 0.016 g of the Trichoderma harzianum lysing enzymes per gram of wet weight. Mix gently with a Teflon or glass rod and incubate for 30-60 minutes at 30°C in a water bath.\n",
            "7. To check the formation of protoplasts, shake the cell suspension every 10 minutes. Withdraw 20 µL of the suspension and mix with 980 µL of distilled water. Measure the optical density at 600 nm.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a detailed protocol for the isolation of mitochondria from Ustilago maydis protoplasts. The steps for protoplast preparation are clearly described, including the culture of cells, collection, suspension, and incubation. However, the article does not mention the subsequent steps for mitochondrial isolation. Therefore, the protocol is incomplete in terms of the mitochondrial isolation process.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Mitochondrial isolation steps:\n",
            "\n",
            "1. Transfer the pellet of protoplasts to a Teflon pestle homogenizer and add cold solution C (40 mL/25-30 g of the initial wet weight of cells) containing the complete cocktail of protease inhibitors plus PMSF (1 mM final concentration).\n",
            "2. Attach the Teflon pestle to the electric drill. Connect the drill to the variable autotransformer (rheostat), select an output intensity of 40, and homogenize for 20 cycles. Avoid the formation of foam and bubbles.\n",
            "3. After homogenization, bring the volume to 130 mL with cold solution C (containing protease inhibitors). Transfer the suspension to 50 mL polycarbonate tubes and eliminate cell debris and nuclei by centrifuging at 3,800 × g and 4°C for 10 min.\n",
            "4. Recover the supernatant from the polycarbonate tubes and centrifuge at 17,000 × g and 4°C for 10 min. Carefully decant the supernatant. Then, with the buffer remaining in the tube and with the help of a soft paintbrush (number 8 or 12, according to the biomass), suspend the mitochondrial pellet. Keep mitochondria at 4°C for same-day experiments, or at -70°C to be used later.\n",
            "5. Determine the protein concentration of your sample by the Lowry method (Lowry et al., 1951). Prepare a stock solution of 1 mg/mL BSA and construct a standard curve in the range of 5 to 100 µg protein. Dilute the sample tenfold using 0.4% sodium deoxycholate solution and use 5-10 µL of the diluted sample for protein determination.\n",
            "\n",
            "Conclusion:\n",
            "The article provides a detailed protocol for mitochondrial isolation from Ustilago maydis protoplasts. The steps are presented clearly, with specific instructions on centrifugation, homogenization, and protein determination. However, there is a gap in the protocol regarding the composition and preparation of solutions A and B, which are mentioned but not described.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. Start the magnetic stirring.\n",
            "2. Add 2 µM safranine (5.4 µL from 1 mM stock solution).\n",
            "3. Add the mitochondrial sample (0.5-1.0 mg protein).\n",
            "4. Place the cuvette in the spectrophotometer and close the cover.\n",
            "5. Adjust the baseline.\n",
            "6. Add the substrate of interest (e.g., 10 mM succinate or 0.15 mM NADH) through a small hole in the chamber cover using a Hamilton syringe.\n",
            "7. To collapse the membrane potential generated by succinate or any other respiratory substrate, add 2-10 μM FCCP.\n",
            "\n",
            "**Uncertainties or Incompleteness:**\n",
            "- The article does not provide details on the duration of magnetic stirring.\n",
            "- The article does not specify the type of cuvette or spectrophotometer used.\n",
            "- The article does not mention the specific wavelength used for adjusting the baseline.\n",
            "- The article does not provide information on the volume of substrate added or the rate of addition.\n",
            "- The article does not specify the duration of incubation with lytic enzymes for protoplast formation.\n",
            "- The article does not provide details on the specific lytic enzymes used.\n",
            "- The article does not mention the specific method used for the measurement of optical density at 600 nm.\n",
            "- The article does not provide information on the specific method used for the measurement of respiratory activity and membrane potential.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a general outline of the mitochondrial isolation steps, including the addition of safranine, the mitochondrial sample, and the substrate of interest. However, there are several protocol gaps and missing details, such as the duration of magnetic stirring, the specific cuvette and spectrophotometer used, the volume and rate of substrate addition, the duration of incubation with lytic enzymes, the specific lytic enzymes used, and the specific methods for optical density measurement, respiratory activity, and membrane potential measurement. These gaps make it difficult to replicate the protocol accurately without additional information.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Error in summarizing article: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4144 tokens (3344 in the messages, 800 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            " Occured in generate_summary function\n",
            "Error in summarizing article: argument of type 'NoneType' is not iterable\n",
            " Using last summary\n",
            "Protocol: Based on the provided information, the steps for mitochondrial isolation from Ustilago maydis protoplasts are as follows:\n",
            "\n",
            "1. Incubate cells in a water bath for protoplast formation.\n",
            "2. Start with at least 20 g of wet weight of cells for high-quality mitochondria preparations.\n",
            "3. Keep the temperature at 4°C after protoplast formation.\n",
            "4. Prepare a fresh mitochondrial preparation for oxygen consumption experiments.\n",
            "5. To achieve inhibition of serine proteases, prepare PMSF just before use and never use stored PMSF.\n",
            "\n",
            "The composition of the system for mitochondrial isolation is as follows:\n",
            "- 300 mM sorbitol\n",
            "- 10 mM Hepes pH 7.0\n",
            "- 0.33 mM EGTA\n",
            "- 0.5 mg (protein) mitochondria\n",
            "- 0.2% BSA\n",
            "- 2 μM safranine\n",
            "\n",
            "The recipes for the solutions used in the isolation process are as follows:\n",
            "\n",
            "Solution A:\n",
            "- (NH4)2SO4 0.6 M (79.26 g/L)\n",
            "- KH2PO4 20 mM (2.72 g/L)\n",
            "\n",
            "Solution B:\n",
            "- Sucrose 0.8 M (273.84 g/L)\n",
            "- Trizma base-HCl (pH 7.0) 10 mM (1.21 g/L)\n",
            "- EDTA 2 mM (0.74 g/L)\n",
            "- KH2PO4 20 mM (2.72 g/L)\n",
            "- BSA 0.3% (3.0 g/L)\n",
            "\n",
            "Solution C:\n",
            "- Sucrose 0.4 M (137 g/L)\n",
            "- Trizma base-HCl (pH 7.0) 10 mM (1.21 g/L)\n",
            "- EDTA 2 mM (0.74 g/L)\n",
            "- KH2PO4 20 mM (2.72 g/L)\n",
            "- BSA 0.3% (3.0 g/L)\n",
            "\n",
            "PMSF:\n",
            "- PMSF 1 M (0.0871 g/0.5 mL of DMSO)\n",
            "\n",
            "Solution D:\n",
            "- Glucose 1 M (1.80 g/10 mL)\n",
            "\n",
            "Solution E:\n",
            "- Tris-HCl (pH 7.0) 250 mM (1.51 g/50 mL)\n",
            "\n",
            "YPD culture medium:\n",
            "- Glucose 0.5% (5 g/L)\n",
            "- Select yeast extract 0.5% (5 g/L)\n",
            "- Bacteriological peptone 0.25% (2.5 g/L)\n",
            "- Sterilize at 120 °C for 15 min\n",
            "- For solid YPD, add 2% agar (20 g/L)\n",
            "\n",
            "Solution F:\n",
            "- Sorbitol 300 mM (54.65 g/L)\n",
            "- Hepes 20 mM, pH 7.0 (4.76 g/L)\n",
            "- EGTA 0.33 mM (125.51 mg/L)\n",
            "- BSA 0.2% (2 g/L)\n",
            "\n",
            "FCCP solution:\n",
            "- FCCP 1 mM (1.27 mg/5 mL)\n",
            "\n",
            "Safranine solution:\n",
            "- Safranine 1 mM (3.51 mg/10 mL)\n",
            "\n",
            "Succinate solution:\n",
            "- Succinate 1 M (1.3507 g/5 mL)\n",
            "\n",
            "NADH solution:\n",
            "- NADH 20 mM (0.07094 g/5 mL)\n",
            "\n",
            "KCN solution:\n",
            "- KCN (0.0815 g/5 mL)\n",
            "\n",
            "n-Octyl gallate Solution:\n",
            "- n-Octyl gallate (0.0028 g/5 mL)\n",
            "\n",
            "In conclusion, the provided information includes the steps for mitochondrial isolation from Ustilago maydis protoplasts, the composition of the isolation system, and the recipes for the solutions used. However, the specific protocol steps for mitochondrial isolation are not provided, and there\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Protocol: **Article:** Sierra-Campos, E., Velázquez, I., Matuz-Mares, D., Villavicencio-Queijeiro, A., & Pardo, J. P. (2009). Functional properties of the Ustilago maydis alternative oxidase under oxidative stress conditions. Mitochondrion, 9(2), 96-102.\n",
            "\n",
            "**Section:** \"Isolation of Mitochondria from Ustilago maydis Protoplasts\"\n",
            "\n",
            "**Steps:**\n",
            "1. \"Mitochondria were isolated from Ustilago maydis protoplasts.\"\n",
            "2. \"Protoplasts were obtained by enzymatic digestion of U. maydis cells.\"\n",
            "3. \"The protoplasts were homogenized in a medium containing 0.6 M mannitol, 10 mM potassium phosphate buffer pH 7.5, 1 mM EGTA, and 0.1% (w/v) BSA.\"\n",
            "4. \"The homogenate was centrifuged at 800 x g for 10 min to remove cell debris and unbroken cells.\"\n",
            "5. \"The supernatant was collected and centrifuged at 10,000 x g for 10 min to obtain a crude mitochondrial pellet.\"\n",
            "6. \"The crude mitochondrial pellet was resuspended in the same medium used for homogenization.\"\n",
            "7. \"The resuspended pellet was layered on top of a 30% Percoll gradient and centrifuged at 20,000 x g for 30 min.\"\n",
            "8. \"The mitochondrial band was collected and washed twice with the same medium.\"\n",
            "9. \"The final mitochondrial pellet was resuspended in a small volume of the same medium.\"\n",
            "\n",
            "**Conclusion:** The article provides a detailed protocol for isolating mitochondria from Ustilago maydis protoplasts. The steps include enzymatic digestion of cells, homogenization, centrifugation to remove debris, Percoll gradient centrifugation, and washing of the mitochondrial pellet. The protocol is complete and provides sufficient information for the isolation process.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.txt\n",
            "Protocol: **Article:** Isolation and reconstruction of cardiac mitochondria from SBEM images using a deep learning-based method\n",
            "\n",
            "**Introduction:**\n",
            "1. \"Mitochondrial morphology is maintained by a balance between the dynamic processes of fission and fusion.\"\n",
            "2. \"Electron micrograph images can provide snapshots of the interplay of these processes, allowing estimation of the homeostasis between them.\"\n",
            "3. \"Morphological defects in mitochondrial morphology are commonly reported in pathologies associated with cardiac dysfunction, such as cardiomyopathies, diabetes mellitus, liver diseases, neurodegeneration, aging, and cancer.\"\n",
            "4. \"Defects in mitochondrial morphology are usually evaluated qualitatively, and quantitative 2-D data are available, but 3-D measurements are scarce.\"\n",
            "\n",
            "**Conclusion:** The introduction provides background information on mitochondrial morphology and its association with cardiac dysfunction. It mentions the limitations of qualitative evaluation and the scarcity of 3-D measurements.\n",
            "\n",
            "Note: The introduction does not provide specific steps for mitochondrial isolation.\n",
            "\n",
            "**Overall Protocol Completeness:** Incomplete. The article does not provide specific steps for mitochondrial isolation.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"For segmentation, we used CDeep3M (Haberl et al., 2018), a cloud-based deep learning network for imaging microscopy datasets. The interior volumes and exterior boundaries of mitochondria were separately segmented.\"\n",
            "2. \"For the separation processing, we propose a method to isolate individual mitochondria whose assessment is hampered by packed alignment and the dense structure of the inner mitochondrial membrane. This method isolates individual mitochondria by subtracting thresholded inner and outer boundaries and using erosion-dilation and expansion to maintain membrane connectivity in 3-D.\"\n",
            "3. \"Finally, we used the new method to segment mitochondria in SBEM datasets from two mouse ventricular cardiomyocytes, and to analyze their sizes and morphologies.\"\n",
            "\n",
            "**Incompleteness or Uncertainty:**\n",
            "\n",
            "- The article does not provide specific details about the steps involved in the segmentation and separation processes.\n",
            "- The article does not mention the specific parameters or thresholds used in the segmentation and separation methods.\n",
            "- The article does not describe the exact steps used to analyze the sizes and morphologies of the segmented mitochondria.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The article describes the use of CDeep3M, a cloud-based deep learning network, for the segmentation of mitochondrial interior volumes and exterior boundaries. It also proposes a method for the separation of individual mitochondria using thresholding, erosion-dilation, and expansion. However, the article lacks specific details and parameters for these steps, as well as the steps for analyzing the sizes and morphologies of the segmented mitochondria.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for mitochondrial isolation:\n",
            "\n",
            "1. \"Mitochondrial interior labels and 6-pixel-wide boundary labels were exported.\"\n",
            "2. \"Mitochondria were automatically separated using the interior volume and boundary label probability distributions.\"\n",
            "3. \"The division process consisted of three steps: (a) subtraction of boundary probability from interior probability; (b) 3-D kernels extraction; and (c) expansion.\"\n",
            "4. \"Subtraction of boundary from interior probability distributions: Conventional image-processing techniques were applied using both interior and boundary probability distributions to separate mitochondria accurately. A threshold of learned boundary probability was set to eliminate most cristae but leave the outer membrane. This thresholded boundary probability was subtracted from the interior probability to highlight the mitochondrial boundary (subtracted images).\"\n",
            "5. \"3-D kernels extraction: Subtracted images were treated with 3-D Gaussian filtering (21 × 21 × 3 pixels) to weaken the incorrect connections and fill the scoops by cristae, and then binarized to separate mitochondria. 3-D filling of the remaining and closed mitochondrial space extracted the kernels (one per mitochondrion). Very small kernels were mostly artifacts and were therefore eliminated when below a threshold.\"\n",
            "6. \"Expansion: Pixel-by-pixel expansion was applied iteratively until the boundary was encountered or a pixel already identified as belonging to another mitochondrion was encountered.\"\n",
            "\n",
            "**Incompleteness:** The article does not provide specific details on the techniques used for conventional image processing or the threshold values used for the subtraction of boundary probability from interior probability.\n",
            "\n",
            "**Conclusion:** The article provides a general overview of the steps involved in mitochondrial isolation, including the use of interior and boundary probability distributions, subtraction of boundary probability from interior probability, 3-D kernels extraction, and pixel-by-pixel expansion. However, specific details regarding the techniques used for image processing and the threshold values are missing, which makes it difficult to replicate the protocol accurately.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, there is no mention of a specific article or any relevant sections related to mitochondrial isolation. Therefore, it is not possible to extract mitochondrial isolation steps from this text.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Protocol: Based on the provided text, there is no information about mitochondrial isolation steps. The text mainly discusses the accuracy and effects of intensity adjustment on segmentation in the context of cell images. Therefore, it is not possible to extract any mitochondrial isolation steps from this text.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Protocol: Based on the provided text, it is not possible to extract the steps for mitochondrial isolation. The text primarily discusses the separation and assessment of mitochondria, but does not provide any specific details about the isolation process.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"This study employed advances in deep learning-based image analysis to automate 3-D segmentation and isolation of individual mitochondria from SBEM images and achieved accuracies comparable to human manual segmentation.\"\n",
            "2. \"Our method can analyze interactions between nanoscopic ultrastructures.\"\n",
            "3. \"For example, we would like to target electron-dense bridges between mitochondria and the sarcoplasmic reticulum and intermitochondrial junctions electrically connecting adjacent mitochondria.\"\n",
            "4. \"Three-dimensional analysis of healthy mouse cardiomyocyte mitochondria was performed for the full volume of two SBEM stacks of image slices.\"\n",
            "5. \"The number of reconstructed mitochondria was 3636, of which 2315 were non-edge-cut, and the total number of cross-sectional contours was 45,125.\"\n",
            "6. \"The volume fraction of mitochondria was 26% and 29% in cells A and B, respectively.\"\n",
            "\n",
            "**Incompleteness or Uncertainties:**\n",
            "- The article does not provide specific steps for mitochondrial isolation. It focuses on the use of deep learning-based image analysis for segmentation and isolation of individual mitochondria.\n",
            "- The article mentions targeting electron-dense bridges between mitochondria and the sarcoplasmic reticulum and intermitochondrial junctions, but it does not describe the isolation process for these structures.\n",
            "\n",
            "**Conclusion:**\n",
            "The article does not provide a complete protocol for mitochondrial isolation. It mainly discusses the use of deep learning-based image analysis for segmentation and isolation of individual mitochondria. The mention of targeting specific structures suggests a potential focus for future research, but the steps for isolating these structures are not described.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4231 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: Based on the provided text, it is difficult to identify the specific steps for mitochondrial isolation. The text primarily discusses the size and morphology of mitochondria in different studies, as well as the clustering and potential fusion of mitochondria. There is no clear mention of a protocol or detailed steps for isolating mitochondria. Therefore, it is not possible to accurately extract the steps for mitochondrial isolation from this text.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Based on the provided text, there is no specific section that describes the steps for mitochondrial isolation. The text mainly discusses the movement, fission, and fusion of mitochondria within cardiomyocytes, as well as the limitations and future perspectives of image-based modeling. Therefore, there are no isolation steps to extract from this article.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: **Article:** PLoS Biol 2 27\n",
            "\n",
            "**Relevant Sections:** \"Materials and Methods,\" \"Mitochondrial Isolation\"\n",
            "\n",
            "**Extracted Steps:**\n",
            "1. \"Mitochondria were isolated from mouse liver by differential centrifugation.\"\n",
            "2. \"Liver tissue was homogenized in a buffer containing 0.25 M sucrose, 10 mM Tris-HCl (pH 7.4), and 1 mM EDTA.\"\n",
            "3. \"The homogenate was centrifuged at 600 x g for 10 minutes at 4°C to remove nuclei and unbroken cells.\"\n",
            "4. \"The supernatant was collected and centrifuged at 12,000 x g for 15 minutes at 4°C to pellet the mitochondria.\"\n",
            "5. \"The mitochondrial pellet was resuspended in a buffer containing 0.25 M sucrose and 10 mM Tris-HCl (pH 7.4).\"\n",
            "\n",
            "**Incompleteness/Uncertainties:** None\n",
            "\n",
            "**Conclusion:** The article provides a complete protocol for mitochondrial isolation from mouse liver using differential centrifugation. No protocol gaps or uncertainties are noted.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Protocol: **Article:** Mitochondrial morphology transitions and functions: Implications for retrograde signaling Am J Physiol - Regul Integr Comp Physiol 304\n",
            "\n",
            "**1.** \"Mitochondrial isolation was performed as previously described (Pinali et al., 2013; Pinali and Kitmitto, 2014).\" (No specific steps mentioned)\n",
            "\n",
            "**2.** \"Cardiac mitochondria were isolated by differential centrifugation.\" (No further details provided)\n",
            "\n",
            "**3.** \"Mitochondria were isolated using a standard differential centrifugation method.\" (No further details provided)\n",
            "\n",
            "**4.** \"Mitochondria were isolated by differential centrifugation as previously described (Smith and Page, 1976).\" (No further details provided)\n",
            "\n",
            "**5.** \"Mitochondria were isolated by differential centrifugation using a sucrose gradient.\" (No further details provided)\n",
            "\n",
            "**Conclusion:** The article mentions several instances of mitochondrial isolation using differential centrifugation, but no specific steps or protocols are provided. The protocol gaps make it difficult to determine the exact procedure used for mitochondrial isolation.\n",
            "Storing data for file_id: Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.txt\n",
            "Protocol: Based on the provided article, the steps for isolating mitochondria from procyclic Trypanosoma brucei are as follows:\n",
            "\n",
            "1. Growth and Harvesting of Cells: \"Procyclic T. brucei cells are grown in SDM-79 medium supplemented with 10% fetal calf serum (FCS) at 27°C to a density of 1-2 × 10^7 cells/ml. Cells are harvested by centrifugation at 1,500 × g for 10 min at 4°C.\"\n",
            "\n",
            "2. Lysis under Hypotonic Conditions: \"The cells are lysed under hypotonic conditions.\"\n",
            "\n",
            "3. Isolation of Mitoplast Vesicles on Percoll Gradients: \"Mitoplast vesicles are isolated on Percoll gradients.\"\n",
            "\n",
            "OR\n",
            "\n",
            "2.1 Isolation of Mitochondria: Isotonic Procedure\n",
            "2.1.1 Growth and Harvesting of Cells: \"Procyclic T. brucei cells are grown in SDM-79 medium supplemented with 10% fetal calf serum (FCS) at 27°C to a density of 1-2 × 10^7 cells/ml. Cells are harvested by centrifugation at 1,500 × g for 10 min at 4°C.\"\n",
            "\n",
            "2. Lysis by N2 Cavitation: \"Lysis occurs isotonically by N2 cavitation.\"\n",
            "\n",
            "3. Isolation of Mitochondrial Vesicles by Nycodenz Gradient Centrifugation: \"The mitochondrial vesicles are isolated by Nycodenz gradient centrifugation.\"\n",
            "\n",
            "Note: The article mentions two methods for isolating mitochondria from procyclic Trypanosoma brucei. The first method involves lysis under hypotonic conditions and isolation of mitoplast vesicles on Percoll gradients. The second method involves lysis by N2 cavitation and isolation of mitochondrial vesicles by Nycodenz gradient centrifugation. The article does not provide the specific steps for each method, only mentioning the general approach. Therefore, the protocol gaps exist in terms of the detailed steps for each method.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Error in summarizing article: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4457 tokens (3657 in the messages, 800 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            " Occured in generate_summary function\n",
            "Error in summarizing article: argument of type 'NoneType' is not iterable\n",
            " Using last summary\n",
            "Error in extracting knowledge: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4250 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error in extracting combined knowledge: Error in extracting knowledge\n",
            " Trying again\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "**Hypotonic Cell Breakage and Deoxyribonuclease Digestion:**\n",
            "1. Prepare hypotonic lysis buffer as a 10X stock: 1 mM Tris-HCl, pH 8.0, 1 mM EDTA, pH 8.0.\n",
            "2. Use a 40-mL Dounce tissue homogenizer with a large clearance pestle.\n",
            "3. Use a 5-L Pressure vessel, Luer-Lok syringe, and hypodermic needles (no. 26 and no. 25).\n",
            "4. Prepare a sucrose stock of 1.75 M.\n",
            "5. Use a centrifuge with a fixed-angle rotor, 8 × 50 mL capacity, and eight 50-mL centrifuge tubes.\n",
            "6. Prepare STM buffer as a 4X stock: 20 mM Tris-HCl, pH 8.0, 0.25 M sucrose, 5 mM MgCl2.\n",
            "7. Use Deoxyribonuclease (DNase) I from bovine pancreas, grade II.\n",
            "8. Prepare STE buffer as a 4X stock: 20 mM Tris-HCl, pH 8.0, 0.25 M sucrose, 2 mM EDTA, pH 8.0.\n",
            "\n",
            "**Percoll Step Gradients:**\n",
            "1. Use an ultracentrifuge with a large swinging bucket ultracentrifuge rotor, 6 × 38.5 mL capacity, and six Ultra-Clear centrifuge tubes (38.5 mL).\n",
            "2. Use Percoll 100% and keep it at 4°C.\n",
            "3. Prepare STE buffer containing 20, 25, 30, 35, and 75% (v/v) of Percoll each and keep it at 4°C.\n",
            "4. Use a 40-mL Dounce tissue homogenizer with a small clearance pestle.\n",
            "5. Use a 10-mL syringe with an attached 100-μL glass capillary.\n",
            "\n",
            "**Removal of Percoll and Storage:**\n",
            "1. Use a centrifuge with a fixed-angle rotor, 8 × 50 mL capacity, and eight 50-mL centrifuge tubes.\n",
            "2. Use STE buffer.\n",
            "3. Use a BCA protein assay kit.\n",
            "4. Use fatty acid-free bovine serum albumin (BSA) and prepare a 100-mg/mL stock.\n",
            "\n",
            "**Isolation of Mitochondria: Isotonic Procedure:**\n",
            "**Growth and Harvesting of Cells:**\n",
            "Refer to Subheading 2.1.1 for details.\n",
            "\n",
            "**Isotonic Cell Breakage, DNase Digestion, and Low-Speed Spins:**\n",
            "1. Prepare SoTE buffer as a 2X stock: 20 mM Tris-HCl, pH 7.5, 0.6 M sorbitol, 2 mM EDTA, pH 7.5.\n",
            "2. Use a cell disruption bomb for N2 cavitation with a capacity of 920 mL.\n",
            "3. Use a centrifuge with a fixed-angle rotor, 8 × 50 mL capacity, and 50-mL centrifuge tubes.\n",
            "4. Prepare SoTM buffer as a 4X stock: 20 mM Tris-HCl, pH 8.0, 0.6 M sorbitol, 5 mM MgCl2.\n",
            "5. Use a Luer-Lok syringe and a hypodermic needle (no. 25).\n",
            "6. Use DNase I from bovine pancreas, grade II.\n",
            "7. Use EDTA stock: 0.5 M, pH 7.5.\n",
            "\n",
            "**Nycodenz Step Gradients:**\n",
            "1. Use an ultracentrifuge with a large swinging bucket ultracentrifuge rotor, 6 × 38.5 mL\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Protocol: Steps for mitochondrial isolation:\n",
            "\n",
            "1. Spin cells in 500-mL centrifuge bottles in a fixed-angle rotor at 4°C for 10 min at 11,000 ×g.\n",
            "2. Determine cell concentration by microscopic counting using a disposable counting chamber during centrifugation.\n",
            "3. Remove the medium immediately after centrifugation.\n",
            "4. Add fresh cell culture to the same set of centrifuge bottles and repeat step 3.\n",
            "5. Resuspend pellets in a small volume of wash buffer, combine pellets in one bottle, add wash buffer to approximately 450 mL, and spin as in step 3.\n",
            "6. Resuspend cell pellet in hypotonic lysis buffer at 1.2 × 10^9 cells/mL.\n",
            "7. Homogenize the cell suspension in a 40-mL glass Dounce tissue homogenizer using the large clearance pestle.\n",
            "8. Pour the suspension into a pressure vessel sitting in an ice bath and containing a magnetic stirrer.\n",
            "9. Close the vessel, lock the outlet valve, and apply 5 bars of pressure using N2.\n",
            "10. Attach a hypodermic needle to the outlet of the pressure vessel.\n",
            "11. Open the outlet valve, collect the suspension in a glass beaker on ice, and measure the volume.\n",
            "12. Add one-sixth volume of 1.75 M sucrose stock to reestablish isotonic conditions.\n",
            "13. Examine lysis microscopically.\n",
            "14. Spin lysate in 50-mL centrifuge tubes in a fixed-angle rotor at 4°C for 10 min at 17,500 g.\n",
            "15. Pour out supernatants. The cloudy supernatants represent the cytosolic fraction.\n",
            "16. Resuspend pellets by vortexing in one-sixth volume of STM buffer and pool in a single tube.\n",
            "17. Estimate the total volume of the resuspended pellets.\n",
            "18. Add solid DNase I to a final concentration of 0.1 mg/mL.\n",
            "19. Push the extract through a hypodermic needle using a Luer-Lok syringe.\n",
            "20. Incubate for 45 min at 4°C, repeating step 19 during the incubation.\n",
            "21. Add an equal volume of STE buffer.\n",
            "22. Add 1/125 of the total volume of EDTA stock (0.5 M).\n",
            "23. Spin in 50-mL centrifuge tubes at 4°C for 10 min at 17,500 g. The resulting pellet will be very soft, so the cloudy supernatant should be removed with a pipet.\n",
            "24. Precool a large swinging bucket ultracentrifuge rotor.\n",
            "25. Determine the number of gradients needed (two, four, or six) and load each gradient with lysate corresponding to 2.0-3.5 × 10^10 cell-equivalents.\n",
            "26. Prepare Percoll step gradients by overlaying cold 35% Percoll containing STE buffer with cold 30/25/20% Percoll containing STE buffer in ultracentrifuge tubes.\n",
            "27. Resuspend the pellets from step 23 in a total volume of 3-6 mL 75% Percoll containing STE buffer per gradient and pool them.\n",
            "28. Homogenize the pooled pellets in a 40-mL glass Dounce tissue homogenizer using the small clearance pestle.\n",
            "29. Place equal volumes of the samples (3-6 mL) at the bottom of each gradient using a 10-mL hypodermic syringe with the attached glass capillary.\n",
            "30. Balance the gradients with 20% Percoll containing STE buffer on an electronic balance.\n",
            "31. Spin the gradients in the large swinging bucket ultracentrifuge rotor at 4°C for 45 min at 100,000 g.\n",
            "32. After centrifugation, the gradients should show three bands, with the middle one at the 25/30% Percoll interphase\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Protocol: Steps for mitochondrial isolation from the article:\n",
            "\n",
            "1. Dilute the combined pellets in the resulting four tubes to 50 mL with STE buffer and repeat steps 2 and 3. The obtained pellets will be tighter now, and essentially all STE can be removed.\n",
            "2. Combine all four pellets into one tube, dilute to 50 mL with STE buffer, and repeat steps 2 and 3.\n",
            "3. Resuspend the mitoplast pellet in a small volume of STE buffer and examine it in the microscope. The fraction should look as shown in Fig 1A.\n",
            "4. Take a small aliquot and determine the protein concentration by the BCA protein assay kit.\n",
            "5. Aliquots of mitoplasts can directly be flash frozen in liquid N2 and stored at -70°C. However, the best way to preserve the membrane integrity of the mitoplast is to add one-ninth volume of 100 mg/mL fatty acid-free BSA before freezing.\n",
            "\n",
            "Isotonic Procedure:\n",
            "1. For growth and harvesting of cells, refer to Subheading 3.1.1.\n",
            "2. Resuspend pellet in SoTE buffer at 2 × 10^9 cells/mL. Take a small sample as a control for the microscopic examination of the extent of the cell lysis.\n",
            "3. Homogenize in a 40-mL glass Dounce tissue homogenizer using the large clearance pestle.\n",
            "4. Put cell suspension into the cell disruption bomb sitting in an ice bath. Close the bomb and lock the outlet valve.\n",
            "5. Apply 55 bars using N2 and close the inlet valve. Incubate for 30 min under constant stirring while the bomb sits in an ice bath.\n",
            "6. Depressurize the cell disruption bomb and collect the foamy suspension.\n",
            "7. Examine lysis microscopically by comparing samples before and after lysis.\n",
            "8. Spin lysate in 50-mL centrifuge tubes in a fixed-angle rotor at 4°C for 10 min at 24,000 g. Pour out supernatants. The supernatants represent the cytosolic fraction and may be kept depending on the experiment.\n",
            "\n",
            "DNase Digestion:\n",
            "1. Resuspend pellet in equal volume of SoTM buffer.\n",
            "2. Add solid DNase I to 0.1 mg/mL final concentration.\n",
            "3. Push the extract through a hypodermic needle (no 25, orange) using a Luer-Lok syringe.\n",
            "4. Incubate for 45 min at 4°C. During incubation, repeat step 3. The viscosity should drop during DNase digestion.\n",
            "5. Add an equal volume of STE buffer.\n",
            "6. Add 1/125 volume of EDTA stock (0.5 M).\n",
            "\n",
            "Low-Speed Spins:\n",
            "1. Spin lysate in 50-mL centrifuge tubes in a fixed-angle rotor at 4°C for 10 min at 490 g. Fill tubes to the top; if necessary, add SoTE buffer.\n",
            "2. Transfer supernatant to a beaker and keep on ice. The pellet will be very soft; thus, leave 1–2 mL of the supernatant in the tube.\n",
            "3. Resuspend each pellet in approx 10–15 mL SoTE by homogenizing in a 40-mL glass Dounce tissue homogenizer using the large clearance pestle. Pool supernatants.\n",
            "4. Spin in 50-mL centrifuge tubes in a fixed-angle rotor at 4°C for 10 min at 375 g. Fill tubes to the top; if necessary, add SoTE buffer.\n",
            "5. Pool supernatants with the previous ones.\n",
            "6. Distribute the pooled supernatants to 50-mL centrifuge tubes and spin in a fixed-angle rotor at 4°C for 10 min at 24,000 g. Discard supernatants.\n",
            "\n",
            "Nycodenz Step Gradients:\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Protocol: 1. Prepare Nycodenz step gradients: pipet 8 mL cold 28% Nycodenz containing SoTE buffer into each 38.5-mL ultracentrifuge tube. Carefully overlay 8 mL each of cold 25/21/18% Nycodenz containing SoTE buffer.\n",
            "2. Resuspend pellets in a total volume of 3-6 mL 50% Nycodenz containing SoTE buffer per gradient, pool them, and homogenize in a 40-mL glass Dounce tissue homogenizer using the small clearance pestle.\n",
            "3. Place equal volumes of the samples (3-6 mL) at the bottom of each gradient using a 10-mL hypodermic syringe with the attached glass capillary.\n",
            "4. Balance gradients with 18% Nycodenz containing SoTE buffer on an electronic balance.\n",
            "5. Spin gradients in the large swinging bucket ultracentrifuge rotor at 4°C for 45 min at 100,000 g.\n",
            "6. After centrifugation, the gradients should show three bands. The middle band at the 25/28% Nycodenz interphase, which is the most prominent one, is enriched for mitochondrial vesicles.\n",
            "7. Collect approximately 5 mL of this band (25/28% Nycodenz interphase) using the same 10-mL hypodermic syringe with the attached glass capillary used for loading.\n",
            "8. Distribute the collected 25/28% Nycodenz interphase mitochondrial fractions of all gradients into 50-mL centrifuge tubes and dilute at least fivefold with SoTE buffer.\n",
            "9. Cap tubes with two layers of parafilm and mix by inversion.\n",
            "10. Spin in a fixed-angle rotor at 4°C for 15 min at 33,000 g and discard supernatants.\n",
            "11. Resuspend pellets in approximately 1 mL per gradient of SoTE buffer and pool. Distribute the resulting suspension into 1.5-mL Eppendorf tubes.\n",
            "12. Spin in an Eppendorf centrifuge at approximately 10,000 g and discard as much supernatant as possible.\n",
            "13. Resuspend the mitochondrial pellet in a small volume of SoTE buffer and examine by microscope. The fraction should look as shown in Fig 1B.\n",
            "14. Take a small aliquot and determine the protein concentration by the BCA protein assay kit.\n",
            "15. Aliquots of mitochondrial vesicles can directly be flash frozen in liquid N2 and stored at -70°C. However, the best way to preserve the membrane integrity of the mitochondrial fraction is to add one-ninth volume of 100-mg/mL fatty acid-free BSA before freezing.\n",
            "\n",
            "**Incompleteness:** The article does not provide details for steps 11 and 14.\n",
            "\n",
            "**Conclusion:** The protocol for mitochondrial isolation is mostly complete, with clear steps for each stage of the process. However, there are two missing steps, 11 and 14, which are not described in the article. These missing steps may contain important information for the protocol.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. If no pressure vessel is available, cells can be lysed manually using a 20- to 40-mL Luer-Lok syringe and pushing them once or twice with as much force as possible through a no 26 hypodermic needle.\n",
            "2. The time the lysed cells remain in the hypotonic lysis buffer before the sucrose is added is critical and should be minimized since otherwise the mitochondrial vesicles will lyse as well.\n",
            "3. Lysis is expected to be complete. Thus, cell fragments, flagella, and floating vesicles but no live cells are observed.\n",
            "4. A white floating layer will appear on the solution. This layer probably represents broken membranes and is indicative of efficient cell lysis; it can most easily be removed using a paper tissue.\n",
            "5. DNase digestion is essential to allow efficient separation on either Percoll or Nycodenz gradients.\n",
            "6. Addition of EDTA complexes the magnesium and thus stops the DNase digestion. Furthermore, addition of EDTA also serves to prevent aggregation of mitochondrial preparations, which is observed in the presence of magnesium.\n",
            "7. It is recommended to load no more than 3.5 × 10^10 cell equivalents on the gradients, although higher loadings may be tolerated.\n",
            "8. The dilution of the pellet with 75% Percoll containing STE buffer in the hypotonic procedure or with the 50% Nycodenz containing SoTE buffer in the isotonic preparation needs to be sufficient to allow the suspension to sink beneath the lowest layers of the step gradients.\n",
            "9. It is best to insert the capillary into the gradient along the tube wall and to keep it there until the whole sample has been applied. First, load a small volume of the sample only and wait a few seconds to make sure it remains at the bottom of the tube. If it floats up, then remove the syringe, add more of the 75% Percoll containing STE buffer for the hypotonic procedure or 50% Nycodenz containing SoTE for the isotonic preparation, and try again.\n",
            "10. Microscopic examination shows that the top band (20/25% Percoll interphase), which is the most intense one, mainly contains flagella and some cell fragments, whereas the lowest band (35/75% Percoll interphase), which normally is the least intense, contains a uniform population of vesicular structures of unknown origin that are much smaller than the ones observed in the mitoplast fraction. Large mitoplast vesicles are seen in all three fractions but are most enriched in the central part of the gradient (25/30% Percoll interphase).\n",
            "11. The main contaminants of the central mitoplast fraction are flagella. Depending on the cell line and the efficiency of the cell lysis, there can be variations in the relative intensities of the three zones. Furthermore, if only little material is loaded on the gradient, then it is possible that no clear accumulation of material is seen in the central zone. Although the yield will be low in this case, it is still worthwhile to collect the zone and to proceed with the purification as a mitoplast pellet may only become visible after the washes.\n",
            "12. Percoll does not mix easily, so vigorous mixing is important. The more the Percoll is diluted, the tighter the pellets will be that are obtained after centrifugation.\n",
            "13. The obtained yields can be quite variable but are typically in the range of 20 mg protein or 250–300 µg RNA per 10^11 cells.\n",
            "14. Although the integrity of the mitoplasts or the mitochondrial fractions during storage at -70°C is best maintained in the presence of BSA, it should be considered that the presence of BSA might interfere with some downstream applications, such as Western blot analysis.\n",
            "15. Pressure will slightly drop during incubation; therefore, readjust to 55 bar after the first 5 min.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: **Article:** Schneider, A., and Marechal-Drouard, L. (2000). Mitochondrial tRNA import: are there distinct mechanisms. Trends Cell Biol, 10, 509–513.\n",
            "\n",
            "**1. Read Article:**\n",
            "I have read the article and identified the relevant section for mitochondrial isolation.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "- \"Mitochondria were isolated from procyclic Trypanosoma brucei as described previously (Braly et al., 1974).\" (Schneider and Marechal-Drouard, 2000)\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "The article refers to a previous study by Braly et al. (1974) for the detailed protocol of mitochondrial isolation. The steps are not provided in this article.\n",
            "\n",
            "**4. Format Steps:**\n",
            "1. Mitochondria were isolated from procyclic Trypanosoma brucei as described previously (Braly et al., 1974).\n",
            "\n",
            "**5. Indicate Uncertainties:**\n",
            "None.\n",
            "\n",
            "**Conclusion:**\n",
            "The article mentions the isolation of mitochondria from procyclic Trypanosoma brucei but does not provide the detailed protocol. The reader is referred to a previous study by Braly et al. (1974) for the complete protocol.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.txt\n",
            "Protocol: **Isolation of Mitochondria from Tissue Culture Cells**\n",
            "\n",
            "**Materials:**\n",
            "- Cell pellet derived from 1 –5×10^9 tissue culture cells\n",
            "- MS homogenization buffer (1× and 2.5×)\n",
            "- RSB hypo buffer\n",
            "\n",
            "**Equipment:**\n",
            "- Centrifuge tubes\n",
            "- Dounce homogenizer (15 mL) with a tight-fitting B pestle or Potter-Elvehjem homogenizer (5 mL) with a Teflon pestle\n",
            "- Phase contrast microscope\n",
            "\n",
            "**Method:**\n",
            "1. \"Resuspend the cell pellet in 11 mL of ice-cold RSB hypo buffer and transfer the cells to a 15-mL Dounce homogenizer. Alternatively, resuspend the cell pellet in 9 mL of ice-cold RSB hypo buffer and transfer 3 mL of the cells at a time to a 5-mL Potter-Elvehjem homogenizer with a Teflon pestle.\"\n",
            "2. Allow the cells to swell for 5-10 min. Check the progress of the swelling using a phase-contrast microscope.\n",
            "3. Break open the swollen cells with several strokes of the B pestle. If a Potter-Elvehjem homogenizer is used, break open the cells with the Teflon pestle rotating at 1600 rpm.\n",
            "4. Check the degree of homogenization with a phase-contrast microscope. Naked nuclei, smaller organelles, and a small number of unbroken cells should be present if cell lysis was successful. Troubleshooting may be required.\n",
            "5. Immediately add 8 mL of 2.5× MS homogenization buffer to give a final concentration of 1× MS homogenization buffer. Cover the top of the homogenizer with Parafilm and mix by inverting a couple of times. Save a portion of the homogenate if marker enzyme assays are to be performed later.\n",
            "6. Transfer the homogenate to a centrifuge tube for differential centrifugation. Rinse the homogenizer with a small amount of 1× MS homogenization buffer and add it to the homogenate. Bring the volume to 30 mL with 1× homogenization MS buffer.\n",
            "7. Centrifuge the homogenate at 1300 g for 5 min to remove nuclei, unbroken cells, and large membrane fragments.\n",
            "8. Pour the supernatant into a clean centrifuge tube. Be careful not to collect the loose top of the pellet with the supernatant.\n",
            "9. Repeat Steps 6 and 7 two more times.\n",
            "10. Transfer the supernatant to a clean centrifuge tube and pellet the mitochondria at 7,000 g–17,000 g for 15 min.\n",
            "11. Discard the supernatant and wipe out the inside of the tube.\n",
            "12. Wash the mitochondria by resuspending the pellet in 1× MS buffer and repeating the 7,000 g–17,000 g sedimentation. This step is not necessary if a density gradient will be performed.\n",
            "13. Discard the supernatant and resuspend the pellet in a buffer suitable for subsequent work. The mitochondria can be stored at -80˚C for at least 1 year for some purposes.\n",
            "\n",
            "**Troubleshooting:**\n",
            "- Problem (Step 4): Too many or not enough cells have lysed. No troubleshooting steps provided.\n",
            "\n",
            "**Conclusion:**\n",
            "The protocol for isolating mitochondria from tissue culture cells is provided in the article. The protocol starts with resuspending the cell pellet in RSB hypo buffer, followed by swelling of the cells and breaking them open using a homogenizer. The homogenate is then subjected to differential centrifugation to remove nuclei, unbroken cells, and large membrane fragments. The supernatant is collected, and the mitochondria are pelleted and washed.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_tissue_culture_cells\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: Steps for isolating mitochondria from tissue culture cells:\n",
            "\n",
            "1. Resuspend cells in at least 5-10 times the volume of the cell pellet in a hypotonic buffer (MS Homogenization Buffer, 1x) containing:\n",
            "   - 210 mM mannitol\n",
            "   - 70 mM sucrose\n",
            "   - 5 mM Tris-HCl (pH 7.5)\n",
            "   - 1 mM EDTA (pH 7.5)\n",
            "\n",
            "2. Perform homogenization using a Dounce homogenizer, which disrupts swollen tissue culture cells by pressure change. As the pestle is pressed down, the pressure around the cell increases. When the cell slips past the end of the pestle, the sudden decrease in pressure causes the cell to rupture. If the pestle is very tight-fitting, there may be some mechanical breakage as well. If an excessive number of strokes are needed for good cell breakage, a tighter-fitting homogenizer is needed.\n",
            "\n",
            "3. Harvest the cells in stationary growth phase when the fewest cells will be actively dividing if purifying mitochondria to isolate mitochondrial DNA. Substitute CaCl2 for MgCl2 in the RSB Hypo Buffer to stabilize the nuclear membrane.\n",
            "\n",
            "4. Omit washing the mitochondrial pellet and any density gradient purification if purifying mitochondria to isolate mitochondrial DNA.\n",
            "\n",
            "5. Resuspend and lyse the mitochondrial pellet from Step 10 if purifying mitochondria to isolate mitochondrial DNA.\n",
            "\n",
            "6. Purify the mitochondrial DNA from any remaining nuclear DNA.\n",
            "\n",
            "**Incompleteness:** The article does not provide information on the number of strokes needed for homogenization or the specific steps for purifying mitochondrial DNA.\n",
            "\n",
            "**Conclusion:** The article provides a basic protocol for isolating mitochondria from tissue culture cells. However, there are some gaps in the protocol, such as the number of strokes needed for homogenization and the specific steps for purifying mitochondrial DNA. Additional information or modifications may be required for specific purposes, such as isolating mitochondrial DNA.\n",
            "Storing data for file_id: Isolation_of_mitochondria_from_tissue_culture_cells\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_mitochondria_from_tissue_culture_cells.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.txt\n",
            "Protocol: Steps for Isolation of Mitochondria from Minimal Quantities of Mouse Skeletal Muscle:\n",
            "\n",
            "1. Thaw frozen stores of 0.25% Trypsin, Isolation Buffer for Mitochondria (IBM) 1 and IBM2 in a 37 °C water bath.\n",
            "2. Rinse glassware and dissecting instruments in 70% ethanol followed by high purity water.\n",
            "3. Prepare 0.05% trypsin solution from 0.25% trypsin stock by diluting 1 part trypsin in 4 parts IBM1.\n",
            "4. Mix protease and phosphatase inhibitor cocktail with cell lysis buffer (1:100 ratio) in a 1.5 ml microcentrifuge tube with screw cap.\n",
            "5. Aliquot 5 ml (5 ml/mouse) of 0.05% trypsin into a 15 ml plastic tube.\n",
            "\n",
            "**Note:** The article does not provide a complete protocol for mitochondrial isolation. The listed steps only cover the setup process and the preparation of trypsin solution. The actual isolation steps are not mentioned.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Protocol: Mitochondrial Isolation Steps:\n",
            "\n",
            "1. Euthanize a mouse by CO2 inhalation, followed by cervical dislocation.\n",
            "2. Remove red muscle from the quadriceps and gastrocnemius muscle, which includes the soleus (~75-100 mg total) as described in the following steps:\n",
            "   - Peel the skin towards the mouse.\n",
            "   - Remove the fat pad over the quadriceps origin point.\n",
            "   - Cut the quadriceps tendon that is attached to the patella with fine-tipped scissors.\n",
            "   - Slowly snip the aponeurosis between the bone and the quadriceps, while avoiding the femoral artery, to liberate the quadriceps from the bone.\n",
            "   - Cut the tendon with fine-tipped scissors at the origin point on the femur to liberate the quadriceps muscle and place the quadriceps muscle in chilled PBS.\n",
            "   - Remove visible adipose tissue over the quadriceps with scissors.\n",
            "   - Flip the quadriceps over so that the portion of the muscle that was overlying the femur is facing up.\n",
            "   - Open the quadriceps muscle with forceps in a fanning motion.\n",
            "   - Remove the two visible red muscle portions from each lobe with fine-tipped scissors and place in a beaker containing 5 ml of chilled IBM1.\n",
            "   - Cut the skin overlying the Achilles tendon with fine-tipped scissors and peel the skin toward the mouse.\n",
            "   - Cut the exposed Achilles tendon and peel the muscle towards the body of the mouse.\n",
            "   - Cut the tendon at the lateral and medial condyles of the femur with fine-tipped scissors to liberate the gastrocnemius and place the gastrocnemius attached to its tendons in chilled PBS.\n",
            "   - Flip the gastrocnemius over and peel off the soleus muscle and place into the beaker containing 5 ml of chilled IBM1.\n",
            "   - Fan open the gastrocnemius.\n",
            "   - Remove the three visual red muscle portions (two lateral and one medial superficial red strips) with fine-tipped scissors and transfer them to the beaker containing 5 ml of chilled IBM1.\n",
            "3. Remove another ~75-100 mg of red muscle from the other leg of the mouse (as described in step 2.2) and place into a 1.5 ml microcentrifuge tube with protease and phosphatase inhibitor cocktail and cell lysis buffer (50 mM Tris-HCL, 1 mM EDTA, 150 mM NaCl, 1% Sodium dodecyl sulfate, 0.5% Sodium deoxycholate, 1% Polyoxyethylene (9) nonylphenylether, branched). Immediately flash-freeze this second sample in liquid nitrogen for later use in Western blotting (see step 6.1).\n",
            "4. Place a pre-chilled, flat plastic surface on ice in a large bucket. Pour a drop of IBM1 onto the plastic surface and use tweezers to place all of the sectioned red muscle (step 2.2.4 and 2.2.7) in IBM1 droplet. Mince the muscle tissue for 2 min using 3 single-edge razor blades by changing razors every 40 sec.\n",
            "5. Transfer the minced tissue to a new beaker with 5 ml of fresh IBM1 by holding the plastic surface over the beaker and scraping the muscle into the beaker with a razor blade. Take this solution and drain it through a 100 µm cell strainer placed onto a 50 ml conical tube.\n",
            "6. Blot the tissue with a delicate task wiper and then transfer it into 5 ml of the 0.05% trypsin solution. Use tweezers to remove the tissue from the task wiper.\n",
            "7. Incubate the muscle tissue on ice for 30 min in 0.05% trypsin solution.\n",
            "8.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Protocol: Steps for mitochondrial isolation:\n",
            "\n",
            "1. Cut off the end of a pipette tip and gently homogenize the pellet in IBM2 with mixing and stirring motions.\n",
            "2. Add another 4.5 ml of IBM2 to the mixture after the pellet is fully suspended.\n",
            "3. Spin the IBM2/tissue homogenate at 8,000 x g for 10 min at 4 °C.\n",
            "4. Remove the supernatant and gently, but completely re-suspend the pellet by adding two 25 µl increments of IBM2 with a pipette tip with its point cut off. Gently stir and mix the pellet after each addition of 25 µl of IBM2. Place this mitochondrial stock on ice.\n",
            "\n",
            "Note: The protocol does not provide information on the initial steps of mitochondrial isolation, such as tissue collection and homogenization.\n",
            "\n",
            "Conclusion: The provided steps outline the isolation of mitochondria from a tissue homogenate. However, the protocol lacks information on the initial steps of tissue collection and homogenization.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: 1. Prepare Stock Solutions:\n",
            "- Tris/HCl, pH 7.4: 2M, 250 mL, 78.8 g\n",
            "- Tris/HCl, pH 7.4: 1M, 250 mL, 39.39 g\n",
            "- KCl: 1M, 500 mL, 37.28 g\n",
            "- Tris Base: 1M, 100 mL, 12.11 g\n",
            "- EDTA, pH 8: 0.5M, 100 mL (in 1M Tris Base), 20.81 g\n",
            "- EGTA, pH 7.2: 0.1M, 100 mL (in 1M Tris Base), 3.801 g\n",
            "\n",
            "2. Prepare Isolation Buffer for Mitochondria 1 (IBM1):\n",
            "- D-Mannitol: 200 mM, 10.9 g\n",
            "- Sucrose: 70 mM, 7.188 g\n",
            "- EGTA/Tris Base: 0.1M, 15 mL, 5 mM\n",
            "- Tris-HCl, pH 7.4: 1M, 3 mL, 10 mM\n",
            "\n",
            "3. Prepare Isolation Buffer for Mitochondria 2 (IBM2):\n",
            "- Substrate Medium:\n",
            "  - Pyruvate/Malate: 10 mM/5 mM, RCR 16.2 ± 4.6\n",
            "  - Succinate/Rotenone: 10 mM/2 μM, RCR 10.6 ± 3.8\n",
            "  - Palmitoyl L-Carnitine/Malate: 40 μM/1 mM Malate, RCR 6.7 ± 0.6\n",
            "  - Glutamate/Malate: 10 mM/10 mM, RCR 8.6 ± 0.4\n",
            "\n",
            "**Conclusion:** The article provides detailed information on the preparation of stock solutions and isolation buffers for mitochondrial isolation. However, the actual steps for mitochondrial isolation are not mentioned in the provided text.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. Tissue mincing using razor blades after tissue excision.\n",
            "2. Homogenization at a reduced rotor speed of 80 rpm.\n",
            "3. Resuspension of the mitochondria pellet into IBM2.\n",
            "\n",
            "**Incompleteness or Uncertainties:**\n",
            "- The article does not provide a complete protocol for mitochondrial isolation.\n",
            "- The specific details of the homogenization and resuspension steps are not mentioned.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides some steps for mitochondrial isolation, including tissue mincing, homogenization at a reduced speed, and resuspension. However, the protocol is incomplete, lacking specific details for the homogenization and resuspension steps. Further information is needed to fully replicate the isolation procedure.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Protocol: **Article:** Fernandez-Vizarra, E., Lopez-Perez, M.J., & Enriquez, J.A Isolation of biogenetically competent mitochondria from mammalian tissues and cultured cells Methods (San Diego, Calif.) 26, 292-297, (2002)\n",
            "\n",
            "**1. Read Article:**\n",
            "The article is about a method for isolating mitochondria from mammalian tissues and cultured cells.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "- \"Tissue samples were homogenized in a Potter-Elvehjem homogenizer with 10 strokes at 900 rpm in ice-cold isolation medium (0.25 M sucrose, 10 mM Tris-HCl pH 7.4, 1 mM EDTA, 0.1% BSA)\"\n",
            "- \"The homogenate was centrifuged at 800 g for 10 min at 4°C to remove nuclei, unbroken cells, and large debris\"\n",
            "- \"The supernatant was collected and centrifuged at 10,000 g for 10 min at 4°C to pellet the mitochondria\"\n",
            "- \"The mitochondrial pellet was resuspended in isolation medium and centrifuged again at 10,000 g for 10 min at 4°C\"\n",
            "- \"The final mitochondrial pellet was resuspended in a small volume of isolation medium for further analysis\"\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "The article does not mention the specific duration or speed of the centrifugation steps. It also does not provide details about the resuspension volume in the final step.\n",
            "\n",
            "**4. Format Steps:**\n",
            "1. Homogenize tissue samples in a Potter-Elvehjem homogenizer with 10 strokes at 900 rpm in ice-cold isolation medium (0.25 M sucrose, 10 mM Tris-HCl pH 7.4, 1 mM EDTA, 0.1% BSA).\n",
            "2. Centrifuge the homogenate at 800 g for 10 min at 4°C to remove nuclei, unbroken cells, and large debris.\n",
            "3. Collect the supernatant and centrifuge it at 10,000 g for 10 min at 4°C to pellet the mitochondria.\n",
            "4. Resuspend the mitochondrial pellet in isolation medium and centrifuge it again at 10,000 g for 10 min at 4°C.\n",
            "5. Resuspend the final mitochondrial pellet in a small volume of isolation medium for further analysis.\n",
            "\n",
            "**5. Indicate Uncertainties:**\n",
            "- The duration and speed of the centrifugation steps are not mentioned.\n",
            "- The article does not specify the resuspension volume in the final step.\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides a protocol for isolating mitochondria from mammalian tissues and cultured cells. The steps mentioned are clear and complete, except for the missing details about centrifugation speed and duration, as well as the resuspension volume in the final step.\n",
            "Storing data for file_id: Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.txt\n",
            "Protocol: **Title: A critical comparison between two classical and a kit-based method for mitochondria isolation**\n",
            "\n",
            "**Authors:** Sonja Hartwig, Christian Feckler, Stefan Lehr, Katrin Wallbrecht, Heike Wolgast, Dirk Müller-Wieland, and Jörg Kotzka\n",
            "\n",
            "**Affiliations:** \n",
            "1. Institute of Clinical Biochemistry and Pathobiochemistry, German Diabetes Center at the Heinrich-Heine- University Düsseldorf, Leibniz Center for Diabetes Research, Düsseldorf, Germany\n",
            "2. Qiagen GmbH, Hilden, Germany\n",
            "3. Institute for Diabetes Research, Department of General Internal Medicine, Asklepios Klinik St Georg, Hamburg, Germany\n",
            "\n",
            "**Received:** April 18, 2008\n",
            "**Revised:** February 5, 2009\n",
            "**Accepted:** February 14, 2009\n",
            "\n",
            "**Objective:** Compare three methods for isolating intact mitochondria from mouse liver tissues based on yield, purity, and activity.\n",
            "\n",
            "1. \"Mitochondria were isolated by sucrose density gradient ultracentrifugation, free-flow electrophoresis, or a commercially available kit-based method.\" \n",
            "(Note: The article does not provide details of the specific steps involved in each method.)\n",
            "\n",
            "2. \"Our analyses show that the sophisticated (and most expensive) free-flow electrophoresis method enables isolation of intact mitochondria with an enrichment of approximately 70%.\"\n",
            "\n",
            "3. \"Using the classical density centrifugation method is very laborious and time-consuming, but delivers about 57% intact mitochondria.\"\n",
            "\n",
            "4. \"Using standard laboratory equipment in a quick and simple procedure, the kit provides approximately 50% intact mitochondria, suitable for most standard investigations.\"\n",
            "\n",
            "**Conclusion:**\n",
            "The article compares three methods for isolating intact mitochondria: sucrose density gradient ultracentrifugation, free-flow electrophoresis, and a commercially available kit-based method. The free-flow electrophoresis method is the most sophisticated and expensive, resulting in approximately 70% enrichment of intact mitochondria. The classical density centrifugation method is laborious and time-consuming but yields about 57% intact mitochondria. The kit-based method, using standard laboratory equipment, provides approximately 50% intact mitochondria. The article does not provide detailed protocols for each method, leaving gaps in the understanding of the isolation steps.\n",
            "Storing data for file_id: A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "Protocol: Steps for mitochondrial isolation from the article:\n",
            "\n",
            "1. \"Prior to centrifugation, samples were prepared according to the workflow shown in Fig 1A.\"\n",
            "2. \"Crude mitochondria fraction (fraction F) was layered onto the linear sucrose gradient.\"\n",
            "3. \"The collected mitochondria fraction (lane H) banded in a sucrose density of 41%.\"\n",
            "4. \"Marker enzyme assays showed a fraction with a mitochondrial purity of approximately 57%.\"\n",
            "5. \"Electron micrographs demonstrated the structural integrity of mitochondria, including the double-layer membrane and matrix.\"\n",
            "6. \"The traditional gradient centrifugation method resulted in a substantial enrichment of functional mitochondria.\"\n",
            "7. \"Differential centrifugations were performed before mitochondrial separation with ZE-FFE.\"\n",
            "8. \"FFE led to an enrichment of mitochondria and a notable decrease of contaminating organelles.\"\n",
            "9. \"Marker enzyme analysis showed a high enrichment of mitochondria (approximately 70%) with minor contamination by other organelles.\"\n",
            "10. \"Electron micrographs showed mitochondria with intact outer and inner membranes.\"\n",
            "11. \"The commercially available Qproteome TM mitochondria isolation kit can be performed using standard laboratory equipment.\"\n",
            "12. \"Specific enrichment of mitochondria and depletion of other organelles was demonstrated by Western blot analyses.\"\n",
            "13. \"Calculating the respective levels of organelle contamination by enzyme assays indicated an overall mitochondrial purity of approximately 50%.\"\n",
            "14. \"Electron micrographs and the JC-1 assay indicated that the kit-based method yielded structurally and functionally intact mitochondria.\"\n",
            "\n",
            "Incompleteness or uncertainties:\n",
            "- The article does not provide detailed steps for the preparation of samples prior to centrifugation.\n",
            "- The article does not provide specific details about the differential centrifugation steps before mitochondrial separation with ZE-FFE.\n",
            "- The article does not provide specific details about the steps involved in using the Qproteome TM mitochondria isolation kit.\n",
            "\n",
            "Conclusion:\n",
            "The article provides a detailed description of the steps involved in the sucrose density gradient method and the ZE-FFE method for mitochondrial isolation. However, there are missing protocol details for the sample preparation steps and the use of the Qproteome TM mitochondria isolation kit.\n",
            "Storing data for file_id: A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "Protocol: Based on the provided text, it is difficult to extract the specific steps for mitochondrial isolation. The text mainly discusses the comparison of different methods and the analysis of protein patterns. The steps for mitochondrial isolation are not clearly described.\n",
            "Storing data for file_id: A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "Protocol: **Mitochondrial Isolation Steps:**\n",
            "\n",
            "1. \"The complete isolation process – from tissue removal to purified mitochondrial fraction – can be performed in less than 2 h, whereas the two other methods require significantly more time (longer centrifugation/electrophoresis, SDH assay to select mitochondria-containing fractions).\"\n",
            "2. \"Moreover, due to the recommended tissue disruption by a special rotor stator, the overall recovery of protein in the mitochondria fraction is very high (approximately 8%), but the purity of prepared mitochondria is slightly compromised.\"\n",
            "\n",
            "**Conclusion:**\n",
            "The article provides some information on the mitochondrial isolation process. The complete process can be performed in less than 2 hours and involves tissue removal, centrifugation/electrophoresis, and the SDH assay to select mitochondria-containing fractions. However, there are gaps in the protocol, such as the specific centrifugation and electrophoresis steps, as well as the details of the SDH assay. The article also mentions tissue disruption using a special rotor stator, which leads to a high recovery of protein in the mitochondria fraction but compromises the purity of the prepared mitochondria.\n",
            "Storing data for file_id: A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "Protocol: **Article:** Weber, G., Islinger, M., Weber, P., Eckerskorn, C., Vo ¨lkl, A., Efficient separation and analysis of peroxisomal membrane proteins using free-flow isoelectric focusing. Electrophoresis 2004, 25, 1735–1747.\n",
            "\n",
            "**1. Read Article:**\n",
            "The article focuses on the efficient separation and analysis of peroxisomal membrane proteins using free-flow isoelectric focusing.\n",
            "\n",
            "**2. Extract Steps:**\n",
            "The article does not provide a detailed protocol for mitochondrial isolation. It primarily discusses the separation and analysis of peroxisomal membrane proteins using free-flow isoelectric focusing.\n",
            "\n",
            "**3. Note Incompleteness:**\n",
            "The article does not provide a complete protocol for mitochondrial isolation. It does not mention specific steps or reagents for mitochondrial isolation.\n",
            "\n",
            "**4. Format Steps:**\n",
            "N/A (No specific steps provided)\n",
            "\n",
            "**5. Indicate Uncertainties:**\n",
            "N/A (No specific steps provided)\n",
            "\n",
            "**Conclusion:**\n",
            "The article does not provide a detailed protocol for mitochondrial isolation. It focuses on the separation and analysis of peroxisomal membrane proteins using free-flow isoelectric focusing. Therefore, there are no specific steps or details to extract for mitochondrial isolation.\n",
            "Storing data for file_id: A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.json\n",
            "Successfully summarized /home/epas/Documents/MitoMAVEN/full_texts/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation.pdf\n",
            "Summarizing /home/epas/Documents/MitoMAVEN/full_texts/A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t.pdf\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t.json\n",
            "References extracted successfully to /home/epas/Documents/MitoMAVEN/full_texts/A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t.txt\n",
            "Protocol: **Steps:**\n",
            "\n",
            "1. \"We describe the isolation of E histolytica mitosomes by cellular fractionation and density gradient centrifugation.\"\n",
            "2. \"Mitosome isolation protocol based on various methods used to isolate mitochondria.\"\n",
            "3. \"Mitosome isolation protocol in sufficient detail to enable the isolation of large numbers of mitosomes.\"\n",
            "4. \"Entamoeba histolytica cells (trophozoites) strain HM-1:IMSS Clone 9 (ATCC 50528).\"\n",
            "5. \"LYI-S-2 axenic culture medium, pH 6.8 supplemented with 2% Vitamin Mix 18 and 10% adult bovine serum.\"\n",
            "6. \"Borosilicate glass culture tubes, 16 ×125 mm.\"\n",
            "7. \"225-cm2 Cell culture flasks, rectangular, angled neck, polystyrene, TC-treated with plug seal cap.\"\n",
            "8. \"1M Dithiothreitol (DTT): dissolve 0.154 g of DTT in 1 mL of H2O.\"\n",
            "9. \"1M MOPS-HCl, pH 7.2: add 23.12 g MOPS to 80 mL of H2O and adjust the pH to 7.2 using concentrated HCl, adjust to a final volume of 100 mL using H2O.\"\n",
            "10. \"2.5M sucrose/100 mM MOPS pH 7.2: add 85.58 g sucrose to 10 mL of 1M MOPS-HCl pH 7.2 and subsequently add enough H2O to make up to 100 mL.\"\n",
            "\n",
            "**Incompleteness:**\n",
            "- No specific details are provided for the mitosome isolation protocol.\n",
            "- No information is given about the density gradient centrifugation method.\n",
            "\n",
            "**Conclusion:**\n",
            "The article mentions the isolation of E histolytica mitosomes using cellular fractionation and density gradient centrifugation. However, the specific steps of the mitosome isolation protocol and the details of the density gradient centrifugation method are not provided. Therefore, the protocol is incomplete, and additional information is needed to fully understand the isolation procedure.\n",
            "Storing data for file_id: A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t\n",
            "Successfully saved data to /home/epas/Documents/MitoMAVEN/full_texts/A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t.json\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge, protocol, organisms):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"mitochondria\": {\"protocol\": protocol, \"organisms\": organisms},\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Isolation Protocol\":\n",
        "        with open(\"get_isolation_protocol.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Organisms\":\n",
        "        with open(\"get_organisms.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0125\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Isolation Protocol\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Isolation Protocol\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Organisms\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Organisms\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0125\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0125\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    try:\n",
        "        topic = request_topics(summary)\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topic and hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None, None\n",
        "    finally:\n",
        "        return topic, questions\n",
        "def extract_protocol(article):\n",
        "    # NLP logic to extract protocol from the article\n",
        "    protocol = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "    try:\n",
        "        protocol = generate_summary(text=article, summary_type=\"Get Isolation Protocol\")\n",
        "        print(f\"Protocol: {protocol}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting protocol: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    finally:\n",
        "        return protocol\n",
        "\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        #if theres an error use an SPR Compresed knowledge\n",
        "        compressed_knowledge = generate_summary(text=knowledge, summary_type=\"SPR\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=compressed_knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        try:\n",
        "            extract_references_from_pdf(file_path, references_path)\n",
        "            with open(references_path) as f:\n",
        "                references = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting references: {e}\")\n",
        "            references = \"\"\n",
        "    else:\n",
        "        references = extract_references(file_path)\n",
        "          \n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        article_text = pdf_to_text(file_path)\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the Mitochondria Isolation Protocol \n",
        "            protocol = extract_protocol(chunk)\n",
        "\n",
        "            if protocol:\n",
        "                organsims = generate_summary(text=protocol, summary_type=\"Get Organisms\")\n",
        "            else:\n",
        "                organsims = \"\"\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            if refined_sumary:\n",
        "                topic, questions = extract_info(refined_sumary)\n",
        "            else:\n",
        "                topic, questions = \"\", \"\"\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic,\n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets,\n",
        "                        protocol=protocol,\n",
        "                        organisms=organsims\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\") or file_name.endswith(\".mmd\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    # Replace spaces with underscores\n",
        "    \n",
        "    remove_spaces = file_name.replace(\" \", \"_\")\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "#OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/gpt-researcher/outputs/\" # mmd files\n",
        "#OUTPUT_FOLDER = \"/home/epas/Downloads/POB2/\"\n",
        "OUTPUT_FOLDER = \"/home/epas/Documents/MitoMAVEN/full_texts/\"\n",
        "article_list = get_article_list(filetype=\"pdf\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open each of the .json to see the results \n",
        "\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (2.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic) (4.8.0)\n",
            "Requirement already satisfied: instructor in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.4.8)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (3.9.1)\n",
            "Requirement already satisfied: docstring-parser<0.16,>=0.15 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (0.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (1.10.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (2.6.0)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.1.0->instructor) (4.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.2->instructor) (2.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.17.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: certifi in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: openai in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (1.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (2.6.0)\n",
            "Requirement already satisfied: sniffio in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n",
            "Requirement already satisfied: PyPDF2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.0.1)\n",
            "Error in checking if file is bibliography: [Errno 21] Is a directory: '/Volumes/Backup Plus/REMARKABLE/New Folder With Items'\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/mml-book.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/mml-book.txt\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: mml-book\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/mml-book.json\n",
            "Successfully summarized /Volumes/Backup Plus/REMARKABLE/mml-book.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.txt\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: Lambda-calculus and combinators\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.json\n",
            "Successfully summarized /Volumes/Backup Plus/REMARKABLE/Lambda-calculus and combinators.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._Lambda-calculus and combinators.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).txt\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Error in extracting topics: Connection error.\n",
            "Error in extracting hypothetical questions: Connection error.\n",
            "Storing data for file_id: R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1)\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._mml-book.pdf\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._mml-book.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._mml-book.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._mml-book.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._mml-book.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.pdf' -o '/Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.txt'' returned non-zero exit status 2.\n",
            "Error extracting references: [Errno 2] No such file or directory: \"/Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.txt\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: pdfx [-h] [-d OUTPUT_DIRECTORY] [-c] [-j] [-v] [-t] [-o OUTPUT_FILE]\n",
            "            [--version]\n",
            "            pdf\n",
            "pdfx: error: unrecognized arguments: Hard, and Where to Start.pdf -o /Volumes/Backup Plus/REMARKABLE/AI Alignment Why Its Hard, and Where to Start.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/AI Alignment Why It's Hard, and Where to Start.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.txt'' returned non-zero exit status 2.\n",
            "Error extracting references: [Errno 2] No such file or directory: \"/Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.txt\"\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._AI Alignment Why It's Hard, and Where to Start.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/Definability of Truth in Probabilistic Logic_pdf.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Definability of Truth in Probabilistic Logic_pdf.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: pdfx [-h] [-d OUTPUT_DIRECTORY] [-c] [-j] [-v] [-t] [-o OUTPUT_FILE]\n",
            "            [--version]\n",
            "            pdf\n",
            "pdfx: error: unrecognized arguments: Hard, and Where to Start.pdf -o /Volumes/Backup Plus/REMARKABLE/._AI Alignment Why Its Hard, and Where to Start.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/Definability of Truth in Probabilistic Logic_pdf.txt\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Definability of Truth in Probabilistic Logic_pdf.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Definability of Truth in Probabilistic Logic_pdf.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._Definability of Truth in Probabilistic Logic_pdf.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/Chapter 7part1 - Lee - Tagged.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Chapter 7part1 - Lee - Tagged.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/Chapter 7part1 - Lee - Tagged.txt\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Chapter 7part1 - Lee - Tagged.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Chapter 7part1 - Lee - Tagged.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._Chapter 7part1 - Lee - Tagged.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Overcoming Bias _ Moral uncertainty – towards a solution__pdf.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/Overcoming Bias _ Moral uncertainty – towards a solution__pdf.txt\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._Overcoming Bias _ Moral uncertainty – towards a solution__pdf.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/acs_analchem_1c03244.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/acs_analchem_1c03244.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/acs_analchem_1c03244.txt\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/acs_analchem_1c03244.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/acs_analchem_1c03244.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._acs_analchem_1c03244.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/Analysis of Good_Bad Goal Architectures.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/Analysis of Good_Bad Goal Architectures.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "References extracted successfully to /Volumes/Backup Plus/REMARKABLE/Analysis of Good_Bad Goal Architectures.txt\n",
            "Error in summarizing article: Connection error.\n",
            " Occured in generate_summary function\n",
            "Error in extracting entities: Connection error.\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Analysis of Good_Bad Goal Architectures.pdf: cannot unpack non-iterable NoneType object\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/Analysis of Good_Bad Goal Architectures.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.json\n",
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.pdf' -o '/Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._Analysis of Good_Bad Goal Architectures.pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n",
            "ERROR 4: Invalid PDF (No /Root object! - Is this really a PDF?)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf' -o '/Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).txt'' returned non-zero exit status 4.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).txt'\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "Error summarizing /Volumes/Backup Plus/REMARKABLE/._R_ Greger, U_ Windhorst (auth_), Prof_ Dr_ Rainer Greger, Prof_ Dr_ Uwe Windhorst (eds_) - Comprehensive Human Physiology_ From Cellular Mechanisms to Integration-Springer-Verlag Berlin Heidelberg (19 (1).pdf\n",
            "Summarizing /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.pdf\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/bin/pdfx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/cli.py\", line 158, in main\n",
            "    pdf = pdfx.PDFx(args.pdf)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/__init__.py\", line 128, in __init__\n",
            "    self.reader = PDFMinerBackend(self.stream)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/backends.py\", line 236, in __init__\n",
            "    refs = self.resolve_PDFObjRef(page.annots)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/backends.py\", line 273, in resolve_PDFObjRef\n",
            "    return [self.resolve_PDFObjRef(item) for item in obj_ref]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/backends.py\", line 273, in <listcomp>\n",
            "    return [self.resolve_PDFObjRef(item) for item in obj_ref]\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/pdfx/backends.py\", line 295, in resolve_PDFObjRef\n",
            "    if \"URI\" in obj_resolved:\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: argument of type 'NoneType' is not iterable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Command 'pdfx -v '/Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.pdf' -o '/Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.txt'' returned non-zero exit status 1.\n",
            "Error extracting references: [Errno 2] No such file or directory: '/Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.txt'\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n",
            "Error in summarizing article: Text cannot be empty\n",
            " Using last summary\n",
            "Storing data for file_id: AnatomyandPhysiology-OP\n",
            "Successfully saved data to /Volumes/Backup Plus/REMARKABLE/AnatomyandPhysiology-OP.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 642\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article_info \u001b[38;5;129;01min\u001b[39;00m article_list:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 642\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43mIncrementally_Refine_Article_Summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully summarized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 539\u001b[0m, in \u001b[0;36mIncrementally_Refine_Article_Summary\u001b[0;34m(article_info)\u001b[0m\n\u001b[1;32m    536\u001b[0m refined_sumary, entities \u001b[38;5;241m=\u001b[39m get_entity_dense_sumary(chunk, initial_summary)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# Extract Knowledge from the article and entities\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m knowledge_triplets \u001b[38;5;241m=\u001b[39m \u001b[43mextract_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_entities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_entities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirty_entities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdirty_entities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Extract the topic and hypothetical questions from the refined summary\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refined_sumary:\n",
            "Cell \u001b[0;32mIn[6], line 437\u001b[0m, in \u001b[0;36mextract_knowledge\u001b[0;34m(article, clean_entities, dirty_entities)\u001b[0m\n\u001b[1;32m    434\u001b[0m     knowledge \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dirty_knowledge\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     combined_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_knowledge_graph_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_knowledge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m     knowledge \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m combined_knowledge\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "Cell \u001b[0;32mIn[6], line 280\u001b[0m, in \u001b[0;36mbuild_knowledge_graph_request\u001b[0;34m(article, clean_entities, dirty_entities, prev_knowledge)\u001b[0m\n\u001b[1;32m    278\u001b[0m client \u001b[38;5;241m=\u001b[39m instructor\u001b[38;5;241m.\u001b[39mpatch(OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key))\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-0125\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_system_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGet Knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     knowledge \u001b[38;5;241m=\u001b[39m parse_response(response)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m knowledge\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/instructor/patch.py:392\u001b[0m, in \u001b[0;36mwrap_chatcompletion.<locals>.new_chatcompletion_sync\u001b[0;34m(response_model, validation_context, max_retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_chatcompletion_sync\u001b[39m(\n\u001b[1;32m    383\u001b[0m     response_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    388\u001b[0m ):\n\u001b[1;32m    389\u001b[0m     response_model, new_kwargs \u001b[38;5;241m=\u001b[39m handle_response_model(\n\u001b[1;32m    390\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model, kwargs\u001b[38;5;241m=\u001b[39mkwargs, mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m    391\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretry_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/instructor/patch.py:300\u001b[0m, in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retries \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Excepts ValidationError, and JSONDecodeError\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m         stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39musage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    904\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install instructor\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "from asyncio import protocols\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "def extract_urls(reference_text):\n",
        "    # Regular expression pattern for identifying URLs\n",
        "    url_pattern = re.compile(r'https?://[^\\s,]+')\n",
        "    urls = url_pattern.findall(reference_text)\n",
        "    return urls\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_id): \n",
        "        self.file_id = file_id\n",
        "        self.file_path = f\"{OUTPUT_FOLDER}{file_id}.json\"\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize with empty data\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, clean_entities,dirty_entities, file_id, article, references, topic, hypothetical_questions, knowledge):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"clean_entities\": clean_entities,\n",
        "            \"dirty_entities\": dirty_entities,\n",
        "            \"references\": references,\n",
        "            \"topics\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,\n",
        "            \"knowledge_triplets\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(data)\n",
        "        print(f\"Storing data for file_id: {file_id}\")  # Log storing action\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump(content, f)\n",
        "            print(f\"Successfully saved data to {self.file_path}\")  # Log success message\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.file_path}: {e}\")  # Log error message  \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Knowledge\":\n",
        "        with open(\"get_knowlege_graph_triples.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Isolation Protocol\":\n",
        "        with open(\"get_isolation_protocol.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Organisms\":\n",
        "        with open(\"get_organisms.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    return f\"{system_prompt}\"\n",
        "\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0125\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Isolation Protocol\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Isolation Protocol\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"Get Organisms\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Get Entities')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Organisms\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    all_entities_dict = {}\n",
        "    clean_entities,  dirty_entities = get_entities(article)\n",
        "    all_entities_dict[\"clean_entities\"] = clean_entities\n",
        "    all_entities_dict[\"dirty_entities\"] = dirty_entities\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in clean_entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], all_entities_dict\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0125\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return clean_and_separate_entities(entities)\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        "\n",
        "\n",
        "def build_knowledge_graph_request(article, clean_entities=None, dirty_entities=None, prev_knowledge=None):\n",
        "        request = f\"Article: {article}\\n\\n\"\n",
        "        if clean_entities:\n",
        "            request += f\"Clean Entities: {clean_entities}\\n\\n\"\n",
        "        if dirty_entities:\n",
        "            request += f\"Dirty Entities: {dirty_entities}\\n\\n\"\n",
        "        if prev_knowledge:\n",
        "            request += f\"Do Not Repeat Previous Knowledge: {prev_knowledge}\\n\\n\"\n",
        "        \n",
        "        client = instructor.patch(OpenAI(api_key=api_key))\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0125\",\n",
        "                temperature=0.6,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Knowledge\")},\n",
        "                    {\"role\": \"user\", \"content\": request}\n",
        "                ],\n",
        "            )\n",
        "            knowledge = parse_response(response)\n",
        "            return knowledge\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting knowledge: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            raise ValueError(\"Error in extracting knowledge\")\n",
        "\n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=600 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def request_topics(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        topic = extract_topics_with_justification(parse_response(response))\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topics: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return topic\n",
        "\n",
        "def request_hypothetical_questions(summary):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.4,\n",
        "        max_retries=3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "            {\"role\": \"user\", \"content\": summary}])\n",
        "        questions = extract_hypothetical_questions(parse_response(response))\n",
        "        #questions = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    return questions\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    try:\n",
        "        topic = request_topics(summary)\n",
        "        questions = request_hypothetical_questions(summary)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting topic and hypothetical questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None, None\n",
        "    finally:\n",
        "        return topic, questions\n",
        "def extract_protocol(article):\n",
        "    # NLP logic to extract protocol from the article\n",
        "    protocol = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "    try:\n",
        "        protocol = generate_summary(text=article, summary_type=\"Get Isolation Protocol\")\n",
        "        print(f\"Protocol: {protocol}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting protocol: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    finally:\n",
        "        return protocol\n",
        "\n",
        "\n",
        "def extract_knowledge(article, clean_entities, dirty_entities):\n",
        "    # NLP logic to extract knowledge from the article\n",
        "    knowledge = \"\"\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    try:\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting clean knowledge: {e}\\n Trying again\")\n",
        "        clean_knowledge = build_knowledge_graph_request(article=article, clean_entities=clean_entities)\n",
        "        knowledge += clean_knowledge\n",
        "    try:\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting dirty knowledge: {e}\\n Trying again\")\n",
        "        dirty_knowledge = build_knowledge_graph_request(article=article, dirty_entities=dirty_entities)\n",
        "        knowledge += dirty_knowledge\n",
        "\n",
        "    try:\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting combined knowledge: {e}\\n Trying again\")\n",
        "        #if theres an error use an SPR Compresed knowledge\n",
        "        compressed_knowledge = generate_summary(text=knowledge, summary_type=\"SPR\")\n",
        "        combined_knowledge = build_knowledge_graph_request(article=article, prev_knowledge=compressed_knowledge)\n",
        "        knowledge += combined_knowledge\n",
        "    return clean_entity_relationships(knowledge)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def extract_references_from_pdf(pdf_path, output_path):\n",
        "    # Construct the command\n",
        "    command = f\"pdfx -v '{pdf_path}' -o '{output_path}'\"\n",
        "\n",
        "    # Run the command\n",
        "    try:\n",
        "        subprocess.run(command, check=True, shell=True)\n",
        "        print(f\"References extracted successfully to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "#pdf_path = \"/path/to/your/pdf.pdf\"\n",
        "#output_path = \"/path/to/output/file.txt\"\n",
        "#extract_references_from_pdf(pdf_path, output_path)\n",
        "from PyPDF2 import PdfReader \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore(file_id)\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract references from PDF\n",
        "        references_path = f\"{OUTPUT_FOLDER}{file_id}.txt\"\n",
        "        try:\n",
        "            extract_references_from_pdf(file_path, references_path)\n",
        "            with open(references_path) as f:\n",
        "                references = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting references: {e}\")\n",
        "            references = \"\"\n",
        "    else:\n",
        "        try:\n",
        "            references = extract_references(file_path)  \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting references: {e}\")\n",
        "            references = \"\"\n",
        "    urls = extract_urls(references)\n",
        "    if urls:\n",
        "        # create dictionary of urls and references\n",
        "        references = {\"urls\": urls, \"references\": references}\n",
        "    #print(f\"References: {references}\")\n",
        "    if article_info[\"file_type\"] == \"pdf\":\n",
        "        # Extract text from PDF\n",
        "        try:\n",
        "            article_text = pdf_to_text(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from PDF: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        with open(file_path) as f:\n",
        "            article_text = f.read()\n",
        "    \n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract Knowledge from the article and entities\n",
        "            knowledge_triplets = extract_knowledge(chunk, clean_entities=entities[\"clean_entities\"], dirty_entities=entities[\"dirty_entities\"])\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            if refined_sumary:\n",
        "                topic, questions = extract_info(refined_sumary)\n",
        "            else:\n",
        "                topic, questions = \"\", \"\"\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            \n",
        "            store.store(summary=refined_sumary, \n",
        "                        file_id=file_id, \n",
        "                        clean_entities=entities[\"clean_entities\"],\n",
        "                        dirty_entities=entities[\"dirty_entities\"],\n",
        "                        article=chunk_name, \n",
        "                        references=references, \n",
        "                        topic=topic,\n",
        "                        hypothetical_questions=questions,\n",
        "                        knowledge=knowledge_triplets,\n",
        "                        )\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "    try:\n",
        "        with codecs.open(file_path, 'rb') as f:\n",
        "            first_line = f.readline()\n",
        "            if b'# Bibliography Recommendation Report:' in first_line:\n",
        "                return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in checking if file is bibliography: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_article_list(filetype=\"md\"):\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # Skip bibliography files\n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "        if not file_name.endswith(filetype):\n",
        "            continue\n",
        "        else:\n",
        "            if file_name.endswith(\".md\") or file_name.endswith(\".mmd\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "            \n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                file_id = get_file_id(file_name)\n",
        "                summary_file_path = os.path.join(OUTPUT_FOLDER, f\"{file_id}.json\")\n",
        "\n",
        "                if not os.path.exists(summary_file_path):\n",
        "                    info = {\n",
        "                        \"file_id\": file_id, \n",
        "                        \"file_path\": file_path,\n",
        "                        \"file_type\": filetype\n",
        "                    }\n",
        "                    articles.append(info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    # Replace spaces with underscores\n",
        "    \n",
        "    remove_spaces = file_name.replace(\" \", \"_\")\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "\n",
        "\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\" \n",
        "#OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/GitHub/gpt-researcher/outputs/\"\n",
        "#OUTPUT_FOLDER = \"gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"Literature_Review/gpt_researcher_outputs/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/chemical_structure_report/\"\n",
        "#OUTPUT_FOLDER = \"/home/epas/Programming/gpt-researcher/outputs/\" # mmd files\n",
        "#OUTPUT_FOLDER = \"/home/epas/Downloads/POB2/\"\n",
        "#OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/DR NASH 1st meeting/Yeast/Exactly_What_im_looking_for/\"\n",
        "OUTPUT_FOLDER = \"/Volumes/Backup Plus/REMARKABLE/\"\n",
        "article_list = get_article_list(filetype=\"pdf\")\n",
        "if article_list:\n",
        "    for article_info in article_list:\n",
        "        print(f\"Summarizing {article_info['file_path']}\")\n",
        "        success = Incrementally_Refine_Article_Summary(article_info)\n",
        "        if success:\n",
        "            print(f\"Successfully summarized {article_info['file_path']}\")\n",
        "        else:\n",
        "            print(f\"Error summarizing {article_info['file_path']}\")\n",
        "else:\n",
        "    print(\"No articles to summarize\")\n",
        "# open each of the .json to see the results \n",
        "\n",
        "OUTPUT_FOLDER = \"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/MD/make_JSON1/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renamed duell-et-al-1964-isolation-and-properties-of-intact-mitochondria-from-spheroplasts-of-yeast.json to duell-et-al-1964-isolation-and-properties-of-intact-mitochondria-from-spheroplasts-of-yeast_json.md\n",
            "Renamed Extracted_JSON_from_paper_Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria.json to Extracted_JSON_from_paper_Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria_json.md\n",
            "Renamed Chris Meisinger_isolation-of-yeast-mitochondria.json to Chris_Meisinger_isolation-of-yeast-mitochondria_json.md\n",
            "Renamed Isolation_of_mitochondria_from_Saccharomy_cescerevisiae_using_magnetic_bead_affinity_purification.json to Isolation_of_mitochondria_from_Saccharomy_cescerevisiae_using_magnetic_bead_affinity_purification_json.md\n",
            "Renamed Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris.json to Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris_json.md\n",
            "Renamed Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria.json to Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria_json.md\n",
            "Renamed ABCAM_Mitochondrial_purification_protocol.json to ABCAM_Mitochondrial_purification_protocol_json.md\n",
            "Renamed braun2009_Yeast_Mitochondria_Purified_by_Free_Flow_Electrophoresis.json to braun2009_Yeast_Mitochondria_Purified_by_Free_Flow_Electrophoresis_json.md\n",
            "Renamed Purification_of_Mitochondria_from_Yeast_Cells.json to Purification_of_Mitochondria_from_Yeast_Cells_json.md\n",
            "Renamed izawa2017_Isolation_of_Mitochondria_from_Saccharomyces_cerevisiae.json to izawa2017_Isolation_of_Mitochondria_from_Saccharomyces_cerevisiae_json.md\n",
            "Renamed Isolation_and_characterization_of_highly_purified_mitochondrial_outer_membranes_of_the_yeast.json to Isolation_and_characterization_of_highly_purified_mitochondrial_outer_membranes_of_the_yeast_json.md\n",
            "Renamed Advanced_tools_for_the_analysis_of_protein_phosphorylation_in_yeast_mitochondria.json to Advanced_tools_for_the_analysis_of_protein_phosphorylation_in_yeast_mitochondria_json.md\n",
            "Renamed Assessment_of_Submitochondrial_Protein_Localization_in_Budding_Yeast_Saccharomyces_cerevisiae.json to Assessment_of_Submitochondrial_Protein_Localization_in_Budding_Yeast_Saccharomyces_cerevisiae_json.md\n",
            "Renamed Mirosław_Kozłowski_Stable_preparation_of_yeast_mitochondria_and_mitoplasts_synthesizing_specific_polypeptides_1988.json to Mirosław_Kozłowski_Stable_preparation_of_yeast_mitochondria_and_mitoplasts_synthesizing_specific_polypeptides_1988_json.md\n"
          ]
        }
      ],
      "source": [
        "# rename every file \"/home/epas/Documents/MitoMAVEN/full_texts/\" replacing the spaces with underscores \n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "from numpy import full\n",
        "\n",
        "def rename_files(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".json\"):\n",
        "            new_filename = \"\" + re.sub(r\"\\s+\", \"_\", filename)\n",
        "            new_filename_with_pdf = new_filename.replace(\".json\", \"_json.md\")\n",
        "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename_with_pdf))\n",
        "            print(f\"Renamed {filename} to {new_filename_with_pdf}\")\n",
        "\n",
        "# Example usage\n",
        "#rename_files(\"/Users/tomriddle1/Documents/REOR/Mitochondria Maven/Extraction Techniques OVERVIEW/DOCX/\")\n",
        "rename_files(\"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/DR NASH 1st meeting/Yeast/Exactly_What_im_looking_for/JSON copy/\")           \n",
        "def convert_docx_to_md(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            new_filename = filename.replace(\".docx\", \".md\")\n",
        "            full_path = os.path.join(directory, filename)\n",
        "            new_full_path = os.path.join(\"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/Converted_DOCX/\", new_filename)\n",
        "            filename = full_path\n",
        "            new_filename = new_full_path\n",
        "            print(f\"Converting {filename} to {new_filename}\")\n",
        "            os.system(f\"pandoc {filename} -f docx+styles -t markdown -s -o {new_filename}\")\n",
        "\n",
        "# Example usage\n",
        "#convert_docx_to_md(\"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/DOCX/\")\n",
        "\n",
        "def convert_json_to_html(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".json\"):\n",
        "            new_filename = filename.replace(\".json\", \".html\")\n",
        "            full_path = os.path.join(directory, filename)\n",
        "            new_full_path = os.path.join(\"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/HTML/\", new_filename)\n",
        "            filename = full_path\n",
        "            new_filename = new_full_path\n",
        "            print(f\"Converting {filename} to {new_filename}\")\n",
        "            os.system(f\"pandoc {filename} -f json -t html -s -o {new_filename}\")\n",
        "\n",
        "# Example usage\n",
        "#convert_json_to_html(\"/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/JSON/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Protocol for 'Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences' saved to 'Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation' saved to 'Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation_protocol.txt'.\n",
            "Protocol for 'Isolation_of_brain_mitochondria_from_neonatal_mice' saved to 'Isolation_of_brain_mitochondria_from_neonatal_mice_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue' saved to 'Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris' saved to 'Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris_protocol.txt'.\n",
            "Protocol for 'Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts' saved to 'Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts_protocol.txt'.\n",
            "Protocol for 'Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex' saved to 'Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana' saved to 'Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana_protocol.txt'.\n",
            "Protocol for 'Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana' saved to 'Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts' saved to 'Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts_protocol.txt'.\n",
            "Protocol for 'Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas' saved to 'Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei' saved to 'Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_tissue_culture_cells' saved to 'Isolation_of_mitochondria_from_tissue_culture_cells_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_' saved to 'Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput__protocol.txt'.\n",
            "Protocol for 'A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation' saved to 'A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation_protocol.txt'.\n",
            "Protocol for 'A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t' saved to 'A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t_protocol.txt'.\n",
            "Protocol for 'A_method_for_isolating_intact_mitochondria_and_nuclei_from_the_same_homogenate,_and_the_influen' saved to 'A_method_for_isolating_intact_mitochondria_and_nuclei_from_the_same_homogenate,_and_the_influen_protocol.txt'.\n",
            "Protocol for 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_percoll_density_gradient_ce' saved to 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_percoll_density_gradient_ce_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondrial_subpopulations_from_skeletal_muscle_Optimizing_recovery_and_preservi' saved to 'Isolation_of_mitochondrial_subpopulations_from_skeletal_muscle_Optimizing_recovery_and_preservi_protocol.txt'.\n",
            "Protocol for 'Qualitative_Characterization_of_the_Rat_Liver_Mitochondrial_Lipidome_Using_All_Ion_Fragmentatio' saved to 'Qualitative_Characterization_of_the_Rat_Liver_Mitochondrial_Lipidome_Using_All_Ion_Fragmentatio_protocol.txt'.\n",
            "Protocol for 'Preparation_of_highly_coupled_rat_heart_mitochondria' saved to 'Preparation_of_highly_coupled_rat_heart_mitochondria_protocol.txt'.\n",
            "Protocol for 'Measurement_of_mitochondrial_respiratory_chain_enzymatic_activities_in_Drosophila_melanogaster_' saved to 'Measurement_of_mitochondrial_respiratory_chain_enzymatic_activities_in_Drosophila_melanogaster__protocol.txt'.\n",
            "Protocol for 'Optimization_of_preparation_of_mitochondria_from_25-100_mg_skeletal_muscle' saved to 'Optimization_of_preparation_of_mitochondria_from_25-100_mg_skeletal_muscle_protocol.txt'.\n",
            "Protocol for 'The_isolation_of_coupled_mitochondria_from_Physarum_polycephalum_and_their_response_to_Ca2+' saved to 'The_isolation_of_coupled_mitochondria_from_Physarum_polycephalum_and_their_response_to_Ca2+_protocol.txt'.\n",
            "Protocol for 'Simultaneous_isolation_of_pure_and_intact_chloroplasts_and_mitochondria_from_moss_as_the_basis_' saved to 'Simultaneous_isolation_of_pure_and_intact_chloroplasts_and_mitochondria_from_moss_as_the_basis__protocol.txt'.\n",
            "Protocol for 'Isolation_of_functional_mitochondria_from_rat_kidney_and_skeletal_muscle_without_manual_homogen' saved to 'Isolation_of_functional_mitochondria_from_rat_kidney_and_skeletal_muscle_without_manual_homogen_protocol.txt'.\n",
            "Protocol for 'A_microcalorimetric_study_of_the_effect_of_La3+_on_mitochondria_isolated_from_Star-Cross_288_ch' saved to 'A_microcalorimetric_study_of_the_effect_of_La3+_on_mitochondria_isolated_from_Star-Cross_288_ch_protocol.txt'.\n",
            "Protocol for 'A_simplified_method_to_isolate_rice_mitochondria' saved to 'A_simplified_method_to_isolate_rice_mitochondria_protocol.txt'.\n",
            "Protocol for 'Tightly_coupled_mitochondria_from_human_early_placenta' saved to 'Tightly_coupled_mitochondria_from_human_early_placenta_protocol.txt'.\n",
            "Protocol for 'A_semi-automated_method_for_isolating_functionally_intact_mitochondria_from_cultured_cells_and_' saved to 'A_semi-automated_method_for_isolating_functionally_intact_mitochondria_from_cultured_cells_and__protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_with_high_respiratory_control_from_primary_cultures_of_neurons_and_as' saved to 'Isolation_of_mitochondria_with_high_respiratory_control_from_primary_cultures_of_neurons_and_as_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_techniques_for_mitochondria_technique_for_rat_liver_mitochondria' saved to 'Rapid_isolation_techniques_for_mitochondria_technique_for_rat_liver_mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Metabolic_Assessment_of_Cancer_Cell_Mitochondria' saved to 'Isolation_and_Metabolic_Assessment_of_Cancer_Cell_Mitochondria_protocol.txt'.\n",
            "Protocol for 'A_novel,_simple_and_rapid_method_for_the_isolation_of_mitochondria_which_exhibit_respiratory_co' saved to 'A_novel,_simple_and_rapid_method_for_the_isolation_of_mitochondria_which_exhibit_respiratory_co_protocol.txt'.\n",
            "Protocol for 'Efficient_isolation_of_pure_and_functional_mitochondria_from_mouse_tissues_using_automated_tiss' saved to 'Efficient_isolation_of_pure_and_functional_mitochondria_from_mouse_tissues_using_automated_tiss_protocol.txt'.\n",
            "Protocol for 'Delivery_of_mitochondria_confers_cardioprotection_through_mitochondria_replenishment_and_metabo' saved to 'Delivery_of_mitochondria_confers_cardioprotection_through_mitochondria_replenishment_and_metabo_protocol.txt'.\n",
            "Protocol for 'Mitochondria_from_the_hepatopancreas_of_the_marine_clam_Mercenaria_mercenaria_substrate_prefere' saved to 'Mitochondria_from_the_hepatopancreas_of_the_marine_clam_Mercenaria_mercenaria_substrate_prefere_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_of_metabolically_active_mitochondria_from_rat_brain_and_subregions_using_Percol' saved to 'Rapid_isolation_of_metabolically_active_mitochondria_from_rat_brain_and_subregions_using_Percol_protocol.txt'.\n",
            "Protocol for 'Improved_method_for_isolation_of_mitochondria_from_chick_breast_muscle_using_Nagarse' saved to 'Improved_method_for_isolation_of_mitochondria_from_chick_breast_muscle_using_Nagarse_protocol.txt'.\n",
            "Protocol for 'Isolation_of_rat_adrenocortical_mitochondria' saved to 'Isolation_of_rat_adrenocortical_mitochondria_protocol.txt'.\n",
            "Protocol for 'An_improved_method_with_a_wider_applicability_to_isolate_plant_mitochondria_for_mtDNA_extractio' saved to 'An_improved_method_with_a_wider_applicability_to_isolate_plant_mitochondria_for_mtDNA_extractio_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_and_purification_of_functional_platelet_mitochondria_using_a_discontinuous_Perc' saved to 'Rapid_isolation_and_purification_of_functional_platelet_mitochondria_using_a_discontinuous_Perc_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_and_mitochondrial_RNA_from_Crithidia_fasciculata' saved to 'Isolation_of_mitochondria_and_mitochondrial_RNA_from_Crithidia_fasciculata_protocol.txt'.\n",
            "Protocol for 'Purity_matters_A_workflow_for_the_valid_high-resolution_lipid_profiling_of_mitochondria_from_ce' saved to 'Purity_matters_A_workflow_for_the_valid_high-resolution_lipid_profiling_of_mitochondria_from_ce_protocol.txt'.\n",
            "Protocol for 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_Percoll_density_gradient_ce' saved to 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_Percoll_density_gradient_ce_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_the_CNS' saved to 'Isolation_of_mitochondria_from_the_CNS_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Isolation_and_Real-Time_Monitoring_of_MOMP' saved to 'Mitochondrial_Isolation_and_Real-Time_Monitoring_of_MOMP_protocol.txt'.\n",
            "Protocol for 'Isolation_and_functional_assessment_of_mitochondria_from_small_amounts_of_mouse_brain_tissue' saved to 'Isolation_and_functional_assessment_of_mitochondria_from_small_amounts_of_mouse_brain_tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_animal_tissue' saved to 'Isolation_of_mitochondria_from_animal_tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Large_Amounts_of_Highly_Pure_Mitochondria_for_Omics_Studies' saved to 'Isolation_of_Large_Amounts_of_Highly_Pure_Mitochondria_for_Omics_Studies_protocol.txt'.\n",
            "Protocol for 'Optimization_of_differential_filtration-based_mitochondrial_isolation_for_mitochondrial_transpl' saved to 'Optimization_of_differential_filtration-based_mitochondrial_isolation_for_mitochondrial_transpl_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Structural_Studies_of_Mitochondria_from_Pea_Roots' saved to 'Isolation_and_Structural_Studies_of_Mitochondria_from_Pea_Roots_protocol.txt'.\n",
            "Protocol for 'A_high-yield_preparative_method_for_isolation_of_rat_liver_mitochondria' saved to 'A_high-yield_preparative_method_for_isolation_of_rat_liver_mitochondria_protocol.txt'.\n",
            "Protocol for 'The_isolation_and_properties_of_mitochondria_from_rat_pancreas' saved to 'The_isolation_and_properties_of_mitochondria_from_rat_pancreas_protocol.txt'.\n",
            "Protocol for 'Comparison_of_three_methods_for_mitochondria_isolation_from_the_human_liver_cell_line_(HepG2)' saved to 'Comparison_of_three_methods_for_mitochondria_isolation_from_the_human_liver_cell_line_(HepG2)_protocol.txt'.\n",
            "Protocol for 'A_rapid_method_for_the_isolation_of_intact_mitochondria_from_isolated_rat_liver_cells' saved to 'A_rapid_method_for_the_isolation_of_intact_mitochondria_from_isolated_rat_liver_cells_protocol.txt'.\n",
            "Protocol for 'An_Update_on_Isolation_of_Functional_Mitochondria_from_Cells_for_Bioenergetics_Studies' saved to 'An_Update_on_Isolation_of_Functional_Mitochondria_from_Cells_for_Bioenergetics_Studies_protocol.txt'.\n",
            "Protocol for 'Purification_of_Functional_Platelet_Mitochondria_Using_a_Discontinuous_Percoll_Gradient' saved to 'Purification_of_Functional_Platelet_Mitochondria_Using_a_Discontinuous_Percoll_Gradient_protocol.txt'.\n",
            "Protocol for 'Affordable_de_novo_generation_of_fish_mitogenomes_using_amplification-free_enrichment_of_mitoch' saved to 'Affordable_de_novo_generation_of_fish_mitogenomes_using_amplification-free_enrichment_of_mitoch_protocol.txt'.\n",
            "Protocol for 'Isolation_and_quality_control_of_functional_mitochondria' saved to 'Isolation_and_quality_control_of_functional_mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_for_biogenetical_studies_An_update' saved to 'Isolation_of_mitochondria_for_biogenetical_studies_An_update_protocol.txt'.\n",
            "Protocol for 'Optimal_isolation_of_mitochondria_for_proteomic_analyses' saved to 'Optimal_isolation_of_mitochondria_for_proteomic_analyses_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Physiologically_Active_and_Intact_Mitochondria_from_Chickpea' saved to 'Isolation_of_Physiologically_Active_and_Intact_Mitochondria_from_Chickpea_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Electron_Microscopic_Analysis_of_Liver_Cancer_Cell_Mitochondria' saved to 'Isolation_and_Electron_Microscopic_Analysis_of_Liver_Cancer_Cell_Mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_and_comparative_proteomic_analysis_of_mitochondria_from_the_pulp_of_ripening_citrus_f' saved to 'Isolation_and_comparative_proteomic_analysis_of_mitochondria_from_the_pulp_of_ripening_citrus_f_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_structure_and_function_are_disrupted_by_standard_isolation_methods' saved to 'Mitochondrial_structure_and_function_are_disrupted_by_standard_isolation_methods_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_ascites_tumor_cells_permeabilized_with_digitonin' saved to 'Isolation_of_mitochondria_from_ascites_tumor_cells_permeabilized_with_digitonin_protocol.txt'.\n",
            "Protocol for 'Protocol_for_mitochondrial_isolation_and_sub-cellular_localization_assay_for_mitochondrial_prot' saved to 'Protocol_for_mitochondrial_isolation_and_sub-cellular_localization_assay_for_mitochondrial_prot_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Isolation_and_Purification_from_Mouse_Spinal_Cord' saved to 'Mitochondrial_Isolation_and_Purification_from_Mouse_Spinal_Cord_protocol.txt'.\n",
            "Protocol for 'Mouse_Liver_Mitochondria_Isolation,_Size_Fractionation,_and_Real-time_MOMP_Measurement' saved to 'Mouse_Liver_Mitochondria_Isolation,_Size_Fractionation,_and_Real-time_MOMP_Measurement_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Purification_of_Mitochondria_from_Cell_Culture_for_Proteomic_Analyses' saved to 'Isolation_and_Purification_of_Mitochondria_from_Cell_Culture_for_Proteomic_Analyses_protocol.txt'.\n",
            "Protocol for 'Two-Step_Tag-Free_Isolation_of_Mitochondria_for_Improved_Protein_Discovery_and_Quantification' saved to 'Two-Step_Tag-Free_Isolation_of_Mitochondria_for_Improved_Protein_Discovery_and_Quantification_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_and_purification_of_mitochondria_for_transplantation_by_tissue_dissociation_and' saved to 'Rapid_isolation_and_purification_of_mitochondria_for_transplantation_by_tissue_dissociation_and_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_cultured_cells_and_liver_tissue_biopsies_for_molecular_and_bioch' saved to 'Isolation_of_mitochondria_from_cultured_cells_and_liver_tissue_biopsies_for_molecular_and_bioch_protocol.txt'.\n",
            "Protocol for 'An_improved_technique_for_the_isolation_of_mitochondria_from_plant_tissue' saved to 'An_improved_technique_for_the_isolation_of_mitochondria_from_plant_tissue_protocol.txt'.\n",
            "Protocol for 'Characterization_of_growth_plate_mitochondria' saved to 'Characterization_of_growth_plate_mitochondria_protocol.txt'.\n",
            "Protocol for 'Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria' saved to 'Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria_protocol.txt'.\n",
            "Protocol for 'Scalable_Isolation_of_Mammalian_Mitochondria_for_Nucleic_Acid_and_Nucleoid_Analysis' saved to 'Scalable_Isolation_of_Mammalian_Mitochondria_for_Nucleic_Acid_and_Nucleoid_Analysis_protocol.txt'.\n",
            "Protocol for 'Assay_of_succinate_dehydrogenase_activity_by_the_tetrazolium_method_evaluation_of_an_improved_t' saved to 'Assay_of_succinate_dehydrogenase_activity_by_the_tetrazolium_method_evaluation_of_an_improved_t_protocol.txt'.\n",
            "Protocol for 'Magnetic_nanoparticles_an_improved_method_for_mitochondrial_isolation' saved to 'Magnetic_nanoparticles_an_improved_method_for_mitochondrial_isolation_protocol.txt'.\n",
            "Protocol for 'Preparation_of_physiologically_active_inside-out_vesicles_from_plant_inner_mitochondrial_membra' saved to 'Preparation_of_physiologically_active_inside-out_vesicles_from_plant_inner_mitochondrial_membra_protocol.txt'.\n",
            "Protocol for 'Preservation_of_mitochondrial_functional_integrity_in_mitochondria_isolated_from_small_cryopres' saved to 'Preservation_of_mitochondrial_functional_integrity_in_mitochondria_isolated_from_small_cryopres_protocol.txt'.\n",
            "Protocol for 'Isolation_of_functional_pure_mitochondria_by_superparamagnetic_microbeads' saved to 'Isolation_of_functional_pure_mitochondria_by_superparamagnetic_microbeads_protocol.txt'.\n",
            "Protocol for 'An_Improved_Method_for_Preparation_of_Uniform_and_Functional_Mitochondria_from_Fresh_Liver' saved to 'An_Improved_Method_for_Preparation_of_Uniform_and_Functional_Mitochondria_from_Fresh_Liver_protocol.txt'.\n",
            "Protocol for 'Isolation_and_functional_analysis_of_mitochondria_from_cultured_cells_and_mouse_tissue' saved to 'Isolation_and_functional_analysis_of_mitochondria_from_cultured_cells_and_mouse_tissue_protocol.txt'.\n",
            "Protocol for 'Mitochondria_and_peroxisomes_from_the_cellular_slime_mould_Dictyostelium_discoideum._Isolation_' saved to 'Mitochondria_and_peroxisomes_from_the_cellular_slime_mould_Dictyostelium_discoideum._Isolation__protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Respiration_of_Platelets_Comparison_of_Isolation_Methods' saved to 'Mitochondrial_Respiration_of_Platelets_Comparison_of_Isolation_Methods_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Intact_Mitochondria_from_Skeletal_Muscle_by_Differential_Centrifugation_for_High-r' saved to 'Isolation_of_Intact_Mitochondria_from_Skeletal_Muscle_by_Differential_Centrifugation_for_High-r_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_by_gentle_cell_membrane_disruption,_and_their_subsequent_characteriza' saved to 'Isolation_of_mitochondria_by_gentle_cell_membrane_disruption,_and_their_subsequent_characteriza_protocol.txt'.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_protocols(input_file):\n",
        "    \"\"\"\n",
        "    Extracts and saves protocols from the input text file based on the file_id.\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Regular expression to identify the line with file_id\n",
        "    file_id_pattern = re.compile(r\"Storing data for file_id: (.+)\")\n",
        "    protocol_data = {}\n",
        "    current_id = None\n",
        "    for line in lines:\n",
        "        file_id_match = file_id_pattern.search(line)\n",
        "        if file_id_match:\n",
        "            current_id = file_id_match.group(1)  # Capture the current file_id\n",
        "            protocol_data[current_id] = []  # Initialize a list to hold protocol lines\n",
        "        elif current_id:\n",
        "            protocol_data[current_id].append(line)  # Append protocol lines under the current file_id\n",
        "\n",
        "    # Saving each protocol to a separate file based on file_id\n",
        "    for file_id, protocol_lines in protocol_data.items():\n",
        "        output_filename = f\"{file_id}_protocol.txt\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.writelines(protocol_lines)\n",
        "            print(f\"Protocol for '{file_id}' saved to '{output_filename}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_filename = \"/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/All_Extraction_Protocols.txt\"\n",
        "    extract_protocols(input_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Protocol for 'Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondria_isolated_from_lipid_droplets_of_white_adipose_tissue_reveal_functional_differences_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Rapid_isolation_of_respiring_skeletal_muscle_mitochondria_using_nitrogen_cavitation_protocol.txt'.\n",
            "Protocol for 'Isolation_of_brain_mitochondria_from_neonatal_mice' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_brain_mitochondria_from_neonatal_mice_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Mitochondria_From_Fresh_Mice_Lung_Tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_bioenergetic_characterization_of_mitochondria_from_Pichia_pastoris_protocol.txt'.\n",
            "Protocol for 'Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Organelle_isolation_functional_mitochondria_from_mouse_liver,_muscle_and_cultured_fibroblasts_protocol.txt'.\n",
            "Protocol for 'Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_functionally_active_and_highly_purified_neuronal_mitochondria_from_human_cortex_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_Respiratory_Measurements_of_Mitochondria_from_Arabidopsis_thaliana_protocol.txt'.\n",
            "Protocol for 'Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_intact,_functional_mitochondria_from_the_model_plant_Arabidopsis_thaliana_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Mitochondria_from_Ustilago_maydis_Protoplasts_protocol.txt'.\n",
            "Protocol for 'Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_reconstruction_of_cardiac_mitochondria_from_SBEM_images_using_a_deep_learning-bas_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_procyclic_Trypanosoma_brucei_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_tissue_culture_cells' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_tissue_culture_cells_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput_' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Mitochondria_from_Minimal_Quantities_of_Mouse_Skeletal_Muscle_for_High_Throughput__protocol.txt'.\n",
            "Protocol for 'A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_critical_comparison_between_two_classical_and_a_kit-based_method_for_mitochondria_isolation_protocol.txt'.\n",
            "Protocol for 'A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_mitosome_purification_protocol_based_on_percoll_density_gradients_and_its_use_in_validating_t_protocol.txt'.\n",
            "Protocol for 'A_method_for_isolating_intact_mitochondria_and_nuclei_from_the_same_homogenate,_and_the_influen' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_method_for_isolating_intact_mitochondria_and_nuclei_from_the_same_homogenate,_and_the_influen_protocol.txt'.\n",
            "Protocol for 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_percoll_density_gradient_ce' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_percoll_density_gradient_ce_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondrial_subpopulations_from_skeletal_muscle_Optimizing_recovery_and_preservi' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondrial_subpopulations_from_skeletal_muscle_Optimizing_recovery_and_preservi_protocol.txt'.\n",
            "Protocol for 'Qualitative_Characterization_of_the_Rat_Liver_Mitochondrial_Lipidome_Using_All_Ion_Fragmentatio' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Qualitative_Characterization_of_the_Rat_Liver_Mitochondrial_Lipidome_Using_All_Ion_Fragmentatio_protocol.txt'.\n",
            "Protocol for 'Preparation_of_highly_coupled_rat_heart_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Preparation_of_highly_coupled_rat_heart_mitochondria_protocol.txt'.\n",
            "Protocol for 'Measurement_of_mitochondrial_respiratory_chain_enzymatic_activities_in_Drosophila_melanogaster_' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Measurement_of_mitochondrial_respiratory_chain_enzymatic_activities_in_Drosophila_melanogaster__protocol.txt'.\n",
            "Protocol for 'Optimization_of_preparation_of_mitochondria_from_25-100_mg_skeletal_muscle' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Optimization_of_preparation_of_mitochondria_from_25-100_mg_skeletal_muscle_protocol.txt'.\n",
            "Protocol for 'The_isolation_of_coupled_mitochondria_from_Physarum_polycephalum_and_their_response_to_Ca2+' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/The_isolation_of_coupled_mitochondria_from_Physarum_polycephalum_and_their_response_to_Ca2+_protocol.txt'.\n",
            "Protocol for 'Simultaneous_isolation_of_pure_and_intact_chloroplasts_and_mitochondria_from_moss_as_the_basis_' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Simultaneous_isolation_of_pure_and_intact_chloroplasts_and_mitochondria_from_moss_as_the_basis__protocol.txt'.\n",
            "Protocol for 'Isolation_of_functional_mitochondria_from_rat_kidney_and_skeletal_muscle_without_manual_homogen' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_functional_mitochondria_from_rat_kidney_and_skeletal_muscle_without_manual_homogen_protocol.txt'.\n",
            "Protocol for 'A_microcalorimetric_study_of_the_effect_of_La3+_on_mitochondria_isolated_from_Star-Cross_288_ch' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_microcalorimetric_study_of_the_effect_of_La3+_on_mitochondria_isolated_from_Star-Cross_288_ch_protocol.txt'.\n",
            "Protocol for 'A_simplified_method_to_isolate_rice_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_simplified_method_to_isolate_rice_mitochondria_protocol.txt'.\n",
            "Protocol for 'Tightly_coupled_mitochondria_from_human_early_placenta' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Tightly_coupled_mitochondria_from_human_early_placenta_protocol.txt'.\n",
            "Protocol for 'A_semi-automated_method_for_isolating_functionally_intact_mitochondria_from_cultured_cells_and_' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_semi-automated_method_for_isolating_functionally_intact_mitochondria_from_cultured_cells_and__protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_with_high_respiratory_control_from_primary_cultures_of_neurons_and_as' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_with_high_respiratory_control_from_primary_cultures_of_neurons_and_as_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_techniques_for_mitochondria_technique_for_rat_liver_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Rapid_isolation_techniques_for_mitochondria_technique_for_rat_liver_mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Metabolic_Assessment_of_Cancer_Cell_Mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_Metabolic_Assessment_of_Cancer_Cell_Mitochondria_protocol.txt'.\n",
            "Protocol for 'A_novel,_simple_and_rapid_method_for_the_isolation_of_mitochondria_which_exhibit_respiratory_co' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_novel,_simple_and_rapid_method_for_the_isolation_of_mitochondria_which_exhibit_respiratory_co_protocol.txt'.\n",
            "Protocol for 'Efficient_isolation_of_pure_and_functional_mitochondria_from_mouse_tissues_using_automated_tiss' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Efficient_isolation_of_pure_and_functional_mitochondria_from_mouse_tissues_using_automated_tiss_protocol.txt'.\n",
            "Protocol for 'Delivery_of_mitochondria_confers_cardioprotection_through_mitochondria_replenishment_and_metabo' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Delivery_of_mitochondria_confers_cardioprotection_through_mitochondria_replenishment_and_metabo_protocol.txt'.\n",
            "Protocol for 'Mitochondria_from_the_hepatopancreas_of_the_marine_clam_Mercenaria_mercenaria_substrate_prefere' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondria_from_the_hepatopancreas_of_the_marine_clam_Mercenaria_mercenaria_substrate_prefere_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_of_metabolically_active_mitochondria_from_rat_brain_and_subregions_using_Percol' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Rapid_isolation_of_metabolically_active_mitochondria_from_rat_brain_and_subregions_using_Percol_protocol.txt'.\n",
            "Protocol for 'Improved_method_for_isolation_of_mitochondria_from_chick_breast_muscle_using_Nagarse' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Improved_method_for_isolation_of_mitochondria_from_chick_breast_muscle_using_Nagarse_protocol.txt'.\n",
            "Protocol for 'Isolation_of_rat_adrenocortical_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_rat_adrenocortical_mitochondria_protocol.txt'.\n",
            "Protocol for 'An_improved_method_with_a_wider_applicability_to_isolate_plant_mitochondria_for_mtDNA_extractio' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/An_improved_method_with_a_wider_applicability_to_isolate_plant_mitochondria_for_mtDNA_extractio_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_and_purification_of_functional_platelet_mitochondria_using_a_discontinuous_Perc' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Rapid_isolation_and_purification_of_functional_platelet_mitochondria_using_a_discontinuous_Perc_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_and_mitochondrial_RNA_from_Crithidia_fasciculata' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_and_mitochondrial_RNA_from_Crithidia_fasciculata_protocol.txt'.\n",
            "Protocol for 'Purity_matters_A_workflow_for_the_valid_high-resolution_lipid_profiling_of_mitochondria_from_ce' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Purity_matters_A_workflow_for_the_valid_high-resolution_lipid_profiling_of_mitochondria_from_ce_protocol.txt'.\n",
            "Protocol for 'Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_Percoll_density_gradient_ce' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Purification_of_functional_mouse_skeletal_muscle_mitochondria_using_Percoll_density_gradient_ce_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_the_CNS' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_the_CNS_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Isolation_and_Real-Time_Monitoring_of_MOMP' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondrial_Isolation_and_Real-Time_Monitoring_of_MOMP_protocol.txt'.\n",
            "Protocol for 'Isolation_and_functional_assessment_of_mitochondria_from_small_amounts_of_mouse_brain_tissue' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_functional_assessment_of_mitochondria_from_small_amounts_of_mouse_brain_tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_animal_tissue' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_animal_tissue_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Large_Amounts_of_Highly_Pure_Mitochondria_for_Omics_Studies' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Large_Amounts_of_Highly_Pure_Mitochondria_for_Omics_Studies_protocol.txt'.\n",
            "Protocol for 'Optimization_of_differential_filtration-based_mitochondrial_isolation_for_mitochondrial_transpl' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Optimization_of_differential_filtration-based_mitochondrial_isolation_for_mitochondrial_transpl_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Structural_Studies_of_Mitochondria_from_Pea_Roots' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_Structural_Studies_of_Mitochondria_from_Pea_Roots_protocol.txt'.\n",
            "Protocol for 'A_high-yield_preparative_method_for_isolation_of_rat_liver_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_high-yield_preparative_method_for_isolation_of_rat_liver_mitochondria_protocol.txt'.\n",
            "Protocol for 'The_isolation_and_properties_of_mitochondria_from_rat_pancreas' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/The_isolation_and_properties_of_mitochondria_from_rat_pancreas_protocol.txt'.\n",
            "Protocol for 'Comparison_of_three_methods_for_mitochondria_isolation_from_the_human_liver_cell_line_(HepG2)' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Comparison_of_three_methods_for_mitochondria_isolation_from_the_human_liver_cell_line_(HepG2)_protocol.txt'.\n",
            "Protocol for 'A_rapid_method_for_the_isolation_of_intact_mitochondria_from_isolated_rat_liver_cells' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/A_rapid_method_for_the_isolation_of_intact_mitochondria_from_isolated_rat_liver_cells_protocol.txt'.\n",
            "Protocol for 'An_Update_on_Isolation_of_Functional_Mitochondria_from_Cells_for_Bioenergetics_Studies' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/An_Update_on_Isolation_of_Functional_Mitochondria_from_Cells_for_Bioenergetics_Studies_protocol.txt'.\n",
            "Protocol for 'Purification_of_Functional_Platelet_Mitochondria_Using_a_Discontinuous_Percoll_Gradient' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Purification_of_Functional_Platelet_Mitochondria_Using_a_Discontinuous_Percoll_Gradient_protocol.txt'.\n",
            "Protocol for 'Affordable_de_novo_generation_of_fish_mitogenomes_using_amplification-free_enrichment_of_mitoch' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Affordable_de_novo_generation_of_fish_mitogenomes_using_amplification-free_enrichment_of_mitoch_protocol.txt'.\n",
            "Protocol for 'Isolation_and_quality_control_of_functional_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_quality_control_of_functional_mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_for_biogenetical_studies_An_update' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_for_biogenetical_studies_An_update_protocol.txt'.\n",
            "Protocol for 'Optimal_isolation_of_mitochondria_for_proteomic_analyses' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Optimal_isolation_of_mitochondria_for_proteomic_analyses_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Physiologically_Active_and_Intact_Mitochondria_from_Chickpea' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Physiologically_Active_and_Intact_Mitochondria_from_Chickpea_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Electron_Microscopic_Analysis_of_Liver_Cancer_Cell_Mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_Electron_Microscopic_Analysis_of_Liver_Cancer_Cell_Mitochondria_protocol.txt'.\n",
            "Protocol for 'Isolation_and_comparative_proteomic_analysis_of_mitochondria_from_the_pulp_of_ripening_citrus_f' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_comparative_proteomic_analysis_of_mitochondria_from_the_pulp_of_ripening_citrus_f_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_structure_and_function_are_disrupted_by_standard_isolation_methods' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondrial_structure_and_function_are_disrupted_by_standard_isolation_methods_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_ascites_tumor_cells_permeabilized_with_digitonin' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_ascites_tumor_cells_permeabilized_with_digitonin_protocol.txt'.\n",
            "Protocol for 'Protocol_for_mitochondrial_isolation_and_sub-cellular_localization_assay_for_mitochondrial_prot' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Protocol_for_mitochondrial_isolation_and_sub-cellular_localization_assay_for_mitochondrial_prot_protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Isolation_and_Purification_from_Mouse_Spinal_Cord' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondrial_Isolation_and_Purification_from_Mouse_Spinal_Cord_protocol.txt'.\n",
            "Protocol for 'Mouse_Liver_Mitochondria_Isolation,_Size_Fractionation,_and_Real-time_MOMP_Measurement' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mouse_Liver_Mitochondria_Isolation,_Size_Fractionation,_and_Real-time_MOMP_Measurement_protocol.txt'.\n",
            "Protocol for 'Isolation_and_Purification_of_Mitochondria_from_Cell_Culture_for_Proteomic_Analyses' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_Purification_of_Mitochondria_from_Cell_Culture_for_Proteomic_Analyses_protocol.txt'.\n",
            "Protocol for 'Two-Step_Tag-Free_Isolation_of_Mitochondria_for_Improved_Protein_Discovery_and_Quantification' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Two-Step_Tag-Free_Isolation_of_Mitochondria_for_Improved_Protein_Discovery_and_Quantification_protocol.txt'.\n",
            "Protocol for 'Rapid_isolation_and_purification_of_mitochondria_for_transplantation_by_tissue_dissociation_and' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Rapid_isolation_and_purification_of_mitochondria_for_transplantation_by_tissue_dissociation_and_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_from_cultured_cells_and_liver_tissue_biopsies_for_molecular_and_bioch' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_from_cultured_cells_and_liver_tissue_biopsies_for_molecular_and_bioch_protocol.txt'.\n",
            "Protocol for 'An_improved_technique_for_the_isolation_of_mitochondria_from_plant_tissue' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/An_improved_technique_for_the_isolation_of_mitochondria_from_plant_tissue_protocol.txt'.\n",
            "Protocol for 'Characterization_of_growth_plate_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Characterization_of_growth_plate_mitochondria_protocol.txt'.\n",
            "Protocol for 'Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Genome-wide_analysis_of_RNA_extracted_from_isolated_mitochondria_protocol.txt'.\n",
            "Protocol for 'Scalable_Isolation_of_Mammalian_Mitochondria_for_Nucleic_Acid_and_Nucleoid_Analysis' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Scalable_Isolation_of_Mammalian_Mitochondria_for_Nucleic_Acid_and_Nucleoid_Analysis_protocol.txt'.\n",
            "Protocol for 'Assay_of_succinate_dehydrogenase_activity_by_the_tetrazolium_method_evaluation_of_an_improved_t' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Assay_of_succinate_dehydrogenase_activity_by_the_tetrazolium_method_evaluation_of_an_improved_t_protocol.txt'.\n",
            "Protocol for 'Magnetic_nanoparticles_an_improved_method_for_mitochondrial_isolation' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Magnetic_nanoparticles_an_improved_method_for_mitochondrial_isolation_protocol.txt'.\n",
            "Protocol for 'Preparation_of_physiologically_active_inside-out_vesicles_from_plant_inner_mitochondrial_membra' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Preparation_of_physiologically_active_inside-out_vesicles_from_plant_inner_mitochondrial_membra_protocol.txt'.\n",
            "Protocol for 'Preservation_of_mitochondrial_functional_integrity_in_mitochondria_isolated_from_small_cryopres' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Preservation_of_mitochondrial_functional_integrity_in_mitochondria_isolated_from_small_cryopres_protocol.txt'.\n",
            "Protocol for 'Isolation_of_functional_pure_mitochondria_by_superparamagnetic_microbeads' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_functional_pure_mitochondria_by_superparamagnetic_microbeads_protocol.txt'.\n",
            "Protocol for 'An_Improved_Method_for_Preparation_of_Uniform_and_Functional_Mitochondria_from_Fresh_Liver' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/An_Improved_Method_for_Preparation_of_Uniform_and_Functional_Mitochondria_from_Fresh_Liver_protocol.txt'.\n",
            "Protocol for 'Isolation_and_functional_analysis_of_mitochondria_from_cultured_cells_and_mouse_tissue' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_and_functional_analysis_of_mitochondria_from_cultured_cells_and_mouse_tissue_protocol.txt'.\n",
            "Protocol for 'Mitochondria_and_peroxisomes_from_the_cellular_slime_mould_Dictyostelium_discoideum._Isolation_' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondria_and_peroxisomes_from_the_cellular_slime_mould_Dictyostelium_discoideum._Isolation__protocol.txt'.\n",
            "Protocol for 'Mitochondrial_Respiration_of_Platelets_Comparison_of_Isolation_Methods' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Mitochondrial_Respiration_of_Platelets_Comparison_of_Isolation_Methods_protocol.txt'.\n",
            "Protocol for 'Isolation_of_Intact_Mitochondria_from_Skeletal_Muscle_by_Differential_Centrifugation_for_High-r' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_Intact_Mitochondria_from_Skeletal_Muscle_by_Differential_Centrifugation_for_High-r_protocol.txt'.\n",
            "Protocol for 'Isolation_of_mitochondria_by_gentle_cell_membrane_disruption,_and_their_subsequent_characteriza' saved to '/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/Isolation_of_mitochondria_by_gentle_cell_membrane_disruption,_and_their_subsequent_characteriza_protocol.txt'.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def extract_protocols(input_file, output_folder):\n",
        "    \"\"\"\n",
        "    Extracts protocols based on \"Storing data for file_id\" markers and saves them into a specified output folder.\n",
        "    Handles multiple instances of the same file_id by aggregating protocols under the same file_id.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    \n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    file_id_pattern = re.compile(r\"Storing data for file_id: (.+)\")\n",
        "    protocol_data = {}\n",
        "    current_id = None\n",
        "\n",
        "    for line in lines:\n",
        "        file_id_match = file_id_pattern.search(line)\n",
        "        if file_id_match:\n",
        "            # When a new file_id is found, switch to it but check if it already exists\n",
        "            new_id = file_id_match.group(1)\n",
        "            if new_id != current_id:  # Avoid resetting if the same ID appears consecutively\n",
        "                current_id = new_id\n",
        "                # Initialize a list to hold protocol lines under the current file_id if it doesn't exist\n",
        "                if current_id not in protocol_data:\n",
        "                    protocol_data[current_id] = []\n",
        "            elif current_id not in protocol_data:\n",
        "                protocol_data[current_id] = []\n",
        "            # Append protocol lines under the current file_id\n",
        "        protocol_data[current_id].append(line)\n",
        "\n",
        "    # Saving each protocol to a separate file within the specified output folder\n",
        "    for file_id, protocol_lines in protocol_data.items():\n",
        "        output_filename = os.path.join(output_folder, f\"Mitochondria_Isolation_Protocol_from_paper_{file_id}_protocol.txt\")\n",
        "        with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.writelines(protocol_lines)\n",
        "            print(f\"Protocol for '{file_id}' saved to '{output_filename}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_filename = \"/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/All_Extraction_Protocols.txt\"\n",
        "    output_directory = \"/home/epas/Documents/MitoMAVEN/full_texts/PROTCOLS/\"  # Specify your desired output directory here\n",
        "    extract_protocols(input_filename, output_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Specify the directory containing the JSON files\n",
        "json_directory = '/Users/tomriddle1/Documents/REOR/MitochondriaMaven/DR NASH 1st meeting/Yeast/Exactly_What_im_looking_for/'\n",
        "output_directory = '/Users/tomriddle1/Documents/REOR/MitochondriaMaven/DR NASH 1st meeting/Yeast/Exactly_What_im_looking_for/PROTCOLS/'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Pattern to match all JSON files in the directory\n",
        "json_pattern = os.path.join(json_directory, '*.json')\n",
        "\n",
        "for json_file_path in glob.glob(json_pattern):\n",
        "    # Initialize a list to store all protocols for the current JSON file\n",
        "    protocols = []\n",
        "    \n",
        "    # Open and read the JSON file\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "        \n",
        "        # Check if data is a list\n",
        "        if isinstance(data, list):\n",
        "            # Process each item in the list\n",
        "            for item in data:\n",
        "                # Assuming each item in the list is a dictionary that could contain 'mitochondria'\n",
        "                # Extract the mitochondrial isolation protocol if available and append to the protocols list\n",
        "                protocol = item.get('mitochondria', {}).get('protocol', None)\n",
        "                if protocol:\n",
        "                    protocols.append(protocol)\n",
        "        else:\n",
        "            # Handle the case where data is not a list (e.g., a single dictionary)\n",
        "            protocol = data.get('mitochondria', {}).get('protocol', None)\n",
        "            if protocol:\n",
        "                protocols.append(protocol)\n",
        "    \n",
        "    # Only proceed if we have collected any protocols\n",
        "    if protocols:\n",
        "        # Prepare the output file path with the same name as the JSON file\n",
        "        base_name = os.path.basename(json_file_path)\n",
        "        new_file_name = os.path.splitext(base_name)[0] + '_protocols.txt'\n",
        "        output_file_path = os.path.join(output_directory, new_file_name)\n",
        "        \n",
        "        # Define the header text\n",
        "        text_header = \"\"\"**Enhanced Prompt for Mitochondrial Isolation Protocol Compilation**\n",
        "\n",
        "**Objective:**\n",
        "Assemble a detailed mitochondrial isolation protocol from segments scattered throughout an article. Use the given information to construct a cohesive and comprehensive guide, ensuring clarity and reproducibility for future research applications.\n",
        "\n",
        "**Output Format:**\n",
        "\n",
        "1. **Title:** Provide a succinct, descriptive title that clearly indicates the focus and methodological uniqueness of the protocol, if applicable.\n",
        "\n",
        "2. **Materials:**\n",
        "  - **Equipment:** List all equipment needed, specifying types and models where possible to aid in reproducibility. \n",
        "  - **Reagents:** Detail all reagents, including concentrations and suppliers. Highlight if specific brands or qualities are necessary.\n",
        "  - **Buffers and Solutions:** Provide exact compositions, pH values, and preparation steps for all buffers and solutions used in the protocol.\n",
        "\n",
        "3. **Detailed Protocol:**\n",
        "  - Break down the protocol into enumerated steps, offering as much detail as possible. Include specifics on tissue preparation, homogenization techniques, centrifugation speeds and temperatures, and any filtration procedures.\n",
        "  - Where information from different segments overlaps or contradicts, reconcile these to form the most accurate and practical step-by-step guide.\n",
        "\n",
        "4. **Assays Performed:**\n",
        "  - Enumerate any assays used to assess mitochondrial DNA, integrity, morphology, and function. Provide detailed methodologies, including reagent compositions, incubation times, and analysis techniques.\n",
        "\n",
        "5. **Notes on Uncertainties:**\n",
        "  - Acknowledge any uncertainties or gaps in the provided information. Suggest how these might be resolved or indicate how they might impact the protocol's execution.\n",
        "\n",
        "6. **Additional Information:**\n",
        "  - Include any extra details or tips that could enhance the protocol's effectiveness, such as advice on improving yield or purity, or considerations for specific research applications.\n",
        "\n",
        "7. **Conclusion:**\n",
        "  - Summarize the protocol's key features and its applicability to mitochondrial isolation research. Highlight any novel techniques or important considerations that distinguish it from standard protocols.\n",
        "\n",
        "**Guidelines for Enhancement:**\n",
        "\n",
        "- **Clarity and Detail:** Ensure each step of the protocol is described with sufficient detail to be easily replicated in another laboratory setting. Use clear, concise language and avoid jargon where possible.\n",
        "\n",
        "- **Addressing Uncertainties:** Where the article's information is incomplete, suggest standard practices or alternatives. Encourage users to adapt the protocol based on their specific equipment and materials availability.\n",
        "\n",
        "- **Practical Tips:** Incorporate practical advice and troubleshooting tips throughout the protocol. These can be based on common issues encountered during mitochondrial isolation processes or on insights gained from the article's context.\n",
        "\n",
        "- **Innovation and Adaptation:** If the article hints at novel approaches or adaptations to standard procedures, highlight these and discuss their potential benefits and limitations.\n",
        "\n",
        "This enhanced prompt structure aims to guide the synthesis of fragmented information into a coherent, detailed, and practical mitochondrial isolation protocol, facilitating ease of replication and adaptation in diverse research contexts.\n",
        "TEXT:\\n\"\"\"\n",
        "        text_header = \"\"\n",
        "        # Write the header and all collected protocols to the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(text_header + \"\\n\")  # Write the header with a newline before protocols\n",
        "            for protocol in protocols:\n",
        "                output_file.write(protocol + \"\\n\\n\")  # Separate protocols by two newlines for readability\n",
        "\n",
        "print(\"Extraction completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Specify the directory containing the JSON files\n",
        "#json_directory = '/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/JSON_copy/'\n",
        "#output_directory = '/Users/tomriddle1/Documents/REOR/MitochondriaMaven/Storage/Questions/'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Pattern to match all JSON files in the directory\n",
        "json_pattern = os.path.join(json_directory, '*.json')\n",
        "\n",
        "for json_file_path in glob.glob(json_pattern):\n",
        "    questions = []\n",
        "    \n",
        "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "        \n",
        "        if isinstance(data, list):\n",
        "            for item in data:\n",
        "                # Check if 'hypothetical_questions' is present and is a list\n",
        "                if 'hypothetical_questions' in item and isinstance(item['hypothetical_questions'], list):\n",
        "                    for question_item in item['hypothetical_questions']:\n",
        "                        # Extract the question and append it to the questions list\n",
        "                        question = question_item.get('question', None)\n",
        "                        if question:\n",
        "                            questions.append(question)\n",
        "        else:\n",
        "            # Similar handling for when data is a single dictionary\n",
        "            if 'hypothetical_questions' in data and isinstance(data['hypothetical_questions'], list):\n",
        "                for question_item in data['hypothetical_questions']:\n",
        "                    question = question_item.get('question', None)\n",
        "                    if question:\n",
        "                        questions.append(question)\n",
        "\n",
        "    if questions:\n",
        "        base_name = os.path.basename(json_file_path)\n",
        "        new_file_name = os.path.splitext(base_name)[0] + '_questions.json'\n",
        "        output_file_path = os.path.join(output_directory, new_file_name)\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            # Let json.dump handle the conversion of the list to a JSON formatted string\n",
        "            json.dump(questions, output_file, indent=2)  # Use indent for pretty printing\n",
        "\n",
        "print(\"Extraction completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/epas/miniconda3/envs/autogen/bin/pdfx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/cli.py\", line 158, in main\n",
            "    pdf = pdfx.PDFx(args.pdf)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/__init__.py\", line 128, in __init__\n",
            "    self.reader = PDFMinerBackend(self.stream)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/backends.py\", line 207, in __init__\n",
            "    self.metadata.update(xmp_to_dict(metadata))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 93, in xmp_to_dict\n",
            "    return XmpParser(xmp).meta\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/site-packages/pdfx/libs/xmp.py\", line 43, in __init__\n",
            "    self.tree = ET.XML(xmp)\n",
            "                ^^^^^^^^^^^\n",
            "  File \"/home/epas/miniconda3/envs/autogen/lib/python3.11/xml/etree/ElementTree.py\", line 1338, in XML\n",
            "    parser.feed(text)\n",
            "xml.etree.ElementTree.ParseError: unbound prefix: line 5, column 220\n"
          ]
        }
      ],
      "source": [
        "!pdfx -v '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.pdf' -o '/home/epas/Documents/MitoMAVEN/full_texts/Isolation_of_brain_mitochondria_from_neonatal_mice.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ChatGPT Message Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def analyze_file(file_path, output_file_path, document_name):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.read().split('\\n')\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            current_message = \"\"\n",
        "            message_started = False\n",
        "            sender = \"\"\n",
        "            message_number = 0\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line.lower().startswith('user') or line.lower().startswith('chatgpt'):\n",
        "                    if message_started:  # End of a message\n",
        "                        message_number += 1\n",
        "                        word_count = count_words(current_message.strip())\n",
        "                        output_file.write(f\"{sender} Line number {i}, Message number {message_number}, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "                        current_message = \"\"\n",
        "                    message_started = True\n",
        "                    sender = \"User\" if line.lower().startswith('user') else \"ChatGPT\"\n",
        "                    continue\n",
        "                if message_started:\n",
        "                    current_message += \" \" + line\n",
        "\n",
        "            # Add the last message if it exists\n",
        "            if current_message:\n",
        "                message_number += 1\n",
        "                word_count = count_words(current_message.strip())\n",
        "                output_file.write(f\"{sender} Last message, Document: {document_name}, (Word Count: {word_count}):\\n{current_message}\\n\\n---\\n\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your text file\n",
        "# Replace 'output_messages.txt' with the path for the output file\n",
        "# Add the document name (e.g., 'ChatGPT_history.txt')\n",
        "file_path = 'ChatGPT_history.txt'\n",
        "output_file_path = 'output_messages.txt'\n",
        "document_name = 'ChatGPT_history'  # This is the document name without the extension\n",
        "analyze_file(file_path, output_file_path, document_name)\n",
        "print(\"Messages have been written to the output file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class MessageAnalysisStore:\n",
        "    def __init__(self, output_file_path):\n",
        "        self.output_file_path = output_file_path\n",
        "        self._create_file_if_not_exists()\n",
        "\n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.output_file_path):\n",
        "            empty_data = []\n",
        "            self._save(empty_data)\n",
        "\n",
        "    def store(self, analyzed_data):\n",
        "        existing_data = self.load()\n",
        "        existing_data.append(analyzed_data)\n",
        "        self._save(existing_data)\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.output_file_path):\n",
        "            with open(self.output_file_path, \"r\") as file:\n",
        "                return json.load(file)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def _save(self, content):\n",
        "        try:\n",
        "            with open(self.output_file_path, \"w\") as file:\n",
        "                json.dump(content, file, indent=4)\n",
        "            print(f\"Successfully saved data to {self.output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to {self.output_file_path}: {e}\")\n",
        "\n",
        "# Assuming the required classes and functions from your new code are already defined and imported\n",
        "# like SummaryStore, generate_summary, get_entities, extract_knowledge, etc.\n",
        "\n",
        "def extract_messages_with_citation(lines: List[str], sender_keyword: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extracts messages with citation from the given lines based on the sender keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    messages_with_citation = []\n",
        "    current_message = \"\"\n",
        "    message_started = False\n",
        "    citation_info = \"\"\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(sender_keyword):\n",
        "            if message_started:\n",
        "                # End of the current message, add it to the list\n",
        "                messages_with_citation.append((current_message.strip(), citation_info))\n",
        "                current_message = \"\"\n",
        "            message_started = True\n",
        "            citation_info = line  # Capture the line with sender info as citation\n",
        "        elif message_started:\n",
        "            current_message += \" \" + line\n",
        "\n",
        "    # Add the last message if it exists\n",
        "    if current_message:\n",
        "        messages_with_citation.append((current_message.strip(), citation_info))\n",
        "\n",
        "    return messages_with_citation\n",
        "\n",
        "def analyze_conversation(message: str, citation: str, sender_keyword: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyzes a single conversation message, extracting and summarizing information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate an initial summary\n",
        "        if sender_keyword == \"ChatGPT\":\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"Entity Dense\")\n",
        "        else:\n",
        "            initial_summary = generate_summary(text=message, summary_type=\"SPR\")\n",
        "\n",
        "        # Extract entities and knowledge\n",
        "        entities = get_entities(message)\n",
        "        knowledge = extract_knowledge(message, entities[\"clean_entities\"], entities[\"dirty_entities\"])\n",
        "\n",
        "        # Extract the topic and hypothetical questions from the summary\n",
        "        topic, questions = extract_info(initial_summary)\n",
        "\n",
        "        analyzed_data = {\n",
        "            \"id\": citation,\n",
        "            \"sender\": sender_keyword,\n",
        "            \"message\": message,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": questions,\n",
        "            \"clean_entities\": entities[\"clean_entities\"],\n",
        "            \"dirty_entities\": entities[\"dirty_entities\"],\n",
        "            \"summary\": initial_summary,\n",
        "            \"knowledge\": knowledge,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        return analyzed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred in analyzing conversation: {e}\")\n",
        "\n",
        "def extract_and_analyze_messages(file_path: str, output_file_path: str, sender_keyword: str, log_file_path: str):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    messages_with_citation = extract_messages_with_citation(lines, sender_keyword)\n",
        "    store = MessageAnalysisStore(output_file_path)\n",
        "    error_log = []\n",
        "\n",
        "    for message, citation in messages_with_citation:\n",
        "        try:\n",
        "            analyzed_data = analyze_conversation(message, citation, sender_keyword)\n",
        "            store.store(analyzed_data)\n",
        "            time.sleep(15)  # Delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            error_info = {\"citation\": citation, \"error\": str(e), \"timestamp\": datetime.now().isoformat()}\n",
        "            # Appending to the error log\n",
        "            error_log.append(error_info)\n",
        "            with open(log_file_path, \"a\") as log_file:\n",
        "                json.dump(error_info, log_file, indent=4)\n",
        "                log_file.write(\"\\n\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Messages analysis completed. Data saved to {output_file_path}\")\n",
        "    if error_log:\n",
        "        print(f\"Errors logged to {log_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'output_messages.txt'\n",
        "output_file_path_user = 'analyzed_user_messages.json'\n",
        "log_file_path_user = 'error_log_user.json'\n",
        "extract_and_analyze_messages(file_path, output_file_path_user, 'user', log_file_path_user)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install neo4j\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MATCH (n) \n",
        "DETACH DELETE n\n",
        "\n",
        "CREATE CONSTRAINT FOR ()-[r:RELATED_TO]-() REQUIRE r.id IS UNIQUE\n",
        "\n",
        "// Shared Entities  \n",
        "MATCH (a1:ArticleID)-[:MENTIONS]->(e)<-[:MENTIONS]-(a2:ArticleID) \n",
        "WHERE e:CleanEntity OR e:DirtyEntity\n",
        "WITH DISTINCT a1, a2, collect(e.name) AS shared \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Entity'}]->(a2)\n",
        "ON CREATE SET r.entities = shared\n",
        "\n",
        "// Shared Topics\n",
        "MATCH (a1)-[:HAS_TOPIC]->(t1)<-[:HAS_TOPIC]-(a2) \n",
        "WITH a1, a2, split(t1.name, ' ') AS words1\n",
        "UNWIND words1 AS word1\n",
        "MATCH (a3)-[:HAS_TOPIC]->(t2)<-[:HAS_TOPIC]-(a4)\n",
        "WHERE word1 =~ '(?i).*?' + word1  \n",
        "WITH DISTINCT a1, a3, collect(word1) AS shared\n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared Topic Word'}]->(a3)\n",
        "ON CREATE SET r.words = shared\n",
        "\n",
        "// Shared Subjects\n",
        "MATCH (a:ArticleID)-[:HAS_TRIPLET]->(s:Subject)<-[:HAS_TRIPLET]-(b:ArticleID)\n",
        "WHERE id(a) < id(b) \n",
        "WITH DISTINCT a, b, collect(s.name) AS shared \n",
        "MERGE (a)-[r:RELATED_TO {type:'Shared Subject'}]->(b)  \n",
        "ON CREATE SET r.subjects = shared\n",
        "\n",
        "// Shared URLs\n",
        "MATCH (a1)-[:HAS_REFERENCE]->(r)<-[:HAS_REFERENCE]-(a2)\n",
        "WHERE r:Reference AND id(a1) < id(a2)  \n",
        "WITH DISTINCT a1, a2, collect(r.url) AS shared  \n",
        "MERGE (a1)-[r:RELATED_TO {type:'Shared URL'}]->(a2)\n",
        "ON CREATE SET r.urls = shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import dateutil.parser  # You need to install the python-dateutil package\n",
        "# 11226 13863\n",
        "\n",
        "def escape_string(text):\n",
        "    # Helper function to escape special characters in strings\n",
        "    return text.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "def create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp):\n",
        "    # Convert timestamp from string to Unix timestamp (seconds since epoch)\n",
        "    try:\n",
        "        timestamp_unix = dateutil.parser.parse(timestamp).timestamp()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing timestamp: {timestamp}. Error: {e}\")\n",
        "        timestamp_unix = 0  # Default to 0 or some other value in case of parsing failure\n",
        "    # Ensure ArticleID node exists (create if it doesn't)\n",
        "    session.run(f\"MERGE (articleID:ArticleID {{id: '{file_id}'}})\")\n",
        "\n",
        "    # Create or update the Article node and link it to ArticleID\n",
        "    session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) MERGE (article:Article) MERGE (articleID)-[:IDENTIFIES]->(article)\")\n",
        "\n",
        "    # Create a new Chunk node with a unique identifier, timestamp, and link it to ArticleID\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "    create_chunk_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"CREATE (chunk:Chunk {{id: '{chunk_id}', content: '{chunk_content}', timestamp: '{timestamp_unix}'}}) \"\n",
        "        f\"CREATE (articleID)-[:HAS_CHUNK]->(chunk)\"\n",
        "    )\n",
        "    session.run(create_chunk_query)\n",
        "\n",
        "    # Link this chunk to the previous chunk if it's not the first one\n",
        "    if chunk_index > 0:\n",
        "        previous_chunk_id = f\"{file_id}_{chunk_index - 1}\"\n",
        "        link_chunks_query = (\n",
        "            f\"MATCH (prevChunk:Chunk {{id: '{previous_chunk_id}'}}), (currChunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "            f\"CREATE (prevChunk)-[:NEXT]->(currChunk)\"\n",
        "        )\n",
        "        session.run(link_chunks_query)\n",
        "\n",
        "# Functions to link nodes to ArticleID\n",
        "def link_topic_to_article(session, file_id, topic_name, justification):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (topic:Topic {{name: '{topic_name}', justification: '{justification}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TOPIC]->(topic)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_entity_to_article(session, file_id, entity_name, clean=True):\n",
        "    entity_type = \"CleanEntity\" if clean else \"DirtyEntity\"\n",
        "    # Don't add irrelevant entities: \n",
        "    if entity_name in ['Abstract Concepts:', 'References:', 'Key Phrases:', 'Keywords:', 'Entities:', 'Topics:', 'Concepts:', 'Final Output:', 'Introduction', 'Conclusion', 'Summary']:\n",
        "        return\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (entity:{entity_type} {{name: '{entity_name}'}}) \"\n",
        "        f\"MERGE (articleID)-[:MENTIONS]->(entity)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_question_to_article(session, file_id, question_content, question_type):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (hq:HypotheticalQuestion {{content: '{question_content}', type: '{question_type}'}}) \"\n",
        "        f\"MERGE (articleID)-[:HAS_QUESTION]->(hq)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_triplet_to_article(session, file_id, subject, target, relationship):\n",
        "    query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "        f\"MERGE (subj:Subject {{name: '{subject}'}}) \"\n",
        "        f\"MERGE (targ:Target {{name: '{target}'}}) \"\n",
        "        f\"MERGE (rel:Relationship {{type: '{relationship}'}}) \"\n",
        "        f\"MERGE (subj)-[:HAS_RELATIONSHIP]->(rel)-[:TARGETS]->(targ) \"\n",
        "        f\"MERGE (articleID)-[:HAS_TRIPLET]->(subj)\"\n",
        "    )\n",
        "    session.run(query)\n",
        "def link_summary_to_chunk(session, file_id, chunk_index, summary_content):\n",
        "    # Unique identifier for the chunk\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Link the summary to the corresponding chunk\n",
        "    link_summary_query = (\n",
        "        f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) \"\n",
        "        f\"MERGE (summary:Summary {{content: '{summary_content}'}}) \"\n",
        "        f\"MERGE (chunk)-[:HAS_SUMMARY]->(summary)\"\n",
        "    )\n",
        "    session.run(link_summary_query)\n",
        "def link_url_references_to_article(session, file_id, urls):\n",
        "    for url in urls:\n",
        "        escaped_url = escape_string(url)\n",
        "        query = (\n",
        "            f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "            f\"MERGE (ref:Reference {{url: '{escaped_url}'}}) \"\n",
        "            f\"WITH articleID, ref \"\n",
        "            f\"MERGE (articleID)-[:HAS_REFERENCE]->(ref)\"\n",
        "        )\n",
        "        session.run(query)\n",
        "def link_textual_references_to_article(session, file_id, textual_references):\n",
        "    for reference in textual_references.split('\\n'):  # Assuming each reference is on a new line\n",
        "        if reference.strip():\n",
        "            escaped_reference = escape_string(reference)\n",
        "            query = (\n",
        "                f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) \"\n",
        "                f\"MERGE (textRef:TextualReference {{text: '{escaped_reference}'}}) \"\n",
        "                f\"WITH articleID, textRef \"\n",
        "                f\"MERGE (articleID)-[:HAS_TEXTUAL_REFERENCE]->(textRef)\"\n",
        "            )\n",
        "            session.run(query)\n",
        "\n",
        "def process_knowledge_triplet(session, file_id, subject, target, relationship):\n",
        "    # Constructing a query to check if the specific triplet combination already exists\n",
        "    check_triplet_query = (\n",
        "        f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TRIPLET]->\"\n",
        "        f\"(subj:Subject {{name: '{subject}'}})-[:HAS_RELATIONSHIP]->\"\n",
        "        f\"(rel:Relationship {{type: '{relationship}'}})-[:TARGETS]->\"\n",
        "        f\"(targ:Target {{name: '{target}'}}) \"\n",
        "        f\"RETURN subj, rel, targ\"\n",
        "    )\n",
        "    if not session.run(check_triplet_query).single():\n",
        "        # Linking the triplet only if it doesn't exist\n",
        "        link_triplet_to_article(session, file_id, subject, target, relationship)\n",
        "def process_json_file(file_path, session):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        # Group records by file_id\n",
        "        grouped_records = {}\n",
        "        for record in data:\n",
        "            file_id = record['file_id']\n",
        "            grouped_records.setdefault(file_id, []).append(record)\n",
        "\n",
        "        for file_id, records in grouped_records.items():\n",
        "            try:\n",
        "                # Check if the ArticleID already exists\n",
        "                check_query = f\"MATCH (articleID:ArticleID {{id: '{file_id}'}}) RETURN articleID\"\n",
        "                result = session.run(check_query).single()\n",
        "                if result:\n",
        "                    logging.info(f\"File ID {file_id} already exists in the database. Processing only new chunks.\")\n",
        "                else:\n",
        "                    logging.info(f\"File ID {file_id} does not exist. Processing all chunks.\")\n",
        "\n",
        "                for index, record in enumerate(records):\n",
        "                    chunk_content = escape_string(record['article'])\n",
        "                    timestamp = record.get('timestamp', '')\n",
        "                    summary_content = escape_string(record['summary']) if \"summary\" in record else \"\"\n",
        "\n",
        "                    # Process each chunk\n",
        "                    process_record_with_chunks(session, file_id, chunk_content, index, timestamp, summary_content, record)\n",
        "\n",
        "                logging.info(f\"Successfully processed file: {file_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file: {file_path}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "def process_record_with_chunks(session, file_id, chunk_content, chunk_index, timestamp, summary_content, record):\n",
        "    chunk_id = f\"{file_id}_{chunk_index}\"\n",
        "\n",
        "    # Check and process each chunk and its summary\n",
        "    if not session.run(f\"MATCH (chunk:Chunk {{id: '{chunk_id}'}}) RETURN chunk\").single():\n",
        "        create_or_update_article_with_chunks(session, file_id, chunk_content, chunk_index, timestamp)\n",
        "        if summary_content:\n",
        "            link_summary_to_chunk(session, file_id, chunk_index, summary_content)\n",
        "\n",
        "    # Process Topics\n",
        "    for topic in record.get(\"topics\", []):\n",
        "        topic_name = escape_string(topic['topic'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TOPIC]->(topic:Topic {{name: '{topic_name}'}}) RETURN topic\").single():\n",
        "            justification = escape_string(topic.get('justification', ''))\n",
        "            link_topic_to_article(session, file_id, topic_name, justification)\n",
        "\n",
        "    # Process Entities (both clean and dirty)\n",
        "    for clean, entities in [(True, record.get(\"clean_entities\", [])), (False, record.get(\"dirty_entities\", []))]:\n",
        "        for entity in entities:\n",
        "            entity_name = escape_string(entity)\n",
        "            if entity_name.strip() and not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:MENTIONS]->(entity {{name: '{entity_name}'}}) RETURN entity\").single():\n",
        "                link_entity_to_article(session, file_id, entity_name, clean)\n",
        "\n",
        "    # Process Hypothetical Questions\n",
        "    for question in record.get(\"hypothetical_questions\", []):\n",
        "        question_content = escape_string(question['question'])\n",
        "        if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_QUESTION]->(hq:HypotheticalQuestion {{content: '{question_content}'}}) RETURN hq\").single():\n",
        "            question_type = escape_string(question.get('question_type', ''))\n",
        "            link_question_to_article(session, file_id, question_content, question_type)\n",
        "\n",
        "    # Process Knowledge Triplets\n",
        "    for triplet in record.get(\"knowledge_triplets\", []):\n",
        "        subject = escape_string(triplet['subject'])\n",
        "        target = escape_string(triplet['target'])\n",
        "        relationship = escape_string(triplet['relationship'])\n",
        "        process_knowledge_triplet(session, file_id, subject, target, relationship)\n",
        "\n",
        "    # Process URL References\n",
        "    if \"references\" in record and \"urls\" in record[\"references\"]:\n",
        "        for url in record[\"references\"][\"urls\"]:\n",
        "            escaped_url = escape_string(url)\n",
        "            if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_REFERENCE]->(ref:Reference {{url: '{escaped_url}'}}) RETURN ref\").single():\n",
        "                link_url_references_to_article(session, file_id, [url])\n",
        "\n",
        "    # Process Textual References\n",
        "    if \"references\" in record and \"textual_references\" in record[\"references\"]:\n",
        "        for reference in record[\"references\"][\"textual_references\"].split('\\n'):\n",
        "            if reference.strip():\n",
        "                escaped_reference = escape_string(reference)\n",
        "                if not session.run(f\"MATCH (articleID:ArticleID {{id: '{file_id}'}})-[:HAS_TEXTUAL_REFERENCE]->(textRef:TextualReference {{text: '{escaped_reference}'}}) RETURN textRef\").single():\n",
        "                    link_textual_references_to_article(session, file_id, [reference])\n",
        "\n",
        "def add_jsons_to_neo4j(output_folder):\n",
        "    with driver.session() as session:\n",
        "        for file_name in os.listdir(output_folder):\n",
        "            if file_name.endswith(\".json\"):\n",
        "                file_path = os.path.join(output_folder, file_name)\n",
        "                process_json_file(file_path, session)\n",
        "                print(f\"Successfully processed file: {file_path}\")\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_folder4 = \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder1 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/Chemical_Structure_json/\"\n",
        "    output_folder2 = \"/home/epas/Programming/gpt-researcher/outputs/\"\n",
        "    output_folder3 = \"/home/epas/Programming/ResearchAgentSwarm/Literature_Review/gpt_researcher_outputs/\"\n",
        "    for output_folder in [output_folder1, output_folder2, output_folder3, output_folder4]:\n",
        "        try:\n",
        "            add_jsons_to_neo4j(output_folder)\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding jsons to Neo4j database: {e}\")\n",
        "            continue\n",
        "    print(\"Successfully added data to Neo4j database.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding = embeddings.embed_query(\"text\")\n",
        "print(f\"Embedding for text: {embedding}\\n With shape: {len(embedding)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Configuration for HuggingFaceEmbeddings\n",
        "model_name = \"WhereIsAI/UAE-Large-V1\"  # or another model of your choice\n",
        "model_kwargs = {'device': 'mps'}  # use 'cuda' for GPU acceleration if available\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "def LoadEmbedding(node_type, text_properties):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Handle multiple properties (concatenation) for a single embedding\n",
        "        if isinstance(text_properties, list) and len(text_properties) > 1:\n",
        "            properties_str = \" + ' ' + \".join([f\"n.{prop}\" for prop in text_properties])\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, {properties_str} AS text\"\n",
        "        else:\n",
        "            text_property = text_properties[0]\n",
        "            query = f\"MATCH (n:{node_type}) WHERE n.embedding IS NULL RETURN id(n) AS id, n.{text_property} AS text\"\n",
        "\n",
        "        result = session.run(query)\n",
        "        count = 0\n",
        "        for record in result:\n",
        "            id = record[\"id\"]\n",
        "            text = record[\"text\"]\n",
        "\n",
        "            # Generate the embedding\n",
        "            embedding = embeddings.embed_query(text)\n",
        "            print(f\"Embedding for {node_type} with id {id}: With shape: {len(embedding)}\")\n",
        "            cypher = \"MATCH (n) WHERE id(n) = $id SET n.embedding = $embedding\"\n",
        "            session.run(cypher, embedding=embedding, id=id)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Processed {count} {node_type} nodes for property @{' and '.join(text_properties)}.\")\n",
        "    return count\n",
        "\n",
        "# Example usages\n",
        "LoadEmbedding(\"Chunk\", [\"content\"])\n",
        "LoadEmbedding(\"CleanEntity\", [\"name\"])\n",
        "LoadEmbedding(\"DirtyEntity\", [\"name\"])\n",
        "LoadEmbedding(\"Subject\", [\"name\"])\n",
        "LoadEmbedding(\"Target\", [\"name\"])\n",
        "LoadEmbedding(\"Relationship\", [\"type\"])\n",
        "LoadEmbedding(\"Topic\", [\"name\", \"justification\"])  # Concatenating name and justification for topics\n",
        "LoadEmbedding(\"HypotheticalQuestion\", [\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "def create_indexes(uri, user, password):\n",
        "    # Establish a connection to the Neo4j database\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "    # Define Cypher queries for creating the indexes\n",
        "    create_full_text_index_query = \"\"\"\n",
        "    CALL db.index.fulltext.createNodeIndex(\"textIndex\", [\"Chunk\", \"CleanEntity\", \"DirtyEntity\", \"HypotheticalQuestion\"], [\"content\", \"name\"])\n",
        "    \"\"\"\n",
        "    create_vector_index_query = \"\"\"\n",
        "    CALL db.index.vector.createNodeIndex(\"chunkVectorIndex\", \"Embedding\", \"embedding\", 1024, \"COSINE\")\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # Create Full-Text Index#\n",
        "        #session.run(create_full_text_index_query) # Neo.ClientError.Procedure.ProcedureNotFound\n",
        "        #print(\"Full-text index created.\")\n",
        "\n",
        "        # Create Vector Index\n",
        "        session.run(create_vector_index_query)\n",
        "        print(\"Vector index created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity):\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "    with driver.session() as session:\n",
        "        # Drop the old index\n",
        "        session.run(f\"DROP INDEX {old_index_name}\")\n",
        "\n",
        "        # Create the new vector index\n",
        "        create_vector_index_query = f\"\"\"\n",
        "        CALL db.index.vector.createNodeIndex(\"{new_index_name}\", \"{label}\", \"{property}\", {dimensions}, \"{similarity}\")\n",
        "        \"\"\"\n",
        "        session.run(create_vector_index_query)\n",
        "        print(f\"Vector index {new_index_name} created.\")\n",
        "\n",
        "    # Close the driver connection\n",
        "    driver.close()\n",
        "\n",
        "# Usage\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "old_index_name = \"chunkVectorIndex\"  # Replace with the actual name of the index to drop\n",
        "new_index_name = \"chunkVectorIndex\"  # Replace with your new index name\n",
        "label = \"Embedding\"  # Replace with the appropriate label\n",
        "property = \"embedding\"  # Replace with the correct property\n",
        "dimensions = 1024\n",
        "similarity = \"COSINE\"\n",
        "\n",
        "#modify_indexes(old_index_name, new_index_name, label, property, dimensions, similarity)\n",
        "\n",
        "\n",
        "create_indexes(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def generate_query_vector(text, model_name=\"WhereIsAI/UAE-Large-V1\", model_kwargs={'device': 'mps'}):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "    return embeddings.embed_query(text)\n",
        "\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "def query_similar_nodes(uri, user, password, vector_index_name, query_vector, top_k=10):\n",
        "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    similar_nodes = []\n",
        "    with driver.session() as session:\n",
        "        query = f\"\"\"\n",
        "        CALL db.index.vector.queryNodes('{vector_index_name}', {top_k}, $queryVector)\n",
        "        YIELD node, score\n",
        "        RETURN node, score\n",
        "        \"\"\"\n",
        "        result = session.run(query, queryVector=query_vector)\n",
        "        for record in result:\n",
        "            node = record[\"node\"]\n",
        "            score = record[\"score\"]\n",
        "            similar_nodes.append((node, score))\n",
        "    driver.close()\n",
        "    return similar_nodes\n",
        "\n",
        "\n",
        "query_text = \"X-ray crystallography\"\n",
        "query_vector = generate_query_vector(query_text)\n",
        "print(f\"Query vector: {query_vector}\\n With shape: {len(query_vector)}\")\n",
        "uri = NEO4J_URI\n",
        "user = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "vector_index_name = \"chunkVectorIndex\"  # Replace with your actual vector index name\n",
        "\n",
        "similar_nodes = query_similar_nodes(uri, user, password, vector_index_name, query_vector)\n",
        "for node, score in similar_nodes:\n",
        "    print(node, score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_URI=\"bolt://:7687\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"12345678\"\n",
        "\n",
        "\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "with driver:\n",
        "    driver.verify_connectivity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import OllamaEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from typing import List, Any\n",
        "#from utils import BaseLogger\n",
        "from langchain.chains import GraphCypherQAChain \n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" \n",
        "def load_embedding_model(embedding_model_name: str, config={}):\n",
        "    if embedding_model_name == \"ollama\":\n",
        "        embeddings = OllamaEmbeddings(\n",
        "            base_url=config[\"ollama_base_url\"], model=\"llama2\"\n",
        "        )\n",
        "        dimension = 4096\n",
        "        #logger.info(\"Embedding: Using Ollama\")\n",
        "    elif embedding_model_name == \"openai\":\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        dimension = 1536\n",
        "        #logger.info(\"Embedding: Using OpenAI\")\n",
        "    elif embedding_model_name == \"WhereIsAI/UAE-Large-V1\":\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"WhereIsAI/UAE-Large-V1\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 1024\n",
        "        #logger.info(\"Embedding: Using HuggingFace\")\n",
        "    else:\n",
        "        embeddings = SentenceTransformerEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\", cache_folder=\"Literature_Review/cache/\"\n",
        "        )\n",
        "        dimension = 384\n",
        "        #logger.info(\"Embedding: Using SentenceTransformer\")\n",
        "    return embeddings, dimension\n",
        "\n",
        "\n",
        "def load_llm(llm_name: str, config={}):\n",
        "    if llm_name == \"gpt-4\":\n",
        "        logger.info(\"LLM: Using GPT-4\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True)\n",
        "    elif llm_name == \"gpt-3.5\":\n",
        "        logger.info(\"LLM: Using GPT-3.5\")\n",
        "        return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "    elif len(llm_name):\n",
        "        logger.info(f\"LLM: Using Ollama: {llm_name}\")\n",
        "        return ChatOllama(\n",
        "            temperature=0,\n",
        "            base_url=config[\"ollama_base_url\"],\n",
        "            model=llm_name,\n",
        "            streaming=True,\n",
        "            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.\n",
        "            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.\n",
        "            num_ctx=3072,  # Sets the size of the context window used to generate the next token.\n",
        "        )\n",
        "    logger.info(\"LLM: Using GPT-3.5\")\n",
        "    return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
        "\n",
        "\n",
        "def configure_llm_only_chain(llm):\n",
        "    # LLM only response\n",
        "    template = \"\"\"\n",
        "    You are a helpful assistant that helps with answering general questions.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    \"\"\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "    human_template = \"{text}\"\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [system_message_prompt, human_message_prompt]\n",
        "    )\n",
        "\n",
        "    def generate_llm_output(\n",
        "        user_input: str, callbacks: List[Any], prompt=chat_prompt\n",
        "    ) -> str:\n",
        "        answer = llm(\n",
        "            prompt.format_prompt(\n",
        "                text=user_input,\n",
        "            ).to_messages(),\n",
        "            callbacks=callbacks,\n",
        "        ).content\n",
        "        return {\"answer\": answer}\n",
        "\n",
        "    return generate_llm_output\n",
        "\n",
        "\n",
        "def configure_qa_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response\n",
        "    general_system_template = \"\"\"\n",
        "    Use the following context to answer the question at the end.\n",
        "    The context contains article summaries, related topics, and hypothetical questions.\n",
        "    Make sure to rely on accurate information from the summaries and topics.\n",
        "    If a particular article or topic in the context is useful, refer to it in your response.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Your response should be concise and based on the given context.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS questionEmb, score\n",
        "        MATCH (questionEmb) <-[:HAS_EMBEDDING]- (article:Article)\n",
        "        OPTIONAL MATCH (article)-[:HAS_SUMMARY]->(summary:Summary)\n",
        "        OPTIONAL MATCH (article)-[:HAS_TOPIC]->(topic:Topic)\n",
        "        RETURN '##Article ID: ' + article.id + '\\n' \n",
        "            + '##Summary: ' + coalesce(summary.content, 'No summary available') + '\\n'\n",
        "            + '##Related Topics: ' + coalesce(topic.name, 'No topics available') AS text, \n",
        "            score AS similarity\n",
        "        ORDER BY similarity DESC LIMIT 5\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=3375,\n",
        "    )\n",
        "    return kg_qa\n",
        "\n",
        "# ADDED\n",
        "# >>>> Extended to support vector search over strucutured chunking\n",
        "\n",
        "def configure_qa_structure_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n",
        "    # RAG response based on vector search and retrieval of structured chunks\n",
        "    \n",
        "    sample_query = \"\"\"\n",
        "    // 0 - prepare question and its embedding \n",
        "        MATCH (ch:Chunk) -[:HAS_EMBEDDING]-> (chemb) \n",
        "        WHERE ch.block_idx = 19\n",
        "        WITH ch.sentences AS question, chemb.value AS qemb\n",
        "        // 1 - search chunk vectors\n",
        "        CALL db.index.vector.queryNodes($index_name, $k, qemb) YIELD node, score\n",
        "        // 2 - retrieve connectd chunks, sections and documents\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (answer) -[:HAS_PARENT*]-> (s:Section)\n",
        "        WITH s, score LIMIT 1\n",
        "        MATCH (d:Document) <-[*]- (s) <-[:HAS_PARENT*]- (chunk:Chunk)\n",
        "        WITH d, s, chunk, score ORDER BY chunk.block_idx ASC\n",
        "        // 3 - prepare results\n",
        "        WITH d, collect(chunk) AS chunks, score\n",
        "        RETURN {source: d.url, page: chunks[0].page_idx} AS metadata, \n",
        "            reduce(text = \"\", x IN chunks | text + x.sentences + '.') AS text, score;   \n",
        "    \"\"\"\n",
        "\n",
        "    general_system_template = \"\"\"\n",
        "    You are an assistant providing detailed answers based on specific chunks of articles.\n",
        "    Use the context provided to answer the question at the end.\n",
        "    Ensure that the context is not altered and that your responses are based on the content of the chunks.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    ----\n",
        "    {summaries}\n",
        "    ----\n",
        "    Each answer should include reference to the relevant document and page, as indicated in the context metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    general_user_template = \"Question:```{question}```\"\n",
        "    messages = [\n",
        "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "        HumanMessagePromptTemplate.from_template(general_user_template),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "    qa_chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=qa_prompt,\n",
        "    )\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=embeddings_store_url,\n",
        "        username=username,\n",
        "        password=password,\n",
        "        database='neo4j',  # neo4j by default\n",
        "        index_name=\"chunkVectorIndex\",  # vector by default\n",
        "        node_label=\"Embedding\",  # embedding node label\n",
        "        embedding_node_property=\"embedding\",  # embedding value property\n",
        "        text_node_property=\"content\",  # text by default\n",
        "        retrieval_query=\"\"\"\n",
        "        WITH node AS answerEmb, score\n",
        "        MATCH (answerEmb) <-[:HAS_EMBEDDING]- (chunk:Chunk) -[:HAS_CHUNK]-> (article:Article)\n",
        "        WITH article, chunk, score\n",
        "        ORDER BY score DESC LIMIT 10\n",
        "        MATCH (chunk)-[:NEXT]->(nextChunk:Chunk)\n",
        "        WITH article, chunk, nextChunk, score\n",
        "        RETURN '##Article ID: ' + article.id + '\\n'\n",
        "            + 'Relevant Chunk: ' + chunk.content + '\\n'\n",
        "            + 'Next Chunk: ' + nextChunk.content AS text, \n",
        "            score AS similarity\n",
        "        LIMIT 3\n",
        "\n",
        "    \"\"\",\n",
        "    )\n",
        "\n",
        "    kg_qa = RetrievalQAWithSourcesChain(\n",
        "        combine_documents_chain=qa_chain,\n",
        "        retriever=kg.as_retriever(search_kwargs={\"k\": 25}),\n",
        "        reduce_k_below_max_tokens=False,\n",
        "        max_tokens_limit=7000,      # gpt-4\n",
        "    )\n",
        "    return kg_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "#!pip install streamlit\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "from streamlit.logger import get_logger\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from dotenv import load_dotenv\n",
        "#from utils import (\n",
        "#    extract_title_and_question,\n",
        "#    create_vector_index,\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "# >>>> initialise - environemnt <<<< \n",
        "\n",
        "#load_dotenv(\".env\")\n",
        "\"\"\"\n",
        "url = os.getenv(\"NEO4J_URI\")\n",
        "username = os.getenv(\"NEO4J_USERNAME\")\n",
        "password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "database = os.getenv(\"NEO4J_DATABASE\")\n",
        "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\n",
        "embedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n",
        "llm_name = os.getenv(\"LLM\")\n",
        "\"\"\"\n",
        "url = NEO4J_URI\n",
        "username = NEO4J_USERNAME\n",
        "password = NEO4J_PASSWORD\n",
        "database = \"neo4j\"\n",
        "#ollama_base_url = \"http://localhost:8000\"\n",
        "embedding_model_name = \"WhereIsAI/UAE-Large-V1\"\n",
        "llm_name = \"gpt-4\"\n",
        "\n",
        "\n",
        "# Remapping for Langchain Neo4j integration\n",
        "# os.environ[\"NEO4J_URL\"] = url\n",
        "\n",
        "\n",
        "# >>>> initialise - services <<<< \n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "neo4j_graph = Neo4jGraph(url=url, username=username, password=password, database=database)\n",
        "\n",
        "embeddings, dimension = load_embedding_model(\n",
        "    embedding_model_name)\n",
        "\n",
        "llm = load_llm(llm_name)\n",
        "\n",
        "\n",
        "# llm_chain: LLM only response\n",
        "llm_chain = configure_llm_only_chain(llm)\n",
        "\n",
        "# rag_chain: KG augmented response\n",
        "rag_chain = configure_qa_structure_rag_chain(\n",
        "    llm, embeddings, embeddings_store_url=url, username=username, password=password\n",
        ")\n",
        "\n",
        "# SKIPPED: create_vector_index(neo4j_graph, dimension)\n",
        "\n",
        "# >>>> Class definition - StreamHander <<<< \n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# >>>> Streamlit UI <<<<\n",
        "\n",
        "styl = f\"\"\"\n",
        "<style>\n",
        "    /* not great support for :has yet (hello FireFox), but using it for now */\n",
        "    .element-container:has([aria-label=\"Select RAG mode\"]) {{\n",
        "      position: fixed;\n",
        "      bottom: 33px;\n",
        "      background: white;\n",
        "      z-index: 101;\n",
        "    }}\n",
        "    .stChatFloatingInputContainer {{\n",
        "        bottom: 20px;\n",
        "    }}\n",
        "\n",
        "    /* Generate question text area */\n",
        "    textarea[aria-label=\"Description\"] {{\n",
        "        height: 200px;\n",
        "    }}\n",
        "</style>\n",
        "\"\"\"\n",
        "st.markdown(styl, unsafe_allow_html=True)\n",
        "st.image(\"qna-logo.png\", width=160) \n",
        "\n",
        "# >>>> UI interations <<<<\n",
        "\n",
        "def chat_input():\n",
        "    user_input = st.chat_input(\"What service questions can I help you resolve today?\")\n",
        "\n",
        "    if user_input:\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_input)\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.caption(f\"RAG: {name}\")\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "\n",
        "            # Call chain to generate answers\n",
        "            result = output_function(\n",
        "                {\"question\": user_input, \"chat_history\": []}, callbacks=[stream_handler]\n",
        "            )[\"answer\"]\n",
        "\n",
        "            output = result\n",
        "\n",
        "            st.session_state[f\"user_input\"].append(user_input)\n",
        "            st.session_state[f\"generated\"].append(output)\n",
        "            st.session_state[f\"rag_mode\"].append(name)\n",
        "\n",
        "\n",
        "def display_chat():\n",
        "    # Initialize session state keys if they do not exist\n",
        "    #if \"generated\" not in st.session_state:\n",
        "    #    st.session_state[\"generated\"] = []\n",
        "    if \"user_input\" not in st.session_state:\n",
        "        st.session_state[\"user_input\"] = []\n",
        "    if \"rag_mode\" not in st.session_state:\n",
        "        st.session_state[\"rag_mode\"] = []\n",
        "\n",
        "    # Now you can safely access st.session_state[\"generated\"] and other keys\n",
        "\n",
        "\n",
        "\n",
        "def mode_select() -> str:\n",
        "    options = [\"Disabled\", \"Enabled\"]\n",
        "    return st.radio(\"Select RAG mode\", options, horizontal=True)\n",
        "\n",
        "# >>>>> switch on/off RAG mode\n",
        "\n",
        "name = mode_select()\n",
        "if name == \"LLM only\" or name == \"Disabled\":\n",
        "    output_function = llm_chain\n",
        "elif name == \"Vector + Graph\" or name == \"Enabled\":\n",
        "    output_function = rag_chain\n",
        "\n",
        "\"\"\"\n",
        "def generate_ticket():\n",
        "    # Get high ranked questions\n",
        "    records = neo4j_graph.query(\n",
        "        \"MATCH (q:Question) RETURN q.title AS title, q.body AS body ORDER BY q.score DESC LIMIT 3\"\n",
        "    )\n",
        "    questions = []\n",
        "    for i, question in enumerate(records, start=1):\n",
        "        questions.append((question[\"title\"], question[\"body\"]))\n",
        "    # Ask LLM to generate new question in the same style\n",
        "    questions_prompt = \"\"\n",
        "    for i, question in enumerate(questions, start=1):\n",
        "        questions_prompt += f\"{i}. {question[0]}\\n\"\n",
        "        questions_prompt += f\"{question[1]}\\n\\n\"\n",
        "        questions_prompt += \"----\\n\\n\"\n",
        "\n",
        "    gen_system_template = f\\\"\"\"\n",
        "    You're an expert in formulating high quality questions. \n",
        "    Can you formulate a question in the same style, detail and tone as the following example questions?\n",
        "    {questions_prompt}\n",
        "    ---\n",
        "\n",
        "    Don't make anything up, only use information in the following question.\n",
        "    Return a title for the question, and the question post itself.\n",
        "\n",
        "    Return example:\n",
        "    ---\n",
        "    Title: How do I use the Neo4j Python driver?\n",
        "    Question: I'm trying to connect to Neo4j using the Python driver, but I'm getting an error.\n",
        "    ---\n",
        "    \\\"\"\"\n",
        "    # we need jinja2 since the questions themselves contain curly braces\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "        gen_system_template, template_format=\"jinja2\"\n",
        "    )\n",
        "    q_prompt = st.session_state[f\"user_input\"][-1]\n",
        "    chat_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            system_prompt,\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \\\"\"\"\n",
        "                Respond in the following format or you will be unplugged.\n",
        "                ---\n",
        "                Title: New title\n",
        "                Question: New question\n",
        "                ---\n",
        "                \\\"\"\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "        ]\n",
        "    )\n",
        "    llm_response = llm_chain(\n",
        "        f\"Here's the question to rewrite in the expected format: ```{q_prompt}```\",\n",
        "        [],\n",
        "        chat_prompt,\n",
        "    )\n",
        "    new_title, new_question = extract_title_and_question(llm_response[\"answer\"])\n",
        "    return (new_title, new_question)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def open_sidebar():\n",
        "    st.session_state.open_sidebar = True\n",
        "\n",
        "\n",
        "def close_sidebar():\n",
        "    st.session_state.open_sidebar = False\n",
        "\n",
        "\n",
        "if not \"open_sidebar\" in st.session_state:\n",
        "    st.session_state.open_sidebar = False\n",
        "\"\"\"\n",
        "if st.session_state.open_sidebar:\n",
        "    new_title, new_question = generate_ticket()\n",
        "    with st.sidebar:\n",
        "        st.title(\"Ticket draft\")\n",
        "        st.write(\"Auto generated draft ticket\")\n",
        "        st.text_input(\"Title\", new_title)\n",
        "        st.text_area(\"Description\", new_question)\n",
        "        st.button(\n",
        "            \"Submit to support team\",\n",
        "            type=\"primary\",\n",
        "            key=\"submit_ticket\",\n",
        "            on_click=close_sidebar,\n",
        "        )\n",
        "\"\"\"\n",
        "\n",
        "# >>>> UI: show chat <<<<\n",
        "display_chat()\n",
        "chat_input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j Graph Database Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### *****************************************************************\n",
        "# Neo4j configuration\n",
        "#\n",
        "# For more details and a complete list of settings, please see\n",
        "# https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/\n",
        "#*****************************************************************\n",
        "\n",
        "# Paths of directories in the installation.\n",
        "#server.directories.data=data\n",
        "#server.directories.plugins=plugins\n",
        "#server.directories.logs=logs\n",
        "#server.directories.lib=lib\n",
        "#server.directories.run=run\n",
        "#server.directories.licenses=licenses\n",
        "#server.directories.metrics=metrics\n",
        "#server.directories.dumps.root=data/dumps\n",
        "#server.directories.transaction.logs.root=data/transactions\n",
        "\n",
        "# This setting constrains all `LOAD CSV` import files to be under the `import` directory. Remove or comment it out to\n",
        "# allow files to be loaded from anywhere in the filesystem; this introduces possible security problems. See the\n",
        "# `LOAD CSV` section of the manual for details.\n",
        "server.directories.import=import\n",
        "\n",
        "# Whether requests to Neo4j are authenticated.\n",
        "# To disable authentication, uncomment this line\n",
        "dbms.security.auth_enabled=true\n",
        "\n",
        "# Number of databases in Neo4j is limited.\n",
        "# To change this limit please uncomment and adapt following setting:\n",
        "# dbms.max_databases=100\n",
        "\n",
        "# Enable online backups to be taken from this database.\n",
        "#server.backup.enabled=true\n",
        "\n",
        "# By default the backup service will only listen on localhost.\n",
        "# To enable remote backups you will have to bind to an external\n",
        "# network interface (e.g. 0.0.0.0 for all interfaces).\n",
        "# The protocol running varies depending on deployment. In a cluster this is the\n",
        "# same protocol that runs on server.cluster.listen_address.\n",
        "#server.backup.listen_address=0.0.0.0:6362\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial DBMS Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial DBMS settings are picked up from the config file once, when a cluster first starts, and then transferred into\n",
        "# the running DBMS. This means later changes to the values will not be seen. There are procedures to change the values\n",
        "# after the initial start\n",
        "\n",
        "# Name of the default database (aliases are not supported). Can be changed with the 'dbms.setDefaultDatabase' procedure.\n",
        "#initial.dbms.default_database=neo4j\n",
        "\n",
        "# Initial default number of primary and secondary instances of user databases. If the user does not specify the number\n",
        "# of primaries and secondaries in 'CREATE DATABASE', these values will be used, unless they are overwritten with the\n",
        "# 'dbms.setDefaultAllocationNumbers' procedure.\n",
        "#initial.dbms.default_primaries_count=1\n",
        "#initial.dbms.default_secondaries_count=0\n",
        "\n",
        "#********************************************************************\n",
        "# Memory Settings\n",
        "#********************************************************************\n",
        "#\n",
        "# Memory settings are specified kibibytes with the 'k' suffix, mebibytes with\n",
        "# 'm' and gibibytes with 'g'.\n",
        "# If Neo4j is running on a dedicated server, then it is generally recommended\n",
        "# to leave about 2-4 gigabytes for the operating system, give the JVM enough\n",
        "# heap to hold all your transaction state and query context, and then leave the\n",
        "# rest for the page cache.\n",
        "\n",
        "# Java Heap Size: by default the Java heap size is dynamically calculated based\n",
        "# on available system resources. Uncomment these lines to set specific initial\n",
        "# and maximum heap size.\n",
        "server.memory.heap.initial_size=512m\n",
        "server.memory.heap.max_size=2g\n",
        "\n",
        "# The amount of memory to use for mapping the store files.\n",
        "# The default page cache memory assumes the machine is dedicated to running\n",
        "# Neo4j, and is heuristically set to 50% of RAM minus the Java heap size.\n",
        "#server.memory.pagecache.size=10g\n",
        "\n",
        "# Limit the amount of memory that all of the running transaction can consume.\n",
        "# The default value is 70% of the heap size limit.\n",
        "#dbms.memory.transaction.total.max=256m\n",
        "\n",
        "# Limit the amount of memory that a single transaction can consume.\n",
        "# By default there is no limit.\n",
        "#db.memory.transaction.max=16m\n",
        "\n",
        "#*****************************************************************\n",
        "# Network connector configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# With default configuration Neo4j only accepts local connections.\n",
        "# Use 0.0.0.0 to bind to all network interfaces on the machine. If you want to only use a specific interface\n",
        "# (such as a private IP address on AWS, for example) then use that IP address instead.\n",
        "server.default_listen_address=0.0.0.0\n",
        "\n",
        "# You can also choose a specific network interface, and configure a non-default\n",
        "# port for each connector, by setting their individual listen_address.\n",
        "\n",
        "# The address at which this server can be reached by its clients. This may be the server's IP address or DNS name, or\n",
        "# it may be the address of a reverse proxy which sits in front of the server. This setting may be overridden for\n",
        "# individual connectors below.\n",
        "#server.default_advertised_address=localhost\n",
        "\n",
        "# You can also choose a specific advertised hostname or IP address, and\n",
        "# configure an advertised port for each connector, by setting their\n",
        "# individual advertised_address.\n",
        "\n",
        "# By default, encryption is turned off.\n",
        "# To turn on encryption, an ssl policy for the connector needs to be configured\n",
        "# Read more in SSL policy section in this file for how to define a SSL policy.\n",
        "\n",
        "# Bolt connector\n",
        "server.bolt.enabled=true\n",
        "#server.bolt.tls_level=DISABLED\n",
        "server.bolt.listen_address=:7687\n",
        "#server.bolt.advertised_address=:7687\n",
        "\n",
        "# HTTP Connector. There can be zero or one HTTP connectors.\n",
        "server.http.enabled=true\n",
        "server.http.listen_address=:7474\n",
        "#server.http.advertised_address=:7474\n",
        "\n",
        "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
        "server.https.enabled=false\n",
        "#server.https.listen_address=:7473\n",
        "#server.https.advertised_address=:7473\n",
        "\n",
        "# Number of Neo4j worker threads.\n",
        "#server.threads.worker_count\n",
        "\n",
        "#*****************************************************************\n",
        "# SSL policy configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Each policy is configured under a separate namespace, e.g.\n",
        "#    dbms.ssl.policy.<scope>.*\n",
        "#    <scope> can be any of 'bolt', 'https', 'cluster' or 'backup'\n",
        "#\n",
        "# The scope is the name of the component where the policy will be used\n",
        "# Each component where the use of an ssl policy is desired needs to declare at least one setting of the policy.\n",
        "# Allowable values are 'bolt', 'https', 'cluster' or 'backup'.\n",
        "\n",
        "# E.g if bolt and https connectors should use the same policy, the following could be declared\n",
        "#   dbms.ssl.policy.bolt.base_directory=certificates/default\n",
        "#   dbms.ssl.policy.https.base_directory=certificates/default\n",
        "# However, it's strongly encouraged to not use the same key pair for multiple scopes.\n",
        "#\n",
        "# N.B: Note that a connector must be configured to support/require\n",
        "#      SSL/TLS for the policy to actually be utilized.\n",
        "#\n",
        "# see: dbms.connector.*.tls_level\n",
        "\n",
        "# SSL settings (dbms.ssl.policy.<scope>.*)\n",
        "#  .base_directory       Base directory for SSL policies paths. All relative paths within the\n",
        "#                        SSL configuration will be resolved from the base dir.\n",
        "#\n",
        "#  .private_key          A path to the key file relative to the '.base_directory'.\n",
        "#\n",
        "#  .private_key_password The password for the private key.\n",
        "#\n",
        "#  .public_certificate   A path to the public certificate file relative to the '.base_directory'.\n",
        "#\n",
        "#  .trusted_dir          A path to a directory containing trusted certificates.\n",
        "#\n",
        "#  .revoked_dir          Path to the directory with Certificate Revocation Lists (CRLs).\n",
        "#\n",
        "#  .verify_hostname      If true, the server will verify the hostname that the client uses to connect with. In order\n",
        "#                        for this to work, the server public certificate must have a valid CN and/or matching\n",
        "#                        Subject Alternative Names.\n",
        "#\n",
        "#  .client_auth          How the client should be authorized. Possible values are: 'none', 'optional', 'require'.\n",
        "#\n",
        "#  .tls_versions         A comma-separated list of allowed TLS versions. By default only TLSv1.2 is allowed.\n",
        "#\n",
        "#  .trust_all            Setting this to 'true' will ignore the trust truststore, trusting all clients and servers.\n",
        "#                        Use of this mode is discouraged. It would offer encryption but no security.\n",
        "#\n",
        "#  .ciphers              A comma-separated list of allowed ciphers. The default ciphers are the defaults of\n",
        "#                        the JVM platform.\n",
        "\n",
        "# Bolt SSL configuration\n",
        "#dbms.ssl.policy.bolt.enabled=true\n",
        "#dbms.ssl.policy.bolt.base_directory=certificates/bolt\n",
        "#dbms.ssl.policy.bolt.private_key=private.key\n",
        "#dbms.ssl.policy.bolt.public_certificate=public.crt\n",
        "#dbms.ssl.policy.bolt.client_auth=NONE\n",
        "\n",
        "# Https SSL configuration\n",
        "#dbms.ssl.policy.https.enabled=true\n",
        "#dbms.ssl.policy.https.base_directory=certificates/https\n",
        "#dbms.ssl.policy.https.private_key=private.key\n",
        "#dbms.ssl.policy.https.public_certificate=public.crt\n",
        "#dbms.ssl.policy.https.client_auth=NONE\n",
        "\n",
        "# Cluster SSL configuration\n",
        "#dbms.ssl.policy.cluster.enabled=true\n",
        "#dbms.ssl.policy.cluster.base_directory=certificates/cluster\n",
        "#dbms.ssl.policy.cluster.private_key=private.key\n",
        "#dbms.ssl.policy.cluster.public_certificate=public.crt\n",
        "\n",
        "# Backup SSL configuration\n",
        "#dbms.ssl.policy.backup.enabled=true\n",
        "#dbms.ssl.policy.backup.base_directory=certificates/backup\n",
        "#dbms.ssl.policy.backup.private_key=private.key\n",
        "#dbms.ssl.policy.backup.public_certificate=public.crt\n",
        "\n",
        "#*****************************************************************\n",
        "# Logging configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# To enable HTTP logging, uncomment this line\n",
        "#dbms.logs.http.enabled=true\n",
        "\n",
        "# To enable GC Logging, uncomment this line\n",
        "#server.logs.gc.enabled=true\n",
        "\n",
        "# GC Logging Options\n",
        "# see https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-BE93ABDC-999C-4CB5-A88B-1994AAAC74D5\n",
        "#server.logs.gc.options=-Xlog:gc*,safepoint,age*=trace\n",
        "\n",
        "# Number of GC logs to keep.\n",
        "#server.logs.gc.rotation.keep_number=5\n",
        "\n",
        "# Size of each GC log that is kept.\n",
        "#server.logs.gc.rotation.size=20m\n",
        "\n",
        "# Log executed queries. One of OFF, INFO and VERBOSE. INFO logs queries longer than a given threshold, VERBOSE logs start and end of all queries.\n",
        "#db.logs.query.enabled=VERBOSE\n",
        "\n",
        "# If the execution of query takes more time than this threshold, the query is logged. If set to zero then all queries\n",
        "# are logged. Only used if `db.logs.query.enabled` is set to INFO\n",
        "#db.logs.query.threshold=0\n",
        "\n",
        "# Include parameters for the executed queries being logged (this is enabled by default).\n",
        "#db.logs.query.parameter_logging_enabled=true\n",
        "\n",
        "# The security log is always enabled when `dbms.security.auth_enabled=true`, for addition\n",
        "# configuration, look at $NEO4J_HOME/conf/server-logs.xml\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Uncomment and specify these lines for running Neo4j in a cluster.\n",
        "# See the cluster documentation at https://neo4j.com/docs/ for details.\n",
        "\n",
        "# A comma-separated list of endpoints which a server should contact in order to discover other cluster members. It must\n",
        "# be in the host:port format. For each machine in the cluster, the address will usually be the public ip address of\n",
        "# that machine. The port will be the value used in the setting \"server.discovery.advertised_address\" of that server.\n",
        "#dbms.cluster.discovery.endpoints=localhost:5000,localhost:5001,localhost:5002\n",
        "\n",
        "# Host and port to bind the cluster member discovery management communication.\n",
        "# This is the setting to add to the collection of addresses in dbms.cluster.discovery.endpoints.\n",
        "server.discovery.listen_address=:5002\n",
        "#server.discovery.advertised_address=:5000\n",
        "\n",
        "# Network interface and port for the transaction shipping server to listen on.\n",
        "# Please note that it is also possible to run the backup client against this port so always limit access to it via the\n",
        "# firewall and configure an ssl policy.\n",
        "server.cluster.listen_address=:6001\n",
        "#server.cluster.advertised_address=:6000\n",
        "\n",
        "# Network interface and port for the RAFT server to listen on.\n",
        "server.cluster.raft.listen_address=:7002\n",
        "#server.cluster.raft.advertised_address=:7000\n",
        "\n",
        "# Network interface and port for server-side routing within the cluster. This allows requests to be forwarded\n",
        "# from one cluster member to another, if the requests can't be satisfied by the first member (e.g. write requests\n",
        "# received by a non-leader).\n",
        "server.routing.listen_address=:7688\n",
        "#server.routing.advertised_address=:7688\n",
        "\n",
        "# List a set of names for groups to which this server should belong. This\n",
        "# is a comma-separated list and names should only use alphanumericals\n",
        "# and underscore. This can be used to identify groups of servers in the\n",
        "# configuration for load balancing and replication policies.\n",
        "#\n",
        "# The main intention for this is to group servers, but it is possible to specify\n",
        "# a unique identifier here as well which might be useful for troubleshooting\n",
        "# or other special purposes.\n",
        "#server.groups\n",
        "\n",
        "#*****************************************************************\n",
        "# Initial Server Settings\n",
        "#*****************************************************************\n",
        "\n",
        "# Initial server settings are used as the default values when enabling a server, but can be overridden by specifying\n",
        "# options when calling ENABLE (relevant for servers in a cluster *after* those that form the initial cluster).\n",
        "\n",
        "# Restrict the modes of database that can be hosted on this server\n",
        "# Allowed values:\n",
        "# PRIMARY - Host standalone databases, and members of the consensus quorum for a multi-primary database.\n",
        "# SECONDARY - Only host read replicas, eventually-consistent read-only instances of databases.\n",
        "# NONE - Can host any mode of database\n",
        "#initial.server.mode_constraint=NONE\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Load Balancing\n",
        "#*****************************************************************\n",
        "\n",
        "# N.B: Read the online documentation for a thorough explanation!\n",
        "\n",
        "# Selects the load balancing plugin that shall be enabled.\n",
        "#dbms.routing.load_balancing.plugin=server_policies\n",
        "\n",
        "####### Examples for \"server_policies\" plugin #######\n",
        "\n",
        "# Will select all available servers as the default policy, which is the\n",
        "# policy used when the client does not specify a policy preference. The\n",
        "# default configuration for the default policy is all().\n",
        "#dbms.routing.load_balancing.config.server_policies.default=all()\n",
        "\n",
        "# Will select servers in groups 'group1' or 'group2' under the default policy.\n",
        "#dbms.routing.load_balancing.config.server_policies.default=groups(group1,group2)\n",
        "\n",
        "# Slightly more advanced example:\n",
        "# Will select servers in 'group1', 'group2' or 'group3', but only if there are at least 2.\n",
        "# This policy will be exposed under the name of 'mypolicy'.\n",
        "#dbms.routing.load_balancing.config.server_policies.mypolicy=groups(group1,group2,group3) -> min(2)\n",
        "\n",
        "# Below will create an even more advanced policy named 'regionA' consisting of several rules\n",
        "# yielding the following behaviour:\n",
        "#\n",
        "#            select servers in regionA, if at least 2 are available\n",
        "# otherwise: select servers in regionA and regionB, if at least 2 are available\n",
        "# otherwise: select all servers\n",
        "#\n",
        "# The intention is to create a policy for a particular region which prefers\n",
        "# a certain set of local servers, but which will fallback to other regions\n",
        "# or all available servers as required.\n",
        "#\n",
        "# N.B: The following configuration uses the line-continuation character \\\n",
        "#      which allows you to construct an easily readable rule set spanning\n",
        "#      several lines.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.policyA=\\\n",
        "#groups(regionA) -> min(2);\\\n",
        "#groups(regionA,regionB) -> min(2);\n",
        "\n",
        "# Note that implicitly the last fallback is to always consider all() servers,\n",
        "# but this can be prevented by specifying a halt() as the last rule.\n",
        "#\n",
        "#dbms.routing.load_balancing.config.server_policies.regionA_only=\\\n",
        "#groups(regionA);\\\n",
        "#halt();\n",
        "\n",
        "#*****************************************************************\n",
        "# Cluster Additional Configuration Options\n",
        "#*****************************************************************\n",
        "# The following settings are used less frequently.\n",
        "# If you don't know what these are, you don't need to change these from their default values.\n",
        "\n",
        "# Cluster Routing Connector. Disable the opening of an additional port to allow\n",
        "# for internal communication using the same security configuration as CLUSTER\n",
        "#dbms.routing.enabled=false\n",
        "\n",
        "# The time window within which the loss of the leader is detected and the first re-election attempt is held.\n",
        "# The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.leader_failure_detection_window=20s-23s\n",
        "\n",
        "# The rate at which leader elections happen. Note that due to election conflicts it might take several attempts to\n",
        "# find a leader. The window should be significantly larger than typical communication delays to make conflicts unlikely.\n",
        "#dbms.cluster.raft.election_failure_detection_window=3s-6s\n",
        "\n",
        "# The time limit allowed for a new member to attempt to update its data to match the rest of the cluster.\n",
        "#dbms.cluster.raft.membership.join_timeout=10m\n",
        "\n",
        "# Maximum amount of lag accepted for a new follower to join the Raft group.\n",
        "#dbms.cluster.raft.membership.join_max_lag=10s\n",
        "\n",
        "# Raft log pruning frequency.\n",
        "#dbms.cluster.raft.log.pruning_frequency=10m\n",
        "\n",
        "# The size to allow the raft log to grow before rotating.\n",
        "#dbms.cluster.raft.log.rotation_size=250M\n",
        "\n",
        "# The name of a server_group whose members should be prioritized as leaders for the given database.\n",
        "# This does not guarantee that members of this group will be leader at all times, but the cluster\n",
        "# will attempt to transfer leadership to such a member when possible.\n",
        "# N.B. the final portion of this config key is dynamic and refers to the name of the database being configured.\n",
        "# You may specify multiple `db.cluster.raft.leader_transfer.priority_group.<database-name>=<server-group>` pairs:\n",
        "#db.cluster.raft.leader_transfer.priority_group.foo\n",
        "#db.cluster.raft.leader_transfer.priority_group.neo4j\n",
        "\n",
        "# Which strategy to use when transferring database leaderships around a cluster.\n",
        "# This can be one of `equal_balancing` or `no_balancing`.\n",
        "# `equal_balancing` automatically ensures that each Core server holds the leader role for an equal number of databases.\n",
        "# `no_balancing` prevents any automatic balancing of the leader role.\n",
        "# Note that if a `leadership_priority_group` is specified for a given database,\n",
        "# the value of this setting will be ignored for that database.\n",
        "#dbms.cluster.raft.leader_transfer.balancing_strategy=equal_balancing\n",
        "\n",
        "# The following setting controls how frequently a server hosting a secondary for a given database attempts to\n",
        "# fetch an update from a server hosting a primary for that database\n",
        "#db.cluster.catchup.pull_interval=1s\n",
        "\n",
        "#********************************************************************\n",
        "# Security Configuration\n",
        "#********************************************************************\n",
        "\n",
        "# The authentication and authorization providers that contains both users and roles.\n",
        "# This can be one of the built-in `native` or `ldap` auth providers,\n",
        "# or it can be an externally provided plugin, with a custom name prefixed by `plugin`,\n",
        "# i.e. `plugin-<AUTH_PROVIDER_NAME>`.\n",
        "dbms.security.authentication_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "dbms.security.authorization_providers=native,plugin-com.neo4j.plugin.jwt.auth.JwtAuthPlugin\n",
        "\n",
        "# The time to live (TTL) for cached authentication and authorization info when using\n",
        "# external auth providers (LDAP or plugin). Setting the TTL to 0 will\n",
        "# disable auth caching.\n",
        "#dbms.security.auth_cache_ttl=10m\n",
        "\n",
        "# The maximum capacity for authentication and authorization caches (respectively).\n",
        "#dbms.security.auth_cache_max_capacity=10000\n",
        "\n",
        "# Set to log successful authentication events to the security log.\n",
        "# If this is set to `false` only failed authentication events will be logged, which\n",
        "# could be useful if you find that the successful events spam the logs too much,\n",
        "# and you do not require full auditing capability.\n",
        "#dbms.security.log_successful_authentication=true\n",
        "\n",
        "#================================================\n",
        "# LDAP Auth Provider Configuration\n",
        "#================================================\n",
        "\n",
        "# URL of LDAP server to use for authentication and authorization.\n",
        "# The format of the setting is `<protocol>://<hostname>:<port>`, where hostname is the only required field.\n",
        "# The supported values for protocol are `ldap` (default) and `ldaps`.\n",
        "# The default port for `ldap` is 389 and for `ldaps` 636.\n",
        "# For example: `ldaps://ldap.example.com:10389`.\n",
        "#\n",
        "# NOTE: You may want to consider using STARTTLS (`dbms.security.ldap.use_starttls`) instead of LDAPS\n",
        "# for secure connections, in which case the correct protocol is `ldap`.\n",
        "#dbms.security.ldap.host=localhost\n",
        "\n",
        "# Use secure communication with the LDAP server using opportunistic TLS.\n",
        "# First an initial insecure connection will be made with the LDAP server, and then a STARTTLS command\n",
        "# will be issued to negotiate an upgrade of the connection to TLS before initiating authentication.\n",
        "#dbms.security.ldap.use_starttls=false\n",
        "\n",
        "# The LDAP referral behavior when creating a connection. This is one of `follow`, `ignore` or `throw`.\n",
        "# `follow` automatically follows any referrals\n",
        "# `ignore` ignores any referrals\n",
        "# `throw` throws an exception, which will lead to authentication failure\n",
        "#dbms.security.ldap.referral=follow\n",
        "\n",
        "# The timeout for establishing an LDAP connection. If a connection with the LDAP server cannot be\n",
        "# established within the given time the attempt is aborted.\n",
        "# A value of 0 means to use the network protocol's (i.e., TCP's) timeout value.\n",
        "#dbms.security.ldap.connection_timeout=30s\n",
        "\n",
        "# The timeout for an LDAP read request (i.e. search). If the LDAP server does not respond within\n",
        "# the given time the request will be aborted. A value of 0 means wait for a response indefinitely.\n",
        "#dbms.security.ldap.read_timeout=30s\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authentication Configuration\n",
        "#----------------------------------\n",
        "\n",
        "# LDAP authentication mechanism. This is one of `simple` or a SASL mechanism supported by JNDI,\n",
        "# for example `DIGEST-MD5`. `simple` is basic username\n",
        "# and password authentication and SASL is used for more advanced mechanisms. See RFC 2251 LDAPv3\n",
        "# documentation for more details.\n",
        "#dbms.security.ldap.authentication.mechanism=simple\n",
        "\n",
        "# LDAP user DN template. An LDAP object is referenced by its distinguished name (DN), and a user DN is\n",
        "# an LDAP fully-qualified unique user identifier. This setting is used to generate an LDAP DN that\n",
        "# conforms with the LDAP directory's schema from the user principal that is submitted with the\n",
        "# authentication token when logging in.\n",
        "# The special token {0} is a placeholder where the user principal will be substituted into the DN string.\n",
        "#dbms.security.ldap.authentication.user_dn_template=uid={0},ou=users,dc=example,dc=com\n",
        "\n",
        "# Determines if the result of authentication via the LDAP server should be cached or not.\n",
        "# Caching is used to limit the number of LDAP requests that have to be made over the network\n",
        "# for users that have already been authenticated successfully. A user can be authenticated against\n",
        "# an existing cache entry (instead of via an LDAP server) as long as it is alive\n",
        "# (see `dbms.security.auth_cache_ttl`).\n",
        "# An important consequence of setting this to `true` is that\n",
        "# Neo4j then needs to cache a hashed version of the credentials in order to perform credentials\n",
        "# matching. This hashing is done using a cryptographic hash function together with a random salt.\n",
        "# Preferably a conscious decision should be made if this method is considered acceptable by\n",
        "# the security standards of the organization in which this Neo4j instance is deployed.\n",
        "#dbms.security.ldap.authentication.cache_enabled=true\n",
        "\n",
        "#----------------------------------\n",
        "# LDAP Authorization Configuration\n",
        "#----------------------------------\n",
        "# Authorization is performed by searching the directory for the groups that\n",
        "# the user is a member of, and then map those groups to Neo4j roles.\n",
        "\n",
        "# Perform LDAP search for authorization info using a system account instead of the user's own account.\n",
        "#\n",
        "# If this is set to `false` (default), the search for group membership will be performed\n",
        "# directly after authentication using the LDAP context bound with the user's own account.\n",
        "# The mapped roles will be cached for the duration of `dbms.security.auth_cache_ttl`,\n",
        "# and then expire, requiring re-authentication. To avoid frequently having to re-authenticate\n",
        "# sessions you may want to set a relatively long auth cache expiration time together with this option.\n",
        "# NOTE: This option will only work if the users are permitted to search for their\n",
        "# own group membership attributes in the directory.\n",
        "#\n",
        "# If this is set to `true`, the search will be performed using a special system account user\n",
        "# with read access to all the users in the directory.\n",
        "# You need to specify the username and password using the settings\n",
        "# `dbms.security.ldap.authorization.system_username` and\n",
        "# `dbms.security.ldap.authorization.system_password` with this option.\n",
        "# Note that this account only needs read access to the relevant parts of the LDAP directory\n",
        "# and does not need to have access rights to Neo4j, or any other systems.\n",
        "#dbms.security.ldap.authorization.use_system_account=false\n",
        "\n",
        "# An LDAP system account username to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "# Note that the `dbms.security.ldap.authentication.user_dn_template` will not be applied to this username,\n",
        "# so you may have to specify a full DN.\n",
        "#dbms.security.ldap.authorization.system_username\n",
        "\n",
        "# An LDAP system account password to use for authorization searches when\n",
        "# `dbms.security.ldap.authorization.use_system_account` is `true`.\n",
        "#dbms.security.ldap.authorization.system_password\n",
        "\n",
        "# The name of the base object or named context to search for user objects when LDAP authorization is enabled.\n",
        "# A common case is that this matches the last part of `dbms.security.ldap.authentication.user_dn_template`.\n",
        "#dbms.security.ldap.authorization.user_search_base=ou=users,dc=example,dc=com\n",
        "\n",
        "# The LDAP search filter to search for a user principal when LDAP authorization is\n",
        "# enabled. The filter should contain the placeholder token {0} which will be substituted for the\n",
        "# user principal.\n",
        "#dbms.security.ldap.authorization.user_search_filter=(&(objectClass=*)(uid={0}))\n",
        "\n",
        "# A list of attribute names on a user object that contains groups to be used for mapping to roles\n",
        "# when LDAP authorization is enabled. This setting is ignored when `dbms.ldap_authorization_nested_groups_enabled` is `true`.\n",
        "#dbms.security.ldap.authorization.group_membership_attributes=memberOf\n",
        "\n",
        "# This setting determines whether multiple LDAP search results will be processed (as is required for the lookup of nested groups).\n",
        "# If set to `true` then instead of using attributes on the user object to determine group membership (as specified by\n",
        "# `dbms.security.ldap.authorization.group_membership_attributes`), the `user` object will only be used to determine the user's\n",
        "# Distinguished Name, which will subsequently be used with  `dbms.security.ldap.authorization.user_search_filter`\n",
        "# in order to perform a nested group search. The Distinguished Names of the resultant group search results will be used to determine roles.\n",
        "#dbms.security.ldap.authorization.nested_groups_enabled=false\n",
        "\n",
        "# The search template which will be used to find the nested groups which the user is a member of.\n",
        "# The filter should contain the placeholder token `{0}` which will be substituted with the user's\n",
        "# Distinguished Name (which is found for the specified user principle using `dbms.security.ldap.authorization.user_search_filter`).\n",
        "# The default value specifies Active Directory's LDAP_MATCHING_RULE_IN_CHAIN (aka 1.2.840.113556.1.4.1941) implementation\n",
        "# which will walk the ancestry of group membership for the specified user.\n",
        "#dbms.security.ldap.authorization.nested_groups_search_filter=(&(objectclass=group)(member:1.2.840.113556.1.4.1941:={0}))\n",
        "\n",
        "# An authorization mapping from LDAP group names to Neo4j role names.\n",
        "# The map should be formatted as a semicolon separated list of key-value pairs, where the\n",
        "# key is the LDAP group name and the value is a comma separated list of corresponding role names.\n",
        "# For example: group1=role1;group2=role2;group3=role3,role4,role5\n",
        "#\n",
        "# You could also use whitespaces and quotes around group names to make this mapping more readable,\n",
        "# for example: dbms.security.ldap.authorization.group_to_role_mapping=\\\n",
        "#          \"cn=Neo4j Read Only,cn=users,dc=example,dc=com\"      = reader;    \\\n",
        "#          \"cn=Neo4j Read-Write,cn=users,dc=example,dc=com\"     = publisher; \\\n",
        "#          \"cn=Neo4j Schema Manager,cn=users,dc=example,dc=com\" = architect; \\\n",
        "#          \"cn=Neo4j Administrator,cn=users,dc=example,dc=com\"  = admin\n",
        "#dbms.security.ldap.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# OpenID Connect configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# The display name for the provider. This will be displayed in clients such as Neo4j Browser and Bloom.\n",
        "#dbms.security.oidc.<provider>.display_name\n",
        "\n",
        "# The OIDC auth_flow for clients such as Neo4j Browser and Bloom to use. Supported values are 'pkce' and 'implicit'\n",
        "#dbms.security.oidc.<provider>.auth_flow=pkce\n",
        "\n",
        "# The OpenID Connect Discovery URL for the provider\n",
        "#dbms.security.oidc.<provider>.well_known_discovery_uri\n",
        "\n",
        "# URL of the provider's Authorization Endpoint\n",
        "#dbms.security.oidc.<provider>.auth_endpoint\n",
        "\n",
        "# Parameters to use with the Authorization Endpoint.\n",
        "#dbms.security.oidc.<provider>.auth_params\n",
        "\n",
        "# URL of the provider's OAuth 2.0 Token Endpoint\n",
        "#dbms.security.oidc.<provider>.token_endpoint\n",
        "\n",
        "# Parameters to use with the Token Endpoint.\n",
        "#dbms.security.oidc.<provider>.token_params\n",
        "\n",
        "# URL of the provider's JSON Web Key Set\n",
        "#dbms.security.oidc.<provider>.jwks_uri\n",
        "\n",
        "# URL of the provider's UserInfo Endpoint\n",
        "#dbms.security.oidc.<provider>.user_info_uri\n",
        "\n",
        "# URL that the provider asserts as its issuer identifier. This will be checked against the iss claim in the token\n",
        "#dbms.security.oidc.<provider>.issuer\n",
        "\n",
        "# The expected value for the `aud` claim\n",
        "#dbms.security.oidc.<provider>.audience\n",
        "\n",
        "# The client_id of this client as issued by the provider.\n",
        "#dbms.security.oidc.<provider>.client_id\n",
        "\n",
        "# Whether to fetch the groups claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_groups_from_user_info=false\n",
        "\n",
        "# Whether to fetch the username claim from the user info endpoint on the identity provider. The default is false, read it from the token.\n",
        "#dbms.security.oidc.<provider>.get_username_from_user_info=false\n",
        "\n",
        "# The claim to use for the database username.\n",
        "#dbms.security.oidc.<provider>.claims.username=sub\n",
        "\n",
        "# The claim to use for the database roles.\n",
        "#dbms.security.oidc.<provider>.claims.groups\n",
        "\n",
        "# General parameters to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.params\n",
        "\n",
        "# General config to use with the Identity Provider.\n",
        "#dbms.security.oidc.<provider>.config\n",
        "\n",
        "# An authorization mapping from identity provider group names to Neo4j role names. See dbms.security.ldap.authorization.group_to_role_mapping above\n",
        "# for the format.\n",
        "#dbms.security.oidc.<provider>.authorization.group_to_role_mapping\n",
        "\n",
        "#*****************************************************************\n",
        "# Miscellaneous configuration\n",
        "#*****************************************************************\n",
        "\n",
        "# Compresses the metric archive files.\n",
        "server.metrics.csv.rotation.compression=zip\n",
        "\n",
        "# Determines if Cypher will allow using file URLs when loading data using\n",
        "# `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV`\n",
        "# clauses that load data from the file system.\n",
        "#dbms.security.allow_csv_import_from_file_urls=true\n",
        "\n",
        "\n",
        "# Value of the Access-Control-Allow-Origin header sent over any HTTP or HTTPS\n",
        "# connector. This defaults to '*', which allows broadest compatibility. Note\n",
        "# that any URI provided here limits HTTP/HTTPS access to that URI only.\n",
        "#dbms.security.http_access_control_allow_origin=*\n",
        "\n",
        "# Value of the HTTP Strict-Transport-Security (HSTS) response header. This header\n",
        "# tells browsers that a webpage should only be accessed using HTTPS instead of HTTP.\n",
        "# It is attached to every HTTPS response. Setting is not set by default so\n",
        "# 'Strict-Transport-Security' header is not sent. Value is expected to contain\n",
        "# directives like 'max-age', 'includeSubDomains' and 'preload'.\n",
        "#dbms.security.http_strict_transport_security\n",
        "\n",
        "# Retention policy for transaction logs needed to perform recovery and backups.\n",
        "#db.tx_log.rotation.retention_policy=2 days\n",
        "\n",
        "# Limit the number of IOs the background checkpoint process will consume per second.\n",
        "# This setting is advisory, is ignored in Neo4j Community Edition, and is followed to\n",
        "# best effort in Enterprise Edition.\n",
        "# An IO is in this case a 8 KiB (mostly sequential) write. Limiting the write IO in\n",
        "# this way will leave more bandwidth in the IO subsystem to service random-read IOs,\n",
        "# which is important for the response time of queries when the database cannot fit\n",
        "# entirely in memory. The only drawback of this setting is that longer checkpoint times\n",
        "# may lead to slightly longer recovery times in case of a database or system crash.\n",
        "# A lower number means lower IO pressure, and consequently longer checkpoint times.\n",
        "# Set this to -1 to disable the IOPS limit and remove the limitation entirely,\n",
        "# this will let the checkpointer flush data as fast as the hardware will go.\n",
        "# Removing the setting, or commenting it out, will set the default value of 600.\n",
        "# db.checkpoint.iops.limit=600\n",
        "\n",
        "# Whether or not any database on this instance are read_only by default.\n",
        "# If false, individual databases may be marked as read_only using dbms.database.read_only.\n",
        "# If true, individual databases may be marked as writable using dbms.databases.writable.\n",
        "#dbms.databases.default_to_read_only=false\n",
        "\n",
        "# Comma separated list of JAX-RS packages containing JAX-RS resources, one\n",
        "# package name for each mountpoint. The listed package names will be loaded\n",
        "# under the mountpoints specified. Uncomment this line to mount the\n",
        "# org.neo4j.examples.server.unmanaged.HelloWorldResource.java from\n",
        "# neo4j-server-examples under /examples/unmanaged, resulting in a final URL of\n",
        "# http://localhost:7474/examples/unmanaged/helloworld/{nodeId}\n",
        "#server.unmanaged_extension_classes=org.neo4j.examples.server.unmanaged=/examples/unmanaged\n",
        "\n",
        "# A comma separated list of procedures and user defined functions that are allowed\n",
        "# full access to the database through unsupported/insecure internal APIs.\n",
        "dbms.security.procedures.unrestricted=jwt.security.*\n",
        "\n",
        "# A comma separated list of procedures to be loaded by default.\n",
        "# Leaving this unconfigured will load all procedures found.\n",
        "#dbms.security.procedures.allowlist=apoc.coll.*,apoc.load.*,gds.*\n",
        "\n",
        "# For how long should drivers cache the discovery data from\n",
        "# the dbms.routing.getRoutingTable() procedure. Defaults to 300s.\n",
        "#dbms.routing_ttl=300s\n",
        "\n",
        "#********************************************************************\n",
        "# JVM Parameters\n",
        "#********************************************************************\n",
        "\n",
        "# G1GC generally strikes a good balance between throughput and tail\n",
        "# latency, without too much tuning.\n",
        "server.jvm.additional=-XX:+UseG1GC\n",
        "\n",
        "# Have common exceptions keep producing stack traces, so they can be\n",
        "# debugged regardless of how often logs are rotated.\n",
        "server.jvm.additional=-XX:-OmitStackTraceInFastThrow\n",
        "\n",
        "# Make sure that `initmemory` is not only allocated, but committed to\n",
        "# the process, before starting the database. This reduces memory\n",
        "# fragmentation, increasing the effectiveness of transparent huge\n",
        "# pages. It also reduces the possibility of seeing performance drop\n",
        "# due to heap-growing GC events, where a decrease in available page\n",
        "# cache leads to an increase in mean IO response time.\n",
        "# Try reducing the heap memory, if this flag degrades performance.\n",
        "server.jvm.additional=-XX:+AlwaysPreTouch\n",
        "\n",
        "# Trust that non-static final fields are really final.\n",
        "# This allows more optimizations and improves overall performance.\n",
        "# NOTE: Disable this if you use embedded mode, or have extensions or dependencies that may use reflection or\n",
        "# serialization to change the value of final fields!\n",
        "server.jvm.additional=-XX:+UnlockExperimentalVMOptions\n",
        "server.jvm.additional=-XX:+TrustFinalNonStaticFields\n",
        "\n",
        "# Disable explicit garbage collection, which is occasionally invoked by the JDK itself.\n",
        "server.jvm.additional=-XX:+DisableExplicitGC\n",
        "\n",
        "# Allow Neo4j to use @Contended annotation\n",
        "server.jvm.additional=-XX:-RestrictContended\n",
        "\n",
        "# Restrict size of cached JDK buffers to 1 KB\n",
        "server.jvm.additional=-Djdk.nio.maxCachedBufferSize=1024\n",
        "\n",
        "# More efficient buffer allocation in Netty by allowing direct no cleaner buffers.\n",
        "server.jvm.additional=-Dio.netty.tryReflectionSetAccessible=true\n",
        "\n",
        "# Exits JVM on the first occurrence of an out-of-memory error. Its preferable to restart VM in case of out of memory errors.\n",
        "# server.jvm.additional=-XX:+ExitOnOutOfMemoryError\n",
        "\n",
        "# Expand Diffie Hellman (DH) key size from default 1024 to 2048 for DH-RSA cipher suites used in server TLS handshakes.\n",
        "# This is to protect the server from any potential passive eavesdropping.\n",
        "server.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048\n",
        "\n",
        "# This mitigates a DDoS vector.\n",
        "server.jvm.additional=-Djdk.tls.rejectClientInitiatedRenegotiation=true\n",
        "\n",
        "# Enable remote debugging\n",
        "#server.jvm.additional=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\n",
        "\n",
        "# This filter prevents deserialization of arbitrary objects via java object serialization, addressing potential vulnerabilities.\n",
        "# By default this filter whitelists all neo4j classes, as well as classes from the hazelcast library and the java standard library.\n",
        "# These defaults should only be modified by expert users!\n",
        "# For more details (including filter syntax) see: https://openjdk.java.net/jeps/290\n",
        "#server.jvm.additional=-Djdk.serialFilter=java.**;org.neo4j.**;com.neo4j.**;com.hazelcast.**;net.sf.ehcache.Element;com.sun.proxy.*;org.openjdk.jmh.**;!*\n",
        "\n",
        "# Increase the default flight recorder stack sampling depth from 64 to 256, to avoid truncating frames when profiling.\n",
        "server.jvm.additional=-XX:FlightRecorderOptions=stackdepth=256\n",
        "\n",
        "# Allow profilers to sample between safepoints. Without this, sampling profilers may produce less accurate results.\n",
        "server.jvm.additional=-XX:+UnlockDiagnosticVMOptions\n",
        "server.jvm.additional=-XX:+DebugNonSafepoints\n",
        "\n",
        "# Open modules for neo4j to allow internal access\n",
        "server.jvm.additional=--add-opens=java.base/java.nio=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/java.io=ALL-UNNAMED\n",
        "server.jvm.additional=--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n",
        "\n",
        "# Disable logging JMX endpoint.\n",
        "server.jvm.additional=-Dlog4j2.disable.jmx=true\n",
        "\n",
        "# Limit JVM metaspace and code cache to allow garbage collection. Used by cypher for code generation and may grow indefinitely unless constrained.\n",
        "# Useful for memory constrained environments\n",
        "#server.jvm.additional=-XX:MaxMetaspaceSize=1024m\n",
        "#server.jvm.additional=-XX:ReservedCodeCacheSize=512m\n",
        "\n",
        "# Allow big methods to be JIT compiled.\n",
        "# Useful for big queries and big expressions where cypher code generation can create large methods.\n",
        "#server.jvm.additional=-XX:-DontCompileHugeMethods\n",
        "\n",
        "#********************************************************************\n",
        "# Wrapper Windows NT/2000/XP Service Properties\n",
        "#********************************************************************\n",
        "# WARNING - Do not modify any of these properties when an application\n",
        "#  using this configuration file has been installed as a service.\n",
        "#  Please uninstall the service before modifying this section.  The\n",
        "#  service can then be reinstalled.\n",
        "\n",
        "# Name of the service\n",
        "server.windows_service_name=neo4j\n",
        "\n",
        "#********************************************************************\n",
        "# Other Neo4j system properties\n",
        "#********************************************************************\n",
        "\n",
        "dbms.memory.heap.initial_size=512m\n",
        "dbms.memory.heap.max_size=1G\n",
        "dbms.memory.pagecache.size=512m\n",
        "dbms.backup.enabled=false\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true -Xss1G\n",
        "dbms.jvm.additional=-Dlog4j2.formatMsgNoLookups=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install TTS\n",
        "!pip install speake3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tts --list_models\n",
        "!tts --text \"Hello, how are you?\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: Full Terms & Conditions of access and use can be found at\n",
            "https://www.tandfonline.com/action/journalInformation?journalCode=gcry20\n",
            "Crystallography Reviews\n",
            "ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/gcry20\n",
            "Machine learning applications in macromolecular\n",
            "X-ray crystallography\n",
            "Melanie Vollmar & Gwyndaf Evans\n",
            "To cite this article:  Melanie Vollmar & Gwyndaf Evans (2021) Machine learning applications\n",
            "in macromolecular X-ray crystallography, Crystallography Reviews, 27:2, 54-101, DOI:\n",
            "10.1080/0889311X.2021.1982914\n",
            "To link to this article:  https://doi.org/10.1080/0889311X.2021.1982914\n",
            "© 2021 The Author(s). Published by Informa\n",
            "UK Limited, trading as Taylor & Francis\n",
            "Group\n",
            "Published online: 04 Oct 2021.\n",
            "Submit your article to this journal \n",
            "Article views: 3426\n",
            "View related articles \n",
            "View Crossmark data\n",
            "Citing articles: 2 View citing articles \n",
            "\n",
            " > Text splitted to sentences.\n",
            "['Full Terms & Conditions of access and use can be found at', 'https://www.tandfonline.com/action/journalInformation?', 'journalCode=gcry20', 'Crystallography Reviews', 'ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/gcry20', 'Machine learning applications in macromolecular', 'X-ray crystallography', 'Melanie Vollmar & Gwyndaf Evans', 'To cite this article:  Melanie Vollmar & Gwyndaf Evans (2021) Machine learning applications', 'in macromolecular X-ray crystallography, Crystallography Reviews, 27:2, 54-101, DOI:', '10.1080/0889311X.2021.1982914', 'To link to this article:  https://doi.org/10.1080/0889311X.2021.1982914', '© 2021 The Author(s).', 'Published by Informa', 'UK Limited, trading as Taylor & Francis', 'Group', 'Published online: 04 Oct 2021.', 'Submit your article to this journal', 'Article views: 3426', 'View related articles', 'View Crossmark data', 'Citing articles: 2 View citing articles']\n",
            " > Processing time: 30.841668844223022\n",
            " > Real-time factor: 0.27716775269608646\n",
            " > Saving output to AUDIO_OUTPUTS/page_00.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHYREVIEWS\n",
            "2021,VOL.27,NO.2,54–101https://doi.org/10.1080/0889311X.2021.1982914\n",
            "REVIEW\n",
            "Machine learning applications in macromolecular X-ray\n",
            "crystallography\n",
            "Melanie Vollmaraand Gwyndaf Evansa,b\n",
            "aDiamondLightSourceLtd.,HarwellScience&InnovationCampus,Didcot,UK;bRosalindFranklinInstitute,\n",
            "HarwellScience&InnovationCampus,Didcot,UK\n",
            "ABSTRACT\n",
            "After more than half a century of evolution, machine learning and\n",
            "artificial intelligence, in general, are entering a truly exciting eraof broad application in commercial and research sectors. In X-raycrystallography, and its application to structural biology, machinelearning is finding a home within expert and automated systems,is forecasting experiment and data analysis outcomes, is predictingwhether crystals can be grown and even generating macromolecu-lar structures. This review provides a historical perspective on AI andmachine learning, offers an introduction and guide to its applicationin crystallography and concludes with topical examples of how it iscurrently influencing macromolecular crystallography.ARTICLE HISTORY\n",
            "Received7May2021\n",
            "Accepted16September2021\n",
            "KEYWORDS\n",
            "Machinelearning;bigdata;\n",
            "automation;macromolecular\n",
            "X-raycrystallography;\n",
            "synchrotron;structuralbiology\n",
            "Contents PAGE\n",
            "Introduction 55\n",
            "Summary and evolution of machine learning and artificial intelligence 58\n",
            "Common concepts and terminology 62\n",
            "Supervised,unsupervisedandreinforcementlearning 62\n",
            "Regressionandclassification 62\n",
            "Types of algorithms 63\n",
            "Dimensionalityreduction 63\n",
            "Bayesianalgorithms 63\n",
            "Shallowlearners 65\n",
            "Deeplearners 66\n",
            "Exploratory data analysis and initial assessment 67\n",
            "Findattributes/features/variablesinthedataset 69\n",
            "Conductunivariatedataanalysistoidentifydatadistributions 70\n",
            "Usebivariate/multivariatedataanalysistoidentifyrelationshipsandinteractions 70\n",
            "Detectmissingvalues 71\n",
            "Detectoutliers 71\n",
            "Featureengineering 71\n",
            "CONTACT MelanieVollmar melanie.vollmar@diamond.ac.uk\n",
            "©2021TheAuthor(s).PublishedbyInformaUKLimited,tradingasTaylor&FrancisGroup\n",
            "This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http://creativecommons.\n",
            "org/licenses/by/4.0/ ),whichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperly\n",
            "cited.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHYREVIEWS', '2021,VOL.27,NO.2,54–101https://doi.org/10.1080/0889311X.2021.1982914', 'REVIEW', 'Machine learning applications in macromolecular X-ray', 'crystallography', 'Melanie Vollmaraand Gwyndaf Evansa,b', 'aDiamondLightSourceLtd.', ',HarwellScience&InnovationCampus,Didcot,UK;bRosalindFranklinInstitute,', 'HarwellScience&InnovationCampus,Didcot,UK', 'ABSTRACT', 'After more than half a century of evolution, machine learning and', 'artificial intelligence, in general, are entering a truly exciting eraof broad application in commercial and research sectors.', 'In X-raycrystallography, and its application to structural biology, machinelearning is finding a home within expert and automated systems,is forecasting experiment and data analysis outcomes, is predictingwhether crystals can be grown and even generating macromolecu-lar structures.', 'This review provides a historical perspective on AI andmachine learning, offers an introduction and guide to its applicationin crystallography and concludes with topical examples of how it iscurrently influencing macromolecular crystallography.', 'ARTICLE HISTORY', 'Received7May2021', 'Accepted16September2021', 'KEYWORDS', 'Machinelearning;bigdata;', 'automation;macromolecular', 'X-raycrystallography;', 'synchrotron;structuralbiology', 'Contents PAGE', 'Introduction 55', 'Summary and evolution of machine learning and artificial intelligence 58', 'Common concepts and terminology 62', 'Supervised,unsupervisedandreinforcementlearning 62', 'Regressionandclassification 62', 'Types of algorithms 63', 'Dimensionalityreduction 63', 'Bayesianalgorithms 63', 'Shallowlearners 65', 'Deeplearners 66', 'Exploratory data analysis and initial assessment 67', 'Findattributes/features/variablesinthedataset 69', 'Conductunivariatedataanalysistoidentifydatadistributions 70', 'Usebivariate/multivariatedataanalysistoidentifyrelationshipsandinteractions 70', 'Detectmissingvalues 71', 'Detectoutliers 71', 'Featureengineering 71', 'CONTACT MelanieVollmar melanie.vollmar@diamond.ac.uk', '©2021TheAuthor(s).', 'PublishedbyInformaUKLimited,tradingasTaylor&FrancisGroup', 'This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http://creativecommons.', 'org/licenses/by/4.0/ ),whichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperly', 'cited.']\n",
            " > Processing time: 53.24688720703125\n",
            " > Real-time factor: 0.24472198614055485\n",
            " > Saving output to AUDIO_OUTPUTS/page_01.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 55\n",
            "Performance measures and producing robust results 72\n",
            "Performancemeasures 72\n",
            "Robustness 76\n",
            "Examples of machine learning in the field of biological crystallography 77\n",
            "Serialcrystallography 77\n",
            "Usageinmodelbuildingandrefinementsoftwarepackages 80\n",
            "Decisionmakinginautomateddataanalysispipelines 84\n",
            "Proteincrystallizability 86\n",
            "Crystallizationoutcomeclassification 89\n",
            "Proteinmodelling 90\n",
            "Concluding remarks 93\n",
            "Acknowledgements 94\n",
            "Disclosure statement 94\n",
            "Funding 94\n",
            "ORCID 95\n",
            "References 95\n",
            "Subject Index 101\n",
            "Introduction\n",
            "After more than half a century in its development with numerous false starts along the\n",
            "way,artificialintelligence(AI)anditssub-fieldmachinelearning(ML;nottobeconfused\n",
            "withmaximumlikelihood,amethodfrequentlyemployedinmacromolecularcrystallogra-\n",
            "phy;MX)arenowbeginningtohaveameasurableimpactinMX.Real-worldexamplesformachinelearningandAIapplicationsthatmanyofushavebecomefamiliarwitharedriver-lesscars,speechrecognitionandtargetedadvertisement.Thesesystemsareverymuchofadynamicnaturerequiringnearreal-timedecisionmakingandadaptationondifferentlev-elswithintheapplication.InMX,manyautomatedprocessesarestaticwithadefinedstartand end point running in a linear fashion. Many such processes may run in parallel andcommunicationbetweenthemistypicallylimited.Keydecision-makingpointsinautoma-tionprovideexcellentopportunitiestoapplyMLandAI-basedcontrolandguidanceusingtheexperiencesgainedfromreal-worldapplicationslikethoselistedabove.ForMX,sev-eralfactorshavecontributedtoaburstofactivityandrelativesuccess:first,thereisgreateraccessibilitytosoftware librariesthatallownumerous approaches tomachinelearningtobetriedandevaluatedforMXapplications;second,thereisawealthoftrainingdataavail-able that were the result of decades of macromolecular crystal structure determinations,predominantlyasaby-productofthestructuralgenomicsboomintheearly2000s[ 1–5];\n",
            "andfinally,thewideavailabilitytomanyresearchersofhigh-performancecomputingfacil-itiesmakesthemanagementofthesedataandtrainingofmachinelearningsystemsamoreeasilyachievabletask.\n",
            "Thelast20yearshavewitnessedaremarkablerevolutioninsynchrotronmacromolecu-\n",
            "larX-raycrystallography.Broadapplicationofinstrumentautomation,andexpertcontrol\n",
            "andanalysissystemshaveconspiredtomakeMXaccessibletoafarwiderpoolofstructural\n",
            "and medical biologists, both academia and industry based, who strive to understand the\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 55', 'Performance measures and producing robust results 72', 'Performancemeasures 72', 'Robustness 76', 'Examples of machine learning in the field of biological crystallography 77', 'Serialcrystallography 77', 'Usageinmodelbuildingandrefinementsoftwarepackages 80', 'Decisionmakinginautomateddataanalysispipelines 84', 'Proteincrystallizability 86', 'Crystallizationoutcomeclassification 89', 'Proteinmodelling 90', 'Concluding remarks 93', 'Acknowledgements 94', 'Disclosure statement 94', 'Funding 94', 'ORCID 95', 'References 95', 'Subject Index 101', 'Introduction', 'After more than half a century in its development with numerous false starts along the', 'way,artificialintelligence(AI)anditssub-fieldmachinelearning(ML;nottobeconfused', 'withmaximumlikelihood,amethodfrequentlyemployedinmacromolecularcrystallogra-', 'phy;MX)arenowbeginningtohaveameasurableimpactinMX.Real-worldexamplesformachinelearningandAIapplicationsthatmanyofushavebecomefamiliarwitharedriver-lesscars,speechrecognitionandtargetedadvertisement.', 'Thesesystemsareverymuchofadynamicnaturerequiringnearreal-timedecisionmakingandadaptationondifferentlev-elswithintheapplication.', 'InMX,manyautomatedprocessesarestaticwithadefinedstartand end point running in a linear fashion.', 'Many such processes may run in parallel andcommunicationbetweenthemistypicallylimited.', 'Keydecision-makingpointsinautoma-tionprovideexcellentopportunitiestoapplyMLandAI-basedcontrolandguidanceusingtheexperiencesgainedfromreal-worldapplicationslikethoselistedabove.', 'ForMX,sev-eralfactorshavecontributedtoaburstofactivityandrelativesuccess:first,thereisgreateraccessibilitytosoftware librariesthatallownumerous approaches tomachinelearningtobetriedandevaluatedforMXapplications;second,thereisawealthoftrainingdataavail-able that were the result of decades of macromolecular crystal structure determinations,predominantlyasaby-productofthestructuralgenomicsboomintheearly2000s[ 1–5];', 'andfinally,thewideavailabilitytomanyresearchersofhigh-performancecomputingfacil-itiesmakesthemanagementofthesedataandtrainingofmachinelearningsystemsamoreeasilyachievabletask.', 'Thelast20yearshavewitnessedaremarkablerevolutioninsynchrotronmacromolecu-', 'larX-raycrystallography.', 'Broadapplicationofinstrumentautomation,andexpertcontrol', 'andanalysissystemshaveconspiredtomakeMXaccessibletoafarwiderpoolofstructural', 'and medical biologists, both academia and industry based, who strive to understand the']\n",
            " > Processing time: 44.88263702392578\n",
            " > Real-time factor: 0.23494824302643608\n",
            " > Saving output to AUDIO_OUTPUTS/page_02.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 56 M. VOLLMAR AND G. EVANS\n",
            "Figure 1. CumulativenumberofproteinstructuredepositionssincethestartoftheProteinDataBank.\n",
            "PlottedarethecontributingmethodsX-raydiﬀractionusingsynchrotronradiation(X-raysynchrotron;\n",
            "source:https://biosync.rcsb.org/)orusingothersources(X-rayother),nuclearmagneticresonance(NMR)\n",
            "andcryo-electronmicroscopy(EM)byyear.\n",
            "fundamentalmechanismsofbiologicalmoleculesandcells,thebasisofdiseasesandseek\n",
            "todevelopdrugsandtherapeuticstotacklethem.\n",
            "Around 88% of the ∼180,000 MX structures deposited in the PDB (Protein Data\n",
            "Bank) [6] have come from synchrotron data with currently about 10,000 structures per\n",
            "yearbeingdeposited(Figure 1).ThishasbeenenabledbyautomationofX-raybeamlines,\n",
            "logisticstomanagesampleshippingandhandlingatthesynchrotron,fullyautomated[ 7]\n",
            "or unattended data collection (UDC) [ 8,9], automated data analysis pipelines [ 10]a n d\n",
            "underpinning it all a wealth of investment in X-ray beamline sources, optics, detectors,\n",
            "end-stations [11 ] and expert staff to develop and maintain these complex systems. The\n",
            "COVID-19 pandemic has forced the hand of many synchrotrons in making their beam-linesavailableonlybyremoteaccess.Fortunately,thedevelopmentsatmanysynchrotronsites worldwide over the last two decades have positioned MX perfectly to cope with thepracticalconstraintsimposedbythepandemic.Forexample,atDiamondthroughout2020user access to MX beamlines was almost exclusively remote and in the 11 months fromJune 2020 over 33,000 data sets were measured through UDC, with typically less than 3minbeingusedforeachcrystalsample.Thisisfromover500UDCusersessionsthatwererunalongsidemoretypicalremoteaccesssessionswhereusersinteractivelymeasuredtheirdata.AutomateddatacollectionandanalysisatMXbeamlinesisnowseenasanessentialcomponentofcurrentandfutureoperationsformanysynchrotrons.\n",
            "The automated analysis of the wealth of data arising from modern MX beamlines,\n",
            "as illustrated above, presents a whole new set of challenges. In recent years, a rather\n",
            "straightforward brute-force approach to automated data analysis has been challenged by\n",
            "the evolution of beamline technologies resultingin extraordinarydata rates and data vol-umesfromthelatesthighframerate( >100fps)andmulti-megapixeldetectors(typically\n",
            "between4and16Mpixelsperframe).Thechallengeiscompoundedbythedesirebymanyusers, and indeed the automated systems themselves to have real-time feedback on the\n",
            " > Text splitted to sentences.\n",
            "['56 M. VOLLMAR AND G. EVANS', 'Figure 1.', 'CumulativenumberofproteinstructuredepositionssincethestartoftheProteinDataBank.', 'PlottedarethecontributingmethodsX-raydiﬀractionusingsynchrotronradiation(X-raysynchrotron;', 'source:https://biosync.rcsb.org/)orusingothersources(X-rayother),nuclearmagneticresonance(NMR)', 'andcryo-electronmicroscopy(EM)byyear.', 'fundamentalmechanismsofbiologicalmoleculesandcells,thebasisofdiseasesandseek', 'todevelopdrugsandtherapeuticstotacklethem.', 'Around 88% of the ∼180,000 MX structures deposited in the PDB (Protein Data', 'Bank) [6] have come from synchrotron data with currently about 10,000 structures per', 'yearbeingdeposited(Figure 1).', 'ThishasbeenenabledbyautomationofX-raybeamlines,', 'logisticstomanagesampleshippingandhandlingatthesynchrotron,fullyautomated[ 7]', 'or unattended data collection (UDC) [ 8,9], automated data analysis pipelines [ 10]a n d', 'underpinning it all a wealth of investment in X-ray beamline sources, optics, detectors,', 'end-stations [11 ] and expert staff to develop and maintain these complex systems.', 'The', 'COVID-19 pandemic has forced the hand of many synchrotrons in making their beam-linesavailableonlybyremoteaccess.', 'Fortunately,thedevelopmentsatmanysynchrotronsites worldwide over the last two decades have positioned MX perfectly to cope with thepracticalconstraintsimposedbythepandemic.', 'Forexample,atDiamondthroughout2020user access to MX beamlines was almost exclusively remote and in the 11 months fromJune 2020 over 33,000 data sets were measured through UDC, with typically less than 3minbeingusedforeachcrystalsample.', 'Thisisfromover500UDCusersessionsthatwererunalongsidemoretypicalremoteaccesssessionswhereusersinteractivelymeasuredtheirdata.', 'AutomateddatacollectionandanalysisatMXbeamlinesisnowseenasanessentialcomponentofcurrentandfutureoperationsformanysynchrotrons.', 'The automated analysis of the wealth of data arising from modern MX beamlines,', 'as illustrated above, presents a whole new set of challenges.', 'In recent years, a rather', 'straightforward brute-force approach to automated data analysis has been challenged by', 'the evolution of beamline technologies resultingin extraordinarydata rates and data vol-umesfromthelatesthighframerate( >100fps)andmulti-megapixeldetectors(typically', 'between4and16Mpixelsperframe).', 'Thechallengeiscompoundedbythedesirebymanyusers, and indeed the automated systems themselves to have real-time feedback on the']\n",
            " > Processing time: 47.99934411048889\n",
            " > Real-time factor: 0.2534088887869057\n",
            " > Saving output to AUDIO_OUTPUTS/page_03.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 57\n",
            "quality of data being measured. With the emerging minimization of human intervention\n",
            "fromdataassessmentandanalysis,therehasbeenanincreaseintheimportanceofauto-matedexpert systems to steer such analysis. More fundamentally, as the yearly quantities\n",
            "of measured data from large-scale facilities like Diamond reach many-Petabyte levels thequestion of whether ML and/or AI applications can be used to determine which data tostoreandwhichtodiscardisbeingasked[ 12].\n",
            "Many expert systems to date have represented linear sequences of tasks that require\n",
            "l i t t l ed e c i s i o nm a k i n gb u te i t h e rf a i lo rs u c c e e db a s e do nt h ed a t aq u a l i t y[ 13,14]. How-\n",
            "ever, the evaluation of crystallographic data quality is a contextual and multidimensionalproblem with no single metric telling the full story about the usefulness of a data set inanswering a given scientific question. Trained crystallographers use their experience andknowledge, together with multiple quality indicators [ 15–17], to make decisions about\n",
            "whichsoftwaretouse,whichstructuredeterminationmethodislikelytosucceed,whetheritisworthproceedingwithanalysisorwhetheritmaybebettertoreturntotheexperimen-talstageofdatacollectionorevensamplepreparation[ 18].Butcodingthisexpertiseand\n",
            "knowledgeintoacomprehensive,robustandhelpfulautonomousdataanalysispipelineisnon-trivial.\n",
            "Machinelearningandartificialintelligenceare,however,beginningtoplayanimportant\n",
            "roleinaddressingthisproblembyadoptingtheroleofanexpertcrystallographer.Indeed,structure determination by crystallography is a problem that is particularly well-suitedto the application of machine learning and is a problem that stands to benefit substan-\n",
            "tially from it. The wealth of existing diffraction data and solved structures, captured for\n",
            "macromolecular crystallography in the PDB and the many data sets stored long term atsynchrotronsites,providearichfoundationforMLandAImethods.Aswellasassistinginthe automation of standard structure solution methods, even more challenging structureproblems in crystallography and structural biology around protein structure predictionfromaminoacidsequencedata[ 19],thequestionofcrystallizabilityandeventhedetection\n",
            "ofthepresenceofcrystalsincrystallizationtrials[ 20]cannowbetackled.Brunoetal.[ 20]\n",
            "estimatethatforcommonlyusedexperimentalsetupsinahigh-throughputenvironment(often around 1,000 96-well plates per year) about 1 hour will be needed to manuallyevaluate the crystallization trials in one such plate. Repeat evaluations are usually donet h r o u g h o u tt h el i f e s p a no fs u c hat r i a l .A s s u m i n g1h o u r / p l a t e ,i tw o u l dt a k e4 2f u l ld a y sto evaluate each of the 1,000 plates once. Five repeats will amount to 210 24-hour days.AlthoughnotstateddirectlybyBrunoetal.[ 20]oneimaginesthatadeeplearning-based\n",
            "systemwouldbeabletoachievethesametaskinamuchshortertime.\n",
            "Forthefirsttimeindecades,thereisconsiderable,andrealistic,interestaboutthepracti-\n",
            "calapplicabilityandusefulnessofsuchmethodsandtheresultstheyproduce.Recently,thisinteresthasreachedfever-pitchwiththelatestresultsofthebiennialCriticalAssessmentofproteinStructurePrediction(CASP)[ 21]thatchallengesscientisttosolvethe50-year-old\n",
            "grand challenge of protein folding. Remarkable results from the use of AlphaFold2 have\n",
            "offered a clearer illustration of the potential of AI for tackling one of the most complex\n",
            "problemsinbiology.\n",
            "This review offers a crystallographer’s eye view of artificial intelligence and machine\n",
            "learning, starting with a comprehensive summary of their evolution, then recipes for\n",
            "theirapplicationincrystallographyandfinishingwithrecentexamplesoftheirsuccessfulutilizationinourfield.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 57', 'quality of data being measured.', 'With the emerging minimization of human intervention', 'fromdataassessmentandanalysis,therehasbeenanincreaseintheimportanceofauto-matedexpert systems to steer such analysis.', 'More fundamentally, as the yearly quantities', 'of measured data from large-scale facilities like Diamond reach many-Petabyte levels thequestion of whether ML and/or AI applications can be used to determine which data tostoreandwhichtodiscardisbeingasked[ 12].', 'Many expert systems to date have represented linear sequences of tasks that require', 'l i t t l ed e c i s i o nm a k i n gb u te i t h e rf a i lo rs u c c e e db a s e do nt h ed a t aq u a l i t y[ 13,14].', 'How-', 'ever, the evaluation of crystallographic data quality is a contextual and multidimensionalproblem with no single metric telling the full story about the usefulness of a data set inanswering a given scientific question.', 'Trained crystallographers use their experience andknowledge, together with multiple quality indicators [ 15–17], to make decisions about', 'whichsoftwaretouse,whichstructuredeterminationmethodislikelytosucceed,whetheritisworthproceedingwithanalysisorwhetheritmaybebettertoreturntotheexperimen-talstageofdatacollectionorevensamplepreparation[ 18].', 'Butcodingthisexpertiseand', 'knowledgeintoacomprehensive,robustandhelpfulautonomousdataanalysispipelineisnon-trivial.', 'Machinelearningandartificialintelligenceare,however,beginningtoplayanimportant', 'roleinaddressingthisproblembyadoptingtheroleofanexpertcrystallographer.', 'Indeed,structure determination by crystallography is a problem that is particularly well-suitedto the application of machine learning and is a problem that stands to benefit substan-', 'tially from it.', 'The wealth of existing diffraction data and solved structures, captured for', 'macromolecular crystallography in the PDB and the many data sets stored long term atsynchrotronsites,providearichfoundationforMLandAImethods.', 'Aswellasassistinginthe automation of standard structure solution methods, even more challenging structureproblems in crystallography and structural biology around protein structure predictionfromaminoacidsequencedata[ 19],thequestionofcrystallizabilityandeventhedetection', 'ofthepresenceofcrystalsincrystallizationtrials[ 20]cannowbetackled.', 'Brunoetal.', '[ 20]', 'estimatethatforcommonlyusedexperimentalsetupsinahigh-throughputenvironment(often around 1,000 96-well plates per year) about 1 hour will be needed to manuallyevaluate the crystallization trials in one such plate.', 'Repeat evaluations are usually donet h r o u g h o u tt h el i f e s p a no fs u c hat r i a l .', 'A s s u m i n g1h o u r / p l a t e ,i tw o u l dt a k e4 2f u l ld a y sto evaluate each of the 1,000 plates once.', 'Five repeats will amount to 210 24-hour days.', 'AlthoughnotstateddirectlybyBrunoetal.', '[ 20]oneimaginesthatadeeplearning-based', 'systemwouldbeabletoachievethesametaskinamuchshortertime.', 'Forthefirsttimeindecades,thereisconsiderable,andrealistic,interestaboutthepracti-', 'calapplicabilityandusefulnessofsuchmethodsandtheresultstheyproduce.', 'Recently,thisinteresthasreachedfever-pitchwiththelatestresultsofthebiennialCriticalAssessmentofproteinStructurePrediction(CASP)[ 21]thatchallengesscientisttosolvethe50-year-old', 'grand challenge of protein folding.', 'Remarkable results from the use of AlphaFold2 have', 'offered a clearer illustration of the potential of AI for tackling one of the most complex', 'problemsinbiology.', 'This review offers a crystallographer’s eye view of artificial intelligence and machine', 'learning, starting with a comprehensive summary of their evolution, then recipes for', 'theirapplicationincrystallographyandfinishingwithrecentexamplesoftheirsuccessfulutilizationinourfield.']\n",
            " > Processing time: 69.1887857913971\n",
            " > Real-time factor: 0.24582486746906687\n",
            " > Saving output to AUDIO_OUTPUTS/page_04.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 58 M. VOLLMAR AND G. EVANS\n",
            "Figure 2. Schematic illustration to explain the relationship between artiﬁcial intelligence, machine\n",
            "learninganddeeplearning.\n",
            "Summary and evolution of machine learning and artificial intelligence\n",
            "Statistical methods are the basis for the development of algorithms that form the core of\n",
            "machine learning applications within the field of artificial intelligence. The relationshipbetweenartificialintelligence,machinelearninganddeeplearningisgiveninFigure 2.One\n",
            "keyconceptinartificialintelligenceisthatofnotexplicitlyprogramminganalgorithm,i.e.adeveloperorprogrammerwillnotdirectlyencodethepredictionanswerforeachpossiblesample.Instead,astatisticalmodelwithasetofparameters(weights)representinggeneraldecision/associationrulesiscreatedbasedontheknownpropertiesofasubsetofdata.Thelearned weights can then be applied to any new sample and a prediction about this sam-pleismade.Anyinformationorpatternfoundwithinthedatadistributionisthenusedindecision making after having been discovered by the algorithm itself [ 22,23]. Limitations\n",
            "are given by the design of the algorithm to be trained and discovery of new knowledgeisunlikelyastheygenerallylackhumaningenuityandcross-disciplinaryinsight.Instead,MLandAI-baseda p plica tio nsserveastoolstowo rkwi thlargeda tavol umes,highlyspe-cifictasksandautomateddecisionmakingtospeedupprocesses.Nosinglepersoncanbecredited with the invention of ML and/or AI. Instead, this has been an evolutionary pro-cessovermanydecadeswithuncountedknownandunknowndevelopersbeinginvolved.B e l o ww eg i v es o m ek e ye v e n t st h a tp u s h e dt h e s ed e v e l o p m e n t s .Ag r a p h i c a lt i m e l i n ei sgiveninFigure 3.\n",
            "The underlying idea, for artificial intelligence in particular, is based on a model of the\n",
            "way brain cells (neurons) or nodes in a neural network communicate with each otherand transfer information through electrical signals as was proposed by McCulloch andPitts [24]a n dH e b b[ 25]. Special emphasis is placed on the visual cortex based on the\n",
            "research by Hubel and Wiesel [ 26,27]. An electrical signal can have an activating or\n",
            " > Text splitted to sentences.\n",
            "['58 M. VOLLMAR AND G. EVANS', 'Figure 2. Schematic illustration to explain the relationship between artiﬁcial intelligence, machine', 'learninganddeeplearning.', 'Summary and evolution of machine learning and artificial intelligence', 'Statistical methods are the basis for the development of algorithms that form the core of', 'machine learning applications within the field of artificial intelligence.', 'The relationshipbetweenartificialintelligence,machinelearninganddeeplearningisgiveninFigure 2.One', 'keyconceptinartificialintelligenceisthatofnotexplicitlyprogramminganalgorithm,i.e.adeveloperorprogrammerwillnotdirectlyencodethepredictionanswerforeachpossiblesample.', 'Instead,astatisticalmodelwithasetofparameters(weights)representinggeneraldecision/associationrulesiscreatedbasedontheknownpropertiesofasubsetofdata.', 'Thelearned weights can then be applied to any new sample and a prediction about this sam-pleismade.', 'Anyinformationorpatternfoundwithinthedatadistributionisthenusedindecision making after having been discovered by the algorithm itself [ 22,23].', 'Limitations', 'are given by the design of the algorithm to be trained and discovery of new knowledgeisunlikelyastheygenerallylackhumaningenuityandcross-disciplinaryinsight.', 'Instead,MLandAI-baseda p plica tio nsserveastoolstowo rkwi thlargeda tavol umes,highlyspe-cifictasksandautomateddecisionmakingtospeedupprocesses.', 'Nosinglepersoncanbecredited with the invention of ML and/or AI.', 'Instead, this has been an evolutionary pro-cessovermanydecadeswithuncountedknownandunknowndevelopersbeinginvolved.', 'B e l o ww eg i v es o m ek e ye v e n t st h a tp u s h e dt h e s ed e v e l o p m e n t s .', 'Ag r a p h i c a lt i m e l i n ei sgiveninFigure 3.', 'The underlying idea, for artificial intelligence in particular, is based on a model of the', 'way brain cells (neurons) or nodes in a neural network communicate with each otherand transfer information through electrical signals as was proposed by McCulloch andPitts [24]a n dH e b b[ 25].', 'Special emphasis is placed on the visual cortex based on the', 'research by Hubel and Wiesel [ 26,27].', 'An electrical signal can have an activating or']\n",
            " > Processing time: 41.31247091293335\n",
            " > Real-time factor: 0.24942171520082657\n",
            " > Saving output to AUDIO_OUTPUTS/page_05.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 59\n",
            "Figure 3. Timelinemachinelearningdevelopments.Atimelinetoshowthemajorkeydevelopmentmomentswithinthelastabout80years.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 59', 'Figure 3.', 'Timelinemachinelearningdevelopments.Atimelinetoshowthemajorkeydevelopmentmomentswithinthelastabout80years.']\n",
            " > Processing time: 3.0906949043273926\n",
            " > Real-time factor: 0.2699387740050819\n",
            " > Saving output to AUDIO_OUTPUTS/page_06.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 60 M. VOLLMAR AND G. EVANS\n",
            "deactivatingeffectonaneuronwhichinturncorrespondstoapositiveornegativeweight\n",
            "inacomputationalneuralnetwork.\n",
            "John McCarthy, Alan Turing, Marvin Minsky, Allen Newell and Herbert A. Simon are\n",
            "generally considered to be the founding fathers of artificial intelligence. Alan Turing in1950 [28]d e v i s e dt h eT u r i n gT e s ti nw h i c hac o m p u t e ra i m st oc o n v i n c eah u m a nt h a ti t\n",
            "is not a machine but a human. The test requires a computer algorithm to hold a naturallanguageconversationwithahumanobservedbyanevaluator.Iftheevaluatorcannotdis-tinguish who in the conversation is a machine and who a human, then the algorithm hassuccessfullypassedthetest.Sofar,noAIalgorithmdevelopedwassuccessfulintheTuringtest. In 1952, Marvin Minsky [ 29] developed the first physical implementation of a neu-\n",
            "ralnetworkwithhisStochasticNeuro-AnalogueReinforcementComputer(SNARC).Thesystemwasconstructedof40hardwiredvacuumtubesrepresentingneuronsandtheircon-nectionsandwasabletolearnsimpleconceptsthrough‘rewards’.JohnMcCarthy,MarvinMinsky,NathanielRochesterandClaudeE.ShannonwroteaproposalfortheDartmouthConferencein1956,inwhichtheycoinedtheterm‘artificialintelligence’[ 30].\n",
            "In the 1950s and 60s, Arthur Samuel [ 31,32]w o r k i n ga tI B Md e v e l o p e dt h efi r s ts c o r -\n",
            "ing and reward functions to enable a computer to learn and coined the phrase ‘MachineLearning’ and the idea of ‘learning by generalization’ for the first time. In 1957, intriguedby the ideas of connections between neurons and generalized learning, Frank Rosenblattd e s i g n e dt h efi r s tp e r c e p t r o n[ 33]. A simple version of such a perceptron, depicted in\n",
            "Figure4, is constructed of input nodes randomly connected with each other while their\n",
            "information exchange is altered through weights producing a single output and is thesimplest form of a neural network. Each input node (purple) represents a feature or adendrite in a neuron. Additionally, a constant bias term can be present. Weights can bea p p l i e dt oe a c hf e a t u r eb a s e do nt h e i ri m p o r t a n c eo rs i g n a ls t r e n g t hi nan e u r o n .A l lt h eincoming features and their weights are summed and an activation function is applied.In a binary classification problem, this will be a sigmoid activation function workingwith ‘AND’ and ‘OR’ statements so that a prediction can only be true or false, ‘1’ or ‘0’.A single output, true or false, is produced, a signal sent to other neurons through theaxon, and errors for false predictions are returned to the system via backpropagation.Thewholecalculationisrepeatedmanytimestominimizetheerrorandincreasecorrectpredictions.\n",
            "CoverandHart[ 34]arecreditedfortheideaofthe‘nearestneighbour’rulewhichsetthe\n",
            "starting point for pattern recognition algorithms. In most applications, there is no infor-m a t i o na v a i l a b l ea b o u tt h eu n d e r l y i n gs a m p l ed i s t r i b u t i o nf o re a c hc l a s s .T h ec l a s sl a b e lforeachsampleisthereforeinferredfromthelabelsfoundforalreadyclassifiedsamplesintheneighbourhood.A sassigningaclasslabelisnotinfluencedb ypriorkno wledgefoundin the sample distribution but solely done based on what classes are found in proximity,theclassificationprocessitselfcarriesanerrorandisnotoptimal.\n",
            "Also, in the 1960s the concept of using multiple layers within a neural network was\n",
            "explored by expanding the simple perceptron by two or more layers. This paved the way\n",
            "forfeedforwardneuralnetworks[35,36]andtheuseofbackpropagationinthe1970s[ 37]\n",
            "which allows a neural network to learn from mistakes and adapt to changing situations.\n",
            "Feedforward describes the flow of information from input to output whereas backpropa-gationisan essentialpart ofdeep neural networks as itreturnsfeedbackabout a system’sperformance.Figure 4givesaverysimpleideaofhowthisworksinaperceptron.\n",
            " > Text splitted to sentences.\n",
            "['60 M. VOLLMAR AND G. EVANS', 'deactivatingeffectonaneuronwhichinturncorrespondstoapositiveornegativeweight', 'inacomputationalneuralnetwork.', 'John McCarthy, Alan Turing, Marvin Minsky, Allen Newell and Herbert A. Simon are', 'generally considered to be the founding fathers of artificial intelligence.', 'Alan Turing in1950 [28]d e v i s e dt h eT u r i n gT e s ti nw h i c hac o m p u t e ra i m st oc o n v i n c eah u m a nt h a ti t', 'is not a machine but a human.', 'The test requires a computer algorithm to hold a naturallanguageconversationwithahumanobservedbyanevaluator.', 'Iftheevaluatorcannotdis-tinguish who in the conversation is a machine and who a human, then the algorithm hassuccessfullypassedthetest.', 'Sofar,noAIalgorithmdevelopedwassuccessfulintheTuringtest.', 'In 1952, Marvin Minsky [ 29] developed the first physical implementation of a neu-', 'ralnetworkwithhisStochasticNeuro-AnalogueReinforcementComputer(SNARC).', 'Thesystemwasconstructedof40hardwiredvacuumtubesrepresentingneuronsandtheircon-nectionsandwasabletolearnsimpleconceptsthrough‘rewards’.', 'JohnMcCarthy,MarvinMinsky,NathanielRochesterandClaudeE.ShannonwroteaproposalfortheDartmouthConferencein1956,inwhichtheycoinedtheterm‘artificialintelligence’[ 30].', 'In the 1950s and 60s, Arthur Samuel [ 31,32]w o r k i n ga tI B Md e v e l o p e dt h efi r s ts c o r -', 'ing and reward functions to enable a computer to learn and coined the phrase ‘MachineLearning’ and the idea of ‘learning by generalization’ for the first time.', 'In 1957, intriguedby the ideas of connections between neurons and generalized learning, Frank Rosenblattd e s i g n e dt h efi r s tp e r c e p t r o n[ 33].', 'A simple version of such a perceptron, depicted in', 'Figure4, is constructed of input nodes randomly connected with each other while their', 'information exchange is altered through weights producing a single output and is thesimplest form of a neural network.', 'Each input node (purple) represents a feature or adendrite in a neuron.', 'Additionally, a constant bias term can be present.', 'Weights can bea p p l i e dt oe a c hf e a t u r eb a s e do nt h e i ri m p o r t a n c eo rs i g n a ls t r e n g t hi nan e u r o n .', 'A l lt h eincoming features and their weights are summed and an activation function is applied.', 'In a binary classification problem, this will be a sigmoid activation function workingwith ‘AND’ and ‘OR’ statements so that a prediction can only be true or false, ‘1’ or ‘0’.', 'A single output, true or false, is produced, a signal sent to other neurons through theaxon, and errors for false predictions are returned to the system via backpropagation.', 'Thewholecalculationisrepeatedmanytimestominimizetheerrorandincreasecorrectpredictions.', 'CoverandHart[ 34]arecreditedfortheideaofthe‘nearestneighbour’rulewhichsetthe', 'starting point for pattern recognition algorithms.', 'In most applications, there is no infor-m a t i o na v a i l a b l ea b o u tt h eu n d e r l y i n gs a m p l ed i s t r i b u t i o nf o re a c hc l a s s .', 'T h ec l a s sl a b e lforeachsampleisthereforeinferredfromthelabelsfoundforalreadyclassifiedsamplesintheneighbourhood.', 'A sassigningaclasslabelisnotinfluencedb ypriorkno wledgefoundin the sample distribution but solely done based on what classes are found in proximity,theclassificationprocessitselfcarriesanerrorandisnotoptimal.', 'Also, in the 1960s the concept of using multiple layers within a neural network was', 'explored by expanding the simple perceptron by two or more layers.', 'This paved the way', 'forfeedforwardneuralnetworks[35,36]andtheuseofbackpropagationinthe1970s[ 37]', 'which allows a neural network to learn from mistakes and adapt to changing situations.', 'Feedforward describes the flow of information from input to output whereas backpropa-gationisan essentialpart ofdeep neural networks as itreturnsfeedbackabout a system’sperformance.', 'Figure 4givesaverysimpleideaofhowthisworksinaperceptron.']\n",
            " > Processing time: 79.18877601623535\n",
            " > Real-time factor: 0.253781848755472\n",
            " > Saving output to AUDIO_OUTPUTS/page_07.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 61\n",
            "Figure 4. Simpleschemaofaperceptron,asinglelayerneuralnetwork.\n",
            "Inthefollowingtwodecades,themorestatistics-basedapproachestoalgorithmdesign\n",
            "inmachinelearningseparatedfromthefieldofartificialintelligenceduetolackofprogress\n",
            "in the latter. This was largely, as already mentioned above, due to the lack of appropriatecomputinginfrastructureandresourcestoworkwiththesehighlycomplexsystems.Devel-opments in machine learning at the time focused on probability theory and statistics tocreatepracticalapplications.\n",
            "Akeydevelopmenttomovestatisticalmachinelearningforwardwastheappearanceofa\n",
            "seriesofboostingalgorithms.Boostingallowsamachine-learningalgorithmtoreducethesamplebiaswhenlearningbycombiningtheresultsofaseriesofweaklearnersintoastrongone [38].W eaklea rner sa r ealg o ri thm stha tmak ep r edictio n sbasedo nw eakco rr ela tio n s\n",
            "tothegroundtruthbutarestillbetterthanrandom.Combiningrepetitiveweaklearningpredictors and applying weights, for which different approaches have been developed, topenalizewrongpredictionswillproduceastrongpredictivemodel.\n",
            "From the 1990s on, mainly due to developments in computational infrastructure, the\n",
            "developmentoftheinternetandever-increasingamountsofdata,artificialintelligencehad\n",
            "itsrevivalandhasbeenevolvingeversince.Artificialneuralnetworks(ANN)areacomplex\n",
            "expansionofthesimpleperceptroncapableoflearninghighlycomplextasks.Theyusuallycomprise an input and output layer with a series of hidden layers in between. The com-putational power to detect patterns in given data sits with these hidden layers and theiri n t e r -a n di n t r a - n o d ec o n n e c t i o n s .A n yp a t t e r n sf o u n di nl a r g e ,m u l t i - d i m e n s i o n a ld a t a\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 61', 'Figure 4.', 'Simpleschemaofaperceptron,asinglelayerneuralnetwork.', 'Inthefollowingtwodecades,themorestatistics-basedapproachestoalgorithmdesign', 'inmachinelearningseparatedfromthefieldofartificialintelligenceduetolackofprogress', 'in the latter.', 'This was largely, as already mentioned above, due to the lack of appropriatecomputinginfrastructureandresourcestoworkwiththesehighlycomplexsystems.', 'Devel-opments in machine learning at the time focused on probability theory and statistics tocreatepracticalapplications.', 'Akeydevelopmenttomovestatisticalmachinelearningforwardwastheappearanceofa', 'seriesofboostingalgorithms.', 'Boostingallowsamachine-learningalgorithmtoreducethesamplebiaswhenlearningbycombiningtheresultsofaseriesofweaklearnersintoastrongone [38].', 'W eaklea rner sa r ealg o ri thm stha tmak ep r edictio n sbasedo nw eakco rr ela tio n s', 'tothegroundtruthbutarestillbetterthanrandom.', 'Combiningrepetitiveweaklearningpredictors and applying weights, for which different approaches have been developed, topenalizewrongpredictionswillproduceastrongpredictivemodel.', 'From the 1990s on, mainly due to developments in computational infrastructure, the', 'developmentoftheinternetandever-increasingamountsofdata,artificialintelligencehad', 'itsrevivalandhasbeenevolvingeversince.Artificialneuralnetworks(ANN)areacomplex', 'expansionofthesimpleperceptroncapableoflearninghighlycomplextasks.Theyusuallycomprise an input and output layer with a series of hidden layers in between.', 'The com-putational power to detect patterns in given data sits with these hidden layers and theiri n t e r -a n di n t r a - n o d ec o n n e c t i o n s .', 'A n yp a t t e r n sf o u n di nl a r g e ,m u l t i - d i m e n s i o n a ld a t a']\n",
            " > Processing time: 33.69864010810852\n",
            " > Real-time factor: 0.24924829006991614\n",
            " > Saving output to AUDIO_OUTPUTS/page_08.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 62 M. VOLLMAR AND G. EVANS\n",
            "are usually too complex to be directly detected by a human whether this is tried through\n",
            "dataanalysisorinaprogrammaticway,butarewellsuitedtoANNsofappropriatedesign.For example, long short-term memory (LSTM) neural networks are currently the basisfor speech recognition but were already described in 1997 by Schmidhuber and Hochre-iter [39] .L S T M s‘ r e m e m b e r ’e v e n t st h a th a v eh a p p e n e de a r l yo ni nat i m e l i n ea n dm a k e\n",
            "ac o n n e c t i o nt ot h ec u r r e n ts i t u a t i o n .T h e i rm o r es o p h i s t i c a t e dv e r s i o n sa r et h eb a s i sf o rnaturallanguageprocessingalgorithms.\n",
            "The most recent changes in machine learning and artificial intelligence, which have a\n",
            "directimpactonourlivesarefacialrecognition,self-drivingvehicles,human–robotsharedworkspaces,theInternetofThings,andmanyareasandnewdevelopmentsindataanalysisandhealthcare.W ithever-increasingdataratesinalmostallareasandnewdevelopmentsin algorithms and computing technologies, applications have become increasingly accu-rate,moreefficientandscalable.Ultimately,thishasdrivenmachinelearningandartificialintelligenceapplicationstowardscontinuouslearning.\n",
            "Common concepts and terminology\n",
            "Supervised,unsupervisedandreinforcementlearning\n",
            "Supervisedlearningisthedominantlearningstyleinmanymachinelearningapplications.\n",
            "I treliesonthefactthatthelabel,ifaclassificationofasampleisthedesiredoutcome,oratargetvalue,ifregressionisthemeansofprediction,isknownforeachentityinatrainingset.Patternsfoundinthedataarethenassociatedwiththeknownresultandthealgorithmlearns this connection. Learning continues until the desired level of accuracy has beenreached.\n",
            "Inthecaseofunsupervisedlearning,dataaregiventoasystemwithoutanyadditional\n",
            "informationandthealgorithmdetectspatternsinthedata,whichcanbeusedtogrouporclusterthesamplesthatexhibitsimilarpatterns.Thegoalistofindrulesthatgroupasmanysamples as possible in as few clusters as possible. After identifying groups/clusters, theycanthen,forexample,beassignedlabelsandserveasabasisforsolvingasupervisedclas-sification problem. Unsupervised learning is applied in applications using principles likeregularizationandcompression.Semi-supervisedlearningisamixofbothoftheaboveinthatthedataispartiallylabelledandthealgorithmhastolearnawayofhowtheunlabelleddatacanbeorganizedtofollowthestructureofthesamplesthatalreadyhaveasolution.\n",
            "Reinforcement learning is similar to supervised learning as it uses a mapping between\n",
            "inputandoutputduringtraining.Thewayfeedbackisprovidedtothesystemtoimprove\n",
            "learningis,however,differentfromsupervisedlearning.Insupervisedlearning,thecorrect\n",
            "answer to a mistake is given as feedback, whereas in reinforcement learning a penalty orrewardisgiven.Overtimethesystem,therefore,developsexperiencethroughself-learningbymaximizingrewardandminimizingpunishmentratherthanbybeinggiventhecorrectanswers.\n",
            "Regressionandclassification\n",
            "Regression describes a process rather than being the name of an algorithm class or a\n",
            "p r o b l e m .I td e s c r i b e st h er e l a t i o n s h i pb e t w e e nd i ff e r e n tv a r i a b l e si nad a t a s e ta n dt h i s\n",
            " > Text splitted to sentences.\n",
            "['62 M. VOLLMAR AND G. EVANS', 'are usually too complex to be directly detected by a human whether this is tried through', 'dataanalysisorinaprogrammaticway,butarewellsuitedtoANNsofappropriatedesign.', 'For example, long short-term memory (LSTM) neural networks are currently the basisfor speech recognition but were already described in 1997 by Schmidhuber and Hochre-iter [39] .', 'L S T M s‘ r e m e m b e r ’e v e n t st h a th a v eh a p p e n e de a r l yo ni nat i m e l i n ea n dm a k e', 'ac o n n e c t i o nt ot h ec u r r e n ts i t u a t i o n .', 'T h e i rm o r es o p h i s t i c a t e dv e r s i o n sa r et h eb a s i sf o rnaturallanguageprocessingalgorithms.', 'The most recent changes in machine learning and artificial intelligence, which have a', 'directimpactonourlivesarefacialrecognition,self-drivingvehicles,human–robotsharedworkspaces,theInternetofThings,andmanyareasandnewdevelopmentsindataanalysisandhealthcare.W ithever-increasingdataratesinalmostallareasandnewdevelopmentsin algorithms and computing technologies, applications have become increasingly accu-rate,moreefficientandscalable.', 'Ultimately,thishasdrivenmachinelearningandartificialintelligenceapplicationstowardscontinuouslearning.', 'Common concepts and terminology', 'Supervised,unsupervisedandreinforcementlearning', 'Supervisedlearningisthedominantlearningstyleinmanymachinelearningapplications.', 'I treliesonthefactthatthelabel,ifaclassificationofasampleisthedesiredoutcome,oratargetvalue,ifregressionisthemeansofprediction,isknownforeachentityinatrainingset.', 'Patternsfoundinthedataarethenassociatedwiththeknownresultandthealgorithmlearns this connection.', 'Learning continues until the desired level of accuracy has beenreached.', 'Inthecaseofunsupervisedlearning,dataaregiventoasystemwithoutanyadditional', 'informationandthealgorithmdetectspatternsinthedata,whichcanbeusedtogrouporclusterthesamplesthatexhibitsimilarpatterns.', 'Thegoalistofindrulesthatgroupasmanysamples as possible in as few clusters as possible.', 'After identifying groups/clusters, theycanthen,forexample,beassignedlabelsandserveasabasisforsolvingasupervisedclas-sification problem.', 'Unsupervised learning is applied in applications using principles likeregularizationandcompression.', 'Semi-supervisedlearningisamixofbothoftheaboveinthatthedataispartiallylabelledandthealgorithmhastolearnawayofhowtheunlabelleddatacanbeorganizedtofollowthestructureofthesamplesthatalreadyhaveasolution.', 'Reinforcement learning is similar to supervised learning as it uses a mapping between', 'inputandoutputduringtraining.', 'Thewayfeedbackisprovidedtothesystemtoimprove', 'learningis,however,differentfromsupervisedlearning.', 'Insupervisedlearning,thecorrect', 'answer to a mistake is given as feedback, whereas in reinforcement learning a penalty orrewardisgiven.', 'Overtimethesystem,therefore,developsexperiencethroughself-learningbymaximizingrewardandminimizingpunishmentratherthanbybeinggiventhecorrectanswers.', 'Regressionandclassification', 'Regression describes a process rather than being the name of an algorithm class or a', 'p r o b l e m .', 'I td e s c r i b e st h er e l a t i o n s h i pb e t w e e nd i ff e r e n tv a r i a b l e si nad a t a s e ta n dt h i s']\n",
            " > Processing time: 59.62825894355774\n",
            " > Real-time factor: 0.24602099401105487\n",
            " > Saving output to AUDIO_OUTPUTS/page_09.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 63\n",
            "connectionisrefinedovermultipleiterationswhilemonitoringsomemeasureoferrorto\n",
            "judgetheperformanceofthemodel.Theoutputpredictedbyaregressionmodelisavalueonacontinuousscale.\n",
            "Classifica tionisusedtopredictalabelforasam ple.Alabelcanbeofca tegorical,often\n",
            "a short-hand description of a class, or numerical nature, where the different descriptivecategoriesaretranslatedintouniquenumericalvaluesorasone-hotencoding.Inone-hotencoding,an1Darrayofnumberscorrespondingtothetotallengthofcategoriesiscreated,whereaparticularclassisrepresentedas‘1’andtheremainingclassesas‘0’.Forexample,ifsixclasslabelsarepossible,thentheone-hotencodingforclass‘3’wouldbe[0.0.1.0.0.0.].Thislabelisusedtodistinctlyidentifyasamplebasedonthespecificpatternfoundinthefeaturesprovided.Theoutputpredictedbyaclassificationmodelisavalueonadiscretescale.\n",
            "Types of algorithms\n",
            "The number of available algorithms to choose from is vast. An introduction to all avail-ableformsisbeyondthescopeofthisreview.Figure 5depictsadecisiontreewhichcanbe\n",
            "utilized to narrow down the type of algorithms one may want to explore for given data.As mentioned in the introduction, machine learning and artificial intelligence are datadriveninnatureandnoassumptionsabouttheunderlyingdistributionofsamplesshouldbemade.Nosamplesshouldbeexcludedorbiasesintroducedbyselectingspecificsubtypesof data unless there are valid reasons, i.e. samples have missing values for some featureswhich cannot be imputed or subsampling is used to explore a newly discovered hypothe-sis.Toexploredatafromdifferentperspectivesitmaybenecessarytoexploreaverylargenumberofalgorithms.\n",
            "Dimensionalityreduction\n",
            "Dimensionality reduction is an unsupervised method to describe the data with reduced\n",
            "informationordimensions.Dimensionalityreductionitselfhappensatvariousstepsdur-ingatrainingprocessand,forexample,isusedtocreatethelatentspaceinneuralnetworks.Here we use dimensionality reduction in the context of a data exploration step to visu-alize high dimensional data in a human interpretable way. This is usually done beforemore in-depth training of algorithms. Dimensionality reduction can be used for classi-ficationaswellasregressionproblems.Algorithmsusedfordimensionalityreductionarenot restricted to exploratory data analysis but are machine learning applications in theirownright.Popularalgorithmsare‘PrincipalComponentAnalysis’(PCA)[ 40],‘Principal\n",
            "Component Regression’ (PCR), ‘Partial Least Squares Regression’ (PLSR) [ 41], ‘Multidi-\n",
            "mensionalScaling’(MDS)[ 42,43]and‘LinearDiscriminantAnalysis’(LDA)[ 44,45].The\n",
            "use of LDA or PCA is not restricted to dimensionality reduction as one can see from itsusage in the automated model building package ARP/wARP [ 46,47]o rw h e nc l a s s i f y i n g\n",
            "crystallizationoutcomes[ 48–51](seetheexampleslaterinthisreview).\n",
            "Bayesianalgorithms\n",
            "These can be used to look at classification and regression problems alike. Most com-\n",
            "monlyusedaresystemsapplying‘NaïveBayes���,‘GaussianNaiveBayes’,‘MultinomialNaive\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 63', 'connectionisrefinedovermultipleiterationswhilemonitoringsomemeasureoferrorto', 'judgetheperformanceofthemodel.', 'Theoutputpredictedbyaregressionmodelisavalueonacontinuousscale.', 'Classifica tionisusedtopredictalabelforasam ple.', 'Alabelcanbeofca tegorical,often', 'a short-hand description of a class, or numerical nature, where the different descriptivecategoriesaretranslatedintouniquenumericalvaluesorasone-hotencoding.', 'Inone-hotencoding,an1Darrayofnumberscorrespondingtothetotallengthofcategoriesiscreated,whereaparticularclassisrepresentedas‘1’andtheremainingclassesas‘0’.', 'Forexample,ifsixclasslabelsarepossible,thentheone-hotencodingforclass‘3’wouldbe[0.0.1.0.0.0.].', 'Thislabelisusedtodistinctlyidentifyasamplebasedonthespecificpatternfoundinthefeaturesprovided.', 'Theoutputpredictedbyaclassificationmodelisavalueonadiscretescale.', 'Types of algorithms', 'The number of available algorithms to choose from is vast.', 'An introduction to all avail-ableformsisbeyondthescopeofthisreview.', 'Figure 5depictsadecisiontreewhichcanbe', 'utilized to narrow down the type of algorithms one may want to explore for given data.', 'As mentioned in the introduction, machine learning and artificial intelligence are datadriveninnatureandnoassumptionsabouttheunderlyingdistributionofsamplesshouldbemade.', 'Nosamplesshouldbeexcludedorbiasesintroducedbyselectingspecificsubtypesof data unless there are valid reasons, i.e. samples have missing values for some featureswhich cannot be imputed or subsampling is used to explore a newly discovered hypothe-sis.', 'Toexploredatafromdifferentperspectivesitmaybenecessarytoexploreaverylargenumberofalgorithms.', 'Dimensionalityreduction', 'Dimensionality reduction is an unsupervised method to describe the data with reduced', 'informationordimensions.Dimensionalityreductionitselfhappensatvariousstepsdur-ingatrainingprocessand,forexample,isusedtocreatethelatentspaceinneuralnetworks.Here we use dimensionality reduction in the context of a data exploration step to visu-alize high dimensional data in a human interpretable way.', 'This is usually done beforemore in-depth training of algorithms.', 'Dimensionality reduction can be used for classi-ficationaswellasregressionproblems.', 'Algorithmsusedfordimensionalityreductionarenot restricted to exploratory data analysis but are machine learning applications in theirownright.', 'Popularalgorithmsare‘PrincipalComponentAnalysis’(PCA)[ 40],‘Principal', 'Component Regression’ (PCR), ‘Partial Least Squares Regression’ (PLSR) [ 41], ‘Multidi-', 'mensionalScaling’(MDS)[ 42,43]and‘LinearDiscriminantAnalysis’(LDA)[ 44,45].', 'The', 'use of LDA or PCA is not restricted to dimensionality reduction as one can see from itsusage in the automated model building package ARP/wARP [ 46,47]o rw h e nc l a s s i f y i n g', 'crystallizationoutcomes[ 48–51](seetheexampleslaterinthisreview).', 'Bayesianalgorithms', 'These can be used to look at classification and regression problems alike.', 'Most com-', 'monlyusedaresystemsapplying‘NaïveBayes’,‘GaussianNaiveBayes’,‘MultinomialNaive']\n",
            " > Processing time: 59.29857110977173\n",
            " > Real-time factor: 0.2511064712120223\n",
            " > Saving output to AUDIO_OUTPUTS/page_10.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 64 M. VOLLMAR AND G. EVANS\n",
            "Figure 5. Decisiontreetohelpwithalgorithmchoice.Commonsituationinwhichaparticularalgorithmmaybeusedareexploredwiththehelpofadecisiontree.\n",
            "Theschemacanbeusedasguidancewhenstartingwithanewmachinelearningproject.\n",
            " > Text splitted to sentences.\n",
            "['64 M. VOLLMAR AND G. EVANS', 'Figure 5.', 'Decisiontreetohelpwithalgorithmchoice.', 'Commonsituationinwhichaparticularalgorithmmaybeusedareexploredwiththehelpofadecisiontree.', 'Theschemacanbeusedasguidancewhenstartingwithanewmachinelearningproject.']\n",
            " > Processing time: 4.672960996627808\n",
            " > Real-time factor: 0.24809971774387246\n",
            " > Saving output to AUDIO_OUTPUTS/page_11.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 65\n",
            "Bayes’ [52], ‘Bayesian Networks’ (BN) and ‘Bayesian Belief Networks’ (BBN) [ 53]. They\n",
            "arebasedonapplicationsofBayes’theoremandBayesianstatisticsingeneral[ 54].Anaïve\n",
            "Bayesclassifierhasbeenusedtopredictthechancesofcrystallizabilityforagivenprotein\n",
            "aswillbeexplainedintheexamplesection[ 55].\n",
            "Shallowlearners\n",
            "Shallowlearnerscanbeusedtoaddressbothclassificationandregressionproblems.Below ,\n",
            "wegiveanoverviewofthetypesofalgorithmsavailable.Inafast-movingfieldlikemachinelearning, this list is intended to give an overview of well-tested and established systemsrather than represent any possible algorithm and it will most likely have any very latestdevelopments missing.Thesealgorithmscontain thosethatare wellknown withinstatis-ticssuchas‘OrdinaryLeastSquaresRegression’(OLSR),‘LinearRegression’and‘LogisticRegression’.\n",
            "Adding a penalization function for model complexity to certain regression algorithms\n",
            "forcesapredictortobesimplerandthereforebeingbetteringeneralization.Thisincludesfor example ‘Ridge Regression’ [ 56], ‘Least Absolute Shrinkage and Selection Operator’\n",
            "(LASSO) [57,58] and ‘Elastic Net’. If certain training samples are identified to be cru-cialforthetrainingofamodelthenonewouldliketoselectaninstance-basedalgorithmsuch as ‘k-Nearest Neighbour’ (kNN) [ 59,60] or ‘Support Vector Machines’ (SVM) [ 61].\n",
            "SVMs have been used in several applications in protein crystallography. As we will see\n",
            "in the example section, they have been used to predict the chances of successful crystal-\n",
            "lization of a protein based on its sequence [ 55,62,63]a sw e l la sa s s e s s i n gt h eo u t c o m eo f\n",
            "a crystallization trial [ 51,64]. In the automated model building programme ARP/wARP\n",
            "a SVM is used to identify amino acid side chains following a main chain buildingstep [65]. For all the methods mentioned so far, data normalization is crucial for their\n",
            "operation.\n",
            "Decision tree algorithms such as the most well-known ‘Classification and Regression\n",
            "Tree’ (CART) [ 66]o re n s e m b l e sc r e a t e do ft h e ma i mt om a k ep r e d i c t i o n sb a s e do nt h e\n",
            "actualrealvaluesinthedata,withoutanynormalization.Ateachdecisionpointinthetree,a‘yes/no’resulthastobeproducedbasedontheconditionsappliedatthatmoment.Thiscanbeusedforclassificationandregressionproblemsalikeandthealgorithmsthemselvesare fast and accurate and hence very widely used in machine learning. Ensemble modelsoftenusedecisiontreesasbasealgorithmsandthencombineweakpredictionsofmultiple,hundreds to thousands, of trees to create a strong ensemble result. An ensemble classifierwillcontinuetoincreasethenumberoftreesuntilallsamplesintheavailabledatacanbeclassified.Careshouldbetakenasinextremecasesanensemblewillhaveonetreeforeachsample meaning it has learned the training data but will be of no use when generalizing,as in the case of a new, unknown sample. Decision tree-based algorithms can be used tosolveaclassificationaswellasaregressionproblemandoftenapplysomeformof‘Boost-ing’suchasfoundin‘BootstrappedAggregation’(Bagging)[ 67,68],‘AdaBoost’,‘Gradient\n",
            "Boosting Machines’ (GBM) [69 ] and ‘Gradient Boosted Regression Trees’ (GBRT) [ 70].\n",
            "‘ B o o s t i n g ’i sas e q u e n t i a lw a yo fl e a r n i n g ,w h e r eaw e a kd e c i s i o nt r e er e l i e so ni n f o r m a -tionfromapreviouslearningcyclewhichincreasesthepredictorsexperiencestep-by-stepovertime.‘Bagging’allowsthetreestolearnindependentlyinparallelandcombinestheirexperiencesforthefinalprediction.Thereisalsotheoptiontocreatea‘RandomForest’or\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 65', 'Bayes’ [52], ‘Bayesian Networks’ (BN) and ‘Bayesian Belief Networks’ (BBN) [ 53].', 'They', 'arebasedonapplicationsofBayes’theoremandBayesianstatisticsingeneral[ 54].', 'Anaïve', 'Bayesclassifierhasbeenusedtopredictthechancesofcrystallizabilityforagivenprotein', 'aswillbeexplainedintheexamplesection[ 55].', 'Shallowlearners', 'Shallowlearnerscanbeusedtoaddressbothclassificationandregressionproblems.', 'Below ,', 'wegiveanoverviewofthetypesofalgorithmsavailable.', 'Inafast-movingfieldlikemachinelearning, this list is intended to give an overview of well-tested and established systemsrather than represent any possible algorithm and it will most likely have any very latestdevelopments missing.', 'Thesealgorithmscontain thosethatare wellknown withinstatis-ticssuchas‘OrdinaryLeastSquaresRegression’(OLSR),‘LinearRegression’and‘LogisticRegression’.', 'Adding a penalization function for model complexity to certain regression algorithms', 'forcesapredictortobesimplerandthereforebeingbetteringeneralization.', 'Thisincludesfor example ‘Ridge Regression’ [ 56], ‘Least Absolute Shrinkage and Selection Operator’', '(LASSO) [57,58] and ‘Elastic Net’.', 'If certain training samples are identified to be cru-cialforthetrainingofamodelthenonewouldliketoselectaninstance-basedalgorithmsuch as ‘k-Nearest Neighbour’ (kNN) [ 59,60] or ‘Support Vector Machines’ (SVM) [ 61].', 'SVMs have been used in several applications in protein crystallography.', 'As we will see', 'in the example section, they have been used to predict the chances of successful crystal-', 'lization of a protein based on its sequence [ 55,62,63]a sw e l la sa s s e s s i n gt h eo u t c o m eo f', 'a crystallization trial [ 51,64].', 'In the automated model building programme ARP/wARP', 'a SVM is used to identify amino acid side chains following a main chain buildingstep [65].', 'For all the methods mentioned so far, data normalization is crucial for their', 'operation.', 'Decision tree algorithms such as the most well-known ‘Classification and Regression', 'Tree’ (CART) [ 66]o re n s e m b l e sc r e a t e do ft h e ma i mt om a k ep r e d i c t i o n sb a s e do nt h e', 'actualrealvaluesinthedata,withoutanynormalization.', 'Ateachdecisionpointinthetree,a‘yes/no’resulthastobeproducedbasedontheconditionsappliedatthatmoment.', 'Thiscanbeusedforclassificationandregressionproblemsalikeandthealgorithmsthemselvesare fast and accurate and hence very widely used in machine learning.', 'Ensemble modelsoftenusedecisiontreesasbasealgorithmsandthencombineweakpredictionsofmultiple,hundreds to thousands, of trees to create a strong ensemble result.', 'An ensemble classifierwillcontinuetoincreasethenumberoftreesuntilallsamplesintheavailabledatacanbeclassified.Careshouldbetakenasinextremecasesanensemblewillhaveonetreeforeachsample meaning it has learned the training data but will be of no use when generalizing,as in the case of a new, unknown sample.', 'Decision tree-based algorithms can be used tosolveaclassificationaswellasaregressionproblemandoftenapplysomeformof‘Boost-ing’suchasfoundin‘BootstrappedAggregation’(Bagging)[ 67,68],‘AdaBoost’,‘Gradient', 'Boosting Machines’ (GBM) [69 ] and ‘Gradient Boosted Regression Trees’ (GBRT) [ 70].', '‘ B o o s t i n g ’i sas e q u e n t i a lw a yo fl e a r n i n g ,w h e r eaw e a kd e c i s i o nt r e er e l i e so ni n f o r m a -tionfromapreviouslearningcyclewhichincreasesthepredictorsexperiencestep-by-stepovertime.', '‘Bagging’allowsthetreestolearnindependentlyinparallelandcombinestheirexperiencesforthefinalprediction.', 'Thereisalsotheoptiontocreatea‘RandomForest’or']\n",
            " > Processing time: 72.18722105026245\n",
            " > Real-time factor: 0.2530968674484572\n",
            " > Saving output to AUDIO_OUTPUTS/page_12.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 66 M. VOLLMAR AND G. EVANS\n",
            "‘Extreme-RandomForest’[ 71]inwhichaverylargenumberofveryshallowtreesarebeing\n",
            "combinedandvariousinternalparametersinthealgorithmarebeingselectedwithalevel\n",
            "of randomness. Decision trees, AdaBoost and random forests have been used to classifycrystallization outcomes [ 72,73], predict the likelihood of a protein to crystallize [ 74,75]\n",
            "a n dp r o v i d ead e c i s i o nm a k e ri na u t o m a t e dd a t aa n a l y s i sp i p e l i n e s[ 76]( s e el a t e r\n",
            "examples).\n",
            "Deeplearners\n",
            "The underlying base of state-of-the art deep learning applications is ‘Artificial Neural\n",
            "Network’ (ANN) [ 77] algorithms. These types of neural networks are designed after the\n",
            "principlesfoundinbiologicalneuralnetworkssuchasthebrain’svisualcortex.ANNscanbe used for both regression and classification problems and have far too many variationsto list them all. The most well-known algorithms are ‘Perceptron’ [ 33], ‘Multilayer Per-\n",
            "ceptron’(MLP),‘Back-Propagation’[ 78],‘StochasticGradientDescent’(SGD)and‘Radial\n",
            "BasisFunctionNetwork’(RBFN).Byincreasingthecomplexityofthesenetworksthroughaddinghundredsoflayersandhavingahugenumberofconnectionsbetweentheneurons,deeplearningalgorithmsarebeingcreated.Asmallselectionofverycommonandpopulardeeplearningalgorithmsinclude:‘ConvolutionalNeuralNetworks’(CNNs)[ 36],‘Recur-\n",
            "rent Neural Networks’ (RNNs) [ 79],‘LongShort-TermMemoryNetworks’(LSTMs)[ 39],\n",
            "‘StackedAuto-Encoders’,‘DeepBoltzmannMachines’(DBM)[ 80]and‘DeepBeliefNet-\n",
            "works’ (DBNs) [ 81]. There are also more specialized algorithms for particular fields for\n",
            "example those concerned with ‘Computer Vision’ (CV), ‘Natural Language Processing’(NLP), ‘Recommender Systems’, ‘Reinforcement Learning’ [ 82]a n de v o l u t i o n a r ya l g o -\n",
            "rithms looking at ‘Computational intelligence’. Following the developments and designsof convolutional neural networks, great breakthroughs were achieved with ANNs and AIin other areas. These achievements have now made their way into macromolecular crys-tallography.Thesimplestversion,amulti-layerperceptron,hasbeenusedinanautomatedmodelbuildingpipeline[ 83].Convolutionalneuralnetworkshavebeentrainedtopredict\n",
            "protein crystallizability [ 75,84] and classify crystallization outcomes [20,85]. ResNet and\n",
            "morespecializedneuralnetworkarchitectureshavebeenusedfordiffractionimageclassi-fication[86,87]andproteinmodelling/structureprediction[ 88–91].Wewillexplorethese\n",
            "applications in the example section below. In the field of cryo electron microscopy (EM),the software package cryoSPARC [ 92] uses SGD to identify a set of low-resolution struc-\n",
            "turesbasedontheparticlesfoundintherecordedimages.Crucially,SGDdoesnotrequirean already highly accurate initial model as is normally the case for refinement programsusingBayesianlikelihoodtosolveanoptimizationproblem.TheCNN-basedYOLO(YouOnlyLookOnce)algorithm[ 93]findsitsuseincrYOLO[94],anothercryo-EMpackage.\n",
            "Here, YOLO is used to find thousands of particles in cryo-EM micrographs and to drawbounding boxes around them. Finding the particles is the crucial first step to reconstructa3Dproteinstructureusingcryo-EMdata.Anotherpopularcryo-EMpackage,RELION,\n",
            "on the other hand uses a Bayesian approach to reconstruct a 3D protein model from 2D\n",
            "micrographs[ 95].However,aswearefocusingonmachinelearningandAIapplicationsin\n",
            "crystallography,thereadermayrefertothecitationsgivenabovetogetmoredetailsaboutimplementationsincryo-EM.\n",
            " > Text splitted to sentences.\n",
            "['66 M. VOLLMAR AND G. EVANS', '‘Extreme-RandomForest’[ 71]inwhichaverylargenumberofveryshallowtreesarebeing', 'combinedandvariousinternalparametersinthealgorithmarebeingselectedwithalevel', 'of randomness.', 'Decision trees, AdaBoost and random forests have been used to classifycrystallization outcomes [ 72,73], predict the likelihood of a protein to crystallize [ 74,75]', 'a n dp r o v i d ead e c i s i o nm a k e ri na u t o m a t e dd a t aa n a l y s i sp i p e l i n e s[ 76]( s e el a t e r', 'examples).', 'Deeplearners', 'The underlying base of state-of-the art deep learning applications is ‘Artificial Neural', 'Network’ (ANN) [ 77] algorithms.', 'These types of neural networks are designed after the', 'principlesfoundinbiologicalneuralnetworkssuchasthebrain’svisualcortex.ANNscanbe used for both regression and classification problems and have far too many variationsto list them all.', 'The most well-known algorithms are ‘Perceptron’ [ 33], ‘Multilayer Per-', 'ceptron’(MLP),‘Back-Propagation’[ 78],‘StochasticGradientDescent’(SGD)and‘Radial', 'BasisFunctionNetwork’(RBFN).', 'Byincreasingthecomplexityofthesenetworksthroughaddinghundredsoflayersandhavingahugenumberofconnectionsbetweentheneurons,deeplearningalgorithmsarebeingcreated.Asmallselectionofverycommonandpopulardeeplearningalgorithmsinclude:‘ConvolutionalNeuralNetworks’(CNNs)[ 36],‘Recur-', 'rent Neural Networks’ (RNNs) [ 79],‘LongShort-TermMemoryNetworks’(LSTMs)[ 39],', '‘StackedAuto-Encoders’,‘DeepBoltzmannMachines’(DBM)[ 80]and‘DeepBeliefNet-', 'works’ (DBNs) [ 81].', 'There are also more specialized algorithms for particular fields for', 'example those concerned with ‘Computer Vision’ (CV), ‘Natural Language Processing’(NLP), ‘Recommender Systems’, ‘Reinforcement Learning’ [ 82]a n de v o l u t i o n a r ya l g o -', 'rithms looking at ‘Computational intelligence’.', 'Following the developments and designsof convolutional neural networks, great breakthroughs were achieved with ANNs and AIin other areas.', 'These achievements have now made their way into macromolecular crys-tallography.', 'Thesimplestversion,amulti-layerperceptron,hasbeenusedinanautomatedmodelbuildingpipeline[ 83].', 'Convolutionalneuralnetworkshavebeentrainedtopredict', 'protein crystallizability [ 75,84] and classify crystallization outcomes [20,85].', 'ResNet and', 'morespecializedneuralnetworkarchitectureshavebeenusedfordiffractionimageclassi-fication[86,87]andproteinmodelling/structureprediction[ 88–91].', 'Wewillexplorethese', 'applications in the example section below.', 'In the field of cryo electron microscopy (EM),the software package cryoSPARC [ 92] uses SGD to identify a set of low-resolution struc-', 'turesbasedontheparticlesfoundintherecordedimages.', 'Crucially,SGDdoesnotrequirean already highly accurate initial model as is normally the case for refinement programsusingBayesianlikelihoodtosolveanoptimizationproblem.', 'TheCNN-basedYOLO(YouOnlyLookOnce)algorithm[ 93]findsitsuseincrYOLO[94],anothercryo-EMpackage.', 'Here, YOLO is used to find thousands of particles in cryo-EM micrographs and to drawbounding boxes around them.', 'Finding the particles is the crucial first step to reconstructa3Dproteinstructureusingcryo-EMdata.', 'Anotherpopularcryo-EMpackage,RELION,', 'on the other hand uses a Bayesian approach to reconstruct a 3D protein model from 2D', 'micrographs[ 95].', 'However,aswearefocusingonmachinelearningandAIapplicationsin', 'crystallography,thereadermayrefertothecitationsgivenabovetogetmoredetailsaboutimplementationsincryo-EM.']\n",
            " > Processing time: 67.31160402297974\n",
            " > Real-time factor: 0.24333021434573965\n",
            " > Saving output to AUDIO_OUTPUTS/page_13.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 67\n",
            "Exploratory data analysis and initial assessment\n",
            "E x p l o r a t o r yd a t aa n a l y s i s( E D A )i sa ni m p o r t a n tp r o c e s st oo b t a i nag o o di n i t i a lg r a s p\n",
            "of the data one has available and how these data can be used to answer specific ques-tions or hypotheses. These questions and hypotheses are not apparent from the datastraightawaybutcanbeformulatedwhencarryingoutsomeinitialanalysis.EDAisoftenundervaluedandneglectedsincetrainingalgorithmsanddevelopingpredictivemodelsisconsideredthemoreinterestingchallenge.Sixbasicstepshavebeendefinedwhencarrying\n",
            "outEDA[ 96]:\n",
            "(1) Findattributes/features/variablesinthedataset\n",
            "(2) Conductunivariatedataanalysistoidentifydatadistributions(3) Use bivariate/multivariate data analysis to identify relationships and interactions\n",
            "betweenattributes/features/variables\n",
            "(4) Detect missing values and either remove the corresponding samples or find\n",
            "a way of replacing the values (i.e. using the mean or median for theseattributes/features/variables)\n",
            "(5) Detectoutliersandremovethecorrespondingsampleorsetmin/maxboundariesfor\n",
            "theattributes/features/variables\n",
            "(6) Feature engineering to combine attributes/features/variables for dimensionality\n",
            "reduction(seeabove)and/ortocreatenewattributes/features/variables\n",
            "Here, we introduce these six basic steps to carry out EDA which help to get an intuitionf o rt h ed a t aa n dt h e r e f o r ep r o v i d es o m ei d e a sa b o u th o wt oa n s w e rq u e s t i o n sa n dt e s thypotheses hidden in the data. Additionally, this step is a quality check of the data foritscompletenessandintegrity,identifiesgapsinthedata,whichmayrequirefurthergath-ering of samples, and helps to select the appropriate tools and techniques to finally traina predictive model. A small but representative subset of data is sufficient for such analy-sisandsimplesummarystatisticssuchasmean,median,maximumandminimumvalues,\n",
            "firstandthirdquartilesareagoodstartingpoint.Higher-ordermomentssuchasvariance,\n",
            "kurtosisandskewshouldalsobeexplored.Plottingthedataashistograms,linecharts,boxplots,pairwisescatterplotsandcorrelationmatricesisahelpfulwaytovisuallyexplorethedata.Suchanalysiswillquicklyidentifyoutliersorodditiesinthedata,e.g.modalityofthedata distribution, and may reveal correlations, dependencies and redundancies betweenattributes/features/variables. Tabular data usually have the attributes/features/variables ascolumn headers whereas the rows represent individual samples. Figure 6(a) shows a pair\n",
            "plotmatrixwherepairsoffeaturesareplottedagainsteachotherasscatterplots.Samplesofthenegativeclassinthisbinaryclassificationexamplearegiveninblueandthosebelong-ing to the positive class are coloured orange. Such a set of plots can be used to identifyfeatures that have a linear correlation or those that will be suitable to separate groups ofd a t a .T h ed i a g o n a lp l o t st h ef e a t u r eo ni t s e l fa n di ss h o w na sad e n s i t yd i s t r i b u t i o nf u n c -tion for each class separately. For feature05 the peaks found in the graphs for the twoclasses are no longer fully overlapping and may therefore be of high relevance in deci-sion making. Also the scatter for both sample classes are not fully overlapping and havedifferent centre of mass. Calculating Pearson’s correlation coefficient with associated p-\n",
            "valuesisanotherwaytoidentifylinearcorrelationsbetweenfeatures,seeFigure 6(b).The\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 67', 'Exploratory data analysis and initial assessment', 'E x p l o r a t o r yd a t aa n a l y s i s( E D A )i sa ni m p o r t a n tp r o c e s st oo b t a i nag o o di n i t i a lg r a s p', 'of the data one has available and how these data can be used to answer specific ques-tions or hypotheses.', 'These questions and hypotheses are not apparent from the datastraightawaybutcanbeformulatedwhencarryingoutsomeinitialanalysis.', 'EDAisoftenundervaluedandneglectedsincetrainingalgorithmsanddevelopingpredictivemodelsisconsideredthemoreinterestingchallenge.', 'Sixbasicstepshavebeendefinedwhencarrying', 'outEDA[ 96]:', '(1) Findattributes/features/variablesinthedataset', '(2) Conductunivariatedataanalysistoidentifydatadistributions(3) Use bivariate/multivariate data analysis to identify relationships and interactions', 'betweenattributes/features/variables', '(4) Detect missing values and either remove the corresponding samples or find', 'a way of replacing the values (i.e. using the mean or median for theseattributes/features/variables)', '(5) Detectoutliersandremovethecorrespondingsampleorsetmin/maxboundariesfor', 'theattributes/features/variables', '(6) Feature engineering to combine attributes/features/variables for dimensionality', 'reduction(seeabove)and/ortocreatenewattributes/features/variables', 'Here, we introduce these six basic steps to carry out EDA which help to get an intuitionf o rt h ed a t aa n dt h e r e f o r ep r o v i d es o m ei d e a sa b o u th o wt oa n s w e rq u e s t i o n sa n dt e s thypotheses hidden in the data.', 'Additionally, this step is a quality check of the data foritscompletenessandintegrity,identifiesgapsinthedata,whichmayrequirefurthergath-ering of samples, and helps to select the appropriate tools and techniques to finally traina predictive model.', 'A small but representative subset of data is sufficient for such analy-sisandsimplesummarystatisticssuchasmean,median,maximumandminimumvalues,', 'firstandthirdquartilesareagoodstartingpoint.', 'Higher-ordermomentssuchasvariance,', 'kurtosisandskewshouldalsobeexplored.', 'Plottingthedataashistograms,linecharts,boxplots,pairwisescatterplotsandcorrelationmatricesisahelpfulwaytovisuallyexplorethedata.', 'Suchanalysiswillquicklyidentifyoutliersorodditiesinthedata,e.g.modalityofthedata distribution, and may reveal correlations, dependencies and redundancies betweenattributes/features/variables.', 'Tabular data usually have the attributes/features/variables ascolumn headers whereas the rows represent individual samples.', 'Figure 6', '(a) shows a pair', 'plotmatrixwherepairsoffeaturesareplottedagainsteachotherasscatterplots.', 'Samplesofthenegativeclassinthisbinaryclassificationexamplearegiveninblueandthosebelong-ing to the positive class are coloured orange.', 'Such a set of plots can be used to identifyfeatures that have a linear correlation or those that will be suitable to separate groups ofd a t a .', 'T h ed i a g o n a lp l o t st h ef e a t u r eo ni t s e l fa n di ss h o w na sad e n s i t yd i s t r i b u t i o nf u n c -tion for each class separately.', 'For feature05 the peaks found in the graphs for the twoclasses are no longer fully overlapping and may therefore be of high relevance in deci-sion making.', 'Also the scatter for both sample classes are not fully overlapping and havedifferent centre of mass.', 'Calculating Pearson’s correlation coefficient with associated p-', 'valuesisanotherwaytoidentifylinearcorrelationsbetweenfeatures,seeFigure 6', '(b).', 'The']\n",
            " > Processing time: 71.46185874938965\n",
            " > Real-time factor: 0.26357965220080026\n",
            " > Saving output to AUDIO_OUTPUTS/page_14.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 68 M. VOLLMAR AND G. EVANS\n",
            "range of correlation coefficients is between +1, a perfect positive correlation (coloured\n",
            "blue here), and −1, a perfect negative correlation (coloured red), with 0 (white) mean-\n",
            "i n gt h e r ei sn oc o r r e l a t i o nb e t w e e nap a i ro ff e a t u r e s .T h ed i a g o n a li st h es e l f - c o r r e l a t i o n\n",
            "Figure 6. Collectionofgraphsthatcanbeusedtovisualizetheinputdataandperformanceresults.(a)\n",
            "depictsapairplotfortheeightfeaturesusedintheseexamples.Alldatasamplesofthetrainingsethave\n",
            "beenagainsteachotherandthecolouringisdonebasedonthetwopossibleclasses‘0’inblueand‘1’inorange.(b)isacorrelationmatrix,avisualrepresentationforPearson’scorrelationcoeﬃcientscalculated\n",
            "forfeaturepairsofthetrainingsetdata.(c)–(e)Thethreegraphsshowthehistogram,probabilityden-\n",
            "sity function and the empirical cumulative distribution function for the training set data of feature05,respectively.Thebarchartin(f)givestheimportanceforeachfeatureindecisionmaking.(g)showsthe\n",
            "receiver–operatorcurveforsamplesofclass“1”andthecalculatedarea-under-the-curve.Theconfusion\n",
            "matrixin(h)showsthefourpossibleclassiﬁcationoutcomesfortheexamplebinaryclassiﬁerandhowmany samples fall into each. The classiﬁcation outcomes are used to calculate several quality metrics\n",
            "(classiﬁcationaccuracyanderror,sensitivity,speciﬁcity,false-positiverate,precision,F1score,ROCAUC)\n",
            "ascanbeseenintheradarplotin(i).\n",
            " > Text splitted to sentences.\n",
            "['68 M. VOLLMAR AND G. EVANS', 'range of correlation coefficients is between +1, a perfect positive correlation (coloured', 'blue here), and −1, a perfect negative correlation (coloured red), with 0 (white) mean-', 'i n gt h e r ei sn oc o r r e l a t i o nb e t w e e nap a i ro ff e a t u r e s .', 'T h ed i a g o n a li st h es e l f - c o r r e l a t i o n', 'Figure 6.', 'Collectionofgraphsthatcanbeusedtovisualizetheinputdataandperformanceresults.', '(a)', 'depictsapairplotfortheeightfeaturesusedintheseexamples.', 'Alldatasamplesofthetrainingsethave', 'beenagainsteachotherandthecolouringisdonebasedonthetwopossibleclasses‘0’inblueand‘1’inorange.', '(b)isacorrelationmatrix,avisualrepresentationforPearson’scorrelationcoeﬃcientscalculated', 'forfeaturepairsofthetrainingsetdata.', '(c)–', '(e)Thethreegraphsshowthehistogram,probabilityden-', 'sity function and the empirical cumulative distribution function for the training set data of feature05,respectively.', 'Thebarchartin', '(f)givestheimportanceforeachfeatureindecisionmaking.', '(g)showsthe', 'receiver–operatorcurveforsamplesofclass“1”andthecalculatedarea-under-the-curve.', 'Theconfusion', 'matrixin', '(h)showsthefourpossibleclassiﬁcationoutcomesfortheexamplebinaryclassiﬁerandhowmany samples fall into each.', 'The classiﬁcation outcomes are used to calculate several quality metrics', '(classiﬁcationaccuracyanderror,sensitivity,speciﬁcity,false-positiverate,precision,F1score,ROCAUC)', 'ascanbeseenintheradarplotin', '(i).']\n",
            " > Processing time: 31.099109172821045\n",
            " > Real-time factor: 0.24561988772368779\n",
            " > Saving output to AUDIO_OUTPUTS/page_15.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 69\n",
            "Figure 6. Continued.\n",
            "for a feature given in dark blue showing that a feature is perfectly positively correlated\n",
            "with itself, i.e. ‘1’ or 100%. In Figure 6(c–e), feature05 is plotted as a histogram (c)), a\n",
            "probabilitydensityfunction(PDF;d)andanobservedandtheoretical(empirical)cumu-lative density function ((E)CDF; e) with the mean (red vertical line) and median (greenv e r t i c a ll i n e )c a l c u l a t e df o rt h ef e a t u r em a r k e df o rt h el a t t e rt w o .T h eh i s t o g r a ma n dprobability density function show a clear skew to the left (lower x-values) and the corre-\n",
            "spondingcumulativedensityfunctiondoesnotfollowtheexpected,theoreticalsigmoidalshape. This feature would benefit from some form of feature engineering, e.g. normaliza-tion,toachieveaGaussiandistributionofthedata.Anexamplefromthecrystallographicfield for the effect of skew in the data is histogram matching to identify electron densitymaps representative for a protein with well-defined atomic coordinates and a connectedbackbone[ 97].\n",
            "Findattributes/features/variablesinthedataset\n",
            "Attributes/features/variables of the data define the dimensions of a dataset and can be\n",
            "placed into one of two groups, numerical or categorical. Numerical data usually hold\n",
            "quantitative information about a dataset whereas categorical data provide qualitative\n",
            "information. Attributes/features/variables that are given as categorical data cannot beanalysed through summary statistics in particular if there are many individual cate-gories available and other statistical methods may not produce optimal results. Com-bining several categories into a group is often used to address the problem. This is\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 69', 'Figure 6.', 'Continued.', 'for a feature given in dark blue showing that a feature is perfectly positively correlated', 'with itself, i.e. ‘1’ or 100%.', 'In Figure 6(c–e), feature05 is plotted as a histogram (c)), a', 'probabilitydensityfunction(PDF;d)andanobservedandtheoretical(empirical)cumu-lative density function ((E)CDF; e) with the mean (red vertical line) and median (greenv e r t i c a ll i n e )c a l c u l a t e df o rt h ef e a t u r em a r k e df o rt h el a t t e rt w o .', 'T h eh i s t o g r a ma n dprobability density function show a clear skew to the left (lower x-values) and the corre-', 'spondingcumulativedensityfunctiondoesnotfollowtheexpected,theoreticalsigmoidalshape.', 'This feature would benefit from some form of feature engineering, e.g. normaliza-tion,toachieveaGaussiandistributionofthedata.', 'Anexamplefromthecrystallographicfield for the effect of skew in the data is histogram matching to identify electron densitymaps representative for a protein with well-defined atomic coordinates and a connectedbackbone[ 97].', 'Findattributes/features/variablesinthedataset', 'Attributes/features/variables of the data define the dimensions of a dataset and can be', 'placed into one of two groups, numerical or categorical.', 'Numerical data usually hold', 'quantitative information about a dataset whereas categorical data provide qualitative', 'information.', 'Attributes/features/variables that are given as categorical data cannot beanalysed through summary statistics in particular if there are many individual cate-gories available and other statistical methods may not produce optimal results.', 'Com-bining several categories into a group is often used to address the problem.', 'This is']\n",
            " > Processing time: 33.98366904258728\n",
            " > Real-time factor: 0.24851815793579052\n",
            " > Saving output to AUDIO_OUTPUTS/page_16.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 70 M. VOLLMAR AND G. EVANS\n",
            "particularly important when considering class distribution and balance in a classification\n",
            "problem.\n",
            "Conductunivariatedataanalysistoidentifydatadistributions\n",
            "Calculatingsimplesummarystatisticsforeachattribute/feature/variableandplottingthem\n",
            "individuallyinvaryingwaysisthefirstpartofthisanalysis.Thefocusisonidentifyingthetypes of data distributions found for the different attributes/features/variables, their cen-trality (mean, median, mode) and their dispersion (range, variance, standard deviation,skew,kurtosis).Table 1containsthesummarystatisticsforthedatausedtodrawtheplots\n",
            "in Figure 6. For feature05 for example, the arithmetic mean (sum over all values for this\n",
            "feature divided by the number of samples) is 35.78 and its median (the middle value fortherankedsamples)of28.67.Themodeforfeature05is1,meaningthatthereisonevaluei nt h ed i s t r i b u t i o nt h a ti sf o u n dm o s to f t e n .T h er a n g eo ft h ed a t ao ff e a t u r e 0 5i sd e fi n e dby its minimal and maximal values which are 14.19 and 100.00 respectively. A standarddeviationcanbeuseful tocalculate ifthedataarenormally distributedtodeterminehowmuchthesamplesinthedistributiononaveragedeviatefromthemean.Forfeature05,wefind18.42,althoughthisismeaninglessaswecanseefromthedensitydistributionplotinFigure6( d )t h a tt h es a m p l e sf o rt h i sf e a t u r ea r en o tn o r m a l l yd i s t r i b u t e da n ds o m en o r -\n",
            "m a l i z a t i o ns h o u l db ea p p l i e d .T h ei n t e r - q u a r t i l er a n g ec a nb eu s e dt oa s s e s st h es p r e a dor dispersion of samples if they are not normally distributed. Therefore, in addition to\n",
            "the median, the 25% and 75% quantile are calculated, for which we find 22.51 and 43.54.\n",
            "The difference between them is the inter-quartile range, here 21.03. A small value for theinter-quartilerangemeansasmalldispersionofthesamples.Theskewdescribeswhethertheaverageormeancanbefoundinthecentreoftheplottedsampledistribution.Forfea-ture05,askewof1.49wascalculated,whichmeansthatthesampledistributionisskewedtotheleftwithalongtrailtowardstherightascanindeedbeseeninFigure 6(d). A neg-\n",
            "a t i v ev a l u ei n d i c a t e sas h i f tt ot h er i g h tw i t hat r a i lt ot h el e f t .F o rn o r m a l l yd i s t r i b u t e ddata no skew is detectable and the value should be 0. The kurtosis describes the poin-t i n e s so ft h es a m p l ed i s t r i b u t i o no rh o ww e l lt h es a m p l e sc l u s t e ra r o u n dt h ec e n t r e .F o rfeature05, the kurtosis is 1.82 and Figure 6(d) has a fairly pointy appearance. A negative\n",
            "value would indicate a rather flat sample distribution with a broad spread and proba-bly a large standard deviation. The most commonly found data distributions are normal(Gaussian),Poissonandbinomialdistributions.Formanymachinelearningapplications,normally distributed data are preferred and normalization should be applied to achievethis.\n",
            "Usebivariate/multivariatedataanalysistoidentifyrelationshipsandinteractions\n",
            "Pairwise analysis and multivariate statistics are used to identify interactions and depen-\n",
            "denciesbetweenthedifferentattributes/features/variables.Visualinspectionofthedatain\n",
            "pairwise scatterplots or correlation matrices after calculating Pearson’s correlation coef-\n",
            "ficient is common. The interpretation of results from multivariate analysis of severalattributes/features/variables can be a challenge and often requires domain knowledge ofthedata.Itoftenrequiressometimeandcomputationaleffortandtheuseoffactoranalysistechniquessuchasclusteringordimensionalityreduction(seeabove).\n",
            " > Text splitted to sentences.\n",
            "['70 M. VOLLMAR AND G. EVANS', 'particularly important when considering class distribution and balance in a classification', 'problem.', 'Conductunivariatedataanalysistoidentifydatadistributions', 'Calculatingsimplesummarystatisticsforeachattribute/feature/variableandplottingthem', 'individuallyinvaryingwaysisthefirstpartofthisanalysis.', 'Thefocusisonidentifyingthetypes of data distributions found for the different attributes/features/variables, their cen-trality (mean, median, mode) and their dispersion (range, variance, standard deviation,skew,kurtosis).', 'Table 1containsthesummarystatisticsforthedatausedtodrawtheplots', 'in Figure 6.', 'For feature05 for example, the arithmetic mean (sum over all values for this', 'feature divided by the number of samples) is 35.78 and its median (the middle value fortherankedsamples)of28.67.', 'Themodeforfeature05is1,meaningthatthereisonevaluei nt h ed i s t r i b u t i o nt h a ti sf o u n dm o s to f t e n .', 'T h er a n g eo ft h ed a t ao ff e a t u r e 0 5i sd e fi n e dby its minimal and maximal values which are 14.19 and 100.00 respectively.', 'A standarddeviationcanbeuseful tocalculate ifthedataarenormally distributedtodeterminehowmuchthesamplesinthedistributiononaveragedeviatefromthemean.', 'Forfeature05,wefind18.42,althoughthisismeaninglessaswecanseefromthedensitydistributionplotinFigure6( d )t h a tt h es a m p l e sf o rt h i sf e a t u r ea r en o tn o r m a l l yd i s t r i b u t e da n ds o m en o r -', 'm a l i z a t i o ns h o u l db ea p p l i e d .', 'T h ei n t e r - q u a r t i l er a n g ec a nb eu s e dt oa s s e s st h es p r e a dor dispersion of samples if they are not normally distributed.', 'Therefore, in addition to', 'the median, the 25% and 75% quantile are calculated, for which we find 22.51 and 43.54.', 'The difference between them is the inter-quartile range, here 21.03.', 'A small value for theinter-quartilerangemeansasmalldispersionofthesamples.', 'Theskewdescribeswhethertheaverageormeancanbefoundinthecentreoftheplottedsampledistribution.', 'Forfea-ture05,askewof1.49wascalculated,whichmeansthatthesampledistributionisskewedtotheleftwithalongtrailtowardstherightascanindeedbeseeninFigure 6(d).', 'A neg-', 'a t i v ev a l u ei n d i c a t e sas h i f tt ot h er i g h tw i t hat r a i lt ot h el e f t .', 'F o rn o r m a l l yd i s t r i b u t e ddata no skew is detectable and the value should be 0.', 'The kurtosis describes the poin-t i n e s so ft h es a m p l ed i s t r i b u t i o no rh o ww e l lt h es a m p l e sc l u s t e ra r o u n dt h ec e n t r e .', 'F o rfeature05, the kurtosis is 1.82 and Figure 6(d) has a fairly pointy appearance.', 'A negative', 'value would indicate a rather flat sample distribution with a broad spread and proba-bly a large standard deviation.', 'The most commonly found data distributions are normal(Gaussian),Poissonandbinomialdistributions.', 'Formanymachinelearningapplications,normally distributed data are preferred and normalization should be applied to achievethis.', 'Usebivariate/multivariatedataanalysistoidentifyrelationshipsandinteractions', 'Pairwise analysis and multivariate statistics are used to identify interactions and depen-', 'denciesbetweenthedifferentattributes/features/variables.', 'Visualinspectionofthedatain', 'pairwise scatterplots or correlation matrices after calculating Pearson’s correlation coef-', 'ficient is common.', 'The interpretation of results from multivariate analysis of severalattributes/features/variables can be a challenge and often requires domain knowledge ofthedata.', 'Itoftenrequiressometimeandcomputationaleffortandtheuseoffactoranalysistechniquessuchasclusteringordimensionalityreduction(seeabove).']\n",
            " > Processing time: 80.23137307167053\n",
            " > Real-time factor: 0.25622638858752145\n",
            " > Saving output to AUDIO_OUTPUTS/page_17.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 71\n",
            "Table 1.Summarystatisticsfromdataexplorationstepforexampledata.\n",
            "Minimum Maximum Sum Mean MedianStandard\n",
            "deviation Variance\n",
            "feature01 11.40 100.00 96491.17 94.60 99.11 10.83 117.19\n",
            "feature02 19.92 150.20 48325.97 47.38 43.46 14.75 217.69\n",
            "feature03 −265.75 135.72 27589.24 27.05 23.75 17.71 313.65\n",
            "feature04 1.44 8859.80 475782.90 466.45 302.10 616.14 379628.70\n",
            "feature05 14.19 100.00 36495.72 35.78 28.67 18.42 339.43\n",
            "feature06 26879.00 22807940.00 800712200.00 785011.93 555324.50 1140006.00 1299614000000.00feature07 0.30 0.66 519.86 0.51 0.52 0.07 0.01\n",
            "feature08 9821.00 500916.00 48422080.00 47472.62 36806.00 40210.58 1616891000.00\n",
            "25%quantile 50%quantile 75%quantile skew kurtosis mode\n",
            "feature01 95.40 99.11 99.92 −3.27 12.62 1\n",
            "feature02 37.74 43.46 53.04 1.87 5.92 1020\n",
            "feature03 16.32 23.75 35.02 −3.60 74.00 9\n",
            "feature04 98.60 302.10 545.80 4.59 40.27 7\n",
            "feature05 22.51 28.67 43.54 1.49 1.82 1\n",
            "feature06 325733.00 555324.50 928986.00 10.49 166.02 1feature07 0.46 0.52 0.56 −0.29 −0.37 3\n",
            "feature08 23812.00 36806.00 55337.00 4.20 32.41 3\n",
            "Detectmissingvalues\n",
            "For rows containing null values, if a large enough dataset is available, such samples are\n",
            "usually removed. If sample size is limited, then data can be imputed by, for example,replacing the missing value with the mean or median calculated for the corresponding\n",
            "attribute/feature/variable.Missingdatacanalsobevisualizedinplots,e.g.ifcertainareas\n",
            "revealgaps.Incrystallography,thiscanbecomparedtofillingmissingreflectionsthatcouldnotbemeasuredinanexperimentortoreplacethosewhichhavelargemeasurementerrorstoimprovethechancesofsuccessfulphasing[ 98].\n",
            "Detectoutliers\n",
            "Anyformofoutliersneedstoberemovedbydeletingthecorrespondingsample.Itisalso\n",
            "possible to set minimum/maximum boundaries for the attributes/features/variables andexcludecertainrangesofdataiftheyarenotreliable.Visualassessmentbyusingplotsvery\n",
            "quicklyidentifiesoutliers.CalculatingInter-QuartileRange[99 ]helpstoidentifyoutliers\n",
            "withinasinglefeature.Correlationcoefficients[ 100]a n dfact o ra nal ysis[ 95]ca nbeu sed\n",
            "toidentifyoutliersafterbivariateandmultivariateanalysisrespectively.\n",
            "Featureengineering\n",
            "The main purpose of this step is feature creation and transformation. A very good intro-\n",
            "ductioncanbefoundin[ 101].Thefeaturesasfoundintherawdatamaynotbedescriptive\n",
            "enoughtoidentifyapatternandsomenewfeaturesmayhavetobecreated.Often,features\n",
            "will be combined, for example, in a linear f ashion or into polynomials. Such combina-\n",
            "tions dramatically reduce the complexity of data analysis. Other feature transformations\n",
            "may bedeveloped makinguse ofdomain-specific knowledgeand helptoturn non-linearrelationships between features into linear ones. Standardization is used to improve theunderstanding of the data whereas normalization turns skewed data distributions into\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 71', 'Table 1.', 'Summarystatisticsfromdataexplorationstepforexampledata.', 'Minimum Maximum Sum Mean MedianStandard', 'deviation Variance', 'feature01 11.40 100.00 96491.17 94.60 99.11 10.83 117.19', 'feature02 19.92 150.20 48325.97 47.38 43.46 14.75 217.69', 'feature03 −265.75 135.72 27589.24 27.05 23.75 17.71 313.65', 'feature04 1.44 8859.80 475782.90 466.45 302.10 616.14 379628.70', 'feature05 14.19 100.00 36495.72 35.78 28.67 18.42 339.43', 'feature06 26879.00 22807940.00 800712200.00 785011.93 555324.50 1140006.00 1299614000000.00feature07 0.30 0.66 519.86 0.51 0.52 0.07 0.01', 'feature08 9821.00 500916.00 48422080.00 47472.62 36806.00 40210.58 1616891000.00', '25%quantile 50%quantile 75%quantile skew kurtosis mode', 'feature01 95.40 99.11 99.92 −3.27 12.62 1', 'feature02 37.74 43.46 53.04 1.87 5.92 1020', 'feature03 16.32 23.75 35.02 −3.60 74.00 9', 'feature04 98.60 302.10 545.80 4.59 40.27 7', 'feature05 22.51 28.67 43.54 1.49 1.82 1', 'feature06 325733.00 555324.50 928986.00 10.49 166.02 1feature07 0.46 0.52 0.56 −0.29 −0.37 3', 'feature08 23812.00 36806.00 55337.00 4.20 32.41 3', 'Detectmissingvalues', 'For rows containing null values, if a large enough dataset is available, such samples are', 'usually removed.', 'If sample size is limited, then data can be imputed by, for example,replacing the missing value with the mean or median calculated for the corresponding', 'attribute/feature/variable.', 'Missingdatacanalsobevisualizedinplots,e.g.ifcertainareas', 'revealgaps.', 'Incrystallography,thiscanbecomparedtofillingmissingreflectionsthatcouldnotbemeasuredinanexperimentortoreplacethosewhichhavelargemeasurementerrorstoimprovethechancesofsuccessfulphasing[ 98].', 'Detectoutliers', 'Anyformofoutliersneedstoberemovedbydeletingthecorrespondingsample.', 'Itisalso', 'possible to set minimum/maximum boundaries for the attributes/features/variables andexcludecertainrangesofdataiftheyarenotreliable.', 'Visualassessmentbyusingplotsvery', 'quicklyidentifiesoutliers.', 'CalculatingInter-QuartileRange[99 ]helpstoidentifyoutliers', 'withinasinglefeature.', 'Correlationcoefficients[ 100]a n dfact o ra nal ysis[ 95]ca nbeu sed', 'toidentifyoutliersafterbivariateandmultivariateanalysisrespectively.', 'Featureengineering', 'The main purpose of this step is feature creation and transformation.', 'A very good intro-', 'ductioncanbefoundin[ 101].', 'Thefeaturesasfoundintherawdatamaynotbedescriptive', 'enoughtoidentifyapatternandsomenewfeaturesmayhavetobecreated.', 'Often,features', 'will be combined, for example, in a linear f ashion or into polynomials.', 'Such combina-', 'tions dramatically reduce the complexity of data analysis.', 'Other feature transformations', 'may bedeveloped makinguse ofdomain-specific knowledgeand helptoturn non-linearrelationships between features into linear ones.', 'Standardization is used to improve theunderstanding of the data whereas normalization turns skewed data distributions into']\n",
            " > Processing time: 107.98851299285889\n",
            " > Real-time factor: 0.25500497678778833\n",
            " > Saving output to AUDIO_OUTPUTS/page_18.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 72 M. VOLLMAR AND G. EVANS\n",
            "symmetric ones. If a variable is continuous, then in some cases binning and discretiza-\n",
            "tion may be necessary. This is similar to defining a resolution range for crystallographicdata and then dividing the range into bins so that each bin contains approximately thesamenumberofreflections.Featuresmaybeexcludedthroughsystematictrial-and-errorapproaches or data dimensions may be reduced by looking at dimensionality reductionproceduresmentionedabove.\n",
            "These steps may be repeated several times to get a better insight and may require\n",
            "rescalinginbetweenthedifferentanalyses.\n",
            "Performance measures and producing robust results\n",
            "W orkingwithMLandAIalgorithmsisapurelydata-drivenapproach.Noinitialassump-tions,exclusionsorbiasesaboutthesystemsoneaimstodevelopshouldbemade.\n",
            "Performancemeasures\n",
            "If a supervised classification problem is addressed with a machine learning algorithm,\n",
            "then a confusion matrix is used for performance assessment (see Figure 6(g–i)). In a\n",
            "simple binary case, all correctly identified positive and negative samples are recorded,true positive (TP) and true negative (TN), respectively. Additionally, samples for whichthe prediction and the known ground-truth disagree are counted as false-positive (FP)and false-negative (FN). All positive samples are given as P=TP+FNand all negative\n",
            "samplesas N=TN+FP.Usingthecountsforthedifferentbinaryclassificationoutcomes,\n",
            "thefollowingmetricscanbecalculated:classificationaccuracyanderror,sensitivity,speci-\n",
            "ficity, false-positive rate, precision and F1 score (a metric for test accuracy based on the\n",
            "harmonicmeasureofprecisionandsensitivity/recall).\n",
            "The‘ClassificationAccuracy ’(ACC)isdefinedas\n",
            "ClassificationAccuracy =TP+TN\n",
            "TP+TN+FP+FN(1)\n",
            "and gives an overall performance measure of the predictor for both classes in the binaryclassificationproblem.\n",
            "The ‘Classification error ’ gives details about misclassification events and can be calcu-\n",
            "latedas\n",
            "Classificationerror =FP+FN\n",
            "TP+TN+FP+FN(2)\n",
            "or as 1 – ACC. A classification error of 5% is often used as a benchmark as this is the\n",
            "typicallyobservedhumanclassificationperformance[ 102].\n",
            "The‘Sensitivity’ ,recallortrue-positiverate( TPR)isameasuretojudgehowwellapre-\n",
            "dictordoesincorrectlyidentifyingsamplesofthepositiveclassoutofallpositivecasesandisdefinedas\n",
            "Sensitivityor TPR=TP\n",
            "P=TP\n",
            "TP+FN(3)\n",
            "The ‘Specificity’ or true-negative rate ( TNR) looks at the predictor’s performance in cor-\n",
            "rectly identifying all negative samples out of the total number of negative class samples\n",
            " > Text splitted to sentences.\n",
            "['72 M. VOLLMAR AND G. EVANS', 'symmetric ones.', 'If a variable is continuous, then in some cases binning and discretiza-', 'tion may be necessary.', 'This is similar to defining a resolution range for crystallographicdata and then dividing the range into bins so that each bin contains approximately thesamenumberofreflections.', 'Featuresmaybeexcludedthroughsystematictrial-and-errorapproaches or data dimensions may be reduced by looking at dimensionality reductionproceduresmentionedabove.', 'These steps may be repeated several times to get a better insight and may require', 'rescalinginbetweenthedifferentanalyses.', 'Performance measures and producing robust results', 'W orkingwithMLandAIalgorithmsisapurelydata-drivenapproach.', 'Noinitialassump-tions,exclusionsorbiasesaboutthesystemsoneaimstodevelopshouldbemade.', 'Performancemeasures', 'If a supervised classification problem is addressed with a machine learning algorithm,', 'then a confusion matrix is used for performance assessment (see Figure 6(g–i)).', 'In a', 'simple binary case, all correctly identified positive and negative samples are recorded,true positive (TP) and true negative (TN), respectively.', 'Additionally, samples for whichthe prediction and the known ground-truth disagree are counted as false-positive (FP)and false-negative (FN).', 'All positive samples are given as P=TP+FNand all negative', 'samplesas N=TN+FP.Usingthecountsforthedifferentbinaryclassificationoutcomes,', 'thefollowingmetricscanbecalculated:classificationaccuracyanderror,sensitivity,speci-', 'ficity, false-positive rate, precision and F1 score (a metric for test accuracy based on the', 'harmonicmeasureofprecisionandsensitivity/recall).', 'The‘ClassificationAccuracy ’(ACC)isdefinedas', 'ClassificationAccuracy =TP+TN', 'TP+TN+FP+FN(1)', 'and gives an overall performance measure of the predictor for both classes in the binaryclassificationproblem.', 'The ‘Classification error ’ gives details about misclassification events and can be calcu-', 'latedas', 'Classificationerror =FP+FN', 'TP+TN+FP+FN(2)', 'or as 1 – ACC.', 'A classification error of 5% is often used as a benchmark as this is the', 'typicallyobservedhumanclassificationperformance[ 102].', 'The‘Sensitivity’ ,recallortrue-positiverate( TPR)isameasuretojudgehowwellapre-', 'dictordoesincorrectlyidentifyingsamplesofthepositiveclassoutofallpositivecasesandisdefinedas', 'Sensitivityor TPR=TP', 'P=TP', 'TP+FN(3)', 'The ‘Specificity’ or true-negative rate ( TNR) looks at the predictor’s performance in cor-', 'rectly identifying all negative samples out of the total number of negative class samples']\n",
            " > Processing time: 51.047473192214966\n",
            " > Real-time factor: 0.2445758552114492\n",
            " > Saving output to AUDIO_OUTPUTS/page_19.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 73\n",
            "andiscalculatedbelow\n",
            "Specificityor TNR=TN\n",
            "N=TN\n",
            "TN+FP(4)\n",
            "The‘False-positiverate’( FPR)givesameasureforhowman ysam plesofthenegativeclass\n",
            "havebeenpredictedtowronglybelongtothepositiveclass\n",
            "False-positiverateor FPR=FP\n",
            "N=FP\n",
            "TN+FP(5)\n",
            "Specificity canalsobecalculatedas 1–FPRandthefalse-positiverate isrelatedtothisas 1\n",
            "–TNR.\n",
            "The ‘Precision’ or positive predictive value ( PPV) looks at how many of the samples\n",
            "belonging to the positive class have been correctly identified as such out of all samples\n",
            "assignedtothepositiveclass\n",
            "Precisionor PPV=TP\n",
            "TP+FP(6)\n",
            "The ‘F1 score’ is a trade-off between ‘ Precision’ and ‘Sensitivity’ .I tl o o k sa th o ww e l la\n",
            "predictor does in identifying samples of the positive class while also considering themisclassificationoutcomesoffalse-positivesandfalse-negatives.Itiscalculatedasfollows:\n",
            "F1score=2\n",
            "1\n",
            "precision+1\n",
            "sensitivity=TP\n",
            "TP+FN=FP\n",
            "2(7)\n",
            "Agoodbinaryclassifierwillscorecloseto1or100%forthe‘ Classificationaccuracy’ andthe\n",
            "‘F1score’,althoughthelattercanbesignificantlylowerdependingonwhetheraclassifieris\n",
            "wanted,tohavehigh‘ Precision’ or‘Sensitivity’ .‘Classificationerror’ and‘FPR’areexpected\n",
            "tobeclosetozeroor0%,whereas‘ Sensitivity’ ,‘Specificity’ ,and‘Precision’ shouldallscore\n",
            "close to 1 or 100%. In general, a model cannot achieve perfect predictions unless it haslearnedthedata.\n",
            "Additionally, the area-under-a-curve of a receiver-operating characteristic (ROC) can\n",
            "becalculated.InanROCplot,thefalse-positiverate, FPR,isplottedonthehorizontalaxes\n",
            "whereas the true-positive rate, TPR, is plotted on the vertical axes. In the case of a well-\n",
            "performingbinaryclassifier,thecurvewillexhibitapeaknearthetopleftcornerwhere FPR\n",
            "isminimaland TPRatitsmaximum.Forthearea-under-the-curve(AUC )determinedfor\n",
            "anROCcurve,an AUCvaluecloseto1or100%shouldbefoundifthesystemisperforming\n",
            "well[103].\n",
            "Figure6(h)and(i)giveaconfusionmatrixandradarplotofperformancemetricsfora\n",
            "binaryclassificationpredictorofadecisiontreewithanAdaBoostalgorithm.Suchasystemiscalledanensemblepredictorasthecombineddecisionsofhundredsorthousandsoftreesare evaluated. Classification accuracy and error are global metrics describing the overall\n",
            "performanceofaclassifier.Here,wefind96%fo rtheclassificationaccuracyand4%forthe\n",
            "errorwhenchallengedwiththetestset,datathatwaskeptseparateandhasnotbeenused\n",
            "intraining.Onthewhole,oursystemperformswell.Forsensitivityandspecificity,wefind94% and 97%, respectively, which means that the predictor performs almost equally wellincorrectlyclassifyingsamplesforboth,thepositiveandthenegativeclass.Thisisfurther\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 73', 'andiscalculatedbelow', 'Specificityor TNR=TN', 'N=TN', 'TN+FP(4)', 'The‘False-positiverate’( FPR)givesameasureforhowman ysam plesofthenegativeclass', 'havebeenpredictedtowronglybelongtothepositiveclass', 'False-positiverateor FPR=FP', 'N=FP', 'TN+FP(5)', 'Specificity canalsobecalculatedas 1–FPRandthefalse-positiverate isrelatedtothisas 1', '–TNR.', 'The ‘Precision’ or positive predictive value ( PPV) looks at how many of the samples', 'belonging to the positive class have been correctly identified as such out of all samples', 'assignedtothepositiveclass', 'Precisionor PPV=TP', 'TP+FP(6)', 'The ‘F1 score’ is a trade-off between ‘ Precision’ and ‘Sensitivity’ .', 'I tl o o k sa th o ww e l la', 'predictor does in identifying samples of the positive class while also considering themisclassificationoutcomesoffalse-positivesandfalse-negatives.', 'Itiscalculatedasfollows:', 'F1score=2', '1', 'precision+1', 'sensitivity=TP', 'TP+FN=FP', '2(7)', 'Agoodbinaryclassifierwillscorecloseto1or100%forthe‘ Classificationaccuracy’ andthe', '‘F1score’,althoughthelattercanbesignificantlylowerdependingonwhetheraclassifieris', 'wanted,tohavehigh‘ Precision’ or‘Sensitivity’ .', '‘Classificationerror’ and‘FPR’areexpected', 'tobeclosetozeroor0%,whereas‘ Sensitivity’ ,‘Specificity’ ,and‘Precision’ shouldallscore', 'close to 1 or 100%.', 'In general, a model cannot achieve perfect predictions unless it haslearnedthedata.', 'Additionally, the area-under-a-curve of a receiver-operating characteristic (ROC) can', 'becalculated.', 'InanROCplot,thefalse-positiverate, FPR,isplottedonthehorizontalaxes', 'whereas the true-positive rate, TPR, is plotted on the vertical axes.', 'In the case of a well-', 'performingbinaryclassifier,thecurvewillexhibitapeaknearthetopleftcornerwhere FPR', 'isminimaland TPRatitsmaximum.', 'Forthearea-under-the-curve(AUC )determinedfor', 'anROCcurve,an AUCvaluecloseto1or100%shouldbefoundifthesystemisperforming', 'well[103].', 'Figure6', '(h)and', '(i)giveaconfusionmatrixandradarplotofperformancemetricsfora', 'binaryclassificationpredictorofadecisiontreewithanAdaBoostalgorithm.', 'Suchasystemiscalledanensemblepredictorasthecombineddecisionsofhundredsorthousandsoftreesare evaluated.', 'Classification accuracy and error are global metrics describing the overall', 'performanceofaclassifier.', 'Here,wefind96%fo rtheclassificationaccuracyand4%forthe', 'errorwhenchallengedwiththetestset,datathatwaskeptseparateandhasnotbeenused', 'intraining.', 'Onthewhole,oursystemperformswell.', 'Forsensitivityandspecificity,wefind94% and 97%, respectively, which means that the predictor performs almost equally wellincorrectlyclassifyingsamplesforboth,thepositiveandthenegativeclass.', 'Thisisfurther']\n",
            " > Processing time: 56.23165416717529\n",
            " > Real-time factor: 0.23959907739512806\n",
            " > Saving output to AUDIO_OUTPUTS/page_20.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 74 M. VOLLMAR AND G. EVANS\n",
            "Figure 7. Sample distribution of a bootstrap calculation to determine the conﬁdence interval of a\n",
            "predictor.\n",
            "supported by a small false-positive rate of 3% and a high score for precision of 94%. An\n",
            "ROC curve is plotted in Figure 6(g) and the area-under-the-curve has been calculated to\n",
            "be99%forthisexamplesystemwithapeakclosetothetopleftcorner.Asthesystemusedhere represents an ensemble of decision makers, the importance of each feature for eachensemblemembercanbeaccessedaswasplottedinFigure 6(f).Feature05seemstobeof\n",
            "highimportanceacrossthedifferentdecisionmakersintheensemble.\n",
            "Abovewelookedatmeasurestojudgetheperformanceofasinglealgorithminaclassifi-\n",
            "cationproblem.Aswasmentionedpreviously,itisimpossibletosayatthebeginningwhichalgorithmwillbebestsuitedtosolveone’sproblem.AwayofcomparingtheperformancebetweendifferentalgorithmsisusingastatisticalsignificancetestlikeStudent’s t-test[104]\n",
            "or calculating a confidence interval using a bootstrap method [ 105]a n da p p l y i n gap a i r -\n",
            "wisecomparisonbetweenthedifferentalgorithms.Bootstrappingisaresamplingmethodandusesrandomsamplingwithreplacementtofindthesampledistributionintheavailabledata. The 95% confidence interval for the example system used here is in the probabil-ityrangefrom51%to65%andthecorrespondingsampledistributionfortheunderlyingb o o t s t r a pc a l c u l a t i o ni sg i v e ni nF i g u r e 7. For our predictor, there is a 95% chance that a\n",
            "sampleliesinarangefrom51%to65%foritspredictionprobability.Forapairwisecom-parison,thedifferentalgorithmshavetoberunonthesamesetofdata,i.e.thesamesplit\n",
            "into training and testing data, and should be combined with k-fold cross-validation (see\n",
            "below).\n",
            "Fordeeplearningapplications,asystem’sperformanceisnotonlyjudgedbyclassifica-\n",
            "tionoutcomesalone(seeabove)butalsobylookingatthebehaviourofthelearningcurveovertime.Infact,alldeeplearningmodels,regardlessofwhethertheyhavebeentrainedfor\n",
            " > Text splitted to sentences.\n",
            "['74 M. VOLLMAR AND G. EVANS', 'Figure 7.', 'Sample distribution of a bootstrap calculation to determine the conﬁdence interval of a', 'predictor.', 'supported by a small false-positive rate of 3% and a high score for precision of 94%.', 'An', 'ROC curve is plotted in Figure 6', '(g) and the area-under-the-curve has been calculated to', 'be99%forthisexamplesystemwithapeakclosetothetopleftcorner.', 'Asthesystemusedhere represents an ensemble of decision makers, the importance of each feature for eachensemblemembercanbeaccessedaswasplottedinFigure 6', '(f).', 'Feature05seemstobeof', 'highimportanceacrossthedifferentdecisionmakersintheensemble.', 'Abovewelookedatmeasurestojudgetheperformanceofasinglealgorithminaclassifi-', 'cationproblem.', 'Aswasmentionedpreviously,itisimpossibletosayatthebeginningwhichalgorithmwillbebestsuitedtosolveone’sproblem.', 'AwayofcomparingtheperformancebetweendifferentalgorithmsisusingastatisticalsignificancetestlikeStudent’s t-test[104]', 'or calculating a confidence interval using a bootstrap method [ 105]a n da p p l y i n gap a i r -', 'wisecomparisonbetweenthedifferentalgorithms.', 'Bootstrappingisaresamplingmethodandusesrandomsamplingwithreplacementtofindthesampledistributionintheavailabledata.', 'The 95% confidence interval for the example system used here is in the probabil-ityrangefrom51%to65%andthecorrespondingsampledistributionfortheunderlyingb o o t s t r a pc a l c u l a t i o ni sg i v e ni nF i g u r e 7.', 'For our predictor, there is a 95% chance that a', 'sampleliesinarangefrom51%to65%foritspredictionprobability.', 'Forapairwisecom-parison,thedifferentalgorithmshavetoberunonthesamesetofdata,i.e.thesamesplit', 'into training and testing data, and should be combined with k-fold cross-validation (see', 'below).', 'Fordeeplearningapplications,asystem’sperformanceisnotonlyjudgedbyclassifica-', 'tionoutcomesalone(seeabove)butalsobylookingatthebehaviourofthelearningcurveovertime.', 'Infact,alldeeplearningmodels,regardlessofwhethertheyhavebeentrainedfor']\n",
            " > Processing time: 41.487709045410156\n",
            " > Real-time factor: 0.2538538510946897\n",
            " > Saving output to AUDIO_OUTPUTS/page_21.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 75\n",
            "Figure 8. Fordeeplearningapplications,thetrainingandperformanceofapredictorareoftenassessed\n",
            "usinglearningcurves,thelossandmodelaccuracyinparticular.Theleftcolumn(plotsa–d)showsthe\n",
            "lossovertimewhentrainingaconvolutionalneuralnetworkandtherightcolumn(plotse–h)showsthemodelaccuracyforthesamemodel.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 75', 'Figure 8.', 'Fordeeplearningapplications,thetrainingandperformanceofapredictorareoftenassessed', 'usinglearningcurves,thelossandmodelaccuracyinparticular.Theleftcolumn(plotsa–d)showsthe', 'lossovertimewhentrainingaconvolutionalneuralnetworkandtherightcolumn(plotse–h)showsthemodelaccuracyforthesamemodel.']\n",
            " > Processing time: 6.152574777603149\n",
            " > Real-time factor: 0.24899471751047897\n",
            " > Saving output to AUDIO_OUTPUTS/page_22.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 76 M. VOLLMAR AND G. EVANS\n",
            "aclassificationoraregressionproblem,maketheirfinalpredictionbymeansofregression\n",
            "byapplyingaminimizationfunction,usuallystochasticgradientdescent.Thegoalofthisminimizationistoreducethecostorpenaltyinthesystembycorrectlyidentifyingsamplesandreturningtheinformationoftheoutcomethroughbackpropagationsoweightscanbeadjusted for the next training cycle. After each training cycle, this can be a batch of data,if the total input is too large to be held in memory, and/or at the end of a training epoch,the system is evaluated by using a validation set and the loss is recorded and plotted forthe training and validation set. Monitoring the loss function by plotting a learning curveisthereforeagoodwaytoseewhetherthesystemperformsaccordingtoexpectationandif there is any behaviour that is of concern, e.g. over- or underfitting. For a system wheretrainingisworkingwell,themodelwillgainexperienceovertimeandthelearningcurvewillconvergetowardsamaximumwiththelossapproachingaminimum.Figure 8shows\n",
            "some learning curves and how problems manifest themselves. Plots (a) and (e) give thebehaviour for a good system where the curves for the training and testing set follow thesame pattern overall and where there are few extreme fluctuations. A system where themodelperformsbetteronthetrainingdatathanonthetestingdataisoverfittingandhaslearnedthedata.Thelossforthetrainingdataislower(b)andthemodelaccuracyhigher(f) when comparing to the testing data if the system is overfitting. On the other hand, asone can see from plots (c) and (g), if a model is underfitting, perhaps because it has notrunlongenoughtoreachconvergence,thenforboth,thetrainingandthetestingset,theloss is high and the accuracy low. If the split into training and testing set is set such that\n",
            "thenumberofsamplesfortrainingissmallcomparedtothenumberofsamplesfortesting,\n",
            "thenbothcurveswillfollowasimilartrendbutwithalargegapbetweenthemasin(d)and(h).Aswiththeperformancemeasuresintroducedabove,aperfectsystemthathaslearnedall the data and has optimized all the internal parameters for this data will make no mis-takesandhasalossof‘0’.Suchasystemisexpectedtofailtogeneralizewhenchallengedwith new samples unless they represent a perfect match to the learned data. For classifi-cationproblems,asecondlearningcurvecanbeplottedwhichmonitorstheclassificationaccuracy (see above) as a second metric. An additional testing set is often used as a finalassessmentafterallthetrainingasfinishedandaboveperformancemetricsarecalculated.\n",
            "Robustness\n",
            "T oachieverobustresults,thedatausedfortrainingneedstobepresentedtoanalgorithmin\n",
            "multiplewaystoensurethatthereisconsistencybetweentheindividualresults.First,oneneeds to consider the way data have been split into a training and testing set. Commonlyused splits are 80%, 75%, 70% of the samples for the training set and 20%, 25%, 30% forthe testing set. These splits are generally applicable but may need adjusting based on theproblem at hand. It is also important to consider the distribution of samples. Generally,any predictive tool works on the assumption that the data have an equal distribution forthedifferentoutcomes,i.e.ifclassesaretobepredictedthenallclasseshavetobepresent\n",
            "equallyorforregressionitisexpectedthatsamplescoverthedatarangesmoothlywithfew\n",
            "extremeoutliers.\n",
            "If balanced data distributions cannot be achieved, and they very rarely can, then the\n",
            "imbalanceneedstobeaddressedwhensplittingthedata.Commonly,oneappliesstratifica-tion,amethodthatensuresthattheoveralldatadistributionisretainedinthetrainingand\n",
            " > Text splitted to sentences.\n",
            "['76 M. VOLLMAR AND G. EVANS', 'aclassificationoraregressionproblem,maketheirfinalpredictionbymeansofregression', 'byapplyingaminimizationfunction,usuallystochasticgradientdescent.', 'Thegoalofthisminimizationistoreducethecostorpenaltyinthesystembycorrectlyidentifyingsamplesandreturningtheinformationoftheoutcomethroughbackpropagationsoweightscanbeadjusted for the next training cycle.', 'After each training cycle, this can be a batch of data,if the total input is too large to be held in memory, and/or at the end of a training epoch,the system is evaluated by using a validation set and the loss is recorded and plotted forthe training and validation set.', 'Monitoring the loss function by plotting a learning curveisthereforeagoodwaytoseewhetherthesystemperformsaccordingtoexpectationandif there is any behaviour that is of concern, e.g. over- or underfitting.', 'For a system wheretrainingisworkingwell,themodelwillgainexperienceovertimeandthelearningcurvewillconvergetowardsamaximumwiththelossapproachingaminimum.', 'Figure 8shows', 'some learning curves and how problems manifest themselves.', 'Plots (a) and (e) give thebehaviour for a good system where the curves for the training and testing set follow thesame pattern overall and where there are few extreme fluctuations.', 'A system where themodelperformsbetteronthetrainingdatathanonthetestingdataisoverfittingandhaslearnedthedata.', 'Thelossforthetrainingdataislower(b)andthemodelaccuracyhigher(f) when comparing to the testing data if the system is overfitting.', 'On the other hand, asone can see from plots (c) and (g), if a model is underfitting, perhaps because it has notrunlongenoughtoreachconvergence,thenforboth,thetrainingandthetestingset,theloss is high and the accuracy low.', 'If the split into training and testing set is set such that', 'thenumberofsamplesfortrainingissmallcomparedtothenumberofsamplesfortesting,', 'thenbothcurveswillfollowasimilartrendbutwithalargegapbetweenthemasin(d)and(h).', 'Aswiththeperformancemeasuresintroducedabove,aperfectsystemthathaslearnedall the data and has optimized all the internal parameters for this data will make no mis-takesandhasalossof‘0’.', 'Suchasystemisexpectedtofailtogeneralizewhenchallengedwith new samples unless they represent a perfect match to the learned data.', 'For classifi-cationproblems,asecondlearningcurvecanbeplottedwhichmonitorstheclassificationaccuracy (see above) as a second metric.', 'An additional testing set is often used as a finalassessmentafterallthetrainingasfinishedandaboveperformancemetricsarecalculated.', 'Robustness', 'T oachieverobustresults,thedatausedfortrainingneedstobepresentedtoanalgorithmin', 'multiplewaystoensurethatthereisconsistencybetweentheindividualresults.', 'First,oneneeds to consider the way data have been split into a training and testing set.', 'Commonlyused splits are 80%, 75%, 70% of the samples for the training set and 20%, 25%, 30% forthe testing set.', 'These splits are generally applicable but may need adjusting based on theproblem at hand.', 'It is also important to consider the distribution of samples.', 'Generally,any predictive tool works on the assumption that the data have an equal distribution forthedifferentoutcomes,i.e.ifclassesaretobepredictedthenallclasseshavetobepresent', 'equallyorforregressionitisexpectedthatsamplescoverthedatarangesmoothlywithfew', 'extremeoutliers.', 'If balanced data distributions cannot be achieved, and they very rarely can, then the', 'imbalanceneedstobeaddressedwhensplittingthedata.', 'Commonly,oneappliesstratifica-tion,amethodthatensuresthattheoveralldatadistributionisretainedinthetrainingand']\n",
            " > Processing time: 59.43134784698486\n",
            " > Real-time factor: 0.24146104006658592\n",
            " > Saving output to AUDIO_OUTPUTS/page_23.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 77\n",
            "testingdata.Alternatively,minoritysamplescanbe‘boosted’inaprocesscalled‘oversam-\n",
            "pling’ by creating simulated or synthetic new instances which consider the actual natureof the data as a starting point. A summary and assessment of various oversampling tech-niquesisgiveninBatistaetal.[ 106].Ontheotherhand,onecanalsoreducethenumberof\n",
            "samples in the dominant class/es through ‘downsampling’ by randomly eliminating sam-ples.Iftoomanysamplesareremoved,thiswillhoweveraffectthestabilityofapredictivesystem.\n",
            "K-fold cross-validation [ 107,108]i sa n o t h e rw a yt oi n c r e a s et h es t a b i l i t yo fas y s t e m .\n",
            "Severalcross-validationfoldsarecreatedbytakingasubsetofdatafromthelargertrainingset.Thesystemissetupsuchthateachsamplecanonlybeusedonceinacross-validationfoldbutcanappearmultipletimesinthetrainingdatadependingonhowmanyfoldshavebeenchosen.Inthisprocedure,itmustalsobeinsuredtoapplystratificationtohavepersis-tentdatarepresentationinthedifferentfolds.Iftherearelargedifferencesinperformancebetweenthedifferentcross-validationfolds,thenthesystemisconsideredunstableandthealgorithmand/orparametersneedtobeevaluatedandadjusted.\n",
            "Unlike machine learning where model robustness is usually achieved through cross-\n",
            "validation, in deep learning randomness is the preferred option. As neural networks indeep learning are stochastic, random initial weights at the beginning of a training cycleand shuffling the data before each training epoch are the means by which robustness isensuredforthemodel.Theresultswilldifferforeachcyclebutnottotheextentthattheyare contradictory and over time the system will converge to a minimal value. Validation\n",
            "done with a hold-out set of data will produce similar results each time. Splitting the data\n",
            "into training and testing set to account for model variance still applies, as well as ensur-ing that both sets are representative in terms of class distribution when compared to allthe data. Internally a deep learning model does have an additional layer of randomnessthroughshufflingandinitialweights.Arandomseedcanbedefinedtogivesomelevelofcontrolandrobustnessifreproducibilityisdesired.Toshowrobustnesswhenevaluatingadeeplearningmodelforrealisticapplications,trainingandevaluationshouldberepeatedmultiple times and then standard deviation, standard error and a confidence interval bedeterminedusingallthemodelperformances.\n",
            "T h ee a s i e s tw a yt oe x p l o r eal a r g en u m b e ro fo p t i o n si nad a t a - d r i v e nw a yi st ou s e\n",
            "automation. Here, a pipeline can be created that tries all algorithms one wants to evalu-ate and changes parameters either systematically through a grid search or by randomlyselectingvaluesfromadefinedrange[ 109].\n",
            "Examples of machine learning in the field of biological crystallography\n",
            "Here, we will look at applications of algorithms from the fields of machine learning andartificial intelligence in six areas of protein crystallography: serial crystallography, soft-ware for model building and refinement, decision making in automated data analysis atsynchrotron facilities, protein crystallizability, crystallization outcome classification andstructureprediction.AsummaryofthedifferentapplicationsisgiveninTable 2.\n",
            "Serialcrystallography\n",
            "In serial crystallography a set of diffraction images is produced by irradiating hundreds\n",
            "or thousands of small protein crystals, sometimes less than a micron in size, with a\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 77', 'testingdata.', 'Alternatively,minoritysamplescanbe‘boosted’inaprocesscalled‘oversam-', 'pling’ by creating simulated or synthetic new instances which consider the actual natureof the data as a starting point.', 'A summary and assessment of various oversampling tech-niquesisgiveninBatistaetal.', '[ 106].', 'Ontheotherhand,onecanalsoreducethenumberof', 'samples in the dominant class/es through ‘downsampling’ by randomly eliminating sam-ples.', 'Iftoomanysamplesareremoved,thiswillhoweveraffectthestabilityofapredictivesystem.', 'K-fold cross-validation [ 107,108]i sa n o t h e rw a yt oi n c r e a s et h es t a b i l i t yo fas y s t e m .', 'Severalcross-validationfoldsarecreatedbytakingasubsetofdatafromthelargertrainingset.', 'Thesystemissetupsuchthateachsamplecanonlybeusedonceinacross-validationfoldbutcanappearmultipletimesinthetrainingdatadependingonhowmanyfoldshavebeenchosen.', 'Inthisprocedure,itmustalsobeinsuredtoapplystratificationtohavepersis-tentdatarepresentationinthedifferentfolds.', 'Iftherearelargedifferencesinperformancebetweenthedifferentcross-validationfolds,thenthesystemisconsideredunstableandthealgorithmand/orparametersneedtobeevaluatedandadjusted.', 'Unlike machine learning where model robustness is usually achieved through cross-', 'validation, in deep learning randomness is the preferred option.', 'As neural networks indeep learning are stochastic, random initial weights at the beginning of a training cycleand shuffling the data before each training epoch are the means by which robustness isensuredforthemodel.', 'Theresultswilldifferforeachcyclebutnottotheextentthattheyare contradictory and over time the system will converge to a minimal value.', 'Validation', 'done with a hold-out set of data will produce similar results each time.', 'Splitting the data', 'into training and testing set to account for model variance still applies, as well as ensur-ing that both sets are representative in terms of class distribution when compared to allthe data.', 'Internally a deep learning model does have an additional layer of randomnessthroughshufflingandinitialweights.', 'Arandomseedcanbedefinedtogivesomelevelofcontrolandrobustnessifreproducibilityisdesired.', 'Toshowrobustnesswhenevaluatingadeeplearningmodelforrealisticapplications,trainingandevaluationshouldberepeatedmultiple times and then standard deviation, standard error and a confidence interval bedeterminedusingallthemodelperformances.', 'T h ee a s i e s tw a yt oe x p l o r eal a r g en u m b e ro fo p t i o n si nad a t a - d r i v e nw a yi st ou s e', 'automation.', 'Here, a pipeline can be created that tries all algorithms one wants to evalu-ate and changes parameters either systematically through a grid search or by randomlyselectingvaluesfromadefinedrange[ 109].', 'Examples of machine learning in the field of biological crystallography', 'Here, we will look at applications of algorithms from the fields of machine learning andartificial intelligence in six areas of protein crystallography: serial crystallography, soft-ware for model building and refinement, decision making in automated data analysis atsynchrotron facilities, protein crystallizability, crystallization outcome classification andstructureprediction.', 'AsummaryofthedifferentapplicationsisgiveninTable 2.', 'Serialcrystallography', 'In serial crystallography a set of diffraction images is produced by irradiating hundreds', 'or thousands of small protein crystals, sometimes less than a micron in size, with a']\n",
            " > Processing time: 64.49550080299377\n",
            " > Real-time factor: 0.2501867953705837\n",
            " > Saving output to AUDIO_OUTPUTS/page_24.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 78 M. VOLLMAR AND G. EVANS\n",
            "Table 2.SummarytableofthediscussedmachinelearningandAIapplications.\n",
            "Fieldofapplication Model/softwarepackage Architecture Publicdata Citation\n",
            "Diﬀractionimage\n",
            "classiﬁcationDeepFreak ResNet-50 DiﬀraNet Souzaetal.[86 ]\n",
            "Diﬀractionimage\n",
            "classiﬁcation– AlexNet CXIDB Keetal.[87 ]\n",
            "3Dmodelbuilding Buccaneer ;Coot Multi-layer\n",
            "perceptronPDB Bondetal.[83 ]\n",
            "3Dmodelbuilding ARP/wARP Unidirected\n",
            "nearest-\n",
            "neighbourgraph;SupportvectormachinePDB Chojnowski\n",
            "etal.[65]\n",
            "3Dmodelbuilding ARP/wARP Principal com-\n",
            "ponentanalysisPDB Pereira and\n",
            "Lamzin[46]\n",
            "3Dmodelbuilding ARP/wARP Lineardiscriminant\n",
            "analysisPDB Hattne and\n",
            "Lamzin[47]\n",
            "Decisionmakingin\n",
            "automationMETRIX Decisiontreewith\n",
            "AdaBoostPDB Vollmaretal.[76 ]\n",
            "Protein\n",
            "crystallizabilitySECRET Supportvector\n",
            "machine;NaïveBayesclassiﬁerPDB;TargetDB Smialowski\n",
            "etal.[55]\n",
            "Protein\n",
            "crystallizabilityXtalPred Logarithmic\n",
            "opinionpoolPDB;TargetDB Slabinskietal.[110]\n",
            "Protein\n",
            "crystallizabilityXtalPred-RF Randomforest PSITargetTrack\n",
            "database(http://sbkb.org/\n",
            "index.jsp)Jahandideh\n",
            "etal.[74]\n",
            "Protein\n",
            "crystallizabilityPPCpred Supportvector\n",
            "machinePepcDB Mizianty and\n",
            "Kurgan[62]\n",
            "Protein\n",
            "crystallizabilityPredPPCrys Supportvector\n",
            "machinePepcDB Wangetal.[63 ]\n",
            "Protein\n",
            "cr\n",
            "ystallizabilityDHS-Crystallize Convolutional\n",
            "neuralnetwork;XGBoostWangetal.[63 ] Alavi and\n",
            "Ascher[75]\n",
            "Protein\n",
            "crystallizabilityDeepCrystal Deepconvolutional\n",
            "neuralnetworkWangetal.[63 ] Elbasiretal.[84 ]\n",
            "Crystallization\n",
            "outcome\n",
            "classiﬁcationMARCO Deepconvolutional\n",
            "neuralnetworkhttps://marco.ccr.\n",
            "buﬀalo.edu [20]Brunoetal.[20 ]\n",
            "Crystallization\n",
            "outcome\n",
            "classiﬁcation– Lineardiscriminant\n",
            "analysis– Cumbaaetal.[48 ];\n",
            "Cumbaaetal.[49]\n",
            "Crystallization\n",
            "outcomeclassiﬁcation– Lineardiscriminant\n",
            "analysis–S aitohetal.[50]\n",
            "Crystallization\n",
            "outcomeclassiﬁcation– Decisiontree,\n",
            "Randomforest–B ernetal.[72]\n",
            "Crystallization\n",
            "outcomeclassiﬁcation– Decisiontree,\n",
            "Randomforest–L iuetal.[73]\n",
            "Crystallization\n",
            "outcomeclassiﬁcationALICE Supportvector\n",
            "machine;Lineardiscriminantanalysis– Buchala and\n",
            "Wilson[51]\n",
            "Crystallization\n",
            "outcomeclassiﬁcation– Supportvector\n",
            "machine– Panetal.[64]\n",
            "(continued )\n",
            " > Text splitted to sentences.\n",
            "['78 M. VOLLMAR AND G. EVANS', 'Table 2.', 'SummarytableofthediscussedmachinelearningandAIapplications.', 'Fieldofapplication Model/softwarepackage Architecture Publicdata Citation', 'Diﬀractionimage', 'classiﬁcationDeepFreak ResNet-50 DiﬀraNet Souzaetal.', '[86 ]', 'Diﬀractionimage', 'classiﬁcation– AlexNet CXIDB Keetal.', '[87 ]', '3Dmodelbuilding Buccaneer ;Coot Multi-layer', 'perceptronPDB Bondetal.', '[83 ]', '3Dmodelbuilding ARP/wARP Unidirected', 'nearest-', 'neighbourgraph;SupportvectormachinePDB Chojnowski', 'etal.', '[65]', '3Dmodelbuilding ARP/wARP Principal com-', 'ponentanalysisPDB Pereira and', 'Lamzin[46]', '3Dmodelbuilding ARP/wARP Lineardiscriminant', 'analysisPDB Hattne and', 'Lamzin[47]', 'Decisionmakingin', 'automationMETRIX Decisiontreewith', 'AdaBoostPDB Vollmaretal.', '[76 ]', 'Protein', 'crystallizabilitySECRET Supportvector', 'machine;NaïveBayesclassiﬁerPDB;TargetDB Smialowski', 'etal.[55]', 'Protein', 'crystallizabilityXtalPred Logarithmic', 'opinionpoolPDB;TargetDB Slabinskietal.[110]', 'Protein', 'crystallizabilityXtalPred-RF Randomforest PSITargetTrack', 'database(http://sbkb.org/', 'index.jsp)Jahandideh', 'etal.[74]', 'Protein', 'crystallizabilityPPCpred Supportvector', 'machinePepcDB Mizianty and', 'Kurgan[62]', 'Protein', 'crystallizabilityPredPPCrys Supportvector', 'machinePepcDB Wangetal.', '[63 ]', 'Proteincr', 'ystallizabilityDHS-Crystallize Convolutional', 'neuralnetwork;XGBoostWangetal.', '[63 ] Alavi and', 'Ascher[75]', 'Protein', 'crystallizabilityDeepCrystal Deepconvolutional', 'neuralnetworkWangetal.', '[63 ] Elbasiretal.', '[84 ]', 'Crystallization', 'outcome', 'classiﬁcationMARCO Deepconvolutional', 'neuralnetworkhttps://marco.ccr.', 'buﬀalo.edu [20]Brunoetal.', '[20 ]', 'Crystallization', 'outcome', 'classiﬁcation– Lineardiscriminant', 'analysis– Cumbaaetal.', '[48 ];', 'Cumbaaetal.[49]', 'Crystallization', 'outcomeclassiﬁcation– Lineardiscriminant', 'analysis–S aitohetal.[50]', 'Crystallization', 'outcomeclassiﬁcation– Decisiontree,', 'Randomforest–B ernetal.[72]', 'Crystallization', 'outcomeclassiﬁcation– Decisiontree,', 'Randomforest–L iuetal.[73]', 'Crystallization', 'outcomeclassiﬁcationALICE Supportvector', 'machine;Lineardiscriminantanalysis– Buchala and', 'Wilson[51]', 'Crystallization', 'outcomeclassiﬁcation– Supportvector', 'machine– Panetal.', '[64]', '(continued )']\n",
            " > Processing time: 50.31108093261719\n",
            " > Real-time factor: 0.22476905006050166\n",
            " > Saving output to AUDIO_OUTPUTS/page_25.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 79\n",
            "Table 2.Continued.\n",
            "Fieldofapplication Model/softwarepackage Architecture Publicdata Citation\n",
            "Crystallization\n",
            "outcome\n",
            "classiﬁcationCEEP Neuralnetwork – Spraggonetal.[ 85]\n",
            "Proteinmodelling RaptorX ResNet PDB Wangetal.[88 ]\n",
            "Proteinmodelling AlphaFold ResNet PDB Senioretal.[ 89];\n",
            "Senioretal.[ 90]\n",
            "Proteinmodelling AlphaFold2 Attentionbased\n",
            "networkPDB; various\n",
            "sequencedatabasesJumperetal.[111 ]\n",
            "Proteinmodelling trRosetta ResNet PDB Yangetal.[91 ]\n",
            "Proteinmodelling RoseTTAFold 3-tracknetwork – Baeketal.[112 ]\n",
            "high-intensity beam, often an X-ray free-electron laser. In the process of the experiment,\n",
            "in many cases, the protein crystal gets destroyed or is severely damaged allowing only asingle or few image/s per crystal to be recorded. For data analysis to calculate a protein’selectrondensitytobeabletobuildamodel,onlytheimagesprovidingusefuldatashouldbecombined.Inparticular,imagesthatexhibitnoorverynoisydiffractionandthosethatmayshowpathologiesthatarecurrentlydifficulttocorrectlyaddress,needtobeexcluded.Identifyingthemostsuitablesetofimagesforfurtheranalysisiscurrentlyanentirelyman-ualandlaboriousprocess.Duetotheverylargedataratesanddatavolumesgeneratedbyserialcrystallography,therehasbeenademandformethodstorapidlyclassifyandtriage\n",
            "measuredimagesto,first,determinewhichdatatouseforfurtheranalysisand,second,to\n",
            "determinewhichdatamustbestoredandwhichcanbediscarded.\n",
            "Souza et al. [ 86] investigated the use of machine learning applications to identify and\n",
            "classifyserialcrystallographyimagessothattheycanbecombinedforfurtherdownstreamanalysis.Thedatausedtotrainseveralmachinelearningalgorithmswereacombinationofrealandsyntheticdiffractionimages, DiffraNet.Intotal,fiveclasseswerepredictedforthe\n",
            "syntheticimages,allofthemrepresentingpossibleoutcomesforadiffractionexperiment.Therealimagesweresplitintotwoclasses,containdiffractionordonotcontaindiffraction.Ofthethreealgorithmstested,randomforest,supportvectormachinesandaconvolutionalneural network based on a ResNet-50 architecture, the latter performed best on both sets\n",
            "of images as judged by classification accuracy. The network was named DeepFreak .T h e\n",
            "predictionaccuracytoidentifyimagesbelongingtothefivedifferentclasseswashigherforthesyntheticimagesthanfortherealimageswith98.45%,97.66%and98.5%,respectively,forthethreedifferentalgorithmstried.Fortherealimages,thethreedifferentapproachesachieved an accuracy of 86.81%, 91.1% and 94.51%, respectively. The lower performanceon the real data ought to be expected, as it is difficult to simulate all possible sources ofnoiseanderrorinanexperimentandthereforesyntheticimageswillbeeasiertolearnandtopredictwiththepredictorachievinghigherscores.\n",
            "Anotherconvolutionalneuralnetwork-basedapproachwasdevelopedbyKeetal.[ 87].\n",
            "T h ea l g o r i t h mu s e dt h ea r c h i t e c t u r eo f AlexNet[113]. The data used for training are real\n",
            "imagedatacollectedattheLinacCoherentLightSource(LCLS).AsinSouzaetal.[ 86]sev-\n",
            "eralclasseswerepredictedbasedonthepotentialoutcomeforserialcrystallographydatacollection,inthisapplicationthreeintotal.Thelabelsfortheseclasseswereassignedintwoways, by a human expert crystallographer and an automated system using a spot-findingalgorithm with a threshold. Five datasets representative of commonly acquired data have\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 79', 'Table 2.', 'Continued.', 'Fieldofapplication Model/softwarepackage Architecture Publicdata Citation', 'Crystallization', 'outcome', 'classiﬁcationCEEP Neuralnetwork – Spraggonetal.', '[ 85]', 'Proteinmodelling RaptorX ResNet PDB Wangetal.', '[88 ]', 'Proteinmodelling AlphaFold ResNet PDB Senioretal.', '[ 89];', 'Senioretal.', '[ 90]', 'Proteinmodelling AlphaFold2 Attentionbased', 'networkPDB; various', 'sequencedatabasesJumperetal.', '[111 ]', 'Proteinmodelling trRosetta ResNet PDB Yangetal.', '[91 ]', 'Proteinmodelling RoseTTAFold 3-tracknetwork – Baeketal.', '[112 ]', 'high-intensity beam, often an X-ray free-electron laser.', 'In the process of the experiment,', 'in many cases, the protein crystal gets destroyed or is severely damaged allowing only asingle or few image/s per crystal to be recorded.', 'For data analysis to calculate a protein’selectrondensitytobeabletobuildamodel,onlytheimagesprovidingusefuldatashouldbecombined.', 'Inparticular,imagesthatexhibitnoorverynoisydiffractionandthosethatmayshowpathologiesthatarecurrentlydifficulttocorrectlyaddress,needtobeexcluded.', 'Identifyingthemostsuitablesetofimagesforfurtheranalysisiscurrentlyanentirelyman-ualandlaboriousprocess.', 'Duetotheverylargedataratesanddatavolumesgeneratedbyserialcrystallography,therehasbeenademandformethodstorapidlyclassifyandtriage', 'measuredimagesto,first,determinewhichdatatouseforfurtheranalysisand,second,to', 'determinewhichdatamustbestoredandwhichcanbediscarded.', 'Souza et al.', '[ 86] investigated the use of machine learning applications to identify and', 'classifyserialcrystallographyimagessothattheycanbecombinedforfurtherdownstreamanalysis.', 'Thedatausedtotrainseveralmachinelearningalgorithmswereacombinationofrealandsyntheticdiffractionimages, DiffraNet.', 'Intotal,fiveclasseswerepredictedforthe', 'syntheticimages,allofthemrepresentingpossibleoutcomesforadiffractionexperiment.', 'Therealimagesweresplitintotwoclasses,containdiffractionordonotcontaindiffraction.', 'Ofthethreealgorithmstested,randomforest,supportvectormachinesandaconvolutionalneural network based on a ResNet-50 architecture, the latter performed best on both sets', 'of images as judged by classification accuracy.', 'The network was named DeepFreak .', 'T h e', 'predictionaccuracytoidentifyimagesbelongingtothefivedifferentclasseswashigherforthesyntheticimagesthanfortherealimageswith98.45%,97.66%and98.5%,respectively,forthethreedifferentalgorithmstried.', 'Fortherealimages,thethreedifferentapproachesachieved an accuracy of 86.81%, 91.1% and 94.51%, respectively.', 'The lower performanceon the real data ought to be expected, as it is difficult to simulate all possible sources ofnoiseanderrorinanexperimentandthereforesyntheticimageswillbeeasiertolearnandtopredictwiththepredictorachievinghigherscores.', 'Anotherconvolutionalneuralnetwork-basedapproachwasdevelopedbyKeetal.', '[ 87].', 'T h ea l g o r i t h mu s e dt h ea r c h i t e c t u r eo f AlexNet[113].', 'The data used for training are real', 'imagedatacollectedattheLinacCoherentLightSource(LCLS).', 'AsinSouzaetal.', '[ 86]sev-', 'eralclasseswerepredictedbasedonthepotentialoutcomeforserialcrystallographydatacollection,inthisapplicationthreeintotal.', 'Thelabelsfortheseclasseswereassignedintwoways, by a human expert crystallographer and an automated system using a spot-findingalgorithm with a threshold.', 'Five datasets representative of commonly acquired data have']\n",
            " > Processing time: 64.69723701477051\n",
            " > Real-time factor: 0.23813892304625878\n",
            " > Saving output to AUDIO_OUTPUTS/page_26.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 80 M. VOLLMAR AND G. EVANS\n",
            "beenusedfortraining.TheimageshavebeenmadepubliclyavailableintheCoherentX-\n",
            "ray Imaging Data Bank (CXIDB; [114 ]). Between 70% and 98% of the images across the\n",
            "differentdatasetsweregiventhecorrectlabelbytheCNN,wherebythehumanannotationserved as the ground truth. These success rates were highly specific to the experimentalsetupusedwhencollectingtheimages.Theauthorsalreadydiscussedthis,astheytrainedaCNNonasetofimagesacquiredononetypeofdetectorbutmadepredictionsonimagescollected on a different detector type which results in lower performance. However, thiswasimprovedwhenapplyingsomeimagepre-processingtechniquessuchaslocalcontrastnormalization [ 115] and data augmentation through random cropping. The more inter-\n",
            "estingaspecthereis,thattheauthorsalsoattemptedautomatedspotfindingandcomparetheresultstotheautomatedstepinthediffractiondataintegrationpackageDIALS[ 116],\n",
            "whichcanbeappliedtodiffractiondatacollectedonalargenumberofsources.Although,CNN-basedautomatedspotfindingshowsgenerallylowerperformance,upto86%oftheimagesshowingdiffractioncanbeidentifiedandnearlyallimageswithoutanydiffraction.ItwouldbeinterestingtoseehowthisclassifiercangeneralizewhengivenimagescollectedatotherXFELfacilities.\n",
            "Usageinmodelbuildingandrefinementsoftwarepackages\n",
            "Here, we are looking at multiple ways machine learning and artificial intelligence have\n",
            "been used in programs employed for building protein structures by placing atoms into\n",
            "experimentallyderivedelectrondensitymaps.\n",
            "We first look at Bond et al. [ 83]. They train two neural networks (both a multi-layer\n",
            "perceptron),onetolookattheproteinbackboneormainchainandoneforthesidechains.\n",
            "T h et a r g e ti st op r e d i c tan e wc o r r e c t n e s ss c o r ef o rh o ww e l lt h ea t o m sh a v eb e e np l a c e dintotheelectrondensitymapduringmodelbuilding.\n",
            "Theinputlayerconsistedofonenodeforeachfeature(differentformainchainandside\n",
            "chain features), followed by a single hidden layer with ten neurons and an output layerpredicting a single value, the correctness. The data used for training were selected fromthePDB[ 6].Thefeaturesusedarecommoncrystallographicmodelqualitymetrics,here,\n",
            "calculatedusingtoolsimplementedwithinthemodelbuildingandvisualizationprogramCoot[117,118].Allfeaturesweretransformedtoameanof0andunitvariance.Themean\n",
            "and standard error in the coefficient of determination (COD) for the test set was used toassesstheperformanceof100trainingrepeatseachtimewitharandomlychosenstartingseed.Asascorebetween0and1wasthepredictionoutcome,themachinelearningprob-lem was that of regression. Additionally, the authors applied a threshold for the score of≥0.5andturnedtheregressionintoabinaryclassificationproblem.\n",
            "Using performance metrics for classification to assess the predictor showed that the\n",
            "resultsforthemainchainpredictionsweremorereliablethanforthesidechainwith92.3%and87.6%oftheatomsbeinggiventhecorrectlabel,respectively.Thecorrectlabel,inthiscase, meant that an atom had been found in its proper position in the 3D structure and\n",
            "the predictor was able to identify this, or that an atom was flagged to require attention\n",
            "becauseithadbeenplacedinthewrongpositionduringautomatedmodelbuilding.True-positivesampleswerethose,wheretheatomswe rebuiltwellintotheelectrondensity,and\n",
            "true-negative samples were those requiring manual adjustment. The fit was given by thecorrelationbetweenthemodelandthefitintotheelectrondensity, Z-scoresforthemean,\n",
            " > Text splitted to sentences.\n",
            "['80 M. VOLLMAR AND G. EVANS', 'beenusedfortraining.', 'TheimageshavebeenmadepubliclyavailableintheCoherentX-', 'ray Imaging Data Bank (CXIDB; [114 ]).', 'Between 70% and 98% of the images across the', 'differentdatasetsweregiventhecorrectlabelbytheCNN,wherebythehumanannotationserved as the ground truth.', 'These success rates were highly specific to the experimentalsetupusedwhencollectingtheimages.Theauthorsalreadydiscussedthis,astheytrainedaCNNonasetofimagesacquiredononetypeofdetectorbutmadepredictionsonimagescollected on a different detector type which results in lower performance.', 'However, thiswasimprovedwhenapplyingsomeimagepre-processingtechniquessuchaslocalcontrastnormalization [ 115] and data augmentation through random cropping.', 'The more inter-', 'estingaspecthereis,thattheauthorsalsoattemptedautomatedspotfindingandcomparetheresultstotheautomatedstepinthediffractiondataintegrationpackageDIALS[ 116],', 'whichcanbeappliedtodiffractiondatacollectedonalargenumberofsources.', 'Although,CNN-basedautomatedspotfindingshowsgenerallylowerperformance,upto86%oftheimagesshowingdiffractioncanbeidentifiedandnearlyallimageswithoutanydiffraction.', 'ItwouldbeinterestingtoseehowthisclassifiercangeneralizewhengivenimagescollectedatotherXFELfacilities.', 'Usageinmodelbuildingandrefinementsoftwarepackages', 'Here, we are looking at multiple ways machine learning and artificial intelligence have', 'been used in programs employed for building protein structures by placing atoms into', 'experimentallyderivedelectrondensitymaps.', 'We first look at Bond et al.', '[ 83].', 'They train two neural networks (both a multi-layer', 'perceptron),onetolookattheproteinbackboneormainchainandoneforthesidechains.', 'T h et a r g e ti st op r e d i c tan e wc o r r e c t n e s ss c o r ef o rh o ww e l lt h ea t o m sh a v eb e e np l a c e dintotheelectrondensitymapduringmodelbuilding.', 'Theinputlayerconsistedofonenodeforeachfeature(differentformainchainandside', 'chain features), followed by a single hidden layer with ten neurons and an output layerpredicting a single value, the correctness.', 'The data used for training were selected fromthePDB[ 6].', 'Thefeaturesusedarecommoncrystallographicmodelqualitymetrics,here,', 'calculatedusingtoolsimplementedwithinthemodelbuildingandvisualizationprogramCoot[117,118].', 'Allfeaturesweretransformedtoameanof0andunitvariance.', 'Themean', 'and standard error in the coefficient of determination (COD) for the test set was used toassesstheperformanceof100trainingrepeatseachtimewitharandomlychosenstartingseed.', 'Asascorebetween0and1wasthepredictionoutcome,themachinelearningprob-lem was that of regression.', 'Additionally, the authors applied a threshold for the score of≥0.5andturnedtheregressionintoabinaryclassificationproblem.', 'Using performance metrics for classification to assess the predictor showed that the', 'resultsforthemainchainpredictionsweremorereliablethanforthesidechainwith92.3%and87.6%oftheatomsbeinggiventhecorrectlabel,respectively.', 'Thecorrectlabel,inthiscase, meant that an atom had been found in its proper position in the 3D structure and', 'the predictor was able to identify this, or that an atom was flagged to require attention', 'becauseithadbeenplacedinthewrongpositionduringautomatedmodelbuilding.', 'True-positivesampleswerethose,wheretheatomswe rebuiltwellintotheelectrondensity,and', 'true-negative samples were those requiring manual adjustment.', 'The fit was given by thecorrelationbetweenthemodelandthefitintotheelectrondensity, Z-scoresforthemean,']\n",
            " > Processing time: 64.69460797309875\n",
            " > Real-time factor: 0.2408457864546461\n",
            " > Saving output to AUDIO_OUTPUTS/page_27.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 81\n",
            "bestanddifferencedensity, Z-scoresfortheBfactors,overlapsbetweenatomsandtheres-\n",
            "olution. For the main chain predictor additionally, Ramachandran score and the twist in\n",
            "the peptide plane and flip of its nitrogen and oxygen atoms were recorded. The predictorfor the side chains additionally used a rotamer score. Both predictors showed high false-positiverates,23%forthemainchainpredictionsand21%forthesidechainpredictions,whichmeansthatbothperformlesswellinidentifyingatomsthatmayrequiresomeatten-tionbytheexperimentalist.Themostlikelyreasonforthisisthatthetrainingdatahadveryfew examples of wrongly placed atoms and perhaps the imbalance was not treated appro-priately in the training process. Nevertheless, overall, both predictors helped to improveautomated structure building which in turn will reduce the time a crystallographer mayneedtomanuallyadjustamodel.\n",
            "Thepredictedcorrectnessscoresforeachatomcanbevisualizedin Cootand an auto-\n",
            "mated pruning function for chains, residues and side chains with low results has beenimplementedintheautomatedmodelbuildingpipeline Buccaneer [119,120].Inparticular,\n",
            "using the correctness scores has resulted in significant improvements for high-resolutionstructures.Thismethodisafirststeptowardsimprovingautomatedmodelbuildingusingartificialintelligencecomparedtothecurrentlyavailableprograms.\n",
            "Anothercrystallographicsoftwaresuitemakinguseofmachinelearningtechniquesat\n",
            "different stages of model building is ARP/wARP. One very recent development is the use\n",
            "of machine learning to assign fragments of protein backbone [ 65]. Those fragments are\n",
            "usuallytheresultofautomatedmodelbuildingandfindingthecorrectsequenceregisteris\n",
            "particularlydifficultforlow-resolutiondata.\n",
            "AdatabaseofproteinfragmentsextractedfromthePDBwasusedtotraintwomachine\n",
            "learning applications. As with Bond et al. [ 83], the two applications are addressing main-\n",
            "chainandside-chainatomsseparately.Goodqualityhightomediumresolutionstructures\n",
            "wereusedfortraining.Thestructuresusedfortrainingwerebrokenintofragmentsofdif-feringbackbonelengthandgroupedbysimilarityusingsuperpositioning.Testingwasdonefor two separate groups, one covering medium to low-resolution structures and one forlowtoverylowresolution.Thenumberofsamplesineachgroupwasbalancedbyreducingthoseinthefirsttestsetthroughrandomselectionfromequallysizedbins.Afterattemptingautomatedmodelbuildingwithoutasuppliedsequenceonlythetop50%ofsolutions,aftercomparingwiththedepositedground-truthstructures,waskeptforperformanceanalysisofamachinelearning-basedsequenceclassificationtool.\n",
            "Ifthedepositedstructure,theground-truthandthebuiltmodelagreedwitheachother\n",
            "basedonmainchainandsidechainspecificcriteria,thenaC αor side chain was labelled\n",
            "‘correct’. The loop regions were built using location probabilities. Side chain descriptorswere created using the top 500 rotamers library [ 121] and the different conformations\n",
            "aligned on the peptide plane with the built residues. A Cartesian grid was placed onCαafter alignment with a spacing of 1Å between the grid points. The grid points that\n",
            "matched the electron density region at a given threshold were used to construct an undi-rectednearest-neighbourgraph.Thedimensionsreachedthroughconnectivitywithinthe\n",
            "graph was then compared to the density volume at varying levels of root-mean squared\n",
            "deviations.\n",
            "Including treatments for variation in the data due to experimental and data analysis\n",
            "effects,afinalsidechaindescriptorwasavectorof25elements.Factorsinfluencingthesidechaindescriptorweresolventcontent,dataresolution,WilsonBfactorandphasequalityas\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 81', 'bestanddifferencedensity, Z-scoresfortheBfactors,overlapsbetweenatomsandtheres-', 'olution.', 'For the main chain predictor additionally, Ramachandran score and the twist in', 'the peptide plane and flip of its nitrogen and oxygen atoms were recorded.', 'The predictorfor the side chains additionally used a rotamer score.', 'Both predictors showed high false-positiverates,23%forthemainchainpredictionsand21%forthesidechainpredictions,whichmeansthatbothperformlesswellinidentifyingatomsthatmayrequiresomeatten-tionbytheexperimentalist.', 'Themostlikelyreasonforthisisthatthetrainingdatahadveryfew examples of wrongly placed atoms and perhaps the imbalance was not treated appro-priately in the training process.', 'Nevertheless, overall, both predictors helped to improveautomated structure building which in turn will reduce the time a crystallographer mayneedtomanuallyadjustamodel.', 'Thepredictedcorrectnessscoresforeachatomcanbevisualizedin Cootand an auto-', 'mated pruning function for chains, residues and side chains with low results has beenimplementedintheautomatedmodelbuildingpipeline Buccaneer [119,120].', 'Inparticular,', 'using the correctness scores has resulted in significant improvements for high-resolutionstructures.', 'Thismethodisafirststeptowardsimprovingautomatedmodelbuildingusingartificialintelligencecomparedtothecurrentlyavailableprograms.', 'Anothercrystallographicsoftwaresuitemakinguseofmachinelearningtechniquesat', 'different stages of model building is ARP/wARP.', 'One very recent development is the use', 'of machine learning to assign fragments of protein backbone [ 65].', 'Those fragments are', 'usuallytheresultofautomatedmodelbuildingandfindingthecorrectsequenceregisteris', 'particularlydifficultforlow-resolutiondata.', 'AdatabaseofproteinfragmentsextractedfromthePDBwasusedtotraintwomachine', 'learning applications.', 'As with Bond et al.', '[ 83], the two applications are addressing main-', 'chainandside-chainatomsseparately.', 'Goodqualityhightomediumresolutionstructures', 'wereusedfortraining.', 'Thestructuresusedfortrainingwerebrokenintofragmentsofdif-feringbackbonelengthandgroupedbysimilarityusingsuperpositioning.', 'Testingwasdonefor two separate groups, one covering medium to low-resolution structures and one forlowtoverylowresolution.', 'Thenumberofsamplesineachgroupwasbalancedbyreducingthoseinthefirsttestsetthroughrandomselectionfromequallysizedbins.', 'Afterattemptingautomatedmodelbuildingwithoutasuppliedsequenceonlythetop50%ofsolutions,aftercomparingwiththedepositedground-truthstructures,waskeptforperformanceanalysisofamachinelearning-basedsequenceclassificationtool.', 'Ifthedepositedstructure,theground-truthandthebuiltmodelagreedwitheachother', 'basedonmainchainandsidechainspecificcriteria,thenaC αor side chain was labelled', '‘correct’.', 'The loop regions were built using location probabilities.', 'Side chain descriptorswere created using the top 500 rotamers library [ 121] and the different conformations', 'aligned on the peptide plane with the built residues.', 'A Cartesian grid was placed onCαafter alignment with a spacing of 1Å between the grid points.', 'The grid points that', 'matched the electron density region at a given threshold were used to construct an undi-rectednearest-neighbourgraph.', 'Thedimensionsreachedthroughconnectivitywithinthe', 'graph was then compared to the density volume at varying levels of root-mean squared', 'deviations.', 'Including treatments for variation in the data due to experimental and data analysis', 'effects,afinalsidechaindescriptorwasavectorof25elements.', 'Factorsinfluencingthesidechaindescriptorweresolventcontent,dataresolution,WilsonBfactorandphasequalityas']\n",
            " > Processing time: 63.75925612449646\n",
            " > Real-time factor: 0.2393557798399133\n",
            " > Saving output to AUDIO_OUTPUTS/page_28.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 82 M. VOLLMAR AND G. EVANS\n",
            "theydirectlyaltertheshapeoftheelectrondensity.Itwasfoundthatforsomesidechains,\n",
            "e.g.threonineandvaline,thesidechaindescriptorswereverysimilarastheyfilledasimilarvolumeandshapeofelectrondensity,buttheyhaddifferentmobilityproperties.Determin-ingthesidechaindescriptorwasakeydatapreparationstepforthesuccessfultrainingofa support vector machine as a one-versus-all classifier. The amino acid type for a residuewithin a main-chain fragment was then determined using a separate system in which aseparateclassifierforeachaminoacidhadbeenproduced.Itestimatedtheprobabilityforeachofthe20differentaminoacidsindependentofthesequenceinformationgivenfortheprotein. The 20 different classifiers used a soft-margin and a radial basis function (RBF)kernel. The RBF kernel uses the squared Euclidean distance between two feature vectorsto describe similarity. The closer the two feature vectors are to each other in the featurespacethegreateristhesquaredEuclideandistanceandthemoresimilararethetwosam-plesdescribedbythefeaturevectors.Asigmoidalfunctionwasusedforcalibrationofallthe20classifiers.Runningthesetof20classifiersonthemainchainfragmentgaveprob-ability estimates for each type of residue in the fragment. For the entire fragment itself,this was then recorded in a statistical scoring matrix. The matrix was then used to alignthe fragment with the target sequence and approximate probabilities for each residue inthe alignment were determined. Alignments reaching a confidence level of 99.99% werescoredasaccepted.Comparisonwasthenmadetoanalignmentbetweenthefragmentanda randomly chosen sequence following the same protocol. All accepted alignments werethenassignedtothetargetsequenceusingadirectedgraph,wherethenodesrepresented\n",
            "the aligned fragments and the connections between them were directed if they could be\n",
            "assignedtothesamechain.Thelengthsoftheedgesrepresentreal-spacedistancesbetweenCαatomsattheendsofthefragments.Probabilitieswerecalculatedforthedifferentpos-\n",
            "siblepathsthroughthegraphwhenconnectingthefragmentswiththehighestprobabilitybeingthe most likelypathand the most likelysequence assignment. Themethod workedwell for medium to low-resolution test structures with somewhat lower performance forvery low-resolution models down to 4Å. The predictors performance strongly correlatedwith side chain mobility, where the latter affected the local quality of the electron den-sitymapwhichinturninfluencedtheundirectedgraphusedtoidentifyaminoacidtypes.Short,well-orderedsidechainsworkedwellwithclassificationaccuraciesof96%,93%and72% for glycine, alanine and proline respectively. Large, aromatic ones like tyrosine andtryptophanewereidentifiedwithaccuraciesof67%and60%respectively.Residuesexposedtosolventsontheproteinsurfaceorinsolventchannelsweretheleastlikelytobecorrectlyi d e n t i fi e dw i t ha c c u r a c i e so f5 %f o rl y s i n e ,9 %f o ra s p a r a g i n ea n d1 8 %f o rh i s t i d i n e .F o rboth test sets, the very low-resolution set, in particular, the number of models with lowside chain completeness was reduced and the number of models with a large number ofsidechainscorrectlybuiltwasincreased.Incombinationwiththeexploredloopbuildingalgorithm, the total chain length increased substantially and, overall, the model buildingperformanceof ARP/wARP wasgreatlyimproved.Twonovelexamplesgiveninthepaper\n",
            "takenfromthepackage’swebserviceplatform,exhibitedastrikingimprovementinmodel\n",
            "quality for the new methods. For one example given in the publication, this meant the\n",
            "refinement R/R\n",
            "freefactors improved from 31/36% to 25/29% and for another the change\n",
            "was from 37/45% to 24/29%. From a machine learning point of view, the implementa-tionfollowedcommonlyusedgoodpractice,bytryingtobalancethesampledistributionsbetweendifferentresolutionrangesexploredandbylimitingthenumberofsupportvectors\n",
            " > Text splitted to sentences.\n",
            "['82 M. VOLLMAR AND G. EVANS', 'theydirectlyaltertheshapeoftheelectrondensity.', 'Itwasfoundthatforsomesidechains,', 'e.g.threonineandvaline,thesidechaindescriptorswereverysimilarastheyfilledasimilarvolumeandshapeofelectrondensity,buttheyhaddifferentmobilityproperties.', 'Determin-ingthesidechaindescriptorwasakeydatapreparationstepforthesuccessfultrainingofa support vector machine as a one-versus-all classifier.', 'The amino acid type for a residuewithin a main-chain fragment was then determined using a separate system in which aseparateclassifierforeachaminoacidhadbeenproduced.', 'Itestimatedtheprobabilityforeachofthe20differentaminoacidsindependentofthesequenceinformationgivenfortheprotein.', 'The 20 different classifiers used a soft-margin and a radial basis function (RBF)kernel.', 'The RBF kernel uses the squared Euclidean distance between two feature vectorsto describe similarity.', 'The closer the two feature vectors are to each other in the featurespacethegreateristhesquaredEuclideandistanceandthemoresimilararethetwosam-plesdescribedbythefeaturevectors.', 'Asigmoidalfunctionwasusedforcalibrationofallthe20classifiers.', 'Runningthesetof20classifiersonthemainchainfragmentgaveprob-ability estimates for each type of residue in the fragment.', 'For the entire fragment itself,this was then recorded in a statistical scoring matrix.', 'The matrix was then used to alignthe fragment with the target sequence and approximate probabilities for each residue inthe alignment were determined.', 'Alignments reaching a confidence level of 99.99% werescoredasaccepted.', 'Comparisonwasthenmadetoanalignmentbetweenthefragmentanda randomly chosen sequence following the same protocol.', 'All accepted alignments werethenassignedtothetargetsequenceusingadirectedgraph,wherethenodesrepresented', 'the aligned fragments and the connections between them were directed if they could be', 'assignedtothesamechain.', 'Thelengthsoftheedgesrepresentreal-spacedistancesbetweenCαatomsattheendsofthefragments.', 'Probabilitieswerecalculatedforthedifferentpos-', 'siblepathsthroughthegraphwhenconnectingthefragmentswiththehighestprobabilitybeingthe most likelypathand the most likelysequence assignment.', 'Themethod workedwell for medium to low-resolution test structures with somewhat lower performance forvery low-resolution models down to 4Å.', 'The predictors performance strongly correlatedwith side chain mobility, where the latter affected the local quality of the electron den-sitymapwhichinturninfluencedtheundirectedgraphusedtoidentifyaminoacidtypes.', 'Short,well-orderedsidechainsworkedwellwithclassificationaccuraciesof96%,93%and72% for glycine, alanine and proline respectively.', 'Large, aromatic ones like tyrosine andtryptophanewereidentifiedwithaccuraciesof67%and60%respectively.', 'Residuesexposedtosolventsontheproteinsurfaceorinsolventchannelsweretheleastlikelytobecorrectlyi d e n t i fi e dw i t ha c c u r a c i e so f5 %f o rl y s i n e ,9 %f o ra s p a r a g i n ea n d1 8 %f o rh i s t i d i n e .', 'F o rboth test sets, the very low-resolution set, in particular, the number of models with lowside chain completeness was reduced and the number of models with a large number ofsidechainscorrectlybuiltwasincreased.', 'Incombinationwiththeexploredloopbuildingalgorithm, the total chain length increased substantially and, overall, the model buildingperformanceof ARP/wARP wasgreatlyimproved.', 'Twonovelexamplesgiveninthepaper', 'takenfromthepackage’swebserviceplatform,exhibitedastrikingimprovementinmodel', 'quality for the new methods.', 'For one example given in the publication, this meant the', 'refinement R/R', 'freefactors improved from 31/36% to 25/29% and for another the change', 'was from 37/45% to 24/29%.', 'From a machine learning point of view, the implementa-tionfollowedcommonlyusedgoodpractice,bytryingtobalancethesampledistributionsbetweendifferentresolutionrangesexploredandbylimitingthenumberofsupportvectors']\n",
            " > Processing time: 73.51666188240051\n",
            " > Real-time factor: 0.2448805138982948\n",
            " > Saving output to AUDIO_OUTPUTS/page_29.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 83\n",
            "toavoidoverfitting.Confidenceintervalswereprovidedtodemonstratetherobustnessof\n",
            "thesystem.\n",
            "Anotherexampleimplementedin ARP/wARP isavalidationtoolforproteinmainchain\n",
            "conformation by Pereira and Lamzin [ 46]. The data for developing the application was\n",
            "againbasedonstructuresinthePDB.Dipeptideblockswereextractedfromgoodqualityreleased structures and subjected to some filtering to ensure extracted blocks were reli-able. Pairwise sequence identity was less than 50% to ensure non-redundancy. For eachdipeptide, a Euclidean distance-squared matrix was calculated for a set of distances usedto describe the peptide plane in a protein main chain. Principal component analysis wasthen used to create a new Euclidean orthogonal 3D-space, comprised of the three decor-related principal components. This space was further divided to account for chirality inmolecules. This newly created space and its subdivision can be used to assign probabil-i t i e st ot h eb a c k b o n eg e o m e t r y .T od os o ,n e g a t i v ee x a m p l e sw e r ec r e a t e db yr a n d o m l yplacing atoms into a defined volume and distance matrices and eigenvalues were calcu-lated as before. To define the boundaries for favoured, allowed, generously allowed anddisallowed scores, similar to what is found in a Ramachandran plot [ 122], a cumulative\n",
            "density distribution was calculated for all grid points. Z-scores were calculated for mean,\n",
            "variance, skew and kurtosis for a set of randomly selected structures, assuming that thescores found for C αatoms were following a Gaussian distribution. These Z-scores were\n",
            "then further analysed with PCA to decorrelate the different moments and combine thescores into a new scoring function. This new scoring function was calculated separately\n",
            "for each of the boundaries defined above. This method increased the understanding of\n",
            "backbone geometry further than the commonly used Ramachandran plot. And indeed,t h ea u t h o r sf o u n dt h a ta n g l e s τ,ϕandψcommonly used to judge the geometric qual-\n",
            "ity of a residue in the Ramachandran plot were not directly correlated to their newlydefined score. Instead, it was observed that the first principal component described theextension of the dipeptide, the second principal component gave the twist between thetwo adjacent peptide planes and the third principal component reported bending in thedipeptide.ApplyingtheknowledgegainedfromthePCAanalysisofthedistancesindipep-tideswithinaproteinstructureidentifiedareasinthemainchainwithdistortedgeometry ,whicheitherrequiredadjustmentorcarefulassessmentwhetherthiseffectwasofcatalyticimportance.\n",
            "T h el a s te x a m p l ef o ra na p p l i c a t i o nw i t h i n ARP/wARP is described in Hattne and\n",
            "Lamzin[47].Here,theauthorsdevelopedapattern-recognitionmethodtoidentifyplanar\n",
            "objectsinproteins(mainchainpeptide;aromaticsidechainssuchashistidine,tryptophan,tyrosine and phenylalanine) or the base moiety in DNA/RNA structures. Aromatic sidechain identification was of particular interest as it served as an anchor point for sequencedockingfromwhichautomatedsidechainbuildingalongthebackbonecommenced.Fea-tures were generated locally from electron density maps and then classified using lineardiscriminant analysis by optimizing the contrast between planar and non-planar objects.I tr e l i e do nt h ef a c tt h a tap a r t i c u l a ra r r a n g e m e n to fa t o m sw a ss u r r o u n d e db yas p e c i fi c\n",
            "electrondensityshape.Toextracttheselocalfeatures,sphericalregionsinthenormalized\n",
            "d e n s i t yw e r es a m p l e da n dc o m b i n e di n t oas i n g l ef e a t u r ev e c t o r .T h es i z eo ft h es p h e r eused, determinedthetype ofinformation found inthefeature vector.Toolarge avolumecreated overlaps between adjacent planes and too small a volume missed information. Asinglesizeof3Åtoextractallfeaturesatoncewasdeterminedempiricallywhichresulted\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 83', 'toavoidoverfitting.', 'Confidenceintervalswereprovidedtodemonstratetherobustnessof', 'thesystem.', 'Anotherexampleimplementedin ARP/wARP isavalidationtoolforproteinmainchain', 'conformation by Pereira and Lamzin [ 46].', 'The data for developing the application was', 'againbasedonstructuresinthePDB.Dipeptideblockswereextractedfromgoodqualityreleased structures and subjected to some filtering to ensure extracted blocks were reli-able.', 'Pairwise sequence identity was less than 50% to ensure non-redundancy.', 'For eachdipeptide, a Euclidean distance-squared matrix was calculated for a set of distances usedto describe the peptide plane in a protein main chain.', 'Principal component analysis wasthen used to create a new Euclidean orthogonal 3D-space, comprised of the three decor-related principal components.', 'This space was further divided to account for chirality inmolecules.', 'This newly created space and its subdivision can be used to assign probabil-i t i e st ot h eb a c k b o n eg e o m e t r y .', 'T od os o ,n e g a t i v ee x a m p l e sw e r ec r e a t e db yr a n d o m l yplacing atoms into a defined volume and distance matrices and eigenvalues were calcu-lated as before.', 'To define the boundaries for favoured, allowed, generously allowed anddisallowed scores, similar to what is found in a Ramachandran plot [ 122], a cumulative', 'density distribution was calculated for all grid points.', 'Z-scores were calculated for mean,', 'variance, skew and kurtosis for a set of randomly selected structures, assuming that thescores found for C αatoms were following a Gaussian distribution.', 'These Z-scores were', 'then further analysed with PCA to decorrelate the different moments and combine thescores into a new scoring function.', 'This new scoring function was calculated separately', 'for each of the boundaries defined above.', 'This method increased the understanding of', 'backbone geometry further than the commonly used Ramachandran plot.', 'And indeed,t h ea u t h o r sf o u n dt h a ta n g l e s τ,ϕandψcommonly used to judge the geometric qual-', 'ity of a residue in the Ramachandran plot were not directly correlated to their newlydefined score.', 'Instead, it was observed that the first principal component described theextension of the dipeptide, the second principal component gave the twist between thetwo adjacent peptide planes and the third principal component reported bending in thedipeptide.', 'ApplyingtheknowledgegainedfromthePCAanalysisofthedistancesindipep-tideswithinaproteinstructureidentifiedareasinthemainchainwithdistortedgeometry ,whicheitherrequiredadjustmentorcarefulassessmentwhetherthiseffectwasofcatalyticimportance.', 'T h el a s te x a m p l ef o ra na p p l i c a t i o nw i t h i n ARP/wARP is described in Hattne and', 'Lamzin[47].', 'Here,theauthorsdevelopedapattern-recognitionmethodtoidentifyplanar', 'objectsinproteins(mainchainpeptide;aromaticsidechainssuchashistidine,tryptophan,tyrosine and phenylalanine) or the base moiety in DNA/RNA structures.', 'Aromatic sidechain identification was of particular interest as it served as an anchor point for sequencedockingfromwhichautomatedsidechainbuildingalongthebackbonecommenced.', 'Fea-tures were generated locally from electron density maps and then classified using lineardiscriminant analysis by optimizing the contrast between planar and non-planar objects.', 'I tr e l i e do nt h ef a c tt h a tap a r t i c u l a ra r r a n g e m e n to fa t o m sw a ss u r r o u n d e db yas p e c i fi c', 'electrondensityshape.', 'Toextracttheselocalfeatures,sphericalregionsinthenormalized', 'd e n s i t yw e r es a m p l e da n dc o m b i n e di n t oas i n g l ef e a t u r ev e c t o r .', 'T h es i z eo ft h es p h e r eused, determinedthetype ofinformation found inthefeature vector.', 'Toolarge avolumecreated overlaps between adjacent planes and too small a volume missed information.', 'Asinglesizeof3Åtoextractallfeaturesatoncewasdeterminedempiricallywhichresulted']\n",
            " > Processing time: 78.29149222373962\n",
            " > Real-time factor: 0.2432219876531837\n",
            " > Saving output to AUDIO_OUTPUTS/page_30.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 84 M. VOLLMAR AND G. EVANS\n",
            "inatrade-offbetweenthedifferentclassesandcausedallofthemtohaveacertainlevelof\n",
            "false-positives. The authors considered trying a specific sphere size for each class in theirfuture plans. Agreements between feature vectors in the training set and those extractedfromnewsamplesgaveanideaabouttheplanarityinthelocalpartoftheelectrondensityencodedinthevector.Afterapplyingtransformationsandnormalizationtheresultingfea-ture vectors were classified using a linear discriminant analysis [ 44,45]. The four classes\n",
            "toidentifyweresmallsingle-ringobjects(histidine),largesingle-ringobjects(single-ringnucleotide bases, phenylalanine, tyrosine), double-ring objects (double-ring nucleotidebases, tryptophan) and noise. Overall, the method identified the majority of double-ringobjects,between42%and100%,andlargesingle-ringobjects,between50%and93%,forthefivecasestudiespresented.Forsmall,single-ringobjectsbetween42%and70%ofthecaseswerefound.Astherewasaresolutiondependence,theauthorsfoundareductioninperformancewithadropinresolution.Itwasalsonotedthatasinglestructurewasusedtoextractthefeaturevectorsfortrainingwhichreducedthevarianceintrainingsamples.Toimprovethemethod,alarger,morediversesetoffeaturevectorsneededtobecreatedlook-ingatstructuresofdifferingqualityandoverallresolutionorevenbetterlocalresolution.WilsonBfactors,ormorelikelytheaveragetemperaturefactorfoundforeachringobject,should be included as the method looked at local electron density features and the sameplane element had varying electron density in different parts of the protein or betweennon-crystallographicsymmetrycopies.Theauthorsnotedthatingeneraltherewereonlyfewplanarobjectsinaproteinstructurewhichtheysawasalowsignal-to-noiseratioand\n",
            "it being challenging to identify such systems. In fact, for two of the cases presented, the\n",
            "noise level was high enough to cause the algorithm to find twice as many false-positivesamples than true-positives. Also, each planar object extracted from the selected struc-turewasregardedasatrainingsamplewhichinturnmeantthatthedifferentclassesthatneededtobeidentifiedwerenotpresentinequalnumbers.Asaresult,thesystempublishedwasbiasedtowardsidentifyingdouble-ringobjects.Nevertheless,itwasimplementedforautomatedmodelbuildinginARP/wARPandalreadyprovidesgoodstartingpointsforacrystallographertothenmanuallycompletetheinitialmodel.\n",
            "Decisionmakinginautomateddataanalysispipelines\n",
            "Large-scale user facilities, like synchrotrons, produce huge volumes of experimental data\n",
            "everyyear .Oneofthekeychallengesinhandlingsuchdatavolumesistheiranalysis,asthecomputational resources are finite and typically preclude making brute-force attempts atstructuredeterminationforeachdatasetthatismeasured.Asdiscussedearlier,thetrainedcrystallographerisusuallyabletomakeaneducatedassessmentastothequalityofdataandthe likelihood of it leading to successful structure determination or that it would answerthe scientific question being asked. No single quality metric of crystallographic data canbe relied upon to support this assessment. On the contrary, a crystallographer makes anassessmentbasedonseveralindicatorsofquality.Ultimately,thecrystallographerwantsto\n",
            "a v o i dw a s t i n gt i m eo na n a l y s i n gp o o rq u a l i t yd a t at h a tw i l ly i e l dn on e wi n f o r m a t i o no r\n",
            "results;bettertore-measurethedataoraddmoredata.\n",
            "TheauthorsofthisreviewexploredinVollmaretal.[ 76]machinelearningapplications\n",
            "to triage and assess experimental data and their metadata at early stages of data analy-sistomakemoreintelligentuseofthecomputationalresourcesfordown-streamanalysis.\n",
            " > Text splitted to sentences.\n",
            "['84 M. VOLLMAR AND G. EVANS', 'inatrade-offbetweenthedifferentclassesandcausedallofthemtohaveacertainlevelof', 'false-positives.', 'The authors considered trying a specific sphere size for each class in theirfuture plans.', 'Agreements between feature vectors in the training set and those extractedfromnewsamplesgaveanideaabouttheplanarityinthelocalpartoftheelectrondensityencodedinthevector.', 'Afterapplyingtransformationsandnormalizationtheresultingfea-ture vectors were classified using a linear discriminant analysis [ 44,45].', 'The four classes', 'toidentifyweresmallsingle-ringobjects(histidine),largesingle-ringobjects(single-ringnucleotide bases, phenylalanine, tyrosine), double-ring objects (double-ring nucleotidebases, tryptophan) and noise.', 'Overall, the method identified the majority of double-ringobjects,between42%and100%,andlargesingle-ringobjects,between50%and93%,forthefivecasestudiespresented.', 'Forsmall,single-ringobjectsbetween42%and70%ofthecaseswerefound.', 'Astherewasaresolutiondependence,theauthorsfoundareductioninperformancewithadropinresolution.', 'Itwasalsonotedthatasinglestructurewasusedtoextractthefeaturevectorsfortrainingwhichreducedthevarianceintrainingsamples.', 'Toimprovethemethod,alarger,morediversesetoffeaturevectorsneededtobecreatedlook-ingatstructuresofdifferingqualityandoverallresolutionorevenbetterlocalresolution.', 'WilsonBfactors,ormorelikelytheaveragetemperaturefactorfoundforeachringobject,should be included as the method looked at local electron density features and the sameplane element had varying electron density in different parts of the protein or betweennon-crystallographicsymmetrycopies.', 'Theauthorsnotedthatingeneraltherewereonlyfewplanarobjectsinaproteinstructurewhichtheysawasalowsignal-to-noiseratioand', 'it being challenging to identify such systems.', 'In fact, for two of the cases presented, the', 'noise level was high enough to cause the algorithm to find twice as many false-positivesamples than true-positives.', 'Also, each planar object extracted from the selected struc-turewasregardedasatrainingsamplewhichinturnmeantthatthedifferentclassesthatneededtobeidentifiedwerenotpresentinequalnumbers.', 'Asaresult,thesystempublishedwasbiasedtowardsidentifyingdouble-ringobjects.', 'Nevertheless,itwasimplementedforautomatedmodelbuildinginARP/wARPandalreadyprovidesgoodstartingpointsforacrystallographertothenmanuallycompletetheinitialmodel.', 'Decisionmakinginautomateddataanalysispipelines', 'Large-scale user facilities, like synchrotrons, produce huge volumes of experimental data', 'everyyear .', 'Oneofthekeychallengesinhandlingsuchdatavolumesistheiranalysis,asthecomputational resources are finite and typically preclude making brute-force attempts atstructuredeterminationforeachdatasetthatismeasured.', 'Asdiscussedearlier,thetrainedcrystallographerisusuallyabletomakeaneducatedassessmentastothequalityofdataandthe likelihood of it leading to successful structure determination or that it would answerthe scientific question being asked.', 'No single quality metric of crystallographic data canbe relied upon to support this assessment.', 'On the contrary, a crystallographer makes anassessmentbasedonseveralindicatorsofquality.', 'Ultimately,thecrystallographerwantsto', 'a v o i dw a s t i n gt i m eo na n a l y s i n gp o o rq u a l i t yd a t at h a tw i l ly i e l dn on e wi n f o r m a t i o no r', 'results;bettertore-measurethedataoraddmoredata.', 'TheauthorsofthisreviewexploredinVollmaretal.', '[ 76]machinelearningapplications', 'to triage and assess experimental data and their metadata at early stages of data analy-sistomakemoreintelligentuseofthecomputationalresourcesfordown-streamanalysis.']\n",
            " > Processing time: 70.88050603866577\n",
            " > Real-time factor: 0.26239328393746614\n",
            " > Saving output to AUDIO_OUTPUTS/page_31.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 85\n",
            "The authors focused on the case of experiment al phasing using single/multiple anoma-\n",
            "lous dispersion(S/MAD).Theyexploredcommonly useddataquality metrics,whichare\n",
            "availablefromtheintegrationofrawexperimentaldiffractiondataandsubsequentscalingand merging steps. The raw diffraction images used to produce the features for train-ingwerepubliclyavailableandtheoriginalstructurefactorsandatomiccoordinatesweredepositedinthePDB.Awiderangeofstructuresize,resolution,typeofproteinandhard-ware the data were collected on were covered. The diffraction data were integrated, andphases were calculated using commonly available crystallographic software while largelykeeping default settings. Metadata in the form of common crystallographic data qualitymetrics that were to be used as features, were extracted and stored in a small databasefor ease of management. A set of statistics/features was identified from a large numberof extracted metadata using exploratory data analysis and initial machine learning trials(SVM,decisiontrees,ensemblepredictorsusingbaggingandboosting).Thefeatureswerechosen based on their importance in decision making as identified by EDA and the triedalgorithmsthemselves,i.e.data-drivenratherthanbydomainknowledgeandexclusionorprioritization.Intotal,sixfeatureswereidentified; CC\n",
            "anom(correlationcoefficientbetween\n",
            "Bijvoetdifferences), /Delta1I/σI(fractionalanomalousintensitydifference), manom(mid-slope\n",
            "of the anomalous normal probability), /Delta1F/F(fractional anomalous difference based on\n",
            "structurefactormagnitudes), f/prime/prime\n",
            "theor(theoreticallydeterminedvalueof f’’,imaginarypartof\n",
            "anomalousscatteringcoefficientfortheanomalousscatterer)and dmax,(thelow-resolution\n",
            "limitofthedata).Allbutthelastdescriptorwereexpectedtobeimportantforexperimen-\n",
            "tal phasing as they are recognized indicators of the presence and strength of anomalous\n",
            "signal in data. It was at first surprising to see that the low-resolution limit, dmax,p l a y e d\n",
            "suchasignificantroleinthesuccessofexperimentalphasing,butuponreflectionitiswellknownthatinsolventflatteningmethodsthepresenceoflow-resolutiondataisimportantforthedefinitionoftheproteinenvelopeandsolventregion[ 123,124].Othermetricsoften\n",
            "used by crystallographers to judge the quality of data were discarded through this analy-sisastheywerenotdeemedusefulinansweringthespecificquestionasked.However,forsomemetrics,inparticularthosedescribingtheprecisionofunmerged( R\n",
            "merge,Rmeas)and\n",
            "mergeddata(I/ σ,CC1/2,Rp.i.m.),theauthorswereabletoconfirmlong-heldviews[ 16,125]\n",
            "regarding which of these best describe the overall data quality of a dataset. In particular,it was found that, especially for merged data, CC\n",
            "1/2was an important feature to describe\n",
            "the data and supports its use as an overall data quality metric. The features found wereusedtopredictwhetheradatasetcanbeusedtocalculateexperimentalphasesornot,i.e.a binary classification problem. Structures for which phases were originally determinedusing molecular replacement and where experimental setup meant no anomalous signalwas produced were assigned to the negative class, ‘0’. If experimental phasing was usedwhen determining the published structure, then the data were labelled to be of the posi-tiveclass,‘1’.Althoughmorepositivesampleswerepresentinthedata,theimbalancedidnotrequireanyadditionaladjustmentandapplyingstratificationwhensplittingdataintotrainingandtestingsetaswellasusingappropriateweightsduringtrainingwassufficientto\n",
            "ensurethepredictorwasnotbiasedtowardsthedominantclass.Inasecondroundoftrain-\n",
            "ing only using the six most important features identified, a decision tree with Ada Boost,forgeneralintroductionseeabove,wasfoundasthemoststableandbest-performingpre-dictorasjudgedbycommonlyusedassessmentmetrics.Aprobabilitythresholdof80%forclass ‘1’ was applied to ensure that predictions for this class had to have high confidence.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 85', 'The authors focused on the case of experiment al phasing using single/multiple anoma-', 'lous dispersion(S/MAD).', 'Theyexploredcommonly useddataquality metrics,whichare', 'availablefromtheintegrationofrawexperimentaldiffractiondataandsubsequentscalingand merging steps.', 'The raw diffraction images used to produce the features for train-ingwerepubliclyavailableandtheoriginalstructurefactorsandatomiccoordinatesweredepositedinthePDB.Awiderangeofstructuresize,resolution,typeofproteinandhard-ware the data were collected on were covered.', 'The diffraction data were integrated, andphases were calculated using commonly available crystallographic software while largelykeeping default settings.', 'Metadata in the form of common crystallographic data qualitymetrics that were to be used as features, were extracted and stored in a small databasefor ease of management.', 'A set of statistics/features was identified from a large numberof extracted metadata using exploratory data analysis and initial machine learning trials(SVM,decisiontrees,ensemblepredictorsusingbaggingandboosting).', 'Thefeatureswerechosen based on their importance in decision making as identified by EDA and the triedalgorithmsthemselves,i.e.data-drivenratherthanbydomainknowledgeandexclusionorprioritization.', 'Intotal,sixfeatureswereidentified; CC', 'anom(correlationcoefficientbetween', 'Bijvoetdifferences), /Delta1I/σI(fractionalanomalousintensitydifference), manom(mid-slope', 'of the anomalous normal probability), /Delta1F/F(fractional anomalous difference based on', 'structurefactormagnitudes), f/prime/prime', 'theor(theoreticallydeterminedvalueof f’’,imaginarypartof', 'anomalousscatteringcoefficientfortheanomalousscatterer)and dmax,(thelow-resolution', 'limitofthedata).', 'Allbutthelastdescriptorwereexpectedtobeimportantforexperimen-', 'tal phasing as they are recognized indicators of the presence and strength of anomalous', 'signal in data.', 'It was at first surprising to see that the low-resolution limit, dmax,p l a y e d', 'suchasignificantroleinthesuccessofexperimentalphasing,butuponreflectionitiswellknownthatinsolventflatteningmethodsthepresenceoflow-resolutiondataisimportantforthedefinitionoftheproteinenvelopeandsolventregion[ 123,124].', 'Othermetricsoften', 'used by crystallographers to judge the quality of data were discarded through this analy-sisastheywerenotdeemedusefulinansweringthespecificquestionasked.', 'However,forsomemetrics,inparticularthosedescribingtheprecisionofunmerged( R', 'merge,Rmeas)and', 'mergeddata(I/ σ,CC1/2,Rp.i.m.),theauthorswereabletoconfirmlong-heldviews[ 16,125]', 'regarding which of these best describe the overall data quality of a dataset.', 'In particular,it was found that, especially for merged data, CC', '1/2was an important feature to describe', 'the data and supports its use as an overall data quality metric.', 'The features found wereusedtopredictwhetheradatasetcanbeusedtocalculateexperimentalphasesornot,i.e.a binary classification problem.', 'Structures for which phases were originally determinedusing molecular replacement and where experimental setup meant no anomalous signalwas produced were assigned to the negative class, ‘0’.', 'If experimental phasing was usedwhen determining the published structure, then the data were labelled to be of the posi-tiveclass,‘1’.', 'Althoughmorepositivesampleswerepresentinthedata,theimbalancedidnotrequireanyadditionaladjustmentandapplyingstratificationwhensplittingdataintotrainingandtestingsetaswellasusingappropriateweightsduringtrainingwassufficientto', 'ensurethepredictorwasnotbiasedtowardsthedominantclass.', 'Inasecondroundoftrain-', 'ing only using the six most important features identified, a decision tree with Ada Boost,forgeneralintroductionseeabove,wasfoundasthemoststableandbest-performingpre-dictorasjudgedbycommonlyusedassessmentmetrics.', 'Aprobabilitythresholdof80%forclass ‘1’ was applied to ensure that predictions for this class had to have high confidence.']\n",
            " > Processing time: 75.33543395996094\n",
            " > Real-time factor: 0.2550298822048636\n",
            " > Saving output to AUDIO_OUTPUTS/page_32.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 86 M. VOLLMAR AND G. EVANS\n",
            "Tobeabletousethepredictedprobabilities,thepredictorwascalibratedwithsomeunseen\n",
            "data.Application onsomeentirelydifferentuser data,24samplesintotal,whichwasnotusedfortrainingandtesting,stillproducedgoodresultsbutoverall,significantlylower.Theaccuracyreachedinthechallengewas79%withasensitivityof64%(findingsevenoutof11positivesamples)andaspecificityof92%(finding12outof13negativesamples).Thismeant that the predictor performed better in correctly identifying samples where exper-imental phasing as either SAD or MAD was likely to fail. The authors suggested that themostlikelyreasonforthelowerperformancewasatechnologygapasthepublicdatausedfortrainingthepredictorwasoftenacquiredonahardwaresetupthatwasthestate-of-the-art6,12ormoremonthsinthepastduetothetimeneededtosolveandpublishastructure.Theperformanceduringreal-lifeuseroperationiscurrentlybeingassessedasthepredic-tor has been integrated into the facility’s data analysis pipelines and will be published induecourse.\n",
            "Proteincrystallizability\n",
            "There have been several attempts over the years to create predictive tools for the crystal-\n",
            "lizability of proteins. One of the earlier implementations, the prediction server SECRET,\n",
            "wasbySmialowskietal.[ 55]usingacombinationofsupportvectormachinesandanaïve\n",
            "Bayes classifier. The system was developed using structures determined by either X-raycrystallography or nuclear magnetic resonance (NMR) experiments. The structures were\n",
            "takenfromthePDBandadditionalinformationfromstructuralgenomicsworkfoundin\n",
            "theTargetDB([126]; http://targetdb.pdb.org/ )wasaddedafteradjustmentforthedifferent\n",
            "proteinsizeranges,whicharetypicallyrevealedbythetwotechniques.Physico-chemicalpropertiesofaproteinthatcanbederivedfromitsprimarysequencewereusedtocreatethefeaturesfortraining.Astheaimwastopredictcrystallizability,thenegativesampleswererepresented by NMR structures for which no equivalent crystal structures were availableand the positive samples were structures solved by X-ray crystallography. More recently,duetohigh-throughputworkinstructuralgenomicsinitiatives,wheredatawaspublishedin publicly available databases and data mining, the creation of more sophisticated pre-d i c t i v et o o l sh a sb e e np o s s i b l e .H e r e ,w el o o ka ts o m em o r er e c e n td e v e l o p m e n t si nt h efield using support vector machines, random forest and convolutional neural networkapplications.\n",
            "TheXtalPredserver developed by Slabinski et al. [ 110] is one of the most popular and\n",
            "well-known crystallizability prediction servers. It is based on large-scale data analysis ofopen-sourcestructuralgenomicsresultswhichwasusedtoidentifyfeaturesthatcanhelpto distinguish crystallizable from non-crystallizable proteins [ 127]. Slabinski et al. [110]\n",
            "used a logarithmic opinion pool [ 128] to combine probabilities for different features into\n",
            "a single score. It used physico-chemical features of proteins to place a sequence into oneof five classes of chances of crystallizability: optimal, suboptimal, average, difficult, verydifficult. The next generation of XtalPred server, now XtalPred-RF ,i n v o l v e das u b s t a n -\n",
            "tial change to the underlying algorithm [ 74]. Besides including new features and larger\n",
            "training and testing sets compared to the first published version, the authors also madeuseofrecentdevelopmentsofadvancedmachinelearningandAItechniques.Theauthorsexplored support vector machines (SVM), artificial neural networks (ANN) and randomforest methods. For each system tried, commonly used quality metrics were reported.\n",
            " > Text splitted to sentences.\n",
            "['86 M. VOLLMAR AND G. EVANS', 'Tobeabletousethepredictedprobabilities,thepredictorwascalibratedwithsomeunseen', 'data.', 'Application onsomeentirelydifferentuser data,24samplesintotal,whichwasnotusedfortrainingandtesting,stillproducedgoodresultsbutoverall,significantlylower.', 'Theaccuracyreachedinthechallengewas79%withasensitivityof64%(findingsevenoutof11positivesamples)andaspecificityof92%(finding12outof13negativesamples).', 'Thismeant that the predictor performed better in correctly identifying samples where exper-imental phasing as either SAD or MAD was likely to fail.', 'The authors suggested that themostlikelyreasonforthelowerperformancewasatechnologygapasthepublicdatausedfortrainingthepredictorwasoftenacquiredonahardwaresetupthatwasthestate-of-the-art6,12ormoremonthsinthepastduetothetimeneededtosolveandpublishastructure.', 'Theperformanceduringreal-lifeuseroperationiscurrentlybeingassessedasthepredic-tor has been integrated into the facility’s data analysis pipelines and will be published induecourse.', 'Proteincrystallizability', 'There have been several attempts over the years to create predictive tools for the crystal-', 'lizability of proteins.', 'One of the earlier implementations, the prediction server SECRET,', 'wasbySmialowskietal.', '[ 55]usingacombinationofsupportvectormachinesandanaïve', 'Bayes classifier.', 'The system was developed using structures determined by either X-raycrystallography or nuclear magnetic resonance (NMR) experiments.', 'The structures were', 'takenfromthePDBandadditionalinformationfromstructuralgenomicsworkfoundin', 'theTargetDB([126]; http://targetdb.pdb.org/ )wasaddedafteradjustmentforthedifferent', 'proteinsizeranges,whicharetypicallyrevealedbythetwotechniques.', 'Physico-chemicalpropertiesofaproteinthatcanbederivedfromitsprimarysequencewereusedtocreatethefeaturesfortraining.', 'Astheaimwastopredictcrystallizability,thenegativesampleswererepresented by NMR structures for which no equivalent crystal structures were availableand the positive samples were structures solved by X-ray crystallography.', 'More recently,duetohigh-throughputworkinstructuralgenomicsinitiatives,wheredatawaspublishedin publicly available databases and data mining, the creation of more sophisticated pre-d i c t i v et o o l sh a sb e e np o s s i b l e .', 'H e r e ,w el o o ka ts o m em o r er e c e n td e v e l o p m e n t si nt h efield using support vector machines, random forest and convolutional neural networkapplications.', 'TheXtalPredserver developed by Slabinski et al.', '[ 110] is one of the most popular and', 'well-known crystallizability prediction servers.', 'It is based on large-scale data analysis ofopen-sourcestructuralgenomicsresultswhichwasusedtoidentifyfeaturesthatcanhelpto distinguish crystallizable from non-crystallizable proteins [ 127].', 'Slabinski et al.', '[110]', 'used a logarithmic opinion pool [ 128] to combine probabilities for different features into', 'a single score.', 'It used physico-chemical features of proteins to place a sequence into oneof five classes of chances of crystallizability: optimal, suboptimal, average, difficult, verydifficult.', 'The next generation of XtalPred server, now XtalPred-RF ,i n v o l v e das u b s t a n -', 'tial change to the underlying algorithm [ 74].', 'Besides including new features and larger', 'training and testing sets compared to the first published version, the authors also madeuseofrecentdevelopmentsofadvancedmachinelearningandAItechniques.', 'Theauthorsexplored support vector machines (SVM), artificial neural networks (ANN) and randomforest methods.', 'For each system tried, commonly used quality metrics were reported.']\n",
            " > Processing time: 70.44732785224915\n",
            " > Real-time factor: 0.2473187527133566\n",
            " > Saving output to AUDIO_OUTPUTS/page_33.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 87\n",
            "SVMs were used in binary classification whereas for ANNs depth, complexity and inter-\n",
            "nallyappliedalgorithmsofthenetworkwereexplored.Fortherandomforest,whichwasultimatelyselected,themaximumnumberoftreeswaslimited.Inallcases,almostalltheparameterswerekeptasdefaults.Naturally,thetypeofdataavailablemeantthattheywerehighly imbalanced towards the failure cases, proteins would not crystallize, and under-sampling [ 129] was applied to reduce the number of failure cases. The authors created\n",
            "multiple random forests, each being trained on one of the classes (and hence differentlybalanced training sets) defined in the earlier implementation as multi-class classificationwasnotdirectlyaccessible.Toaddressoverfitting,theauthorsexpandedfromthedatausedtodevelopthefirstversionof XtalPred-RF andlookedatfeatureimportanceandremoved\n",
            "those that were irrelevant for the decision making. Also, sequence similarity was consid-ered,andstepsputinplacetoreduceredundancy.Allmachinelearningmethodstriedhereperformed slightly better than the original XtalPred-RF if trained and tested on the data\n",
            "to develop the first version of the predictor, with the random forest coming top. Severalof the newly introduced features boosted the predictive performance even though someof them are correlated. The random forest in particular can work with these correlations.Comparedtothefirstversionof XtalPredthenewRFversionsetupwithadditionalfeatures\n",
            "improvesaccuracyfrom68%to74%,specificityfrom72%to78%andsensitivityfrom66%to69%.Thehighervalueachievedforthespecificityindicatesthatthepredictorperformsbetterinidentifyingproteinsthatareunlikelytocrystallize.\n",
            "Mizianty and Kurgan [ 61] explored support vector machines in a system named\n",
            "PPCPred to predict the crystallizability for a given protein sequence. They used PepcDB,\n",
            "an extension of TargetDB [126], as data source, which is a further extension to the datausedforthedevelopmentof XtalPred.Ratherthanpredictinganoveralllikelihoodofcrys-\n",
            "tallizability, they give chances of success at different stages along the path from constructto crystal. After careful filtering and data assessment, three datasets of non-crystallizableproteinswerecreatedwitheachsetrepresentingadifferentstepinthecrystallizationpro-cesswherefailureoccurred,andforeachsetaseparatepredictorwastrained.Intotal,fourclass labels were annotated, three for the different stages of crystallization failure and oneforsuccess.Ateachstage,abinaryclassificationwascarriedout.Aswiththenew XtalPred\n",
            "version, the training data had more samples of the failure than the success cases. For thetraining,theauthorshadselectedsimilarfeaturesasJahandidehetal.[ 74]a n dco m b in ed\n",
            "them into a single, numerical feature vector. For each of the possible crystallization out-comesanSVMwastrainedwithfivefoldcross-validationandthepredictionsfromallfourmodels were aggregated into a final result. Three different kernels were explored: radialbasis function, polynomial (which also covers the linear kernel) and sigmoid. In total, 12models were computed and assessed; the 4 possible outcomes multiplied with 3 differentkernels selected above. Several quality metrics are used to assess performance depend-ing on what is being predicted: Matthews correlation coefficient (MCC; [ 130]), accuracy,\n",
            "sensitivity,specificityandreceiveroperatorcharacteristicsfortheareaunderthecurve.AfinaloverallaccuracyandmeanMCCwasthencalculatedafteraggregatingtheresultsfor\n",
            "the four classes. Bootstrapping and Student’s t-test were used to measure statistical sig-\n",
            "nificance.Pearsoncorrelationcoefficientandthecross-validationmethodwerecombined\n",
            "to eliminate large numbers of correlated features so that for each of the final predictors,approximately the same number of elements were in the feature vector. Of the 12 modelsavailable, the best performing for a particular class outcome had their results combined,\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 87', 'SVMs were used in binary classification whereas for ANNs depth, complexity and inter-', 'nallyappliedalgorithmsofthenetworkwereexplored.Fortherandomforest,whichwasultimatelyselected,themaximumnumberoftreeswaslimited.Inallcases,almostalltheparameterswerekeptasdefaults.Naturally,thetypeofdataavailablemeantthattheywerehighly imbalanced towards the failure cases, proteins would not crystallize, and under-sampling [ 129] was applied to reduce the number of failure cases.', 'The authors created', 'multiple random forests, each being trained on one of the classes (and hence differentlybalanced training sets) defined in the earlier implementation as multi-class classificationwasnotdirectlyaccessible.', 'Toaddressoverfitting,theauthorsexpandedfromthedatausedtodevelopthefirstversionof XtalPred-RF andlookedatfeatureimportanceandremoved', 'those that were irrelevant for the decision making.', 'Also, sequence similarity was consid-ered,andstepsputinplacetoreduceredundancy.', 'Allmachinelearningmethodstriedhereperformed slightly better than the original XtalPred-RF if trained and tested on the data', 'to develop the first version of the predictor, with the random forest coming top.', 'Severalof the newly introduced features boosted the predictive performance even though someof them are correlated.', 'The random forest in particular can work with these correlations.', 'Comparedtothefirstversionof XtalPredthenewRFversionsetupwithadditionalfeatures', 'improvesaccuracyfrom68%to74%,specificityfrom72%to78%andsensitivityfrom66%to69%.', 'Thehighervalueachievedforthespecificityindicatesthatthepredictorperformsbetterinidentifyingproteinsthatareunlikelytocrystallize.', 'Mizianty and Kurgan [ 61] explored support vector machines in a system named', 'PPCPred to predict the crystallizability for a given protein sequence.', 'They used PepcDB,', 'an extension of TargetDB [126], as data source, which is a further extension to the datausedforthedevelopmentof XtalPred.', 'Ratherthanpredictinganoveralllikelihoodofcrys-', 'tallizability, they give chances of success at different stages along the path from constructto crystal.', 'After careful filtering and data assessment, three datasets of non-crystallizableproteinswerecreatedwitheachsetrepresentingadifferentstepinthecrystallizationpro-cesswherefailureoccurred,andforeachsetaseparatepredictorwastrained.', 'Intotal,fourclass labels were annotated, three for the different stages of crystallization failure and oneforsuccess.', 'Ateachstage,abinaryclassificationwascarriedout.', 'Aswiththenew XtalPred', 'version, the training data had more samples of the failure than the success cases.', 'For thetraining,theauthorshadselectedsimilarfeaturesasJahandidehetal.', '[ 74]a n dco m b in ed', 'them into a single, numerical feature vector.', 'For each of the possible crystallization out-comesanSVMwastrainedwithfivefoldcross-validationandthepredictionsfromallfourmodels were aggregated into a final result.', 'Three different kernels were explored: radialbasis function, polynomial (which also covers the linear kernel) and sigmoid.', 'In total, 12models were computed and assessed; the 4 possible outcomes multiplied with 3 differentkernels selected above.', 'Several quality metrics are used to assess performance depend-ing on what is being predicted: Matthews correlation coefficient (MCC; [ 130]), accuracy,', 'sensitivity,specificityandreceiveroperatorcharacteristicsfortheareaunderthecurve.', 'AfinaloverallaccuracyandmeanMCCwasthencalculatedafteraggregatingtheresultsfor', 'the four classes.', 'Bootstrapping and Student’s t-test were used to measure statistical sig-', 'nificance.', 'Pearsoncorrelationcoefficientandthecross-validationmethodwerecombined', 'to eliminate large numbers of correlated features so that for each of the final predictors,approximately the same number of elements were in the feature vector.', 'Of the 12 modelsavailable, the best performing for a particular class outcome had their results combined,']\n",
            " > Processing time: 76.15710997581482\n",
            " > Real-time factor: 0.26166742006402394\n",
            " > Saving output to AUDIO_OUTPUTS/page_34.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 88 M. VOLLMAR AND G. EVANS\n",
            "wherebyateachstepaprobabilitythresholdwasapplied.Theoverallperformanceachieved\n",
            "was an improvement to existing methods but accuracy remained similar. For the predic-tionstobeabletocrystallizediffractionqualitycrystals,theachievedaccuracy,specificityand sensitivity were 77%, 85% and 61%, respectively. As with XtalPred, this means that\n",
            "the predictor performs better in identifying proteins that will fail to produce diffractionquality crystals. Considering that data was not assessed and balanced accounting for thedifferentclasses,reconsideringtheclassdistributionmaybeafirststeptofurtherimproveperformance.Thismethodistheclosestintermsofperformancetotheabove-mentionedXtalPredserver and compared to the latter’s original implementation, Mizianty and Kur-\n",
            "gan [62] showed improved prediction success. The newer version of XtalPred-RF from\n",
            "2014hasnotbeenassessed.\n",
            "Wang et al. [ 63]usedsupportvectormachinesinastackedmannerwheretheoutputof\n",
            "oneSVMistheinputtoanother.Moreimportantly,theyproducedawell-curateddatasetfromstructuralgenomicsdata,whichhasnotonlyfounduseinbenchmarkingbutenabledothergroupstodevelopnewapplicationstopredictthechanceofproteincrystallizability.Thiswasofmajorimportance,as,overtime,withnewdevelopmentsinlaboratoryequip-ment and improvements in experimental techniques previously annotated failure cases(proteins that were thought not to be crystallizable) could now be purified and crystal-lized. As a result, previously developed prediction tools became unstable and producedpoor results when presented with new samples. Looking at the implementations by othergroupsinthepast,Wangetal.[ 63]crea tedPredPPCrys (PredictionofProcedurePropen-\n",
            "sityforproteinCrystallization)tonotjustpredictthecrystallizabilityinasingleclass,butrather give chances of success for the individual steps on the way to a crystal structure.Their algorithm made use of a combination of features identified in previous work, i.e.aminoacidindices,types,compositions,physico-chemicalproperties,predictedstructuralfeaturesgeneratedfromthesequence,the PROFEAT server[131]andotherbioinformatics\n",
            "tools.Aswithmostmachinelearningprojects,asignificantamountoftimeandresourceswereusedbytheauthorstocuratethedataintermsofsamplecompleteness,ensuringnon-redundancyofthesamples,definingtheclassestobepredictedanddeterminingthemostimportantfeaturesforeachofthedifferentclasses.Usingcommonqualitymetricsashavebeen introduced above, the predictor created by Wang et al. [ 63]o u t p e r f o r m e dm a n yo f\n",
            "the past implementations on various levels. With a specificity of 76% and a sensitivity of75%thepredictorperformssimilarlywellonbothcaseswhenaproteinmaycrystallizeornot.Theaccuracyachievedis76%whichissimilartowhatwasfoundfor XtalPred-RF and\n",
            "PPCPred.\n",
            "ThemostrecentapproachtopredictproteincrystallizabilityisbyAlaviandAscher[ 75],\n",
            "DHS-Crystallize . Like the previous two methods, the focus here was to analyse a protein\n",
            "sequenceregardingphysico-chemical,sequence-basedandfunctionalfeaturestoestimatehowlikelythiswillresultinaproteincrystal.Theauthorsusedadeepcon volutionalneu-ralnetwork(CNN)toextractfeaturesfromthesequencewhichwerethencombinedwithstructural and physico-chemical features. The higher performance achieved in this sys-\n",
            "temwasattributedtotheautomaticallyidentifiedfeaturesinthesequencewhichallowed\n",
            "forabetterdescriptionoftheproblemthantraditional,hand-craftedfeatures.Thesystemexplored here was based on some previous work that solely relied on a deep CNN andtherawproteinsequence[ 84].Thepredictionsweremadeforabinaryclassificationprob-\n",
            "lem,‘doescrystallize’or‘doesnot’.Trainingwasdonewithpublicly,curateddata[ 63]and\n",
            " > Text splitted to sentences.\n",
            "['88 M. VOLLMAR AND G. EVANS', 'wherebyateachstepaprobabilitythresholdwasapplied.', 'Theoverallperformanceachieved', 'was an improvement to existing methods but accuracy remained similar.', 'For the predic-tionstobeabletocrystallizediffractionqualitycrystals,theachievedaccuracy,specificityand sensitivity were 77%, 85% and 61%, respectively.', 'As with XtalPred, this means that', 'the predictor performs better in identifying proteins that will fail to produce diffractionquality crystals.', 'Considering that data was not assessed and balanced accounting for thedifferentclasses,reconsideringtheclassdistributionmaybeafirststeptofurtherimproveperformance.', 'Thismethodistheclosestintermsofperformancetotheabove-mentionedXtalPredserver and compared to the latter’s original implementation, Mizianty and Kur-', 'gan [62] showed improved prediction success.', 'The newer version of XtalPred-RF from', '2014hasnotbeenassessed.', 'Wang et al.', '[ 63]usedsupportvectormachinesinastackedmannerwheretheoutputof', 'oneSVMistheinputtoanother.', 'Moreimportantly,theyproducedawell-curateddatasetfromstructuralgenomicsdata,whichhasnotonlyfounduseinbenchmarkingbutenabledothergroupstodevelopnewapplicationstopredictthechanceofproteincrystallizability.', 'Thiswasofmajorimportance,as,overtime,withnewdevelopmentsinlaboratoryequip-ment and improvements in experimental techniques previously annotated failure cases(proteins that were thought not to be crystallizable) could now be purified and crystal-lized.', 'As a result, previously developed prediction tools became unstable and producedpoor results when presented with new samples.', 'Looking at the implementations by othergroupsinthepast,Wangetal.', '[ 63]crea tedPredPPCrys (PredictionofProcedurePropen-', 'sityforproteinCrystallization)tonotjustpredictthecrystallizabilityinasingleclass,butrather give chances of success for the individual steps on the way to a crystal structure.', 'Their algorithm made use of a combination of features identified in previous work, i.e.aminoacidindices,types,compositions,physico-chemicalproperties,predictedstructuralfeaturesgeneratedfromthesequence,the PROFEAT server[131]andotherbioinformatics', 'tools.', 'Aswithmostmachinelearningprojects,asignificantamountoftimeandresourceswereusedbytheauthorstocuratethedataintermsofsamplecompleteness,ensuringnon-redundancyofthesamples,definingtheclassestobepredictedanddeterminingthemostimportantfeaturesforeachofthedifferentclasses.', 'Usingcommonqualitymetricsashavebeen introduced above, the predictor created by Wang et al.', '[ 63]o u t p e r f o r m e dm a n yo f', 'the past implementations on various levels.', 'With a specificity of 76% and a sensitivity of75%thepredictorperformssimilarlywellonbothcaseswhenaproteinmaycrystallizeornot.', 'Theaccuracyachievedis76%whichissimilartowhatwasfoundfor XtalPred-RF and', 'PPCPred.', 'ThemostrecentapproachtopredictproteincrystallizabilityisbyAlaviandAscher[ 75],', 'DHS-Crystallize .', 'Like the previous two methods, the focus here was to analyse a protein', 'sequenceregardingphysico-chemical,sequence-basedandfunctionalfeaturestoestimatehowlikelythiswillresultinaproteincrystal.', 'Theauthorsusedadeepcon volutionalneu-ralnetwork(CNN)toextractfeaturesfromthesequencewhichwerethencombinedwithstructural and physico-chemical features.', 'The higher performance achieved in this sys-', 'temwasattributedtotheautomaticallyidentifiedfeaturesinthesequencewhichallowed', 'forabetterdescriptionoftheproblemthantraditional,hand-craftedfeatures.', 'Thesystemexplored here was based on some previous work that solely relied on a deep CNN andtherawproteinsequence[ 84].', 'Thepredictionsweremadeforabinaryclassificationprob-', 'lem,‘doescrystallize’or‘doesnot’.', 'Trainingwasdonewithpublicly,curateddata[ 63]and']\n",
            " > Processing time: 69.48690009117126\n",
            " > Real-time factor: 0.24813213325770322\n",
            " > Saving output to AUDIO_OUTPUTS/page_35.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 89\n",
            "two independent blind test sets were used to assess performance. As with the other two\n",
            "applications above, filtering was carried out regarding sequence redundancy in the dif-ferent datasets. The authors used the same network architecture as published by Elbasiret al. [84] and extracted the features from the output of the fully connected layer. The\n",
            "final connected layer produced 250 features and the results for 10 repeats were concate-nated. 21 physico-chemical features were derived from the protein sequence using theProtlearn software [ 132]. The actual binary classification was then conducted using the\n",
            "XGBoost algorithm [ 133], an ensemble decision tree method. To improve the predictors\n",
            "performance, dimensionality reduction was carried out using PCA to reduce the featurevector to a quarter of its initial size when coming from feature extraction. The data usedin training were balanced by applying appropriate weights. The XGBoost based predic-tor achieved better performance on the test set as well as the two blind sets compared toall the other methods included in this study as judged by commonly used quality met-rics. The results presented by Alavi and Ascher [ 75] give an initial idea of the design\n",
            "of a future system. There were several elements in connection with the dimensionalityreduction step and the XGBoost training where more thorough exploration of parame-t e r sm a yr e s u l ti ne v e nb e t t e rp e r f o r m a n c e .T h i sh a sa l r e a d yb e e nm e n t i o n e db yA l a v iand Ascher [ 75]. Depending on the test set this CNN-based predictor reaches accura-\n",
            "ciesbetween77%and89%withprecisionbetween81%and96%andrecallbetween67%and92%.Noinformationisgivenaboutperformanceonnegativeclasssamplesinformofspecificity.Thisisasignificantimprovementinaccuracyby ∼10%comparedtoprevious\n",
            "methods.\n",
            "Crystallizationoutcomeclassification\n",
            "Large screening campaigns to identify and optimize conditions for novel structures as\n",
            "wellasproducinglargenumbersofcrystalsofconsistentqualityforsmallmoleculesoak-ing experiments are routinely done in structural genomics and pharmaceutical settings.Automated crystallization platforms are often used to facilitate such high throughputexperiments by ensuring safe and managed storage with automated imaging of the crys-tallization trials at set time points. If the crystallization conditions for a target are wellestablished, then results can usually be expected within a few days and crystals can beidentified easily due to their size. However, when screening a large number of conditionsto crystallize a novel protein, it can be challenging to manually assess the large numberof images produced. To support crystallographers in this assessment and to make mostoutofautomatedcrystallizationplatforms,imagerecognitiontechniqueshavebeendevel-opedas,forexample,bytheMAchineRecognitionofCrystallizationOutcomesinitiative(MARCO;[ 20]).\n",
            "ForMARCOthetrainingimageswerecollectedondifferentimagingsystemsatahand\n",
            "full of high-throughput labs. Each system had a specific setup defined by the group sothatthetotalnumberofimageswasreasonablydiverse.Theimageswerelabelledbyusing\n",
            "existingscoringsystemsandcollapsingthemtoacommonsetoffourlabels.Byusingthe\n",
            "existing scores, a mix of people were involved in the labelling, reducing the likelihood ofbias towards one person’s scoring strategy. After adjusting some of the labels from initialtrialsandcleaningthedata,adeepconvolutionalneuralnetworkwascreatedbasedontheworkofLeCunetal.[ 134,135]andRawatandWang[ 136].Somepartsofthearchitecture\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 89', 'two independent blind test sets were used to assess performance.', 'As with the other two', 'applications above, filtering was carried out regarding sequence redundancy in the dif-ferent datasets.', 'The authors used the same network architecture as published by Elbasiret al.', '[84] and extracted the features from the output of the fully connected layer.', 'The', 'final connected layer produced 250 features and the results for 10 repeats were concate-nated.', '21 physico-chemical features were derived from the protein sequence using theProtlearn software [ 132].', 'The actual binary classification was then conducted using the', 'XGBoost algorithm [ 133], an ensemble decision tree method.', 'To improve the predictors', 'performance, dimensionality reduction was carried out using PCA to reduce the featurevector to a quarter of its initial size when coming from feature extraction.', 'The data usedin training were balanced by applying appropriate weights.', 'The XGBoost based predic-tor achieved better performance on the test set as well as the two blind sets compared toall the other methods included in this study as judged by commonly used quality met-rics.', 'The results presented by Alavi and Ascher [ 75] give an initial idea of the design', 'of a future system.', 'There were several elements in connection with the dimensionalityreduction step and the XGBoost training where more thorough exploration of parame-t e r sm a yr e s u l ti ne v e nb e t t e rp e r f o r m a n c e .', 'T h i sh a sa l r e a d yb e e nm e n t i o n e db yA l a v iand Ascher [ 75].', 'Depending on the test set this CNN-based predictor reaches accura-', 'ciesbetween77%and89%withprecisionbetween81%and96%andrecallbetween67%and92%.', 'Noinformationisgivenaboutperformanceonnegativeclasssamplesinformofspecificity.', 'Thisisasignificantimprovementinaccuracyby ∼10%comparedtoprevious', 'methods.', 'Crystallizationoutcomeclassification', 'Large screening campaigns to identify and optimize conditions for novel structures as', 'wellasproducinglargenumbersofcrystalsofconsistentqualityforsmallmoleculesoak-ing experiments are routinely done in structural genomics and pharmaceutical settings.', 'Automated crystallization platforms are often used to facilitate such high throughputexperiments by ensuring safe and managed storage with automated imaging of the crys-tallization trials at set time points.', 'If the crystallization conditions for a target are wellestablished, then results can usually be expected within a few days and crystals can beidentified easily due to their size.', 'However, when screening a large number of conditionsto crystallize a novel protein, it can be challenging to manually assess the large numberof images produced.', 'To support crystallographers in this assessment and to make mostoutofautomatedcrystallizationplatforms,imagerecognitiontechniqueshavebeendevel-opedas,forexample,bytheMAchineRecognitionofCrystallizationOutcomesinitiative(MARCO;[ 20]).', 'ForMARCOthetrainingimageswerecollectedondifferentimagingsystemsatahand', 'full of high-throughput labs.', 'Each system had a specific setup defined by the group sothatthetotalnumberofimageswasreasonablydiverse.', 'Theimageswerelabelledbyusing', 'existingscoringsystemsandcollapsingthemtoacommonsetoffourlabels.', 'Byusingthe', 'existing scores, a mix of people were involved in the labelling, reducing the likelihood ofbias towards one person’s scoring strategy.', 'After adjusting some of the labels from initialtrialsandcleaningthedata,adeepconvolutionalneuralnetworkwascreatedbasedontheworkofLeCunetal.', '[ 134,135]andRawatandWang[ 136].', 'Somepartsofthearchitecture']\n",
            " > Processing time: 68.26541304588318\n",
            " > Real-time factor: 0.24342780052903504\n",
            " > Saving output to AUDIO_OUTPUTS/page_36.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 90 M. VOLLMAR AND G. EVANS\n",
            "were directly taken from Szegedy et al. [ 137]. After adjusting some parts of the network\n",
            "architecturetosuittheproblemandtrainingwiththelargeanddiverseimageset,theclas-\n",
            "sifiers performance surpassed that of already available academic or commercial systemswhentestedbythesamegroupsthatcontributedimages.Thetrainedmodelisopen-sourceand can be run locally by any other laboratory. Performance for these new users with anew, specific local setup may however be lower compared to that reported, presumablyduetothepresenceofspecificcharacteristicsofthelocalsetup.Suchadifferencehasbeenobserv edwhilethesystemhasbeeninusea tthecrystalliza tio nfacili tya tDia mo ndLigh tSourceandamorepersonalizedsystemhasbeendevelopedtobeimplementedatbeamlineVMX-itoaddressdeficiencies(OllyKing,personalcommunication).\n",
            "Before the success of convolutional neural networks and the striking developments in\n",
            "computer vision, earlier algorithms explored other areas of machine learning. Cumbaaetal.[48]andCumbaaandJurasica[ 49]aswellasSaitohetal.[ 50]usedlineardiscriminant\n",
            "analysis to identify which images of drops in crystallization trials contained actual pro-teincrystals.Inthesethreeapproaches,themanuallylabelledimagesprovidedthegroundtruthandthetrainedalgorithmswereusuallyabletocorrectlyidentifyaround85%oftheseimages.DecisiontreeswereexploredbyBernetal.[ 72]andLiuetal.[ 73].Thelatteradds\n",
            "a boosting algorithm to the basic decision tree which allows the usage of more and oftenmarginalfeaturesinthedecisionprocess.BuchalaandWilson[ 51]presentALICE (AnaL-\n",
            "ysis of Images from Crystallization Experiments) which is based on previous work andcombinesanobject-basedmethod[ 138]withwavelettransforms[ 139]withthefinalclas-\n",
            "sificationbeingcarriedoutbyanSVM.Panetal.[ 64]usedanSVM-basedsystemtoclassify\n",
            "imagesofcrystallizationtrials.Althoughtakingsomeoftheworkloadofacrystallographer,theirsystemwasplaguedbyahighfalse-positiverateofnearly40%.Aself-organisingnetwas proposed by Spraggon et al. [ 85] and implemented in their package CEEP(Crystal\n",
            "Experiment Evaluation Program). The algorithms presented here substantially reduce acrystallographer’stimespentexaminingandscoringcrystallizationtrials.Themainprob-lemforthesetechniquesremainsthattheyareveryspecifictothelocalexperimentalsetupthatwasusedtoproducethecrystallizationtrialsandacquirethedropimages.Evenwithastate-of-the-artalgorithmusedinMARCO[20 ]thisproblempersists.\n",
            "Proteinmodelling\n",
            "A very detailed review of the state-of-the-art developments and some historic lead-up\n",
            "regarding protein modelling is given by Gao et al. [ 140]. They focus on deep learning\n",
            "applications that have begun to replace the more traditional statistics and machine learn-ingmethods,coveringstructureprediction,proteinfolding,proteindesignandmoleculardynamicssimulation.\n",
            "Untilafewyearsago,moststructurepredictionimplementationswouldfocusonstruc-\n",
            "tureoptimizationmethodscombinedwithsomeinformationfromevolutionarycouplinganalysis (ECD [ 141]). The assumption here was that two amino acid residues that are in\n",
            "closecontactwitheachotherinathree-dimensionalproteinstructurewillco-evolvesoasnottodisrupttheintegrityoftheproteinstructure. RaptorX[88] and its implementation\n",
            "of deep learning to predict the distances between pairs of residues was a major break-through,outperformedECDandpavedthewayformoresophisticatedapplicationsaswillbediscussedbelow.\n",
            " > Text splitted to sentences.\n",
            "['90 M. VOLLMAR AND G. EVANS', 'were directly taken from Szegedy et al.', '[ 137].', 'After adjusting some parts of the network', 'architecturetosuittheproblemandtrainingwiththelargeanddiverseimageset,theclas-', 'sifiers performance surpassed that of already available academic or commercial systemswhentestedbythesamegroupsthatcontributedimages.', 'Thetrainedmodelisopen-sourceand can be run locally by any other laboratory.', 'Performance for these new users with anew, specific local setup may however be lower compared to that reported, presumablyduetothepresenceofspecificcharacteristicsofthelocalsetup.', 'Suchadifferencehasbeenobserv edwhilethesystemhasbeeninusea tthecrystalliza tio nfacili tya tDia mo ndLigh tSourceandamorepersonalizedsystemhasbeendevelopedtobeimplementedatbeamlineVMX-itoaddressdeficiencies(OllyKing,personalcommunication).', 'Before the success of convolutional neural networks and the striking developments in', 'computer vision, earlier algorithms explored other areas of machine learning.', 'Cumbaaetal.', '[48]andCumbaaandJurasica[ 49]aswellasSaitohetal.', '[ 50]usedlineardiscriminant', 'analysis to identify which images of drops in crystallization trials contained actual pro-teincrystals.Inthesethreeapproaches,themanuallylabelledimagesprovidedthegroundtruthandthetrainedalgorithmswereusuallyabletocorrectlyidentifyaround85%oftheseimages.DecisiontreeswereexploredbyBernetal.', '[ 72]andLiuetal.', '[ 73].', 'Thelatteradds', 'a boosting algorithm to the basic decision tree which allows the usage of more and oftenmarginalfeaturesinthedecisionprocess.', 'BuchalaandWilson[ 51]presentALICE (AnaL-', 'ysis of Images from Crystallization Experiments) which is based on previous work andcombinesanobject-basedmethod[ 138]withwavelettransforms[ 139]withthefinalclas-', 'sificationbeingcarriedoutbyanSVM.Panetal.', '[ 64]usedanSVM-basedsystemtoclassify', 'imagesofcrystallizationtrials.Althoughtakingsomeoftheworkloadofacrystallographer,theirsystemwasplaguedbyahighfalse-positiverateofnearly40%.', 'Aself-organisingnetwas proposed by Spraggon et al.', '[ 85] and implemented in their package CEEP(Crystal', 'Experiment Evaluation Program).', 'The algorithms presented here substantially reduce acrystallographer’stimespentexaminingandscoringcrystallizationtrials.', 'Themainprob-lemforthesetechniquesremainsthattheyareveryspecifictothelocalexperimentalsetupthatwasusedtoproducethecrystallizationtrialsandacquirethedropimages.', 'Evenwithastate-of-the-artalgorithmusedinMARCO[20 ]thisproblempersists.', 'Proteinmodelling', 'A very detailed review of the state-of-the-art developments and some historic lead-up', 'regarding protein modelling is given by Gao et al.', '[ 140].', 'They focus on deep learning', 'applications that have begun to replace the more traditional statistics and machine learn-ingmethods,coveringstructureprediction,proteinfolding,proteindesignandmoleculardynamicssimulation.', 'Untilafewyearsago,moststructurepredictionimplementationswouldfocusonstruc-', 'tureoptimizationmethodscombinedwithsomeinformationfromevolutionarycouplinganalysis (ECD [ 141]).', 'The assumption here was that two amino acid residues that are in', 'closecontactwitheachotherinathree-dimensionalproteinstructurewillco-evolvesoasnottodisrupttheintegrityoftheproteinstructure.', 'RaptorX[88] and its implementation', 'of deep learning to predict the distances between pairs of residues was a major break-through,outperformedECDandpavedthewayformoresophisticatedapplicationsaswillbediscussedbelow.']\n",
            " > Processing time: 64.368812084198\n",
            " > Real-time factor: 0.24586957974227777\n",
            " > Saving output to AUDIO_OUTPUTS/page_37.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 91\n",
            "In[140]theauthorsdiscussdifferentdeeplearningandneuralnetworktypesthathave\n",
            "proven successful in scientific modelling competitions. They give details about the inner\n",
            "structuresofthesedeeplearningalgorithmsandexplainwhatproceduresandalgorithmshavebeenusedtoturnaprotein’saminoacidsequenceandadditionalproteindescriptorsintoa3Dstructure.Alearnablefeaturevectoriscreated,oftenusingalgorithmsfromnat-ural language processing such as Word2Vec or Doc2Vec as was first proposed by Asgariand Mofrad [ 142]. The summary provided by Gao et al. [ 140] gives a good foundation\n",
            "to understand why, when combining all the advances and developments the algorithm,AlphaFold [89,90]designedbyGoogle’sDeepMind,wassosuccessful.\n",
            "As the result of the unprecedented advances in the field of protein structure predic-\n",
            "tion thanks to the work of the developers of RaptorX[88]a n dAlphaFold ([89,90]; see\n",
            "below) another implementation was developed. Yang et al. [ 91]c r e a t e dan e wm e t h o d ,\n",
            "transform-restrained Rosetta ( trRosetta), that incorporated the ideas of [88 ]a n d[89,90]\n",
            "and generally deep residual convolutional neural networks ideas as were used by all thetop-scoring groups during the 13th Critical Assessment of Protein Structure Prediction(CASP13;[ 143])competitionin2018.Thesedeeplearningalgorithmswerecombinedwith\n",
            "coevolutionarycouplingfeaturesdeterminedfrommultiplesequencealignments(MSAs)with the output being distances that yield structures through direct optimization. Yangetal.[91]extendedtheCASP13ideasbychoosingaRosetta-basedoptimizationthatuses\n",
            "a Rosetta energy function, which allows for direct model building. A total of six geomet-ric descriptors were predicted to describe the relative orientation of two residues to each\n",
            "other. Similar to [ 89,90]thepredicteddistancesandorientationsareusedinaminimiza-\n",
            "tionfunctionandtheirprobabilitydistributionsareconvertedintointerresidueinteraction\n",
            "potentials. Those potentials were then combined with a Rosetta centroid level (coarse-grain)energyfunction[ 144]tocreatefoldedstructures.Themodelwiththelowestenergy\n",
            "andthemostbuiltatomswasselectedasfinalmodel.\n",
            "AlphaFold developedbyGoogle’sDeepMind[ 89,90]isastructurepredictiontoolthat\n",
            "uses deep learning and was one of the contestants during CASP13 competition. Theytrainedacomplex2Ddilatedconvolutionalr esidualnetwork(whosebasicdesigniscom-\n",
            "monin2Dimagerecognition)topredictdistancesbetweenpairsofaminoacidsbasedonaprotein’ssequence.Backbonetorsionanglesandrelativeaccessiblesurfaceforeachresiduewerealsopredicted.Furthermore,basedonthecomplexityofthenetworkthesecondarystructure was predicted based on DSSP [ 145] class labels. The data used to predict the\n",
            "pairwise distances were extracted from PDB structures after creating MSAs which pro-duced several hundred features. The structures were curated to ensure non-redundancyand during train-test splitting homologous structures were kept together in the same set.So rather than using fragments of homologous structures for training a predictor, whichis usually done and often combined with contact predictions, they used the smaller-sizeddistance pairs. To avoid overfitting, MSAs were subjected to data augmentation by sub-samplingthealignmentsandthePDBcoordinateshadnoiseadded,resultinginvariationin the target distances, which, combined, increased the number of training samples. In\n",
            "a second step, the derived covariance information was used in a gradient descent proce-\n",
            "dure,usingL-BFGS[146 ]asminimizer,tocreateapotentialofmeanforcewhichwasthen\n",
            "used to describe the shape of a protein. At the time of the CASP13 competition Deep-Mind’sAlphaFold considerably advanced the entire structure prediction field for certain\n",
            "challenges,thefreemodellingclassinparticular.Inthefreemodellingclass,theprovided\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 91', 'In[140]theauthorsdiscussdifferentdeeplearningandneuralnetworktypesthathave', 'proven successful in scientific modelling competitions.', 'They give details about the inner', 'structuresofthesedeeplearningalgorithmsandexplainwhatproceduresandalgorithmshavebeenusedtoturnaprotein’saminoacidsequenceandadditionalproteindescriptorsintoa3Dstructure.', 'Alearnablefeaturevectoriscreated,oftenusingalgorithmsfromnat-ural language processing such as Word2Vec or Doc2Vec as was first proposed by Asgariand Mofrad [ 142].', 'The summary provided by Gao et al.', '[ 140] gives a good foundation', 'to understand why, when combining all the advances and developments the algorithm,AlphaFold [89,90]designedbyGoogle’sDeepMind,wassosuccessful.', 'As the result of the unprecedented advances in the field of protein structure predic-', 'tion thanks to the work of the developers of RaptorX[88]a n dAlphaFold ([89,90]; see', 'below) another implementation was developed.', 'Yang et al.', '[ 91]c r e a t e dan e wm e t h o d ,', 'transform-restrained Rosetta ( trRosetta), that incorporated the ideas of [88 ]a n d[89,90]', 'and generally deep residual convolutional neural networks ideas as were used by all thetop-scoring groups during the 13th Critical Assessment of Protein Structure Prediction(CASP13;[ 143])competitionin2018.', 'Thesedeeplearningalgorithmswerecombinedwith', 'coevolutionarycouplingfeaturesdeterminedfrommultiplesequencealignments(MSAs)with the output being distances that yield structures through direct optimization.', 'Yangetal.', '[91]extendedtheCASP13ideasbychoosingaRosetta-basedoptimizationthatuses', 'a Rosetta energy function, which allows for direct model building.', 'A total of six geomet-ric descriptors were predicted to describe the relative orientation of two residues to each', 'other.', 'Similar to [ 89,90]thepredicteddistancesandorientationsareusedinaminimiza-', 'tionfunctionandtheirprobabilitydistributionsareconvertedintointerresidueinteraction', 'potentials.', 'Those potentials were then combined with a Rosetta centroid level (coarse-grain)energyfunction[ 144]tocreatefoldedstructures.', 'Themodelwiththelowestenergy', 'andthemostbuiltatomswasselectedasfinalmodel.', 'AlphaFold developedbyGoogle’sDeepMind[ 89,90]isastructurepredictiontoolthat', 'uses deep learning and was one of the contestants during CASP13 competition.', 'Theytrainedacomplex2Ddilatedconvolutionalr esidualnetwork(whosebasicdesigniscom-', 'monin2Dimagerecognition)topredictdistancesbetweenpairsofaminoacidsbasedonaprotein’ssequence.Backbonetorsionanglesandrelativeaccessiblesurfaceforeachresiduewerealsopredicted.Furthermore,basedonthecomplexityofthenetworkthesecondarystructure was predicted based on DSSP [ 145] class labels.', 'The data used to predict the', 'pairwise distances were extracted from PDB structures after creating MSAs which pro-duced several hundred features.', 'The structures were curated to ensure non-redundancyand during train-test splitting homologous structures were kept together in the same set.', 'So rather than using fragments of homologous structures for training a predictor, whichis usually done and often combined with contact predictions, they used the smaller-sizeddistance pairs.', 'To avoid overfitting, MSAs were subjected to data augmentation by sub-samplingthealignmentsandthePDBcoordinateshadnoiseadded,resultinginvariationin the target distances, which, combined, increased the number of training samples.', 'In', 'a second step, the derived covariance information was used in a gradient descent proce-', 'dure,usingL-BFGS[146 ]asminimizer,tocreateapotentialofmeanforcewhichwasthen', 'used to describe the shape of a protein.', 'At the time of the CASP13 competition Deep-Mind’sAlphaFold considerably advanced the entire structure prediction field for certain', 'challenges,thefreemodellingclassinparticular.', 'Inthefreemodellingclass,theprovided']\n",
            " > Processing time: 71.76190400123596\n",
            " > Real-time factor: 0.23984709919337843\n",
            " > Saving output to AUDIO_OUTPUTS/page_38.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 92 M. VOLLMAR AND G. EVANS\n",
            "targets were novel structures for which no known homologous structures were available.\n",
            "AlphaFold achieved the highest scores for correctly modelling the backbone trace and\n",
            "hence overall fold of given target proteins. Side chains were a different matter. Using thesamealgorithm,whichdidnotincludestructuralinformationfromhomologuesfortrain-ing, the group still achieved high scores in the template-based modelling category. Theentiresystemrelied,however,ontheaccuracyofthedistancepredictions.\n",
            "In2020thenextiteration,CASP14( https://predictioncenter.org/casp14/index.cgi),saw\n",
            "DeepMindenteringagain,thistimewithanentirelynewsystem, AlphaFold2 [111].Look-\n",
            "ing at the few details provided in the competition’s book of abstracts and including somegeneralknowledgeandideasinthefieldofAIwetrytoidentifysomecomponentsofthisnewsystem.Theinputdatafor AlphaFold2 wereMSAsandtemplatehitsfromhomologues,\n",
            "andtheoutputisafullyfoldedproteinstructureasdefinedbydistances,torsionanglesandatomic coordinates. The predictor itself was based upon convolutional neural networks(CNN) which capture local details either between pixels in 2D or voxels in 3D. CNNs,however, lack more global information, i.e. long-distance interactions in a 3D structure,which need to be found from a 1D sequence. This challenge is close to natural languageprocessing,whereasystemneedstobeabletokeeptrackofwhathappenedatthebegin-ning and middle of a sentence or paragraph (or a particular amino acid in a sequence)and how this affects the end or conclusion (or the position and interaction in a 3D struc-ture). A likely choice of CNN here would be an attention-based network [ 147]t h a ta i m s\n",
            "to find the most important connections between pairs of residues and between a residue\n",
            "and the corresponding one in other sequences in the MSA. At the same time, the predic-\n",
            "tor ignores irrelevant sequences in the alignment and identifies long-range interactionsbetweenaminoacidpairsinamoreholisticwaythanthroughco-evolutionbetweenpairsof amino acids in a structure. A transformer, probably similar to generative pre-trainedtransformer-3 (GPT-3; [ 148]) or deep bidirectional encoder transformer (BERT; [ 149]),\n",
            "could be used to update the protein backbone and build side chains. The entire pro-cess of looking at an MSA, working out connections and building a model was iterativeand repeated a few times over to reach convergence. Finally, the structure was then ‘set-tled’ using coordinate-restrained gradient descent with an Amber force field [ 150]. The\n",
            "predictor did extremely well in the competition in all categories, coming top in the free-modelling competition and achieving high scores in the template-based challenges. Outof a total 43 targets, Alphafold2 achieved the highest score for 37 of these. For five sam-\n",
            "ples the results were comparable to the other groups and for one the submitted structurewasworse.Itshouldbenotedhowever,thatthesystemwasverygoodinpredictingapro-tein crystal structure given a MSA, considering that the training data and features stemf r o mt h eP D B .T h eP D Bi sl a r g e l yd o m i n a t e db yp r o t e i ns t r u c t u r e ss o l v e df r o mp r o t e i ncrystals using X-ray diffraction and is biased towards proteins that are crystallizable, i.e.thereisasparserepresentationofstructuresbeingsolvedbynuclearmagneticresonanceorcryo-electron microscopy experiments, membrane proteins, intrinsically disordered pro-teins and large multi-protein complexes. This may explain why it is challenging, even\n",
            "forAlphaFold2, to correctly predict solvent-exposed protein surfaces and loop regions.\n",
            "S u c ha r e a sa r ek n o w nt ob eh i g h l yfl e x i b l ew h i c hi nt u r nh a m p e r st h e mf r o mf o r m i n g\n",
            "c l o s ea n ds t a b i l i z i n gc o n t a c t st oc r e a t ea no r d e r e dc ry s t a ll a t t i c e .Ah i g hl e v e lo fo r d e ri na protein crystal is necessary to get sufficient resolution to build a correct 3D structure.Unfortunately, highly flexible areas are often the most interesting ones from a biological\n",
            " > Text splitted to sentences.\n",
            "['92 M. VOLLMAR AND G. EVANS', 'targets were novel structures for which no known homologous structures were available.', 'AlphaFold achieved the highest scores for correctly modelling the backbone trace and', 'hence overall fold of given target proteins.', 'Side chains were a different matter.', 'Using thesamealgorithm,whichdidnotincludestructuralinformationfromhomologuesfortrain-ing, the group still achieved high scores in the template-based modelling category.', 'Theentiresystemrelied,however,ontheaccuracyofthedistancepredictions.', 'In2020thenextiteration,CASP14( https://predictioncenter.org/casp14/index.cgi),saw', 'DeepMindenteringagain,thistimewithanentirelynewsystem, AlphaFold2 [111].', 'Look-', 'ing at the few details provided in the competition’s book of abstracts and including somegeneralknowledgeandideasinthefieldofAIwetrytoidentifysomecomponentsofthisnewsystem.', 'Theinputdatafor AlphaFold2 wereMSAsandtemplatehitsfromhomologues,', 'andtheoutputisafullyfoldedproteinstructureasdefinedbydistances,torsionanglesandatomic coordinates.', 'The predictor itself was based upon convolutional neural networks(CNN) which capture local details either between pixels in 2D or voxels in 3D.', 'CNNs,however, lack more global information, i.e. long-distance interactions in a 3D structure,which need to be found from a 1D sequence.', 'This challenge is close to natural languageprocessing,whereasystemneedstobeabletokeeptrackofwhathappenedatthebegin-ning and middle of a sentence or paragraph (or a particular amino acid in a sequence)and how this affects the end or conclusion (or the position and interaction in a 3D struc-ture).', 'A likely choice of CNN here would be an attention-based network [ 147]t h a ta i m s', 'to find the most important connections between pairs of residues and between a residue', 'and the corresponding one in other sequences in the MSA.', 'At the same time, the predic-', 'tor ignores irrelevant sequences in the alignment and identifies long-range interactionsbetweenaminoacidpairsinamoreholisticwaythanthroughco-evolutionbetweenpairsof amino acids in a structure.', 'A transformer, probably similar to generative pre-trainedtransformer-3 (GPT-3; [ 148]) or deep bidirectional encoder transformer (BERT; [ 149]),', 'could be used to update the protein backbone and build side chains.', 'The entire pro-cess of looking at an MSA, working out connections and building a model was iterativeand repeated a few times over to reach convergence.', 'Finally, the structure was then ‘set-tled’ using coordinate-restrained gradient descent with an Amber force field [ 150].', 'The', 'predictor did extremely well in the competition in all categories, coming top in the free-modelling competition and achieving high scores in the template-based challenges.', 'Outof a total 43 targets, Alphafold2 achieved the highest score for 37 of these.', 'For five sam-', 'ples the results were comparable to the other groups and for one the submitted structurewasworse.', 'Itshouldbenotedhowever,thatthesystemwasverygoodinpredictingapro-tein crystal structure given a MSA, considering that the training data and features stemf r o mt h eP D B .', 'T h eP D Bi sl a r g e l yd o m i n a t e db yp r o t e i ns t r u c t u r e ss o l v e df r o mp r o t e i ncrystals using X-ray diffraction and is biased towards proteins that are crystallizable, i.e.thereisasparserepresentationofstructuresbeingsolvedbynuclearmagneticresonanceorcryo-electron microscopy experiments, membrane proteins, intrinsically disordered pro-teins and large multi-protein complexes.', 'This may explain why it is challenging, even', 'forAlphaFold2, to correctly predict solvent-exposed protein surfaces and loop regions.', 'S u c ha r e a sa r ek n o w nt ob eh i g h l yfl e x i b l ew h i c hi nt u r nh a m p e r st h e mf r o mf o r m i n g', 'c l o s ea n ds t a b i l i z i n gc o n t a c t st oc r e a t ea no r d e r e dc ry s t a ll a t t i c e .', 'Ah i g hl e v e lo fo r d e ri na protein crystal is necessary to get sufficient resolution to build a correct 3D structure.', 'Unfortunately, highly flexible areas are often the most interesting ones from a biological']\n",
            " > Processing time: 81.90435886383057\n",
            " > Real-time factor: 0.2520115129008571\n",
            " > Saving output to AUDIO_OUTPUTS/page_39.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 93\n",
            "function perspective and since structure and function are inextricably linked, the ability\n",
            "to correctly predict these more disordered structural regions, or indeed the nature of thedisorder,willbecriticaltofurtheringourunderstandingofbiologicalfunction.Thesameappliestoprotein–proteininteractions,whichagaincaninvolvehighlyflexible/mobilesur-face features of a protein (or to continue with natural language processing to keep trackof different characters in a story and how they affect each other over several chapters orevenbooksinaseries).Anotherchallengeistheplacementofligandmoleculesinbiolog-icallyandchemicallyrelevantinteractionstoboostdrugdesignortopredictfunctionandm e c h a n i s ma n di d e n t i f yt h ea c t i v es i t eo fn o v e lf o l d sf o rw h i c ha n yt y p eo fe x p e r i m e n t a ldata is sparse. So, in all, AlphaFold2 is remarkably good at predicting protein struc-\n",
            "tures with a strong bias towards well-ordered structures like those largely represented inthePDB.\n",
            "Based on the published details for AlphaFold andAlphaFold2 and general domain-\n",
            "specific knowledge, the Baker lab has recently published a new structure predictionalgorithm, RoseTTAFold [112].Baeketal.[ 112]exploreamodelwiththreeparalleltracks,\n",
            "where1Dinformationfromtheaminoacidsequenceiscombinedwith2Ddistancemapsand 3D atomic coordinates. The three tracks talk to each other and exchange informa-tion to optimize weights and parameters in the system. To assess the performance of thesystem blindly, the server has been taking part in the CAMEO experiment [ 151]a n d\n",
            "s of a ro u t p e r f o r m e da l lo t h e rp a r t i c i p a n t s .T h ea l g o r i t h ma i m st oc l o s et h eg a p ,s e e ni nCASP14,thatexistsbetweenacademicgroupsandDeepMindandmakethemethodavail-\n",
            "able to the scientific community. Unfortunately, DeepMind is a private company with\n",
            "proprietary interests and the AlphaFold/2 developers may not be able to fully disclose all\n",
            "details.\n",
            "Concluding remarks\n",
            "I nt h i sr e v i e w ,w eh a v ep r o v i d e dab r i e fh i s t o r ya n dg i v e nat a r g e t e di n t r o d u c t i o nt osome of the key concepts, ideas and algorithms used in machine learning and artificialintelligence and reviewed some of their applications in the field of macromolecular crys-tallography. Machine learning and artificial intelligence are fast-moving areas of researchgreatly accelerated by new developments in computing technology and the availability ofopen-source software: keeping pace with new and emerging methods is in itself a chal-lenge. This review is intended to deliver some starting points for anyone in the structuralb i o l o g yfi e l dw h om i g h tb ec u r i o u sa b o u ta p p l y i n gm a c h i n el e a r n i n gt ot h e i rp r o b l e m .As many research institutions support open data, there are numerous publicly accessi-ble scientific databases and experimental data are produced at an ever-increasing rate.As such, data science, machine learning and artificial intelligence will be new tools thatcan be employed to explore interesting questions from different points of view. The var-ious sources of information can be combined into highly complex sets of data that don o tn e c e s s a r i l ys e r v eas i n g l ep u r p o s e ,f o re x a m p l e ,t os o l e l ys o l v eas t r u c t u r e ,b u tm o r e\n",
            "broadly describe a biological problem. For example, information from a multi-sequence\n",
            "alignmentcanbecombinedwithcontactpredictions,secondarystructurepredictions,pro-teinfolding,functionalannotationsbasedonphylogenyandatomiccoordinateswiththeirpositional deviations from an ensemble of homologues. A combination of such data maybe used to deduct a protein’s structure and function if crystallization has not succeeded,\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 93', 'function perspective and since structure and function are inextricably linked, the ability', 'to correctly predict these more disordered structural regions, or indeed the nature of thedisorder,willbecriticaltofurtheringourunderstandingofbiologicalfunction.', 'Thesameappliestoprotein–proteininteractions,whichagaincaninvolvehighlyflexible/mobilesur-face features of a protein (or to continue with natural language processing to keep trackof different characters in a story and how they affect each other over several chapters orevenbooksinaseries).', 'Anotherchallengeistheplacementofligandmoleculesinbiolog-icallyandchemicallyrelevantinteractionstoboostdrugdesignortopredictfunctionandm e c h a n i s ma n di d e n t i f yt h ea c t i v es i t eo fn o v e lf o l d sf o rw h i c ha n yt y p eo fe x p e r i m e n t a ldata is sparse.', 'So, in all, AlphaFold2 is remarkably good at predicting protein struc-', 'tures with a strong bias towards well-ordered structures like those largely represented inthePDB.', 'Based on the published details for AlphaFold andAlphaFold2 and general domain-', 'specific knowledge, the Baker lab has recently published a new structure predictionalgorithm, RoseTTAFold [112].', 'Baeketal.', '[ 112]exploreamodelwiththreeparalleltracks,', 'where1Dinformationfromtheaminoacidsequenceiscombinedwith2Ddistancemapsand 3D atomic coordinates.', 'The three tracks talk to each other and exchange informa-tion to optimize weights and parameters in the system.', 'To assess the performance of thesystem blindly, the server has been taking part in the CAMEO experiment [ 151]a n d', 's of a ro u t p e r f o r m e da l lo t h e rp a r t i c i p a n t s .', 'T h ea l g o r i t h ma i m st oc l o s et h eg a p ,s e e ni nCASP14,thatexistsbetweenacademicgroupsandDeepMindandmakethemethodavail-', 'able to the scientific community.', 'Unfortunately, DeepMind is a private company with', 'proprietary interests and the AlphaFold/2 developers may not be able to fully disclose all', 'details.', 'Concluding remarks', 'I nt h i sr e v i e w ,w eh a v ep r o v i d e dab r i e fh i s t o r ya n dg i v e nat a r g e t e di n t r o d u c t i o nt osome of the key concepts, ideas and algorithms used in machine learning and artificialintelligence and reviewed some of their applications in the field of macromolecular crys-tallography.', 'Machine learning and artificial intelligence are fast-moving areas of researchgreatly accelerated by new developments in computing technology and the availability ofopen-source software: keeping pace with new and emerging methods is in itself a chal-lenge.', 'This review is intended to deliver some starting points for anyone in the structuralb i o l o g yfi e l dw h om i g h tb ec u r i o u sa b o u ta p p l y i n gm a c h i n el e a r n i n gt ot h e i rp r o b l e m .', 'As many research institutions support open data, there are numerous publicly accessi-ble scientific databases and experimental data are produced at an ever-increasing rate.', 'As such, data science, machine learning and artificial intelligence will be new tools thatcan be employed to explore interesting questions from different points of view.', 'The var-ious sources of information can be combined into highly complex sets of data that don o tn e c e s s a r i l ys e r v eas i n g l ep u r p o s e ,f o re x a m p l e ,t os o l e l ys o l v eas t r u c t u r e ,b u tm o r e', 'broadly describe a biological problem.', 'For example, information from a multi-sequence', 'alignmentcanbecombinedwithcontactpredictions,secondarystructurepredictions,pro-teinfolding,functionalannotationsbasedonphylogenyandatomiccoordinateswiththeirpositional deviations from an ensemble of homologues.', 'A combination of such data maybe used to deduct a protein’s structure and function if crystallization has not succeeded,']\n",
            " > Processing time: 76.74849390983582\n",
            " > Real-time factor: 0.24775425831026252\n",
            " > Saving output to AUDIO_OUTPUTS/page_40.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 94 M. VOLLMAR AND G. EVANS\n",
            "and biochemical assays prove difficult because the protein is unstable. As with ensemble\n",
            "predictors,whereweakpredictionsarecombinedtogiveastronganswer,ifanysinglepieceof information is of low quality or shows a weak signal combining them may allow con-clusions with more certainty to be drawn. Additionally, if some small molecules can bedocked and their geometry can be provided then an AI system may be able to identify ar e a c t i o nm e c h a n i s mi na na c t i v es i t ea n dw h e t h e rap o t e n t i a ld r u gm a yh a v ea n ye ff e c t .Molecular dynamics simulation may then be used to assess how trustworthy such a pre-diction is. Ideas for such systems are already being trialled [ 152,153]. Machine learning\n",
            "a n dA Io ff e raw a yt oe x p l o r en o tj u s tp r o t e i ns t r u c t u r e sb u tam u c hl a r g e rs p a c eo fd a t athat may never be realistically accessible through experiments. This will allow for a goodguessastowhatthegroundtruthmaybeand,withexperienceandtime,willbecomeeverclosertotheperfectsolution.Finally,byusingdata-drivenapproachesthereisarealoppor-tunitytochallengeandexpandestablishedcrystallographicanalysismethods.Bycreatingexpertsystemsthatencapsulatedecadesofexperienceintheassessmentofthequalityandusefulnessofdiffractiondatafromthemanymetricsavailableduringanalysis,X-raybeam-lineswillthemselvesbeabletomakedecisionsonthebeststepstotaketowardssuccessfulcrystallographicstructuredetermination.\n",
            "Acknowledgements\n",
            "W ethankSirDaveStuart,FRS,andDavidHallforfruitfuldiscussionsandcommentswhenwritingthisreview.\n",
            "Disclosure statement\n",
            "Nopotentialconflictofinterestwasreportedbytheauthor(s).\n",
            "Funding\n",
            "ThisworkwassupportedbyBBSRC:[GrantNumberBB/S006699/1].\n",
            "Notes on contributors\n",
            "Gwyndaf Evans was awarded his PhD in Physics from the University of War-\n",
            "wick in 1994 for his work performed at the European Molecular Biology\n",
            "Laboratory in Hamburg on the application of anomalous scattering in macro-\n",
            "molecular crystallography using synchrotron radiation. After Post-Doctoral\n",
            "positions at the Advance Photon Source in Chicago, the MRC Laboratory ofMolecular Biology and Global Phasing Ltd (Cambridge, UK), he joined Dia-\n",
            "mond Light Source on the Harwell Campus in 2004. In 2020, he took up a\n",
            "joint position with the Rosalind Franklin Institute (Oxfordshire, UK) as Head\n",
            "ofTechnologyandbecameDeputyDirectorofLifeSciencesatDiamondLightSource.DrEvansis\n",
            "a leading expert in the use of synchrotron radiation for structural biology and has research groups\n",
            "developing instrumentation, software and methods in X-ray and electron diffraction and imagingattheinterfaceofmeasurementandanalysis.\n",
            " > Text splitted to sentences.\n",
            "['94 M. VOLLMAR AND G. EVANS', 'and biochemical assays prove difficult because the protein is unstable.', 'As with ensemble', 'predictors,whereweakpredictionsarecombinedtogiveastronganswer,ifanysinglepieceof information is of low quality or shows a weak signal combining them may allow con-clusions with more certainty to be drawn.', 'Additionally, if some small molecules can bedocked and their geometry can be provided then an AI system may be able to identify ar e a c t i o nm e c h a n i s mi na na c t i v es i t ea n dw h e t h e rap o t e n t i a ld r u gm a yh a v ea n ye ff e c t .', 'Molecular dynamics simulation may then be used to assess how trustworthy such a pre-diction is.', 'Ideas for such systems are already being trialled [ 152,153].', 'Machine learning', 'a n dA Io ff e raw a yt oe x p l o r en o tj u s tp r o t e i ns t r u c t u r e sb u tam u c hl a r g e rs p a c eo fd a t athat may never be realistically accessible through experiments.', 'This will allow for a goodguessastowhatthegroundtruthmaybeand,withexperienceandtime,willbecomeeverclosertotheperfectsolution.', 'Finally,byusingdata-drivenapproachesthereisarealoppor-tunitytochallengeandexpandestablishedcrystallographicanalysismethods.', 'Bycreatingexpertsystemsthatencapsulatedecadesofexperienceintheassessmentofthequalityandusefulnessofdiffractiondatafromthemanymetricsavailableduringanalysis,X-raybeam-lineswillthemselvesbeabletomakedecisionsonthebeststepstotaketowardssuccessfulcrystallographicstructuredetermination.', 'Acknowledgements', 'W ethankSirDaveStuart,FRS,andDavidHallforfruitfuldiscussionsandcommentswhenwritingthisreview.', 'Disclosure statement', 'Nopotentialconflictofinterestwasreportedbytheauthor(s).', 'Funding', 'ThisworkwassupportedbyBBSRC:[GrantNumberBB/S006699/1].', 'Notes on contributors', 'Gwyndaf Evans was awarded his PhD in Physics from the University of War-', 'wick in 1994 for his work performed at the European Molecular Biology', 'Laboratory in Hamburg on the application of anomalous scattering in macro-', 'molecular crystallography using synchrotron radiation.', 'After Post-Doctoral', 'positions at the Advance Photon Source in Chicago, the MRC Laboratory ofMolecular Biology and Global Phasing Ltd (Cambridge, UK), he joined Dia-', 'mond Light Source on the Harwell Campus in 2004.', 'In 2020, he took up a', 'joint position with the Rosalind Franklin Institute (Oxfordshire, UK) as Head', 'ofTechnologyandbecameDeputyDirectorofLifeSciencesatDiamondLightSource.', 'DrEvansis', 'a leading expert in the use of synchrotron radiation for structural biology and has research groups', 'developing instrumentation, software and methods in X-ray and electron diffraction and imagingattheinterfaceofmeasurementandanalysis.']\n",
            " > Processing time: 53.643494844436646\n",
            " > Real-time factor: 0.24582172181743872\n",
            " > Saving output to AUDIO_OUTPUTS/page_41.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 95\n",
            "Melanie Vollmar didherdoctoralstudiesinstructuralbiologyattheUniversity\n",
            "ofDüsseldorf,Germany,andwasawardedaDr.rer.nat.in2009.AfteraPost-\n",
            "Doctoral position at the Structural Genomics Consortium at the University ofOxford,UK,followedbyanotherattheUniversityofManchester,UK,DrVoll-\n",
            "mar joined Diamond Light Source in 2014. Her work at Diamond combines\n",
            "herstrongexpertiseinproteincrystallographyandstructuralbiologywithdata\n",
            "science,machinelearningandstatistics.\n",
            "ORCID\n",
            "Melanie Vollmar http://orcid.org/0000-0002-9162-9159\n",
            "GwyndafEvans http://orcid.org/0000-0002-6079-2201\n",
            "References\n",
            "[1] Yokoyama S, Matsuo Y, Hirota H, et al. Structural genomics projects in Japan. Prog Biophys\n",
            "Mol Biol. 2000;73:363–376.\n",
            "[2] Cassetta A, Deacon AM, Ealick SE, et al. Development of instrumentation and methods for\n",
            "MADandstructuralgenomicsattheSRS,ESRF,CHESSandElettrafacilities.JSynchrotron\n",
            "Radiat.1999;6:822–833.\n",
            "[3] Weigelt J. Structural genomics – impact on biomedicine and drug discovery. Exp Cell Res.\n",
            "2010;316:1332–1338.\n",
            "[4] Joachimiak A. High-throughput crystallography for structural genomics. Curr Opin Struc\n",
            "Biol.2009;19:573–584.\n",
            "[5] DelagenièreS,BrenchereauP,LaunerL,etal.ISPyb:aninformationmanagementsystemfor\n",
            "synchrotronmacromolecularcrystallography.Bioinformatics. 2011;27:3186–3192.\n",
            "[ 6 ]B e r m a nH M ,W e s t b r o o kJ ,F e n gZ ,e ta l .T h ep r o t e i nd a t ab a n k .N u c l e i cA c i d sR e s .\n",
            "2000;28:235–242.\n",
            "[7] Gelski M. Automation and remote synchrotron data collection. Acta Phys Pol A.\n",
            "2008;114:331–338.\n",
            "[8] Sanchez-Weatherby J, Sandy J, Mikolajek H, et al. VMXi: a fully automated, fully\n",
            "remote, high-flux in situ macromolecular crystallography beamline. J Synchrotron Radiat.\n",
            "2019;26:291–301.\n",
            "[ 9 ]B o w l e rM W ,N u r i z z oD ,B a r r e t tR ,e ta l .M A S S I F - 1 :ab e a m l i n ed e d i c a t e dt ot h ef u l l ya u t o -\n",
            "matic characterization and data collection from crystals of biological macromolecules. JSynchrotronRadiat. 2015;22:1540–1547.\n",
            "[10] WinterG.Xia2:anexpertsystemformacromolecularcrystallographicdatareduction.JAppl\n",
            "Crystallogr. 2010;43:186–190.\n",
            "[11] Winter G, McAuley KE. Automated data collection for macromolecular crystallography.\n",
            "Methods. 2011;55:81–93.\n",
            "[12] Rao R. Phys Today. 2020. Available from:https://physicstoday.scitation.org/do/10.1063/PT.\n",
            "6.2.20200925a/full/.\n",
            "[13] Keegan RM, Winn MD. MrBUMP: an automated pipeline for molecular replacement. Acta\n",
            "CrystallogrD. 2008;D64:119–124.\n",
            "[14] VonrheinC,BlancE,RoversiP,etal.AutomatedstructuresolutionwithautoSHARP.Methods\n",
            "Mol Biol. 2007;364:215–230.\n",
            "[15] DiederichsK,KarplusPA.ImprovedR-factorsfordiffractiondataanalysisinmacromolecular\n",
            "crystallography.NatStructBiol. 1997;4:269–275.\n",
            "[16] Weiss MS, Hilgenfeld R. On the use of the merging R factor as a quality indicator for X-ray\n",
            "data.JApplCrystallogr. 1997;30:203–205.\n",
            "[17] W eissM.GlobalindicatorsofX-raydataquality .JA pplCrystallogr . 2001;34:130–135.\n",
            "[18] EvansP.Scalingandassessmentofdataquality.ActaCrystallogrD. 2006;D62:72–82.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 95', 'Melanie Vollmar didherdoctoralstudiesinstructuralbiologyattheUniversity', 'ofDüsseldorf,Germany,andwasawardedaDr.rer.nat.in2009.', 'AfteraPost-', 'Doctoral position at the Structural Genomics Consortium at the University ofOxford,UK,followedbyanotherattheUniversityofManchester,UK,DrVoll-', 'mar joined Diamond Light Source in 2014.', 'Her work at Diamond combines', 'herstrongexpertiseinproteincrystallographyandstructuralbiologywithdata', 'science,machinelearningandstatistics.', 'ORCID', 'Melanie Vollmar http://orcid.org/0000-0002-9162-9159', 'GwyndafEvans http://orcid.org/0000-0002-6079-2201', 'References', '[1] Yokoyama S, Matsuo Y, Hirota H, et al.', 'Structural genomics projects in Japan.', 'Prog Biophys', 'Mol Biol.', '2000;73:363–376.', '[2] Cassetta A, Deacon AM, Ealick SE, et al.', 'Development of instrumentation and methods for', 'MADandstructuralgenomicsattheSRS,ESRF,CHESSandElettrafacilities.', 'JSynchrotron', 'Radiat.1999;6:822–833.', '[3] Weigelt J. Structural genomics – impact on biomedicine and drug discovery.', 'Exp Cell Res.', '2010;316:1332–1338.', '[4] Joachimiak A. High-throughput crystallography for structural genomics.', 'Curr Opin Struc', 'Biol.2009;19:573–584.', '[5] DelagenièreS,BrenchereauP,LaunerL,etal.', 'ISPyb:aninformationmanagementsystemfor', 'synchrotronmacromolecularcrystallography.', 'Bioinformatics.', '2011;27:3186–3192.', '[ 6 ]B e r m a nH M ,W e s t b r o o kJ ,F e n gZ ,e ta l .', 'T h ep r o t e i nd a t ab a n k .', 'N u c l e i cA c i d sR e s .', '2000;28:235–242.', '[7] Gelski M. Automation and remote synchrotron data collection.', 'Acta Phys Pol A.', '2008;114:331–338.', '[8] Sanchez-Weatherby J, Sandy J, Mikolajek H, et al.', 'VMXi: a fully automated, fully', 'remote, high-flux in situ macromolecular crystallography beamline.', 'J Synchrotron Radiat.', '2019;26:291–301.', '[ 9 ]B o w l e rM W ,N u r i z z oD ,B a r r e t tR ,e ta l .', 'M A S S I F - 1 :ab e a m l i n ed e d i c a t e dt ot h ef u l l ya u t o -', 'matic characterization and data collection from crystals of biological macromolecules.', 'JSynchrotronRadiat.', '2015;22:1540–1547.', '[10] WinterG.Xia2:anexpertsystemformacromolecularcrystallographicdatareduction.', 'JAppl', 'Crystallogr.', '2010;43:186–190.', '[11] Winter G, McAuley KE.', 'Automated data collection for macromolecular crystallography.', 'Methods.', '2011;55:81–93.', '[12] Rao R. Phys Today.', '2020.', 'Available from:https://physicstoday.scitation.org/do/10.1063/PT.', '6.2.20200925a/full/.', '[13] Keegan RM, Winn MD.', 'MrBUMP: an automated pipeline for molecular replacement.', 'Acta', 'CrystallogrD.', '2008;D64:119–124.', '[14] VonrheinC,BlancE,RoversiP,etal.', 'AutomatedstructuresolutionwithautoSHARP.Methods', 'Mol Biol.', '2007;364:215–230.', '[15] DiederichsK,KarplusPA.ImprovedR-factorsfordiffractiondataanalysisinmacromolecular', 'crystallography.', 'NatStructBiol.', '1997;4:269–275.', '[16] Weiss MS, Hilgenfeld R. On the use of the merging R factor as a quality indicator for X-ray', 'data.', 'JApplCrystallogr.', '1997;30:203–205.', '[17] W eissM.GlobalindicatorsofX-raydataquality .', 'JA pplCrystallogr .', '2001;34:130–135.', '[18] EvansP.Scalingandassessmentofdataquality.', 'ActaCrystallogrD.', '2006;D62:72–82.']\n",
            "fˈoːɹtiːn vˈɑːnɹiːn sˈiː,blˈɑ̃ŋk ˈiː,ɹˈoʊvɚsi pˈiː,ˈɛɾəl.\n",
            " [!] Character '̃' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 91.25786113739014\n",
            " > Real-time factor: 0.23877416392315653\n",
            " > Saving output to AUDIO_OUTPUTS/page_42.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 96 M. VOLLMAR AND G. EVANS\n",
            "[19] Yang J, Anishchenko I, Park H, et al. Improved protein structure prediction using predicted\n",
            "interresidueorientations.ProcNatlAcadSciUSA. 2020;117:1496–1503.\n",
            "[20] BrunoAE,CharbonneauP,NewmanJ,etal.Classificationofcrystallizationoutcomesusing\n",
            "deepconvolutionalneuralnetworks.PLoSONE. 2018;13:e0198883.\n",
            "[21] KryshtafovychA,MonastyrskyyB,FidelisK.CASP11statisticsandpredictioncenterevalu-\n",
            "ationsystem.Proteins. 2016;84:15–19.\n",
            "[22] K ohaviR,ProvostF .Glossaryofterms.MachLearn. 1998;30:271–274.\n",
            "[23] BishopCM.Patternrecognitionandmachinelearning.NewYork:Springer; 2006.\n",
            "[24] McCulloch S, Pitts W. A logical calculus of the ideas immanent in nervous activity. B Math\n",
            "Biophys.1943;5:115–133.\n",
            "[25] HebbDO.Theorganizationofbehaviour.Aneurophysiologicaltheory.NewYork:JohnWiley\n",
            "&Sons,Inc.;1949.\n",
            "[26] HubelDH,WieselTN.Receptivefieldsofsingleneuronsinthecat’sstriatecortex.JPhysiol.\n",
            "1959;148:574–591.\n",
            "[27] HubelDH,WieselTN.Receptivefields,binocularinteractionandfunctionalarchitecturein\n",
            "thecat’svisualcortex.JPhysiol. 1962;160:106–154.\n",
            "[28] TuringA.Computingmachineryandintelligence.Mind. 1950;LIX:433–460.\n",
            "[29] Minsky M. A neural-Analogue calculator based upon a probability model of reinforcement.\n",
            "Cambridge(Massachusetts):HarvardUniversityPsychologicalLaboratories; 1952.\n",
            "[30] McCarthyJ,MinskyML,RochesterN,etal.AproposalfortheDartmouthsummerresearch\n",
            "projectonartificialintelligence.AIMag. 1955;27:12–14.\n",
            "[31] Samuel AL. Some studies in machine learning using the game of checkers. IBM J Res Dev.\n",
            "1959;3:210–229.\n",
            "[32] SamuelAL.Somestudiesinmachinelearningusingthegameofcheckers.II-recentprogress.\n",
            "IBMJResDev. 1967;11:601–617.\n",
            "[33] Rosenblatt F. The Perceptron: A Perceiving and Recognizing Automaton. Report: 85–60. -1,\n",
            "CornellAeronauticalLaboratory,Buffalo,NewYork, 1957.\n",
            "[34] Cover T, Hart P. Nearest neighbor pattern classification. IEEE T Inform Theory.\n",
            "1967;13:21–27.\n",
            "[35] Fukushima K. Neural network model for a mechanism of pattern recognition unaffected by\n",
            "shiftinposition-neocognitron.TransIECE. 1979;J62-A:658–665.\n",
            "[36] Fukushima K. Neocogniton: A self-organizing neural network model for a mechanism of\n",
            "patternrecognitionunaffectedbyshiftinposition.BiolCybern. 1980;36:193–202.\n",
            "[37] Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a\n",
            "Taylor expansion of the local rounding errors [Master’s Thesis (in Finnish). ”, University of\n",
            "Helsinki, 1970\n",
            "[38] SchapireRE.Thestrengthofweaklearnability.MachLearn. 1990;5:197–227.\n",
            "[39] HochreiterS,SchmidhuberJ.Longshort-termmemory.NeuralComput. 1997;9:1735–1780.\n",
            "[40] Pearson KLIII. On lines and planes of closest fit to systems of points in space. Philos Mag.\n",
            "1901;2:559–572.\n",
            "[41] Wold S, Sjöström M, Eriksson L. PLS-regression: a basic tool of chemometrics. Chemometr\n",
            "IntellLab. 2001;58:109–130.\n",
            "[42] KruskalJB.Multidimensionalscalingbyoptimizinggoodnessoffittoanonmetrichypothesis.\n",
            "Psychometrika. 1964;29:1–27.\n",
            "[43] Borg I, Groenen P. Modern Multidimensional scaling: theory and applications. New York:\n",
            "Springer; 2005.\n",
            "[44] Fisher RA. The use of multiple measurements in taxonomic problems. Ann Eugen.\n",
            "1936;7:466–475.\n",
            "[45] McLachlan GJ. Discriminant analysis and statistical pattern recognition. In: Wiley series in\n",
            "probability and statistics. A Wiley interscience publication. New York: John Wiley & Sons,Inc.;2004.\n",
            "[46] PereiraJ,LamzinVS.Adistancegeometry-baseddescriptionandvalidationofproteinmain-\n",
            "chainconformation.IUCrJ. 2017;4:657–670.\n",
            " > Text splitted to sentences.\n",
            "['96 M. VOLLMAR AND G. EVANS', '[19] Yang J, Anishchenko I, Park H, et al.', 'Improved protein structure prediction using predicted', 'interresidueorientations.', 'ProcNatlAcadSciUSA.', '2020;117:1496–1503.', '[20] BrunoAE,CharbonneauP,NewmanJ,etal.Classificationofcrystallizationoutcomesusing', 'deepconvolutionalneuralnetworks.PLoSONE.', '2018;13:e0198883.', '[21] KryshtafovychA,MonastyrskyyB,FidelisK.CASP11statisticsandpredictioncenterevalu-', 'ationsystem.', 'Proteins.', '2016;84:15–19.', '[22] K ohaviR,ProvostF .', 'Glossaryofterms.', 'MachLearn.', '1998;30:271–274.', '[23] BishopCM.Patternrecognitionandmachinelearning.', 'NewYork:Springer; 2006.', '[24] McCulloch S, Pitts W. A logical calculus of the ideas immanent in nervous activity.', 'B Math', 'Biophys.1943;5:115–133.', '[25] HebbDO.Theorganizationofbehaviour.', 'Aneurophysiologicaltheory.', 'NewYork:JohnWiley', '&Sons,Inc.', ';1949.', '[26] HubelDH,WieselTN.Receptivefieldsofsingleneuronsinthecat’sstriatecortex.', 'JPhysiol.', '1959;148:574–591.', '[27] HubelDH,WieselTN.Receptivefields,binocularinteractionandfunctionalarchitecturein', 'thecat’svisualcortex.', 'JPhysiol.', '1962;160:106–154.', '[28] TuringA.Computingmachineryandintelligence.', 'Mind.', '1950;LIX:433–460.', '[29] Minsky M. A neural-Analogue calculator based upon a probability model of reinforcement.', 'Cambridge(Massachusetts):HarvardUniversityPsychologicalLaboratories; 1952.', '[30] McCarthyJ,MinskyML,RochesterN,etal.', 'AproposalfortheDartmouthsummerresearch', 'projectonartificialintelligence.', 'AIMag.', '1955;27:12–14.', '[31] Samuel AL.', 'Some studies in machine learning using the game of checkers.', 'IBM J Res Dev.', '1959;3:210–229.', '[32] SamuelAL.Somestudiesinmachinelearningusingthegameofcheckers.', 'II-recentprogress.', 'IBMJResDev.', '1967;11:601–617.', '[33] Rosenblatt F. The Perceptron: A Perceiving and Recognizing Automaton.', 'Report: 85–60.', '-1,', 'CornellAeronauticalLaboratory,Buffalo,NewYork, 1957.', '[34] Cover T, Hart P. Nearest neighbor pattern classification.', 'IEEE T Inform Theory.', '1967;13:21–27.', '[35] Fukushima K. Neural network model for a mechanism of pattern recognition unaffected by', 'shiftinposition-neocognitron.', 'TransIECE.', '1979;J62-A:658–665.', '[36] Fukushima K. Neocogniton: A self-organizing neural network model for a mechanism of', 'patternrecognitionunaffectedbyshiftinposition.', 'BiolCybern.', '1980;36:193–202.', '[37] Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a', 'Taylor expansion of the local rounding errors [Master’s Thesis (in Finnish).', '”, University of', 'Helsinki, 1970', '[38] SchapireRE.Thestrengthofweaklearnability.', 'MachLearn.', '1990;5:197–227.', '[39] HochreiterS,SchmidhuberJ.Longshort-termmemory.', 'NeuralComput.', '1997;9:1735–1780.', '[40] Pearson KLIII.', 'On lines and planes of closest fit to systems of points in space.', 'Philos Mag.', '1901;2:559–572.', '[41] Wold S, Sjöström M, Eriksson L. PLS-regression: a basic tool of chemometrics.', 'Chemometr', 'IntellLab.', '2001;58:109–130.', '[42] KruskalJB.Multidimensionalscalingbyoptimizinggoodnessoffittoanonmetrichypothesis.', 'Psychometrika.', '1964;29:1–27.', '[43] Borg I, Groenen P. Modern Multidimensional scaling: theory and applications.', 'New York:', 'Springer; 2005.', '[44] Fisher RA.', 'The use of multiple measurements in taxonomic problems.', 'Ann Eugen.', '1936;7:466–475.', '[45] McLachlan GJ.', 'Discriminant analysis and statistical pattern recognition.', 'In: Wiley series in', 'probability and statistics.', 'A Wiley interscience publication.', 'New York: John Wiley & Sons,Inc.', ';2004.', '[46] PereiraJ,LamzinVS.Adistancegeometry-baseddescriptionandvalidationofproteinmain-', 'chainconformation.', 'IUCrJ.', '2017;4:657–670.']\n",
            " > Processing time: 96.11750912666321\n",
            " > Real-time factor: 0.23274461417458706\n",
            " > Saving output to AUDIO_OUTPUTS/page_43.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 97\n",
            "[47] Hattne J, Lamzin VS. Pattern recognition-based detection of planar objects in 3D electron\n",
            "densitymaps.ActaCrystallogrD. 2008;D64:834–842.\n",
            "[48] Cumbaa CA, Lauricella A, Fehrman N, et al. Automatic classification of sub-microlitre\n",
            "protein-crystallizationtrialsin1536-wellplates.ActaCrystallogrD. 2003;D59:1619–1627.\n",
            "[49] Cumbaa C, Jurisica I. Automatic classification and pattern discovery in high-throughput\n",
            "proteincrystallizationtrials.JStructFunctGenomics. 2005;6:195–202.\n",
            "[50] Saitoh K, Kawabata K, Asama H, et al. Evaluation of protein crystallization states based on\n",
            "textureinformationderivedfromgreyscaleimages.ActaCrystallogrD. 2005;D61:873–880.\n",
            "[51] BuchalaS,WilsonJC.Improvedclassificationofcrystallizationimagesusingdatafusionand\n",
            "multipleclassifiers.ActaCrystallogrD. 2008;D64:823–833.\n",
            "[52] Manning CD, Raghavan P, Schütze H. Introduction to information retrieval. New York:\n",
            "CambridgeUniversityPress; 2008.\n",
            "[53] Pearl J. Bayesian Networks: A Model of Self-Activated Memory for Evidential Reasoning.\n",
            "UCLA Technical Report CSD-850017. Proceedings of the 7th Conference of the Cognitive\n",
            "ScienceSociety,UniversityofCalifornia,Irvine,CA. 1985:329–334.\n",
            "[54] Bayes Rev T. An essay towards solving a problem in the doctrine of chances. Philos T R Soc\n",
            "Lond.1763;53:370–418.ReprintedinBiometrika,1958;45:293-315.\n",
            "[55] SmialowskiP,SchmidtT,CoxJ,etal.WillMyproteincrystallize?Asequence-basedpredictor.\n",
            "Proteins. 2006;62:343–355.\n",
            "[56] Hoerl AE, Kennard RW. Ridge regression: biased estimation for nonorthogonal problems.\n",
            "Technometrics. 1970;12:55–67.\n",
            "[57] Santosa F, Symes WW. Linear inversion of band-limited reflection seismograms. SIAM J Sci\n",
            "StatComp. 1986;7:1307–1330.\n",
            "[58] TibshiraniR.Regressionshrinkageandselectionviathelasso.JRStatSoc. 1996;58:267–288.\n",
            "[59] AltmanNS.Anintroduction tokernelandnearest-neighbornonparametricregression.Am\n",
            "Stat.1992;46:175–185.\n",
            "[60] FixE,HodgesJL.Discriminatoryanalysis.Nonparametricdiscrimination:Consistencyprop-\n",
            "erties. Report Number 4, Project Number 21-49-004. USAF School of Aviation Medicine,RandolphField,Texas,1951.\n",
            "[61] CortesCV.VN.support-vectornetworks.MachLearn. 1995;20:273–797.\n",
            "[62] MiziantyMJ,KurganL.Sequence-basedpredictionofproteincrystallization,purificationand\n",
            "productionpropensity.Bioinformatics. 2011;27:i24–i33.\n",
            "[63] WangH,WangM,TanH,etal.PredPPCrys:accuratepredictionofsequencecloning,protein\n",
            "production, purification and crystallization propensity from protein sequences using multi-stepheterogeneousfeaturefusionandselection.PloSONE. 2014;9:e105902.\n",
            "[64] Pan S, Shavit G, Penas-Centeno M, et al. Automated classification of protein crystallization\n",
            "images using support vector machines with scale-invariant texture and gabor features. ActaCrystallogrD. 2006;D62:271–279.\n",
            "[65] Chojnowski G, Pereira J, Lamzin VS. Sequence assignment for low-resolution modelling of\n",
            "proteincrystalstructures.ActaCrystallogrD. 2019;D75:753–763.\n",
            "[66] BreimanL,FriedmanJH,OlshenRA,etal.Classificationandregressiontrees.Wadsworth&\n",
            "Brooks/Cole,AdvancedBooks&SoftwareMonterey, 1984.\n",
            "[67] Ho TK. Random decision forests). ICDAR ‘95 Proceedings of the Third International Con-\n",
            "ferenceonDocumentAnalysisandRecognition;1995;1:278.\n",
            "[68] BreimanL.Baggingpredictors.MachLearn. 1996;24:123–140.\n",
            "[69] Friedman JH. Greedy function approximation: A Gradient Boosting machine. Ann Stat.\n",
            "2001;29:1189–1232.\n",
            "[70] MasonL,BaxterJ,BartlettL,etal.BoostingAlgorithmsasGradientDescent.).NIPS’99:Pro-\n",
            "ceedings of the 12th International Conference on Neural Information Processing Systems;1999:512–518.\n",
            "[71] GeurtsP,ErnstD,WehenkelL.Extremelyrandomizedtrees.MachLearn. 2006;63:3–42.\n",
            "[72] Bern M, Goldberg D, Stevens RC, et al. Automatic classification of protein crystallization\n",
            "imagesusingacurve-trackingalgorithm.JApplCryst. 2004;37:279–287.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 97', '[47] Hattne J, Lamzin VS. Pattern recognition-based detection of planar objects in 3D electron', 'densitymaps.', 'ActaCrystallogrD.', '2008;D64:834–842.', '[48] Cumbaa CA, Lauricella A, Fehrman N, et al.', 'Automatic classification of sub-microlitre', 'protein-crystallizationtrialsin1536-wellplates.', 'ActaCrystallogrD.', '2003;D59:1619–1627.', '[49] Cumbaa C, Jurisica I. Automatic classification and pattern discovery in high-throughput', 'proteincrystallizationtrials.', 'JStructFunctGenomics.', '2005;6:195–202.', '[50] Saitoh K, Kawabata K, Asama H, et al.', 'Evaluation of protein crystallization states based on', 'textureinformationderivedfromgreyscaleimages.', 'ActaCrystallogrD.', '2005;D61:873–880.', '[51] BuchalaS,WilsonJC.Improvedclassificationofcrystallizationimagesusingdatafusionand', 'multipleclassifiers.', 'ActaCrystallogrD.', '2008;D64:823–833.', '[52] Manning CD, Raghavan P, Schütze H. Introduction to information retrieval.', 'New York:', 'CambridgeUniversityPress; 2008.', '[53] Pearl J. Bayesian Networks: A Model of Self-Activated Memory for Evidential Reasoning.', 'UCLA Technical Report CSD-850017.', 'Proceedings of the 7th Conference of the Cognitive', 'ScienceSociety,UniversityofCalifornia,Irvine,CA.', '1985:329–334.', '[54] Bayes Rev T. An essay towards solving a problem in the doctrine of chances.', 'Philos T R Soc', 'Lond.1763;53:370–418.', 'ReprintedinBiometrika,1958;45:293-315.', '[55] SmialowskiP,SchmidtT,CoxJ,etal.', 'WillMyproteincrystallize?', 'Asequence-basedpredictor.', 'Proteins.', '2006;62:343–355.', '[56] Hoerl AE, Kennard RW.', 'Ridge regression: biased estimation for nonorthogonal problems.', 'Technometrics.', '1970;12:55–67.', '[57] Santosa F, Symes WW.', 'Linear inversion of band-limited reflection seismograms.', 'SIAM J Sci', 'StatComp.', '1986;7:1307–1330.', '[58] TibshiraniR.Regressionshrinkageandselectionviathelasso.', 'JRStatSoc.', '1996;58:267–288.', '[59] AltmanNS.Anintroduction tokernelandnearest-neighbornonparametricregression.', 'Am', 'Stat.1992;46:175–185.', '[60] FixE,HodgesJL.Discriminatoryanalysis.', 'Nonparametricdiscrimination:Consistencyprop-', 'erties.', 'Report Number 4, Project Number 21-49-004.', 'USAF School of Aviation Medicine,RandolphField,Texas,1951.', '[61] CortesCV.VN.support-vectornetworks.MachLearn.', '1995;20:273–797.', '[62] MiziantyMJ,KurganL.Sequence-basedpredictionofproteincrystallization,purificationand', 'productionpropensity.', 'Bioinformatics.', '2011;27:i24–i33.', '[63] WangH,WangM,TanH,etal.', 'PredPPCrys:accuratepredictionofsequencecloning,protein', 'production, purification and crystallization propensity from protein sequences using multi-stepheterogeneousfeaturefusionandselection.', 'PloSONE.', '2014;9:e105902.', '[64] Pan S, Shavit G, Penas-Centeno M, et al.', 'Automated classification of protein crystallization', 'images using support vector machines with scale-invariant texture and gabor features.', 'ActaCrystallogrD.', '2006;D62:271–279.', '[65] Chojnowski G, Pereira J, Lamzin VS. Sequence assignment for low-resolution modelling of', 'proteincrystalstructures.', 'ActaCrystallogrD.', '2019;D75:753–763.', '[66] BreimanL,FriedmanJH,OlshenRA,etal.', 'Classificationandregressiontrees.', 'Wadsworth&', 'Brooks/Cole,AdvancedBooks&SoftwareMonterey, 1984.', '[67] Ho TK.', 'Random decision forests).', 'ICDAR ‘95 Proceedings of the Third International Con-', 'ferenceonDocumentAnalysisandRecognition;1995;1:278.', '[68] BreimanL.Baggingpredictors.', 'MachLearn.', '1996;24:123–140.', '[69] Friedman JH.', 'Greedy function approximation: A Gradient Boosting machine.', 'Ann Stat.', '2001;29:1189–1232.', '[70] MasonL,BaxterJ,BartlettL,etal.', 'BoostingAlgorithmsasGradientDescent.', ').', 'NIPS’99:Pro-', 'ceedings of the 12th International Conference on Neural Information Processing Systems;1999:512–518.', '[71] GeurtsP,ErnstD,WehenkelL.Extremelyrandomizedtrees.', 'MachLearn.', '2006;63:3–42.', '[72] Bern M, Goldberg D, Stevens RC, et al.', 'Automatic classification of protein crystallization', 'imagesusingacurve-trackingalgorithm.', 'JApplCryst.', '2004;37:279–287.']\n",
            " > Processing time: 112.15640687942505\n",
            " > Real-time factor: 0.240482111098728\n",
            " > Saving output to AUDIO_OUTPUTS/page_44.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 98 M. VOLLMAR AND G. EVANS\n",
            "[73] Liu R, Freund Y, Spraggon G. Image-based crystal detection: a machine learning approach.\n",
            "ActaCrystallogrD. 2008;D64:1187–1195.\n",
            "[74] JahandidehS,JaroszewskiL,GodzikaA.Improvingthechancesofsuccessfulproteinstructure\n",
            "determinationwitharandomforestclassifier.ActaCrystallogrD. 2014;D70:627–635.\n",
            "[75] Alavi A, Ascher DB. DHS-Crystallize: Deep-Hybrid-Sequence based method for predicting\n",
            "proteinCrystallization.Posted:[cited2020Nov13]Availablefrom: https://www.biorxiv.org/\n",
            "content/10.1101/2020.11.13.381301v1 .\n",
            "[76] V ollmarM,P arkh urstJM,J aquesD ,etal.Thep r edictivepo wero fda ta-p r ocessingsta tistics.\n",
            "IUCrJ.2020;7:342–354.\n",
            "[77] BishopCM.Neuralnetworksforpatternrecognition.NewYork:OxfordUniversityPress,Inc;\n",
            "1995.\n",
            "[78] Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagation\n",
            "errors.Nature. 1986;323:533–536.\n",
            "[79] Hopfield JJ. Neural networks and physical systems with emergent collective computational\n",
            "abilities.ProcNatlAcadSciUSA. 1982;79:2554–2558.\n",
            "[80] HintonGE,SalakhutdinovR.AbetterwaytopretraindeepBoltzmannmachines).NIPS’12:\n",
            "Proceedingsofthe25thInternationalConferenceonNeuralInformationProcessingSystems;\n",
            "2012.:2447–2455.\n",
            "[81] Hinton GE, Osindero S, Teh Y-W. A fast learning algorithm for deep believe nets. Neural\n",
            "Comput. 2006;18:1527–1554.\n",
            "[82] Watkins CJCH. Phd thesis [Learning from De layed Rewards, King’s College], Cambridge,\n",
            "UK,1989.\n",
            "[83] Bond PS, Wilson KS, Cowtan KD. Predicting protein model correctness in Coot using\n",
            "machinelearning.ActaCrystallogrD. 2020;D76:713–723.\n",
            "[84] ElbasirA,MoovarkumudalvanB,KunjiK,etal.Deepcrystal:adeeplearningframeworkfor\n",
            "sequence-basedproteincrystallizationprediction.Bioinformatics. 2019;35:2216–2225.\n",
            "[85] SpraggonG,LesleySA,KreuschA,etal.Computationalanalysisofcrystallizationtrials.Acta\n",
            "CrystallogrD. 2002;D58:1915–1923.\n",
            "[86] Souza A, Oliveira LB, Hollatz S, et al. DeepFreak: Learning Crystallography Diffrac-\n",
            "tion Patterns with Automated Machine Learning. Posted: April 26, 2019. Available\n",
            "from:https://arxiv.org/pdf/1904.11834.pdf .\n",
            "[87] KeT-W,BrewsterAS,YuSX,etal.Aconvolutionalneuralnetwork-basedscreeningtoolfor\n",
            "X-rayserialcrystallography.JSynchrotronRadiat. 2018;25:655–670.\n",
            "[88] WangS,SunS,LiZ,etal.AccurateDenovopredictionofproteincontactMapbyultra-deep\n",
            "learningmodel.PLoSComputBiol. 2017;13:e1005324.\n",
            "[89] SeniorAW,EvansR,JumperJ,etal.Proteinstructurepredictionusingmultipledeepneural\n",
            "networksinthe13thCriticalAssessmentofproteinstructureprediction(CASP13).Proteins.\n",
            "2019;87:1141–1148.\n",
            "[90] Senior AW, Evans R, Jumper J, et al. Improved protein structure prediction using potentials\n",
            "fromdeeplearning.Nature. 2020;577:706–710.\n",
            "[91] Yang J, Anishchenko I, Park H, et al. Improved protein structure prediction using predicted\n",
            "interresidueorientations.ProcNatlAcadSciUSA. 2020;117:1496–1503.\n",
            "[92] PunjaniA,RubinsteinJL,FleetDJ,etal.cryoSPARC:algorithmsforrapidunsupervisedcryo-\n",
            "EMstructuredetermination.NatMethods. 2017;14:290–296.\n",
            "[93] Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object\n",
            "Detection.Posted:June8,2015.Availablefrom:https://arxiv.org/pdf/1506.02640.pdf .\n",
            "[94] WagnerT,MerinoF,StabrinM,etal.SPHIRE-crYOLOisafastandaccuratefullyautomated\n",
            "particlepickerforcryo-EM.CommunBiol. 2019;2:218.\n",
            "[95] ScheresSHW.RELION:implementationofaBayesianapproachtocryo-EMstructuredeter-\n",
            "mination.JStructBiol. 2012;180:519–530.\n",
            "[96] TufféryS.DataMiningandstatisticsfordecisionmaking.Chichester:Wiley; 2011.\n",
            "[97] Zhang KYJ, Main P. Histogram matching as a new density modification technique for phase\n",
            "refinementandextensionofproteinmolecules.ActaCrystallogrA. 1990;A46:41–46.\n",
            " > Text splitted to sentences.\n",
            "['98 M. VOLLMAR AND G. EVANS', '[73] Liu R, Freund Y, Spraggon G. Image-based crystal detection: a machine learning approach.', 'ActaCrystallogrD.', '2008;D64:1187–1195.', '[74] JahandidehS,JaroszewskiL,GodzikaA.Improvingthechancesofsuccessfulproteinstructure', 'determinationwitharandomforestclassifier.', 'ActaCrystallogrD.', '2014;D70:627–635.', '[75] Alavi A, Ascher DB.', 'DHS-Crystallize: Deep-Hybrid-Sequence based method for predicting', 'proteinCrystallization.', 'Posted:[cited2020Nov13]Availablefrom: https://www.biorxiv.org/', 'content/10.1101/2020.11.13.381301v1 .', '[76] V ollmarM,P arkh urstJM,J aquesD ,etal.', 'Thep r edictivepo wero fda ta-p r ocessingsta tistics.', 'IUCrJ.2020;7:342–354.', '[77] BishopCM.Neuralnetworksforpatternrecognition.NewYork:OxfordUniversityPress,Inc;', '1995.', '[78] Rumelhart DE, Hinton GE, Williams RJ.', 'Learning representations by back-propagation', 'errors.', 'Nature.', '1986;323:533–536.', '[79] Hopfield JJ.', 'Neural networks and physical systems with emergent collective computational', 'abilities.', 'ProcNatlAcadSciUSA.', '1982;79:2554–2558.', '[80] HintonGE,SalakhutdinovR.AbetterwaytopretraindeepBoltzmannmachines).', 'NIPS’12:', 'Proceedingsofthe25thInternationalConferenceonNeuralInformationProcessingSystems;', '2012.:2447–2455.', '[81] Hinton GE, Osindero S, Teh Y-W.', 'A fast learning algorithm for deep believe nets.', 'Neural', 'Comput.', '2006;18:1527–1554.', '[82] Watkins CJCH.', 'Phd thesis [Learning from De layed Rewards, King’s College], Cambridge,', 'UK,1989.', '[83] Bond PS, Wilson KS, Cowtan KD.', 'Predicting protein model correctness in Coot using', 'machinelearning.', 'ActaCrystallogrD.', '2020;D76:713–723.', '[84] ElbasirA,MoovarkumudalvanB,KunjiK,etal.', 'Deepcrystal:adeeplearningframeworkfor', 'sequence-basedproteincrystallizationprediction.', 'Bioinformatics.', '2019;35:2216–2225.', '[85] SpraggonG,LesleySA,KreuschA,etal.', 'Computationalanalysisofcrystallizationtrials.', 'Acta', 'CrystallogrD.', '2002;D58:1915–1923.', '[86] Souza A, Oliveira LB, Hollatz S, et al.', 'DeepFreak: Learning Crystallography Diffrac-', 'tion Patterns with Automated Machine Learning.', 'Posted: April 26, 2019.', 'Available', 'from:https://arxiv.org/pdf/1904.11834.pdf .', '[87] KeT-W,BrewsterAS,YuSX,etal.Aconvolutionalneuralnetwork-basedscreeningtoolfor', 'X-rayserialcrystallography.JSynchrotronRadiat.', '2018;25:655–670.', '[88] WangS,SunS,LiZ,etal.', 'AccurateDenovopredictionofproteincontactMapbyultra-deep', 'learningmodel.', 'PLoSComputBiol.', '2017;13:e1005324.', '[89] SeniorAW,EvansR,JumperJ,etal.Proteinstructurepredictionusingmultipledeepneural', 'networksinthe13thCriticalAssessmentofproteinstructureprediction(CASP13).', 'Proteins.', '2019;87:1141–1148.', '[90] Senior AW, Evans R, Jumper J, et al.', 'Improved protein structure prediction using potentials', 'fromdeeplearning.', 'Nature.', '2020;577:706–710.', '[91] Yang J, Anishchenko I, Park H, et al.', 'Improved protein structure prediction using predicted', 'interresidueorientations.', 'ProcNatlAcadSciUSA.', '2020;117:1496–1503.', '[92] PunjaniA,RubinsteinJL,FleetDJ,etal.cryoSPARC:algorithmsforrapidunsupervisedcryo-', 'EMstructuredetermination.', 'NatMethods.', '2017;14:290–296.', '[93] Redmon J, Divvala S, Girshick R, et al.', 'You Only Look Once: Unified, Real-Time Object', 'Detection.Posted:June8,2015.Availablefrom:https://arxiv.org/pdf/1506.02640.pdf .', '[94] WagnerT,MerinoF,StabrinM,etal.', 'SPHIRE-crYOLOisafastandaccuratefullyautomated', 'particlepickerforcryo-EM.CommunBiol.', '2019;2:218.', '[95] ScheresSHW.RELION:implementationofaBayesianapproachtocryo-EMstructuredeter-', 'mination.', 'JStructBiol.', '2012;180:519–530.', '[96] TufféryS.DataMiningandstatisticsfordecisionmaking.', 'Chichester:Wiley; 2011.', '[97] Zhang KYJ, Main P. Histogram matching as a new density modification technique for phase', 'refinementandextensionofproteinmolecules.', 'ActaCrystallogrA.', '1990;A46:41–46.']\n",
            " > Processing time: 106.85902190208435\n",
            " > Real-time factor: 0.23801880286596994\n",
            " > Saving output to AUDIO_OUTPUTS/page_45.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 99\n",
            "[98] Jiang M, He H, Cheng Y, et al. Resolution dependence of an Ab initio phasing method in\n",
            "proteinX-raycrystallography.Crystals(Basel). 2018;8:156–173.\n",
            "[99] deSmithMJ.Statisticalanalysishandbook.Edinburgh:TheWinchelseaPress; 2018.\n",
            "[100] Demiralp C, Haas PJ, Parthasarathy S, et al. Foresight: rapid data exploration through\n",
            "guideposts.Posted:September29,2017.Availablefrom:https://arxiv.org/pdf/1709.10513.pdf .\n",
            "[101] Guyon I, Elisseeff A. An introduction to variable and feature selection. JMLR. 2003;3:\n",
            "1157–1182.\n",
            "[102] Dodge S, Karam L. A study and comparison of human and deep learning recog-\n",
            "nition performance under visual distortion. Posted: [Cited 2017 May 6]. Available\n",
            "from:https://arxiv.org/pdf/1705.02498.pdf .\n",
            "[103] HanleyJA,McNeilBJ.Amethodofcomparingtheareasunderreceiveroperatingcharacter-\n",
            "isticcurvesderivedfromthesamecases.Radiology. 1983;148:839–843.\n",
            "[104] GossetWS.Theprobableerrorofamean.Biometrika. 1908;6:1–25.\n",
            "[105] EfronB.Bootstrapmethods:anotherlookatthejackknife.AnnStat. 1979;7:1–26.\n",
            "[106] Batista GEAPA, Prati RC, Monard MC. A Study of the behavior of several methods for bal-\n",
            "ancingmachinelearningtrainingdata.ACMSigkddExplorationsNewsletter. 2004;6:20–29.\n",
            "[107] AllenDM.TherelationshipbetweenvariableSelectionanddataaugmentationandamethod\n",
            "forprediction.Technometrics. 1974;16:125–127.\n",
            "[108] Stone M. Cross-Validatory choice and assessment of statistical predictions. J R Stat Soc B.\n",
            "1974;36:111–147.\n",
            "[109] ClaesenM,DeMoorB.Hyperparametersearchinmachinelearning.Posted[cited2015Feb\n",
            "7].https://arxiv.org/pdf/1502.02127.pdf .\n",
            "[110] SlabinskiL,JaroszewskiL,RychlewskiL,etal.XtalPred:awebserverforpredictionofprotein\n",
            "crystallizability.Bioinformatics. 2007;23:3403–3405.\n",
            "[111] Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with\n",
            "AlphaFold.Nature. 2021;596:583–589.\n",
            "[112] Baek M, DiMaio F, Anishchenko I, et al. Accurate prediction of protein structure and\n",
            "interactionsusinga3-tracknetwork.Science. 2021;373:871–876.\n",
            "[113] KrizhevskyA,SutskeverI,HintonGE.Imagenetclassificationwithdeepconvolutionalneural\n",
            "networks.NIPS;25,Proceedingsofthe12thInternationalConferenceonNeuralInformationProcessingSystems;2012:1097–1105\n",
            "[114] MaiaFRNC.TheCoherentX-rayImagingDatabank.NatMethods. 2012;9:854–855.\n",
            "[115] Jarrett K, Kavukcuoglu K, Ranzato MA, et al. What is the best multi-stage architecture\n",
            "for object recognition?). Proceedings of the 2009 IEEE 12th International Conference on\n",
            "ComputerVision;2009:2146-2153.\n",
            "[116] Winter G, Waterman DG, Parkhurst JM, et al. DIALS: implementation and evaluation of a\n",
            "newintegrationpackage.ActaCrystallogrD. 2018;D74:85–97.\n",
            "[117] EmsleyP,CowtanK.Coot:model-buildingtoolsformoleculargraphics.ActaCrystallogrD.\n",
            "2004;D60:2126–2132.\n",
            "[118] EmsleyP,LohkampB,ScottWG,etal.Featuresanddevelopmentofcoot.ActaCrystallogrD.\n",
            "2010;D66:486–501.\n",
            "[119] CowtanK.TheBuccaneersoftwareforautomatedmodelbuilding.1.Tracingproteinchains.\n",
            "ActaCrystallogrD. 2006;D62:1002–1011.\n",
            "[120] Cowtan K. Fitting molecular fragments into electron density. Acta Crystallogr D.\n",
            "2008;D64:83–89.\n",
            "[121] Lovell SC, Word JM, Richardson JS, et al. The penultimate rotamer library. Proteins.\n",
            "2000;40:389–408.\n",
            "[122] RamachandranGN,RamakrishnanC,SasisekharanV.Stereochemistryofpolypeptidechain\n",
            "configuration.JMolBiol. 1963;7:95–99.\n",
            "[123] WangBC.Resolutionofphaseambiguityinmacromolecularcrystallography.MethodsEnzy-\n",
            "mol.1985;115:90–112.\n",
            "[124] Leslie AGW. A reciprocal-space method for calculating a molecular envelope using the\n",
            "algorithmofB.C.Wang.ActaCrystallogrA. 1987;A43:134–136.\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 99', '[98] Jiang M, He H, Cheng Y, et al.', 'Resolution dependence of an Ab initio phasing method in', 'proteinX-raycrystallography.', 'Crystals(Basel).', '2018;8:156–173.', '[99] deSmithMJ.Statisticalanalysishandbook.', 'Edinburgh:TheWinchelseaPress; 2018.', '[100] Demiralp C, Haas PJ, Parthasarathy S, et al.', 'Foresight: rapid data exploration through', 'guideposts.Posted:September29,2017.Availablefrom:https://arxiv.org/pdf/1709.10513.pdf .', '[101] Guyon I, Elisseeff A. An introduction to variable and feature selection.', 'JMLR.', '2003;3:', '1157–1182.', '[102] Dodge S, Karam L. A study and comparison of human and deep learning recog-', 'nition performance under visual distortion.', 'Posted: [Cited 2017 May 6].', 'Available', 'from:https://arxiv.org/pdf/1705.02498.pdf .', '[103] HanleyJA,McNeilBJ.Amethodofcomparingtheareasunderreceiveroperatingcharacter-', 'isticcurvesderivedfromthesamecases.', 'Radiology.', '1983;148:839–843.', '[104] GossetWS.Theprobableerrorofamean.', 'Biometrika.', '1908;6:1–25.', '[105] EfronB.Bootstrapmethods:anotherlookatthejackknife.', 'AnnStat.', '1979;7:1–26.', '[106] Batista GEAPA, Prati RC, Monard MC.', 'A Study of the behavior of several methods for bal-', 'ancingmachinelearningtrainingdata.', 'ACMSigkddExplorationsNewsletter.', '2004;6:20–29.', '[107] AllenDM.TherelationshipbetweenvariableSelectionanddataaugmentationandamethod', 'forprediction.', 'Technometrics.', '1974;16:125–127.', '[108] Stone M. Cross-Validatory choice and assessment of statistical predictions.', 'J R Stat Soc B.', '1974;36:111–147.', '[109] ClaesenM,DeMoorB.Hyperparametersearchinmachinelearning.Posted[cited2015Feb', '7].', 'https://arxiv.org/pdf/1502.02127.pdf .', '[110] SlabinskiL,JaroszewskiL,RychlewskiL,etal.', 'XtalPred:awebserverforpredictionofprotein', 'crystallizability.', 'Bioinformatics.', '2007;23:3403–3405.', '[111] Jumper J, Evans R, Pritzel A, et al.', 'Highly accurate protein structure prediction with', 'AlphaFold.', 'Nature.', '2021;596:583–589.', '[112] Baek M, DiMaio F, Anishchenko I, et al.', 'Accurate prediction of protein structure and', 'interactionsusinga3-tracknetwork.Science.', '2021;373:871–876.', '[113] KrizhevskyA,SutskeverI,HintonGE.Imagenetclassificationwithdeepconvolutionalneural', 'networks.NIPS;25,Proceedingsofthe12thInternationalConferenceonNeuralInformationProcessingSystems;2012:1097–1105', '[114] MaiaFRNC.TheCoherentX-rayImagingDatabank.', 'NatMethods.', '2012;9:854–855.', '[115] Jarrett K, Kavukcuoglu K, Ranzato MA, et al.', 'What is the best multi-stage architecture', 'for object recognition?', ').', 'Proceedings of the 2009 IEEE 12th International Conference on', 'ComputerVision;2009:2146-2153.', '[116] Winter G, Waterman DG, Parkhurst JM, et al.', 'DIALS: implementation and evaluation of a', 'newintegrationpackage.', 'ActaCrystallogrD.', '2018;D74:85–97.', '[117] EmsleyP,CowtanK.Coot:model-buildingtoolsformoleculargraphics.', 'ActaCrystallogrD.', '2004;D60:2126–2132.', '[118] EmsleyP,LohkampB,ScottWG,etal.', 'Featuresanddevelopmentofcoot.', 'ActaCrystallogrD.', '2010;D66:486–501.', '[119] CowtanK.TheBuccaneersoftwareforautomatedmodelbuilding.1.', 'Tracingproteinchains.', 'ActaCrystallogrD.', '2006;D62:1002–1011.', '[120] Cowtan K. Fitting molecular fragments into electron density.', 'Acta Crystallogr D.', '2008;D64:83–89.', '[121] Lovell SC, Word JM, Richardson JS, et al.', 'The penultimate rotamer library.', 'Proteins.', '2000;40:389–408.', '[122] RamachandranGN,RamakrishnanC,SasisekharanV.Stereochemistryofpolypeptidechain', 'configuration.', 'JMolBiol.', '1963;7:95–99.', '[123] WangBC.Resolutionofphaseambiguityinmacromolecularcrystallography.', 'MethodsEnzy-', 'mol.1985;115:90–112.', '[124] Leslie AGW.', 'A reciprocal-space method for calculating a molecular envelope using the', 'algorithmofB.C.', 'Wang.', 'ActaCrystallogrA.', '1987;A43:134–136.']\n",
            " > Processing time: 108.5399317741394\n",
            " > Real-time factor: 0.2376627276047384\n",
            " > Saving output to AUDIO_OUTPUTS/page_46.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: 100 M. VOLLMAR AND G. EVANS\n",
            "[125] Karplus PA, Diederichs K. Assessing and maximising data quality in macromolecular crys-\n",
            "tallography.CurrOpinStrucBiol. 2015;34:60–68.\n",
            "[126] ChenL,OughtredR,BermanHM,etal.TargetDB :atargetregistrationdatabaseforstructural\n",
            "genomicsprojects.Bioinformatics. 2004;20:2860–2862.\n",
            "[127] SlabinskiL,JaroszewskiL,RodriguesAPC,etal.Thechallengeofproteinstructuredetermi-\n",
            "nation–lessonsfromstructuralgenomics.ProteinSci. 2007;16:2472–2482.\n",
            "[128] Genest C, Weerahandi S, Zidek JV. Aggregating opinions through logarithmic pooling.\n",
            "TheoryDecis. 1984;17:61–70.\n",
            "[129] YenS-J,LeeY-S.Cluster-basedunder-samplingapproachesforimbalancedatadistributions.\n",
            "ExpSystApplic. 2009;36:5718–5727.\n",
            "[130] Matthews BW. Comparison of the predicted and observed secondary structure of T4 phage\n",
            "lysozyme.BBA-ProteinStructM. 1975;405:442–451.\n",
            "[131] Rao HB, Zhu F, Yang GB, et al. Update of PROFEAT: a web server for computing structural\n",
            "and physicochemical features of proteins and peptides from amino acid sequence. Nucleic\n",
            "Acids Res. 2011;39:W385–W390.\n",
            "[132] Protlearn DT. Version 1. Posted: [cited 2020 Oct 3]. Available from: https://github.com/\n",
            "tadorfer/protlearn .\n",
            "[133] Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System). Proceedings of the\n",
            "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;2016:785–794\n",
            "[134] LeCun Y, Boser B, Denker JS, et al. Backpropagation applied to handwritten zip code\n",
            "recognition.NeuralComput. 1989;1:541–551.\n",
            "[135] LeCunY,BengioY,HintonG.Deeplearning.Nature. 2015;521:436–444.\n",
            "[136] Rawat W, Wang Z. Deep convolutional neural networks for image classification: A compre-\n",
            "hensivereview.NeuralComput. 2017;29:2352–2449.\n",
            "[137] Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for com-\n",
            "puter vision. Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\n",
            "Recognition. 2016:2818–2826.\n",
            "[138] Wilson J. Towards the automated evaluation of crystallization trials. Acta Crystallogr D.\n",
            "2002;D58:1907–1914.\n",
            "[139] Watts D, Cowtan K, Wilson J. Automated classification of crystallization experiments using\n",
            "waveletsandstatisticaltexturecharacterizationtechniques.JApplCryst. 2008;41:8–17.\n",
            "[140] GaoW,MahajanSP,SulamJ,etal.Deeplearninginproteinstructuralmodelinganddesign.\n",
            "Patterns.2020;1:100142.\n",
            "[141] Marks DS, Colwell LJ, Sheridan R, et al. Protein 3D structure computed from evolutionary\n",
            "sequencevariation.PLoSONE. 2011;6:e28766.\n",
            "[142] Asgari E, Mofrad MRK. Continuous distributed representation of biological sequences for\n",
            "deepproteomicsandgenomics.PLoSONE. 2015;10:e0141287.\n",
            "[143] Moult J, Pedersen JT, Judson R, et al. A large scale experiment to assess protein structure\n",
            "predictionmethods.Proteins. 1995;23:ii–iv.\n",
            "[144] Rohl CA, Strauss CEM, Misura KMS, et al. Protein structure prediction using rosetta.\n",
            "Methods Enzymol. 2004;383:66–93.\n",
            "[145] Kabsch W, Sander C. Dictionary of protein secondary structure: pattern recognition of\n",
            "hydrogen-bondedandgeometricalfeatures.Biopolymers. 1983;22:2577–2637.\n",
            "[146] LiuDC,NocedalJ.OnthelimitedmemoryBFGSmethodforlargescaleoptimization.Math\n",
            "Program. 1989;45:503–528.\n",
            "[147] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and\n",
            "translate.Posted:[cited2014Dec1]Availablefrom: https://arxiv.org/pdf/1409.0473.pdf .\n",
            "[148] Brown TB, Mann B, Ryder N, et al. Language Models are Few-Shot Learners. Posted: [cited\n",
            "2020May28. https://arxiv.org/pdf/2005.14165.pdf .\n",
            "[149] Devlin J, Chang M-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers\n",
            "forLanguageUnderstanding.Posted:October11,2018.Availablefrom: https://arxiv.org/pdf/\n",
            "1810.04805.pdf.\n",
            " > Text splitted to sentences.\n",
            "['100 M. VOLLMAR AND G. EVANS', '[125] Karplus PA, Diederichs K. Assessing and maximising data quality in macromolecular crys-', 'tallography.', 'CurrOpinStrucBiol.', '2015;34:60–68.', '[126] ChenL,OughtredR,BermanHM,etal.', 'TargetDB :atargetregistrationdatabaseforstructural', 'genomicsprojects.', 'Bioinformatics.', '2004;20:2860–2862.', '[127] SlabinskiL,JaroszewskiL,RodriguesAPC,etal.', 'Thechallengeofproteinstructuredetermi-', 'nation–lessonsfromstructuralgenomics.', 'ProteinSci.', '2007;16:2472–2482.', '[128] Genest C, Weerahandi S, Zidek JV.', 'Aggregating opinions through logarithmic pooling.', 'TheoryDecis.', '1984;17:61–70.', '[129] YenS-J,LeeY-S.Cluster-basedunder-samplingapproachesforimbalancedatadistributions.', 'ExpSystApplic.', '2009;36:5718–5727.', '[130] Matthews BW.', 'Comparison of the predicted and observed secondary structure of T4 phage', 'lysozyme.', 'BBA-ProteinStructM.', '1975;405:442–451.', '[131] Rao HB, Zhu F, Yang GB, et al.', 'Update of PROFEAT: a web server for computing structural', 'and physicochemical features of proteins and peptides from amino acid sequence.', 'Nucleic', 'Acids Res. 2011;39:W385–W390.', '[132] Protlearn DT.', 'Version 1.', 'Posted: [cited 2020 Oct 3].', 'Available from: https://github.com/', 'tadorfer/protlearn .', '[133] Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System).', 'Proceedings of the', '22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;2016:785–794', '[134] LeCun Y, Boser B, Denker JS, et al.', 'Backpropagation applied to handwritten zip code', 'recognition.', 'NeuralComput.', '1989;1:541–551.', '[135] LeCunY,BengioY,HintonG.Deeplearning.', 'Nature.', '2015;521:436–444.', '[136] Rawat W, Wang Z. Deep convolutional neural networks for image classification: A compre-', 'hensivereview.', 'NeuralComput.', '2017;29:2352–2449.', '[137] Szegedy C, Vanhoucke V, Ioffe S, et al.', 'Rethinking the inception architecture for com-', 'puter vision.', 'Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern', 'Recognition.', '2016:2818–2826.', '[138] Wilson J. Towards the automated evaluation of crystallization trials.', 'Acta Crystallogr D.', '2002;D58:1907–1914.', '[139] Watts D, Cowtan K, Wilson J. Automated classification of crystallization experiments using', 'waveletsandstatisticaltexturecharacterizationtechniques.', 'JApplCryst.', '2008;41:8–17.', '[140] GaoW,MahajanSP,SulamJ,etal.', 'Deeplearninginproteinstructuralmodelinganddesign.', 'Patterns.2020;1:100142.', '[141] Marks DS, Colwell LJ, Sheridan R, et al.', 'Protein 3D structure computed from evolutionary', 'sequencevariation.', 'PLoSONE.', '2011;6:e28766.', '[142] Asgari E, Mofrad MRK.', 'Continuous distributed representation of biological sequences for', 'deepproteomicsandgenomics.', 'PLoSONE.', '2015;10:e0141287.', '[143] Moult J, Pedersen JT, Judson R, et al.', 'A large scale experiment to assess protein structure', 'predictionmethods.', 'Proteins.', '1995;23:ii–iv.', '[144] Rohl CA, Strauss CEM, Misura KMS, et al.', 'Protein structure prediction using rosetta.', 'Methods Enzymol.', '2004;383:66–93.', '[145] Kabsch W, Sander C. Dictionary of protein secondary structure: pattern recognition of', 'hydrogen-bondedandgeometricalfeatures.', 'Biopolymers.', '1983;22:2577–2637.', '[146] LiuDC,NocedalJ.OnthelimitedmemoryBFGSmethodforlargescaleoptimization.', 'Math', 'Program.', '1989;45:503–528.', '[147] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and', 'translate.', 'Posted:[cited2014Dec1]Availablefrom: https://arxiv.org/pdf/1409.0473.pdf .', '[148] Brown TB, Mann B, Ryder N, et al.', 'Language Models are Few-Shot Learners.', 'Posted: [cited', '2020May28.', 'https://arxiv.org/pdf/2005.14165.pdf .', '[149] Devlin J, Chang M-W, Lee K, et al.', 'BERT: Pre-training of Deep Bidirectional Transformers', 'forLanguageUnderstanding.', 'Posted:October11,2018.Availablefrom: https://arxiv.org/pdf/', '1810.04805.pdf.']\n",
            " > Processing time: 114.92262506484985\n",
            " > Real-time factor: 0.2402239715748048\n",
            " > Saving output to AUDIO_OUTPUTS/page_47.wav\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: CRYSTALLOGRAPHY REVIEWS 101\n",
            "[150] Cornell WD, Cieplak P, Bayly CI, et al. A second generation force field for the simulation of\n",
            "proteins,NucleicAcids,andorganicmolecules.JAmChemSoc. 1995;117:5179–5197.\n",
            "[151] Haas J, Barbato A, Behringer D, et al. Continuous automated model EvaluatOn (CAMEO)\n",
            "complementing the critical assessment of structure prediction in CASP12. Proteins.\n",
            "2018;86(Suppl1):387–398.\n",
            "[152] Ragoza M, Hochuli J, Idrobo E, et al. Protein-Ligand scoring with convolutional neural\n",
            "networks.JChemInfModel. 2017;57:942–957.\n",
            "[153] ImrieF,BradleyAR,vanderSchaarM,etal.Proteinfamily-specificmodelsusingdeepneural\n",
            "networks and transfer learning improve virtual screening and highlight the need for moredata.JChemInfModel. 2018;58:2319–2330.\n",
            "Subject Index\n",
            "(Artificial)neuralnetwork,(A)NN58,60,61,62,66,75,78,79,86\n",
            "Backpropagation60,76Bayesian63,66Boosting61,65,85(Class/sample)distribution60,67,68,69,70,74Classification60,62,63,65Confusionmatrix68,72CriticalAssessmentofproteinStructurePrediction,CASP57,91,92Cryoelectronmicroscopy,cryo-EM56,66,92Deeplearners/learning55,57,58,66Dimensionality63,67,70Exploratorydataanalysis,EDA63,67Feedforward60Im/balance(ddata)70,76,81,82,85,87,88Macromolecular(X-ray)crystallography,MX55,57,66,93(Multi-layer)perceptron66,78Performance60,63,68,72,73,74,96Receiver-operatingcharacteristic,ROC73Regression62,63,65,66,76,80\n",
            "Shallowlearners/learning65,66\n",
            "Un/supervisedlearning62,63,72\n",
            " > Text splitted to sentences.\n",
            "['CRYSTALLOGRAPHY REVIEWS 101', '[150] Cornell WD, Cieplak P, Bayly CI, et al.', 'A second generation force field for the simulation of', 'proteins,NucleicAcids,andorganicmolecules.', 'JAmChemSoc.', '1995;117:5179–5197.', '[151] Haas J, Barbato A, Behringer D, et al.', 'Continuous automated model EvaluatOn (CAMEO)', 'complementing the critical assessment of structure prediction in CASP12.', 'Proteins.', '2018;86(Suppl1):387–398.', '[152] Ragoza M, Hochuli J, Idrobo E, et al.', 'Protein-Ligand scoring with convolutional neural', 'networks.JChemInfModel.', '2017;57:942–957.', '[153] ImrieF,BradleyAR,vanderSchaarM,etal.Proteinfamily-specificmodelsusingdeepneural', 'networks and transfer learning improve virtual screening and highlight the need for moredata.', 'JChemInfModel.', '2018;58:2319–2330.', 'Subject Index', '(Artificial)neuralnetwork,(A)NN58,60,61,62,66,75,78,79,86', 'Backpropagation60,76Bayesian63,66Boosting61,65,85(Class/sample)distribution60,67,68,69,70,74Classification60,62,63,65Confusionmatrix68,72CriticalAssessmentofproteinStructurePrediction,CASP57,91,92Cryoelectronmicroscopy,cryo-EM56,66,92Deeplearners/learning55,57,58,66Dimensionality63,67,70Exploratorydataanalysis,EDA63,67Feedforward60Im/balance(ddata)70,76,81,82,85,87,88Macromolecular(X-ray)crystallography,MX55,57,66,93(Multi-layer)perceptron66,78Performance60,63,68,72,73,74,96Receiver-operatingcharacteristic,ROC73Regression62,63,65,66,76,80', 'Shallowlearners/learning65,66', 'Un/supervisedlearning62,63,72']\n",
            " > Processing time: 85.6753568649292\n",
            " > Real-time factor: 0.39263930664383384\n",
            " > Saving output to AUDIO_OUTPUTS/page_48.wav\n",
            "1423\n"
          ]
        }
      ],
      "source": [
        "from calendar import c\n",
        "from librosa import ex\n",
        "from regex import E\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "#!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    chunk_idx =0\n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text = page.extract_text()\n",
        "        if chunk_idx <=9:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "        else:\n",
        "            cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "        \n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, shell=True)\n",
        "            chunk_idx += 1\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            # try again but drop the last 5 sentences\n",
        "            text = text.rsplit(\".\", 5)[0]\n",
        "            if chunk_idx <=9:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_0{chunk_idx}.wav'\n",
        "            else:\n",
        "                cmd = f'tts --text \"{text}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path AUDIO_OUTPUTS/page_{chunk_idx}.wav'\n",
        "            try:\n",
        "                subprocess.run(cmd, check=True, shell=True)\n",
        "                chunk_idx += 1\n",
        "            except Exception as e:\n",
        "                # skip this chunk\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                chunk_idx += 1\n",
        "                continue\n",
        "\n",
        "        \n",
        "\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "\n",
        "pdf = \"/Users/tomriddle1/Documents/Machine learning applications in macromolecular X-ray crystallography.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Get device\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# List available 🐸TTS models\n",
        "#print(TTS().list_models())\n",
        "\n",
        "# Init TTS\n",
        "#tts = TTS(\"tts_models/en/ljspeech/vits\").to(device)\n",
        "\n",
        "# Run TTS\n",
        "# Text to speech to a file\n",
        "#tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n",
        "# Init TTS with the target model name\n",
        "#tts = TTS(model_name=\"tts_models/en/ljspeech/vits\", progress_bar=True).to(device)\n",
        "\n",
        "# Run TTS\n",
        "#tts.tts_to_file(text=text, file_path=\"output.wav\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.0.1)\n",
            "142265\n",
            "880\n",
            " > tts_models/en/ljspeech/vits is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Text: fFull Terms & Conditions of access and use can be found at\n",
            "https://www.tandfonline.com/action/journalInformation?journalCode=gcry20\n",
            "Crystallography Reviews\n",
            "ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/gcry20\n",
            "Machine learning applications in macromolecular\n",
            "X-ray crystallography\n",
            "Melanie Vollmar & Gwyndaf Evans\n",
            "To cite this article:  Melanie Vollmar & Gwyndaf Evans (2021) Machine learning applications\n",
            "in macromolecular X-ray crystallography, Crystallography Reviews, 27:2, 54-101, DOI:\n",
            "10.1080/0889311X.2021.1982914\n",
            "To link to this article:  https://doi.org/10.1080/0889311X.2021.1982914\n",
            "© 2021 The Author(s)\n",
            " > Text splitted to sentences.\n",
            "['fFull Terms & Conditions of access and use can be found at', 'https://www.tandfonline.com/action/journalInformation?', 'journalCode=gcry20', 'Crystallography Reviews', 'ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/gcry20', 'Machine learning applications in macromolecular', 'X-ray crystallography', 'Melanie Vollmar & Gwyndaf Evans', 'To cite this article:  Melanie Vollmar & Gwyndaf Evans (2021) Machine learning applications', 'in macromolecular X-ray crystallography, Crystallography Reviews, 27:2, 54-101, DOI:', '10.1080/0889311X.2021.1982914', 'To link to this article:  https://doi.org/10.1080/0889311X.2021.1982914', '© 2021 The Author(s)']\n",
            " > Processing time: 36.32368087768555\n",
            " > Real-time factor: 0.4318484148965242\n",
            " > Saving output to /Users/tomriddle1/Downloads/COLLEGE/DR_BRYAN/output.wav\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from torch import le \n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    # importing required modules \n",
        "    text = \"\"\n",
        "    # creating a pdf reader object \n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages\n",
        "    \n",
        "    # printing number of pages in pdf file \n",
        "    #print(len(reader.pages)) \n",
        "    \n",
        "    # getting a specific page from the pdf file \n",
        "    #page = reader.pages[0] \n",
        "    \n",
        "    # extracting text from page \n",
        "    for page in pages:\n",
        "        text += page.extract_text()\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "    return text\n",
        "#pdf = \"/Users/tomriddle1/Documents/Dr_Bryan_Chemisty.pdf\"\n",
        "pdf = \"/Users/tomriddle1/Documents/Machine learning applications in macromolecular X-ray crystallography.pdf\"\n",
        "text = pdf_to_text(pdf)\n",
        "print(len(text))\n",
        "# Break text into chunks of 5000 words\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "text_chunks = split_to_sentences(text)\n",
        "print(len(text_chunks))\n",
        "!tts --text f\"{text_chunks[0]}\" --model_name \"tts_models/en/ljspeech/vits\" --out_path \"/Users/tomriddle1/Downloads/COLLEGE/DR_BRYAN/output.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda clean --packages -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
