{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: paper-qa in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (3.13.4)\n",
            "Requirement already satisfied: pypdf in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.17.4)\n",
            "Requirement already satisfied: pydantic<2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (1.10.13)\n",
            "Collecting langchain>=0.0.303 (from paper-qa)\n",
            "  Downloading langchain-0.0.353-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.27.8)\n",
            "Requirement already satisfied: faiss-cpu in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (1.7.4)\n",
            "Requirement already satisfied: PyCryptodome in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (3.19.0)\n",
            "Requirement already satisfied: html2text in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (2020.1.16)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-qa) (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (3.8.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (0.0.6)\n",
            "Collecting langchain-core<0.2,>=0.1.4 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading langchain_core-0.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.70 (from langchain>=0.0.303->paper-qa)\n",
            "  Downloading langsmith-0.0.75-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (1.24.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain>=0.0.303->paper-qa) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from openai<1->paper-qa) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<2->paper-qa) (4.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from tiktoken>=0.4.0->paper-qa) (2023.10.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->paper-qa) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.4->langchain>=0.0.303->paper-qa) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.4->langchain>=0.0.303->paper-qa)\n",
            "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.303->paper-qa) (2023.7.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain>=0.0.303->paper-qa) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.303->paper-qa) (1.0.0)\n",
            "Downloading langchain-0.0.353-py3-none-any.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.4-py3-none-any.whl (205 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Installing collected packages: packaging, langsmith, langchain-core, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.38\n",
            "    Uninstalling langsmith-0.0.38:\n",
            "      Successfully uninstalled langsmith-0.0.38\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.3\n",
            "    Uninstalling langchain-core-0.1.3:\n",
            "      Successfully uninstalled langchain-core-0.1.3\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.292\n",
            "    Uninstalling langchain-0.0.292:\n",
            "      Successfully uninstalled langchain-0.0.292\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 4.12.0 requires pydantic>=2.0, but you have pydantic 1.10.13 which is incompatible.\n",
            "aider-chat 0.19.1 requires certifi==2023.11.17, but you have certifi 2023.7.22 which is incompatible.\n",
            "aider-chat 0.19.1 requires charset-normalizer==3.3.2, but you have charset-normalizer 3.2.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires idna==3.6, but you have idna 3.4 which is incompatible.\n",
            "aider-chat 0.19.1 requires numpy==1.26.2, but you have numpy 1.24.0 which is incompatible.\n",
            "aider-chat 0.19.1 requires openai==1.3.7, but you have openai 0.27.8 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic==2.5.2, but you have pydantic 1.10.13 which is incompatible.\n",
            "aider-chat 0.19.1 requires pydantic-core==2.14.5, but you have pydantic-core 2.14.6 which is incompatible.\n",
            "aider-chat 0.19.1 requires tiktoken==0.5.2, but you have tiktoken 0.5.1 which is incompatible.\n",
            "aider-chat 0.19.1 requires urllib3==2.1.0, but you have urllib3 1.26.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.0.353 langchain-core-0.1.4 langsmith-0.0.75 packaging-23.2\n",
            "Collecting git+https://github.com/blackadad/paper-scraper.git\n",
            "  Cloning https://github.com/blackadad/paper-scraper.git to /private/var/folders/q6/z6_5lkkx431989t_6fhf1m2w0000gn/T/pip-req-build-w2ix3org\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/blackadad/paper-scraper.git /private/var/folders/q6/z6_5lkkx431989t_6fhf1m2w0000gn/T/pip-req-build-w2ix3org\n",
            "  Resolved https://github.com/blackadad/paper-scraper.git to commit 6588a9dab249ff7989c362cc1847fee372adfccc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-scraper==1.1.3) (3.8.5)\n",
            "Requirement already satisfied: pybtex in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-scraper==1.1.3) (0.24.0)\n",
            "Requirement already satisfied: pypdf in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from paper-scraper==1.1.3) (3.17.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp->paper-scraper==1.1.3) (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pybtex->paper-scraper==1.1.3) (6.0.1)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pybtex->paper-scraper==1.1.3) (2.0.1)\n",
            "Requirement already satisfied: six in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pybtex->paper-scraper==1.1.3) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->paper-scraper==1.1.3) (3.4)\n",
            "Requirement already satisfied: sentence-transformers in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (4.36.2)\n",
            "Requirement already satisfied: tqdm in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: torchvision in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.16.2)\n",
            "Requirement already satisfied: numpy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.24.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: filelock in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
            "Requirement already satisfied: requests in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: langchain in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (0.0.353)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (0.0.6)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (0.1.4)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (0.0.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (1.24.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.4->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.4->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!pip install paper-qa\n",
        "!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:langchain_community.utils.math:Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=189 request_id=1eab5ceb59d2a883e141b97b7cfd9955 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=983 request_id=ff0d4c297c52eb18849c35017a3f012c response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3135 request_id=b830a96fac97e41e17e41b0e51bd8690 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3155 request_id=528217c39d47726fc80d6b524979aec6 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3579 request_id=abfc95dec5bff84667f3a6c3ec94c402 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2982 request_id=e27adce487f6b2276aee4cec9d7c627c response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=495 request_id=b399528ca45a49045ec666f2bed702c8 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3636 request_id=187c45e06b2aa5c72e2ab4deea8bda10 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2517 request_id=6e75048feba981a8b41014267808d0d3 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3271 request_id=84f49b0612adb4b62ea67f48781fc166 response_code=200\n",
            "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4594 request_id=47262a3a11af7101d29102ec4548e13a response_code=200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\n",
            "\n",
            "The current understanding is that mitochondria play a central role in animal regeneration and aging. They generate key epigenetic modifiers that regulate aging and are involved in various cellular processes such as inflammation, apoptosis, and tissue regeneration. Studies have shown that intervention strategies targeting mitochondrial metabolism, such as injecting intact functional mitochondria or transferring healthy mitochondria, can improve cellular and tissue function, including cognitive performance, muscle atrophy, and tissue regeneration. Future research directions include further investigating the mechanisms of mitochondrial regulation in regeneration and aging, exploring conserved mechanisms across different types of animal regeneration, optimizing delivery methods, and understanding the interplay between mitochondrial function and other cellular processes involved in regeneration (Zhao2023 pages 7-8, Zhao2023 pages 5-5, Zhao2023 pages 1-1, Liu2022 pages 16-16). Efforts may also involve utilizing the regulatory machinery present in biological systems to achieve whole-body regeneration (Zhao2023 pages 7-8).\n",
            "\n",
            "References\n",
            "\n",
            "1. (Zhao2023 pages 7-8): Zhao, Yun, et al. \"Emerging roles of mitochondria in animal regeneration.\" Cell Regeneration, vol. 12, no. 14, 2023, pp. 1-16. doi:10.1186/s13619-023-00158-7.\n",
            "\n",
            "2. (Zhao2023 pages 5-5): Zhao, Yun, et al. \"Emerging roles of mitochondria in animal regeneration.\" Cell Regeneration, vol. 12, no. 14, 2023, pp. 1-16. doi:10.1186/s13619-023-00158-7.\n",
            "\n",
            "3. (Zhao2023 pages 1-1): Zhao, Yun, et al. \"Emerging roles of mitochondria in animal regeneration.\" Cell Regeneration, vol. 12, no. 14, 2023, pp. 1-16. doi:10.1186/s13619-023-00158-7.\n",
            "\n",
            "4. (Liu2022 pages 16-16): Liu, Zonghan, et al. \"Mitochondrial transfer/transplantation: an emerging therapeutic approach for multiple diseases.\" Cell & Bioscience, vol. 12, no. 1, 2022, p. 66. doi:10.1186/s13578-022-00805-7.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialization successful.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/ast.py:276: RuntimeWarning: coroutine 'Docs.aquery' was never awaited\n",
            "  for item in field:\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaperqa\u001b[39;00m\n\u001b[1;32m     12\u001b[0m keyword_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbispecific antibody manufacture\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mpaperscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword_search\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m docs \u001b[38;5;241m=\u001b[39m paperqa\u001b[38;5;241m.\u001b[39mDocs(openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path,data \u001b[38;5;129;01min\u001b[39;00m papers\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:578\u001b[0m, in \u001b[0;36msearch_papers\u001b[0;34m(query, limit, pdir, semantic_scholar_api_key, _paths, _limit, _offset, logger, year, verbose, scraper, batch_size, search_type)\u001b[0m\n\u001b[1;32m    576\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    577\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_search_papers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nest_asyncio.py:116\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     heappop(scheduled)\n\u001b[1;32m    111\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    114\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    119\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/selectors.py:561\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mcontrol(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "This event loop is already running",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaperscraper\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mpaperscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbayesian model selection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded-papers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/paperscraper/lib.py:578\u001b[0m, in \u001b[0;36msearch_papers\u001b[0;34m(query, limit, pdir, semantic_scholar_api_key, _paths, _limit, _offset, logger, year, verbose, scraper, batch_size, search_type)\u001b[0m\n\u001b[1;32m    576\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    577\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_search_papers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_scholar_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    632\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/asyncio/base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 588\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ],
      "source": [
        "import paperscraper\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=10,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nougat-ocr\n",
            "  Downloading nougat_ocr-0.1.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (4.36.2)\n",
            "Collecting timm==0.5.4 (from nougat-ocr)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (3.9.10)\n",
            "Collecting opencv-python-headless (from nougat-ocr)\n",
            "  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: datasets[vision] in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (2.16.0)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr)\n",
            "  Downloading lightning-2.1.3-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (3.8.1)\n",
            "Collecting python-Levenshtein (from nougat-ocr)\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (0.1.99)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Collecting albumentations>=1.0.0 (from nougat-ocr)\n",
            "  Downloading albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: pypdf>=3.1.0 in ./.conda/lib/python3.11/site-packages (from nougat-ocr) (3.17.4)\n",
            "Collecting pypdfium2 (from nougat-ocr)\n",
            "  Downloading pypdfium2-4.25.0-py3-none-macosx_11_0_arm64.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in ./.conda/lib/python3.11/site-packages (from timm==0.5.4->nougat-ocr) (2.1.2)\n",
            "Requirement already satisfied: torchvision in ./.conda/lib/python3.11/site-packages (from timm==0.5.4->nougat-ocr) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in ./.conda/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr) (1.24.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in ./.conda/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr) (1.11.4)\n",
            "Collecting scikit-image>=0.16.1 (from albumentations>=1.0.0->nougat-ocr)\n",
            "  Downloading scikit_image-0.22.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML in ./.conda/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr) (6.0.1)\n",
            "Collecting qudida>=0.0.4 (from albumentations>=1.0.0->nougat-ocr)\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in ./.conda/lib/python3.11/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr) (2023.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in ./.conda/lib/python3.11/site-packages (from lightning<2022,>=2.0.0->nougat-ocr) (23.1)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in ./.conda/lib/python3.11/site-packages (from lightning<2022,>=2.0.0->nougat-ocr) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in ./.conda/lib/python3.11/site-packages (from lightning<2022,>=2.0.0->nougat-ocr) (4.8.0)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (0.20.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (2023.10.3)\n",
            "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./.conda/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (0.3.7)\n",
            "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (2.1.1)\n",
            "Requirement already satisfied: xxhash in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (3.8.5)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in ./.conda/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr) (10.1.0)\n",
            "Requirement already satisfied: click in ./.conda/lib/python3.11/site-packages (from nltk->nougat-ocr) (8.1.7)\n",
            "Requirement already satisfied: joblib in ./.conda/lib/python3.11/site-packages (from nltk->nougat-ocr) (1.3.2)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein->nougat-ocr)\n",
            "  Downloading Levenshtein-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in ./.conda/lib/python3.11/site-packages (from Levenshtein==0.23.0->python-Levenshtein->nougat-ocr) (3.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.3.1)\n",
            "Requirement already satisfied: setuptools in ./.conda/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2022,>=2.0.0->nougat-ocr) (68.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in ./.conda/lib/python3.11/site-packages (from qudida>=0.0.4->albumentations>=1.0.0->nougat-ocr) (1.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr) (2023.7.22)\n",
            "Requirement already satisfied: networkx>=2.8 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr) (3.2.1)\n",
            "Collecting imageio>=2.27 (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr)\n",
            "  Downloading imageio-2.33.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tifffile>=2022.8.12 (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr)\n",
            "  Downloading tifffile-2023.12.9-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting lazy_loader>=0.3 (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr)\n",
            "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: sympy in ./.conda/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (1.12)\n",
            "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr) (2023.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations>=1.0.0->nougat-ocr) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->torch>=1.4->timm==0.5.4->nougat-ocr) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.11/site-packages (from sympy->torch>=1.4->timm==0.5.4->nougat-ocr) (1.3.0)\n",
            "Downloading nougat_ocr-0.1.17-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.25.0-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading Levenshtein-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Downloading scikit_image-0.22.0-cp311-cp311-macosx_12_0_arm64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.33.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.5/134.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2023.12.9-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tifffile, ruamel.yaml.clib, pypdfium2, opencv-python-headless, munch, lightning-utilities, Levenshtein, lazy_loader, imageio, scikit-image, ruamel.yaml, python-Levenshtein, torchmetrics, sconf, qudida, timm, pytorch-lightning, albumentations, lightning, nougat-ocr\n",
            "Successfully installed Levenshtein-0.23.0 albumentations-1.3.1 imageio-2.33.1 lazy_loader-0.3 lightning-2.1.3 lightning-utilities-0.10.0 munch-4.0.0 nougat-ocr-0.1.17 opencv-python-headless-4.8.1.78 pypdfium2-4.25.0 python-Levenshtein-0.23.0 pytorch-lightning-2.1.3 qudida-0.0.4 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 scikit-image-0.22.0 sconf-0.2.5 tifffile-2023.12.9 timm-0.5.4 torchmetrics-1.2.1\n",
            "downloading nougat checkpoint version 0.1.0-small to path /Users/tomriddle1/.cache/torch/hub/nougat-0.1.0-small\n",
            "config.json: 100%|█████████████████████████████| 557/557 [00:00<00:00, 1.05Mb/s]\n",
            "pytorch_model.bin: 100%|█████████████████████| 956M/956M [00:17<00:00, 56.9Mb/s]\n",
            "special_tokens_map.json: 100%|████████████████| 96.0/96.0 [00:00<00:00, 358kb/s]\n",
            "tokenizer.json: 100%|██████████████████████| 2.04M/2.04M [00:00<00:00, 15.3Mb/s]\n",
            "tokenizer_config.json: 100%|████████████████████| 106/106 [00:00<00:00, 863kb/s]\n",
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0%|                                                     | 0/3 [03:12<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/bin/nougat\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/predict.py\", line 167, in main\n",
            "    model_output = model.inference(\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nougat/model.py\", line 580, in inference\n",
            "    last_hidden_state = self.encoder(image_tensors)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nougat/model.py\", line 123, in forward\n",
            "    x = self.model.layers(x)\n",
            "        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 413, in forward\n",
            "    x = blk(x)\n",
            "        ^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 295, in forward\n",
            "    attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 188, in forward\n",
            "    attn = attn + relative_position_bias.unsqueeze(0)\n",
            "           ~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "RuntimeError: MPS backend out of memory (MPS allocated: 8.62 GB, other allocations: 449.77 MB, max allowed: 9.07 GB). Tried to allocate 28.14 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n"
          ]
        }
      ],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (1.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import A\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "\n",
        "\n",
        "class InitialSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n",
        "    yet highly non-specific, containing little information beyond the entities marked as missing.\n",
        "    Use overly verbose languages and fillers (Eg. This text discusses) to reach ~80 words.\n",
        "    \"\"\"\n",
        "\n",
        "    summary: str = Field(\n",
        "        ...,\n",
        "        description=\"This is a summary of the text provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n",
        "    )\n",
        "\n",
        "\n",
        "class RewrittenSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n",
        "\n",
        "    Guidelines\n",
        "    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n",
        "    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
        "    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Text.\n",
        "    - Make space with fusion, compression, and removal of uninformative phrases like \"the text discusses\"\n",
        "    - Missing entities can appear anywhere in the new summary\n",
        "\n",
        "    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n",
        "    \"\"\"\n",
        "    summary: str = Field(\n",
        "        ...,\n",
        "        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n",
        "    )\n",
        "    absent: List[str] = Field(\n",
        "        ...,\n",
        "        default_factory=list,\n",
        "        description=\"This is a list of Entities found absent from the new summary that were present in the previous summary\",\n",
        "    )\n",
        "    missing: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"This is a list of 1-3 informative Entities from the Text that are missing from the new summary which should be included in the next generated summary.\",\n",
        "    )\n",
        "\n",
        "\n",
        "    @field_validator(\"missing\")\n",
        "    def has_missing_entities(cls, missing_entities: List[str]):\n",
        "        if len(missing_entities) == 0:\n",
        "            raise ValueError(\n",
        "                \"You must identify 1-3 informative Entities from the Text which are missing from the previously generated summary to be used in a new summary\"\n",
        "            )\n",
        "        return missing_entities\n",
        "\n",
        "    @field_validator(\"absent\")\n",
        "    def has_no_absent_entities(cls, absent_entities: List[str]):\n",
        "        absent_entity_string = \",\".join(absent_entities)\n",
        "        if len(absent_entities) > 0:\n",
        "            print(f\"Detected absent entities of {absent_entity_string}\")\n",
        "        return absent_entities\n",
        "    \n",
        "\n",
        "def summarize_text(text):\n",
        "        \"\"\"\n",
        "        Summarizes the text using OpenAI's GPT-3.5 Turbo model.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            system_message = \"\"\"# Mission\n",
        "Craft a Sparse Priming Representation (SPR) for a given set of information. The goal is to distill complex concepts into concise, context-rich statements that enable a Large Language Model (LLM) to reconstruct the original idea efficiently.\n",
        "\n",
        "# Persona\n",
        "You are an SPR Writer, skilled in synthesizing complex information into its essential elements. You have a talent for identifying the core of an idea and expressing it in a minimal, yet comprehensive way.\n",
        "\n",
        "# Guiding Principles\n",
        "- **Precision**: Focus on the most crucial elements of the information, ensuring each word adds significant value.\n",
        "- **Clarity**: Maintain clarity in your representations to avoid ambiguity.\n",
        "- **Context Preservation**: Retain enough context to enable accurate reconstruction of the original idea.\n",
        "- **Efficiency**: Aim for the minimal number of words to convey the concept without loss of meaning.\n",
        "\n",
        "# Task\n",
        "1. **Receive Information**: Begin with the user-provided content that needs to be compressed into an SPR.\n",
        "2. **Identify Key Concepts**: Analyze the content to pinpoint its fundamental ideas, themes, or messages.\n",
        "3. **Distill Information**: Transform these key concepts into short, complete sentences that encapsulate the essence of the content.\n",
        "4. **Contextual Embedding**: Ensure that these sentences include necessary context for understanding and reconstruction.\n",
        "5. **Review for Completeness**: Confirm that the SPR conveys the core idea effectively and is free of extraneous details.\n",
        "\n",
        "# Style\n",
        "- **Concise and Direct**: Use clear and straightforward language, avoiding unnecessary embellishments.\n",
        "- **Analytical**: Demonstrate a keen understanding of the essential aspects of the information.\n",
        "\n",
        "# Rules\n",
        "- **No Superfluous Details**: Eliminate any information that doesn’t contribute to understanding the core concept.\n",
        "- **Maintain Integrity of Original Idea**: Ensure that the SPR accurately represents the original content's intent and meaning.\n",
        "- **Brevity is Key**: Strive for the shortest possible representation without losing essential context or meaning.\n",
        "\n",
        "# Output Format\n",
        "\"YOUR COMPRESION HERE (DONT WASTE TIME STATING THIS IN AN SPR JUST WRITE)\"\"\"\n",
        "            user_message = text\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                response_model=InitialSummary  # Adjust as needed\n",
        "            )\n",
        "            return response \n",
        "        except Exception as e:\n",
        "            print(f\"Error in summarizing text: {e}\")\n",
        "            return \"Summary not available.\" \n",
        "\n",
        "def summarize_article(article: str, summary_steps: int = 3):\n",
        "    summary_chain = []\n",
        "\n",
        "    summary = summarize_text(text=article)   \n",
        "    prev_summary = None\n",
        "    summary_chain.append(summary)\n",
        "    total_missing_entities = []\n",
        "    temp = 0.3\n",
        "    try:\n",
        "        for i in range(summary_steps):\n",
        "\n",
        "            missing_entity_message = (\n",
        "                []\n",
        "                if prev_summary is None\n",
        "                else [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"The summary MUST include these Missing Entities: {','.join(prev_summary.missing)}\",\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "            if prev_summary is not None:\n",
        "                total_missing_entities.extend(prev_summary.missing)\n",
        "            new_summary: RewrittenSummary = client.chat.completions.create( \n",
        "\n",
        "\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"\n",
        "### Chain of Density Prompt for Creating Entity-Dense Summaries\n",
        "\n",
        "#### Prompt Introduction and Objective\n",
        "You are tasked with generating progressively concise and informative summaries of an text, focusing on including key entities. Begin with a summary that captures the essence of the text and then refine it in subsequent iterations to increase its density of relevant information.\n",
        "\n",
        "#### Revised Steps for Summary Creation\n",
        "1. **Initial Summary**: Start by writing a comprehensive summary of the text. Focus on capturing the core ideas of the text while maintaining clarity and conciseness. Avoid unnecessary verbosity.\n",
        "\n",
        "2. **Identifying Key Entities**:\n",
        "   - In each iteration, identify any key entities from the text that were not included in your previous summary.\n",
        "   - A key entity should be:\n",
        "     - **Relevant**: Directly related to the main story or themes of the text.\n",
        "     - **Specific**: Descriptive yet concise, ideally in 5 words or fewer.\n",
        "     - **Novel**: Not already included in your previous summary.\n",
        "     - **Faithful**: Accurately represented as in the text.\n",
        "\n",
        "3. **Refining the Summary**:\n",
        "   - Rewrite the summary to include the newly identified key entities. Aim for the same length as the previous summary, enhancing its density and informativeness.\n",
        "   - Employ techniques like fusion, compression, and the removal of redundant phrases to make room for new entities.\n",
        "   - Ensure that all previously included entities and details are retained in each new version of the summary.\n",
        "\n",
        "4. **Repetition and Refinement**:\n",
        "   - Repeat this process for a total of 5 iterations, each time enhancing the summary's entity density and conciseness.\n",
        "\n",
        "#### Guidelines for Effective Summary Writing\n",
        "- **Clarity and Cohesion**: Ensure each version of the summary is clear, cohesive, and can stand alone as a comprehensive overview of the text.\n",
        "- **Balanced Inclusion**: If space is limited, prioritize the most impactful entities for inclusion. Do not sacrifice clarity for the sake of adding more entities.\n",
        "- **Consistency**: Maintain a consistent approach to summarizing, ensuring that each iteration builds logically on the previous one.\n",
        "\n",
        "#### Expected Outcome\n",
        "By the end of the fifth iteration, you should have a highly concise, entity-dense summary that encapsulates the main points and key entities of the text in a clear and accessible manner.\n",
        "                    \"\"\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": f\"Here is the Text: {article}\"},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n",
        "                    },\n",
        "                    *missing_entity_message,\n",
        "                ],\n",
        "                max_retries=3, \n",
        "\n",
        "                temperature=temp,\n",
        "                max_tokens=1000,\n",
        "                response_model=RewrittenSummary,\n",
        "            )\n",
        "            summary_chain.append(new_summary.summary)\n",
        "            print(f\"Summary {i+1}: {new_summary.summary}\")\n",
        "            prev_summary = new_summary\n",
        "            temp = temp + 0.3\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article1: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        # Create the file if it doesn't exist\n",
        "        file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "        # Check if file exists and is not empty\n",
        "        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump({}, f)  # Initialize with an empty dictionary\n",
        "                \n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory.keys():\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"] = {\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "        return str(summary_chain[-1])\n",
        "         \n",
        "\n",
        "    file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "    # Check if file exists and is not empty\n",
        "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump({}, f)  # Initialize with an empty dictionary\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory:\n",
        "                # Ensure summary_chain is a list\n",
        "                if not isinstance(memory[\"summary_chain\"], list):\n",
        "                    memory[\"summary_chain\"] = []\n",
        "\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                # Initialize summary_chain as a list with the first element\n",
        "                memory[\"summary_chain\"] = [{\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}]\n",
        "\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error reading JSON file. The file may be corrupted or improperly formatted.\")\n",
        "        # Handle the error (e.g., reinitialize the file, log the error, etc.)\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump({}, f)\n",
        "        return str(summary_chain[-1])\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article4: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        # Create the file if it doesn't exist\n",
        "        file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump({}, f)\n",
        "                \n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory.keys():\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"] = {\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "        return str(summary_chain[-1])\n",
        "    return str(summary_chain[-1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ast import In\n",
        "from html import entities\n",
        "from arrow import get\n",
        "from celery import chunks\n",
        "from colorama import init\n",
        "from flask import g\n",
        "import openai\n",
        "import json\n",
        "from datetime import datetime\n",
        "from openai.api_resources import OpenAIError\n",
        "from pydantic import ValidationError, validate_arguments\n",
        "from sqlalchemy import all_\n",
        "from torch import ge\n",
        "import re\n",
        "\n",
        "class SummaryInfo(BaseModel):\n",
        "    topic: str = Field(..., description=\"The topic of the text\")\n",
        "    hypothetical_questions: List[str] = Field(default_factory=list, description=\"List of hypothetical questions.\")\n",
        "class SummaryStore:\n",
        "    def __init__(self, file_path=\"summaries.json\"):\n",
        "        self.file_path = file_path \n",
        "\n",
        "    def store(self, summary, entities, article, citation, info: SummaryInfo):\n",
        "        data = {  \n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"entities\": entities, \n",
        "            \"citation\": citation,\n",
        "            \"info\": info.model_dump(), # serialize Pydantic model           \n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        summaries = self.load()\n",
        "        summaries.append(data)\n",
        "\n",
        "        self._save(summaries)\n",
        "\n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        except (FileNotFoundError, json.JSONDecodeError): \n",
        "            return []\n",
        "    \n",
        "    def _save(self, content):\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "           json.dump(content, f)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_prompt(text, prompt_type=\"Enitity Dense\"):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    \n",
        "\n",
        "\n",
        "    return f\"{system_prompt}\\n\\n{text}\"\n",
        "def parse_response(response):\n",
        "    summary = response[\"summary\"] \n",
        "    # extract summary here\n",
        "    return summary\n",
        "\n",
        "client = OpenAIAPIClient()\n",
        "\n",
        "@validate_arguments\n",
        "def generate_summary(text: str, model: str = \"gpt-3.5-turbo\", temp: float = 0.3, max_tokens: int = 800):\n",
        "    \n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        pass\n",
        "    except OpenAIError as e:\n",
        "        # handling\n",
        "        pass\n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    list_of_all_entities = []\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            entities = get_entities(article)\n",
        "            list_of_all_entities.extend(entities) \n",
        "            missing_entities = [entity for entity in entities if entity not in summary_chain[-1]]\n",
        "            request = build_sumary_request(article, summary_chain[-1], missing_entities)\n",
        "            \n",
        "            new_summary = generate_summary(request)  \n",
        "            summary_chain.append(new_summary)\n",
        "        # remove duplicates\n",
        "        list_of_all_entities = list(set(list_of_all_entities))\n",
        "        return summary_chain[-1], list_of_all_entities\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], list(set(list_of_all_entities))\n",
        "    \n",
        "\n",
        "\n",
        "def get_entities(article):\n",
        "    # logic to extract entities from the article\n",
        "    # go through the each sentence of the article and find all entities (using prompt NER) \n",
        "    # you are looking for entities, keywords, and key phrases, and abstract concepts \n",
        "    return [\"entity1\", \"entity2\"]  \n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "\n",
        "def get_citation(article):\n",
        "    # logic to get citation for the article\n",
        "    # use a combination of the filename and or the reference section of the research report\n",
        "    extract_ref_section(article)\n",
        "    get_file_name \n",
        "    \n",
        "    return \"citation\"\n",
        "\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    topic = \"topic\"\n",
        "    questions = [\"question1\", \"question2\"]\n",
        "    return topic, questions\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article):\n",
        "    store = SummaryStore()\n",
        "    citation = get_citation(article)\n",
        "    # Break article into chunks of roughly 800 words, while preserving sentence boundaries\n",
        "    article_chunks = get_article_chunks(article)\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(chunk)\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "            info = SummaryInfo(topic=topic, hypothetical_questions=questions)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            store.store(summary=refined_sumary, entities=entities, article=chunk_name, citation=citation, info=info)\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None \n",
        "\n",
        "def get_article_list():\n",
        "    # logic to get list of articles and check if they have already been summarized\n",
        "    return [\"article1\", \"article2\"]\n",
        "\n",
        "article_list = get_article_list()\n",
        "for article in article_list:\n",
        "    # incrementally refine article summary if unsuccessful try again\n",
        "    while True:\n",
        "        success = Incrementally_Refine_Article_Summary(article)\n",
        "        if success:\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "# open summary.json to see the results \n",
        "with open(\"summaries.json\", \"r\") as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0%|                                                     | 0/3 [02:23<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/bin/nougat\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/predict.py\", line 167, in main\n",
            "    model_output = model.inference(\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nougat/model.py\", line 580, in inference\n",
            "    last_hidden_state = self.encoder(image_tensors)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/nougat/model.py\", line 123, in forward\n",
            "    x = self.model.layers(x)\n",
            "        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 413, in forward\n",
            "    x = blk(x)\n",
            "        ^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 295, in forward\n",
            "    attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/timm/models/swin_transformer.py\", line 179, in forward\n",
            "    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/.conda/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: MPS backend out of memory (MPS allocated: 8.06 GB, other allocations: 1.01 GB, max allowed: 9.07 GB). Tried to allocate 55.12 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
