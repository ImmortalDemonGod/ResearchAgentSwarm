{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'child' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install paper-qa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install git+https://github.com/blackadad/paper-scraper.git\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install sentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/ipykernel/zmqshell.py:658\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'child' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "!pip install paper-qa\n",
        "!pip install git+https://github.com/blackadad/paper-scraper.git\n",
        "!pip install sentence-transformers\n",
        "#!pip install -U angle-emb\n",
        "api_key = \"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\"\n",
        "import os\n",
        "from re import T\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!pip install langchain\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "model_name = \"ggrn/e5-small-v2\" # fast\n",
        "#model_name = \"WhereIsAI/UAE-Large-V1\" # slow\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "TOKENIZERS_PARALLELISM=True\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "!export DOI2PDF='https://sci-hub.ru/'\n",
        "os.environ['DOI2PDF'] = 'https://sci-hub.ru/'\n",
        "#os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import M\n",
        "from paperqa import Docs\n",
        "import os\n",
        "\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key, memory=True, embeddings=embeddings)\n",
        "\n",
        "# load the papers from Mitochondria Papers folder\n",
        "\n",
        "mito_papers = os.listdir('Mitochondria Papers/')\n",
        "\n",
        "for paper in mito_papers:\n",
        "    docs.add(\"Mitochondria Papers/\"+paper, chunk_chars=2500)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Query and print the answer\n",
        "answer = docs.query(\"What is the current understanding of the role of mitochondria in animal regeneration and aging, and what future research directions are being considered to harness these mechanisms for whole-body regeneration?\")\n",
        "print(answer.formatted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open(\"MitochondrialPapers.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "# load\n",
        "with open(\"MitochondrialPapers.pkl\", \"rb\") as f:\n",
        "    docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "from paperqa import Docs\n",
        "\n",
        "try:\n",
        "    docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "    print(\"Initialization successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Initialization failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "# Set the API key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# Initialize Docs with the API key\n",
        "#docs = Docs(llm='gpt-3.5-turbo', openai_api_key=api_key)\n",
        "import paperqa\n",
        "\n",
        "keyword_search = 'bispecific antibody manufacture'\n",
        "papers = paperscraper.search_papers(keyword_search)\n",
        "docs = paperqa.Docs(openai_api_key=api_key)\n",
        "for path,data in papers.items():\n",
        "    try:\n",
        "        #docs.add(path)\n",
        "        print(path, data['title'])\n",
        "    except ValueError as e:\n",
        "        # sometimes this happens if PDFs aren't downloaded or readable\n",
        "        print('Could not read', path, e)\n",
        "answer = docs.query(\"What manufacturing challenges are unique to bispecific antibodies?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import paperscraper\n",
        "papers = paperscraper.search_papers(query='bayesian model selection',\n",
        "                                    limit=10,\n",
        "                                    pdir='downloaded-papers')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nougat-ocr\n",
        "#$ nougat path/to/file.pdf -o output_directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large\")\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"studio-ousia/luke-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/electra-large-discriminator-finetuned-conll03-english\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "text = \"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from entities.\"\n",
        "ner_results = nlp(text)\n",
        "print(ner_results)\n",
        "# save to file txt\n",
        "with open('ner_results.txt', 'w') as f:\n",
        "    print(ner_results, file=f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import A\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "\n",
        "\n",
        "class InitialSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n",
        "    yet highly non-specific, containing little information beyond the entities marked as missing.\n",
        "    Use overly verbose languages and fillers (Eg. This text discusses) to reach ~80 words.\n",
        "    \"\"\"\n",
        "\n",
        "    summary: str = Field(\n",
        "        ...,\n",
        "        description=\"This is a summary of the text provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n",
        "    )\n",
        "\n",
        "\n",
        "class RewrittenSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n",
        "\n",
        "    Guidelines\n",
        "    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n",
        "    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
        "    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Text.\n",
        "    - Make space with fusion, compression, and removal of uninformative phrases like \"the text discusses\"\n",
        "    - Missing entities can appear anywhere in the new summary\n",
        "\n",
        "    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n",
        "    \"\"\"\n",
        "    summary: str = Field(\n",
        "        ...,\n",
        "        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n",
        "    )\n",
        "    absent: List[str] = Field(\n",
        "        ...,\n",
        "        default_factory=list,\n",
        "        description=\"This is a list of Entities found absent from the new summary that were present in the previous summary\",\n",
        "    )\n",
        "    missing: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"This is a list of 1-3 informative Entities from the Text that are missing from the new summary which should be included in the next generated summary.\",\n",
        "    )\n",
        "\n",
        "\n",
        "    @field_validator(\"missing\")\n",
        "    def has_missing_entities(cls, missing_entities: List[str]):\n",
        "        if len(missing_entities) == 0:\n",
        "            raise ValueError(\n",
        "                \"You must identify 1-3 informative Entities from the Text which are missing from the previously generated summary to be used in a new summary\"\n",
        "            )\n",
        "        return missing_entities\n",
        "\n",
        "    @field_validator(\"absent\")\n",
        "    def has_no_absent_entities(cls, absent_entities: List[str]):\n",
        "        absent_entity_string = \",\".join(absent_entities)\n",
        "        if len(absent_entities) > 0:\n",
        "            print(f\"Detected absent entities of {absent_entity_string}\")\n",
        "        return absent_entities\n",
        "    \n",
        "\n",
        "def summarize_text(text):\n",
        "        \"\"\"\n",
        "        Summarizes the text using OpenAI's GPT-3.5 Turbo model.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            system_message = \"\"\"# Mission\n",
        "Craft a Sparse Priming Representation (SPR) for a given set of information. The goal is to distill complex concepts into concise, context-rich statements that enable a Large Language Model (LLM) to reconstruct the original idea efficiently.\n",
        "\n",
        "# Persona\n",
        "You are an SPR Writer, skilled in synthesizing complex information into its essential elements. You have a talent for identifying the core of an idea and expressing it in a minimal, yet comprehensive way.\n",
        "\n",
        "# Guiding Principles\n",
        "- **Precision**: Focus on the most crucial elements of the information, ensuring each word adds significant value.\n",
        "- **Clarity**: Maintain clarity in your representations to avoid ambiguity.\n",
        "- **Context Preservation**: Retain enough context to enable accurate reconstruction of the original idea.\n",
        "- **Efficiency**: Aim for the minimal number of words to convey the concept without loss of meaning.\n",
        "\n",
        "# Task\n",
        "1. **Receive Information**: Begin with the user-provided content that needs to be compressed into an SPR.\n",
        "2. **Identify Key Concepts**: Analyze the content to pinpoint its fundamental ideas, themes, or messages.\n",
        "3. **Distill Information**: Transform these key concepts into short, complete sentences that encapsulate the essence of the content.\n",
        "4. **Contextual Embedding**: Ensure that these sentences include necessary context for understanding and reconstruction.\n",
        "5. **Review for Completeness**: Confirm that the SPR conveys the core idea effectively and is free of extraneous details.\n",
        "\n",
        "# Style\n",
        "- **Concise and Direct**: Use clear and straightforward language, avoiding unnecessary embellishments.\n",
        "- **Analytical**: Demonstrate a keen understanding of the essential aspects of the information.\n",
        "\n",
        "# Rules\n",
        "- **No Superfluous Details**: Eliminate any information that doesn’t contribute to understanding the core concept.\n",
        "- **Maintain Integrity of Original Idea**: Ensure that the SPR accurately represents the original content's intent and meaning.\n",
        "- **Brevity is Key**: Strive for the shortest possible representation without losing essential context or meaning.\n",
        "\n",
        "# Output Format\n",
        "\"YOUR COMPRESION HERE (DONT WASTE TIME STATING THIS IN AN SPR JUST WRITE)\"\"\"\n",
        "            user_message = text\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                temperature=0.3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                response_model=InitialSummary  # Adjust as needed\n",
        "            )\n",
        "            return response \n",
        "        except Exception as e:\n",
        "            print(f\"Error in summarizing text: {e}\")\n",
        "            return \"Summary not available.\" \n",
        "\n",
        "def summarize_article(article: str, summary_steps: int = 3):\n",
        "    summary_chain = []\n",
        "\n",
        "    summary = summarize_text(text=article)   \n",
        "    prev_summary = None\n",
        "    summary_chain.append(summary)\n",
        "    total_missing_entities = []\n",
        "    temp = 0.3\n",
        "    try:\n",
        "        for i in range(summary_steps):\n",
        "\n",
        "            missing_entity_message = (\n",
        "                []\n",
        "                if prev_summary is None\n",
        "                else [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"The summary MUST include these Missing Entities: {','.join(prev_summary.missing)}\",\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "            if prev_summary is not None:\n",
        "                total_missing_entities.extend(prev_summary.missing)\n",
        "            new_summary: RewrittenSummary = client.chat.completions.create( \n",
        "\n",
        "\n",
        "                model=\"gpt-3.5-turbo-0613\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"\n",
        "### Chain of Density Prompt for Creating Entity-Dense Summaries\n",
        "\n",
        "#### Prompt Introduction and Objective\n",
        "You are tasked with generating progressively concise and informative summaries of an text, focusing on including key entities. Begin with a summary that captures the essence of the text and then refine it in subsequent iterations to increase its density of relevant information.\n",
        "\n",
        "#### Revised Steps for Summary Creation\n",
        "1. **Initial Summary**: Start by writing a comprehensive summary of the text. Focus on capturing the core ideas of the text while maintaining clarity and conciseness. Avoid unnecessary verbosity.\n",
        "\n",
        "2. **Identifying Key Entities**:\n",
        "   - In each iteration, identify any key entities from the text that were not included in your previous summary.\n",
        "   - A key entity should be:\n",
        "     - **Relevant**: Directly related to the main story or themes of the text.\n",
        "     - **Specific**: Descriptive yet concise, ideally in 5 words or fewer.\n",
        "     - **Novel**: Not already included in your previous summary.\n",
        "     - **Faithful**: Accurately represented as in the text.\n",
        "\n",
        "3. **Refining the Summary**:\n",
        "   - Rewrite the summary to include the newly identified key entities. Aim for the same length as the previous summary, enhancing its density and informativeness.\n",
        "   - Employ techniques like fusion, compression, and the removal of redundant phrases to make room for new entities.\n",
        "   - Ensure that all previously included entities and details are retained in each new version of the summary.\n",
        "\n",
        "4. **Repetition and Refinement**:\n",
        "   - Repeat this process for a total of 5 iterations, each time enhancing the summary's entity density and conciseness.\n",
        "\n",
        "#### Guidelines for Effective Summary Writing\n",
        "- **Clarity and Cohesion**: Ensure each version of the summary is clear, cohesive, and can stand alone as a comprehensive overview of the text.\n",
        "- **Balanced Inclusion**: If space is limited, prioritize the most impactful entities for inclusion. Do not sacrifice clarity for the sake of adding more entities.\n",
        "- **Consistency**: Maintain a consistent approach to summarizing, ensuring that each iteration builds logically on the previous one.\n",
        "\n",
        "#### Expected Outcome\n",
        "By the end of the fifth iteration, you should have a highly concise, entity-dense summary that encapsulates the main points and key entities of the text in a clear and accessible manner.\n",
        "                    \"\"\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": f\"Here is the Text: {article}\"},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n",
        "                    },\n",
        "                    *missing_entity_message,\n",
        "                ],\n",
        "                max_retries=3, \n",
        "\n",
        "                temperature=temp,\n",
        "                max_tokens=1000,\n",
        "                response_model=RewrittenSummary,\n",
        "            )\n",
        "            summary_chain.append(new_summary.summary)\n",
        "            print(f\"Summary {i+1}: {new_summary.summary}\")\n",
        "            prev_summary = new_summary\n",
        "            temp = temp + 0.3\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article1: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        # Create the file if it doesn't exist\n",
        "        file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "        # Check if file exists and is not empty\n",
        "        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump({}, f)  # Initialize with an empty dictionary\n",
        "                \n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory.keys():\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"] = {\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "        return str(summary_chain[-1])\n",
        "         \n",
        "\n",
        "    file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "    # Check if file exists and is not empty\n",
        "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump({}, f)  # Initialize with an empty dictionary\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory:\n",
        "                # Ensure summary_chain is a list\n",
        "                if not isinstance(memory[\"summary_chain\"], list):\n",
        "                    memory[\"summary_chain\"] = []\n",
        "\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                # Initialize summary_chain as a list with the first element\n",
        "                memory[\"summary_chain\"] = [{\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}]\n",
        "\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error reading JSON file. The file may be corrupted or improperly formatted.\")\n",
        "        # Handle the error (e.g., reinitialize the file, log the error, etc.)\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump({}, f)\n",
        "        return str(summary_chain[-1])\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article4: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        # Create the file if it doesn't exist\n",
        "        file_path = \"memory/Shared_agent_memory.json\"\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump({}, f)\n",
        "                \n",
        "        with open(file_path, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "            if \"summary_chain\" in memory.keys():\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"].append({\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp})\n",
        "            else:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                memory[\"summary_chain\"] = {\"value\": summary_chain[-1], \"missing_entities\": total_missing_entities, \"timestamp\": timestamp}\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(memory, f)\n",
        "        return str(summary_chain[-1])\n",
        "    return str(summary_chain[-1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'file_id': '6f37373983f54caf8eae14058cb786c8', 'file_path': '/home/epas/Programming/ResearchAgentSwarm/Literature Review/gpt_researcher_outputs/6f37373983f54caf8eae14058cb786c8.md'}\n",
            "Success: True for 6f37373983f54caf8eae14058cb786c8\n",
            "Success: True for f8e2a08ffd5149d6a128d23d772abc38\n",
            "Success: True for 15c0aba9ec7a4a2d99fd2a35dad345ed\n",
            "Success: True for 19bc86d438fd4d06b14a99c32595b0ad\n",
            "Success: True for 6a666e95c0674c2d84596d6065a4dad6\n",
            "Success: True for 6a6833e6bf9b4723b80b992cfd1c3c09\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from pydantic import ValidationError, validate_arguments\n",
        "import os\n",
        "import json\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "OUTPUT_FOLDER = \"/home/epas/Programming/ResearchAgentSwarm/Literature Review/gpt_researcher_outputs/\" \n",
        "SUMMARY_JSON = \"summaries.json\"\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "# Optionally set the environment variable (if needed elsewhere)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "class SummaryInfo(BaseModel):\n",
        "    topic: str = Field(..., description=\"The topic of the text\")\n",
        "    hypothetical_questions: List[str] = Field(default_factory=list, description=\"List of hypothetical questions.\")\n",
        "\n",
        "# Enum for prompt types\n",
        "    \n",
        "class SummaryStore:\n",
        "    def __init__(self, file_path=SUMMARY_JSON): \n",
        "        self.file_path = file_path\n",
        "        self._create_file_if_not_exists()\n",
        "    \n",
        "    def _create_file_if_not_exists(self):\n",
        "        if not os.path.exists(self.file_path):\n",
        "            # Initialize empty list\n",
        "            empty_data = [] \n",
        "            self._save(empty_data)\n",
        "    \n",
        "    def store(self, summary, entities, file_id, article, references, topic, hypothetical_questions):\n",
        "        data = { \n",
        "            \"file_id\": file_id,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary,\n",
        "            \"entities\": entities, \n",
        "            \"references\": references,\n",
        "            \"topic\": topic,\n",
        "            \"hypothetical_questions\": hypothetical_questions,         \n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        summaries = self.load()\n",
        "        summaries.append(data)\n",
        "\n",
        "        self._save(summaries)\n",
        "\n",
        "    def load(self):\n",
        "        try:\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            # If load fails, initialize empty file\n",
        "            self._create_file_if_not_exists() \n",
        "            return []\n",
        "    \n",
        "    def _save(self, content):\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "           json.dump(content, f)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def build_system_prompt(prompt_type: str):\n",
        "    # read from file \"entity_dense_prompt.md\"\n",
        "    if prompt_type == \"Enitity Dense\":\n",
        "        with open(\"entity_dense_prompt.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"SPR\":\n",
        "        with open(\"sparse_prime_representation.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Entities\":\n",
        "        with open(\"get_entities.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Topic\":\n",
        "        with open(\"get_topic.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "    if prompt_type == \"Get Hypothetical Questions\":\n",
        "        with open(\"get_hypothetical_questions.md\", \"r\") as f:\n",
        "            system_prompt = f.read()\n",
        "\n",
        "    return f\"{system_prompt}\"\n",
        "def parse_response(response):\n",
        "\n",
        "    # Get the text content from the single completion \n",
        "    completion = response.choices[0]\n",
        "    text = completion.message.content\n",
        "\n",
        "    # Remove unnecessary newlines and whitespace    \n",
        "    text = text.strip()  \n",
        "\n",
        "    # Could add additional parsing logic here \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def generate_summary(text: str, summary_type: str, model: str = \"gpt-3.5-turbo-0613\", temp: float = 0.45, max_tokens: int = 800 ):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    if not text:\n",
        "        raise ValueError(\"Text cannot be empty\")\n",
        "\n",
        "    if temp < 0 or temp > 1:\n",
        "       raise ValueError(\"Temperature should be between 0 and 1\")\n",
        "    \n",
        "    try: \n",
        "        # summarization code\n",
        "        if summary_type == \"Entity Dense\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='Enitity Dense')}\")\n",
        "            system_prompt = build_system_prompt(prompt_type=\"Enitity Dense\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Enitity Dense\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        if summary_type == \"SPR\":\n",
        "            #print(f\"System Prompt: {build_system_prompt(prompt_type='SPR')}\")\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                temperature=temp,\n",
        "                max_tokens=max_tokens,\n",
        "                max_retries=3,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"SPR\")},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "            )\n",
        "        summary = parse_response(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Occured in generate_summary function\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None\n",
        "    \n",
        "    if not summary:\n",
        "        raise RuntimeError(\"Summary generation failed\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def get_entity_dense_sumary(article, initial_summary, num_iterations=3):\n",
        "    summary_chain = [initial_summary]\n",
        "    \n",
        "    list_of_all_entities = []\n",
        "    entities = get_entities(article)\n",
        "    list_of_all_entities.append(entities) \n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            missing_entities = [entity for entity in entities if entity not in summary_chain[-1]]\n",
        "            condensed_entities = generate_summary(text=\",\".join(missing_entities), summary_type=\"SPR\")\n",
        "            request = build_sumary_request(article, summary_chain[-1], condensed_entities)\n",
        "            new_summary = generate_summary(text=request, summary_type=\"Entity Dense\")  \n",
        "            summary_chain.append(new_summary)        \n",
        "        return summary_chain[-1], list_of_all_entities\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarizing article: {e}\\n Using last summary\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return summary_chain[-1], list_of_all_entities\n",
        "    \n",
        "\n",
        "def get_entities(article: str, model=\"gpt-3.5-turbo-0613\"):\n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "\n",
        "    if not article:\n",
        "        raise ValueError(\"Article text cannot be empty\")\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    sentences = split_to_sentences(article)\n",
        "        \n",
        "    chunk_size = 5\n",
        "    overlap = 1\n",
        "    \n",
        "    for i in range(0, len(sentences), chunk_size-overlap): \n",
        "        start = i\n",
        "        end = i + chunk_size\n",
        "        if end > len(sentences):\n",
        "            end = len(sentences)\n",
        "            \n",
        "        chunk = sentences[start:end]\n",
        "        chunk_text = \". \".join(chunk)\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Entities\")},\n",
        "                        {\"role\": \"user\", \"content\": chunk_text}\n",
        "                    ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            entities.extend(_parse_entities(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting entities: {e}\")\n",
        "            # Break out of the loop if there is an error\n",
        "            return None\n",
        "        \n",
        "    return list(set(entities))\n",
        "    \n",
        "\n",
        "def _parse_entities(response):\n",
        "    # Parses the generated response to extract a list of entity strings\n",
        "    entities = [] \n",
        "    entity_text =  parse_response(response)\n",
        "    #print(f'Entity text: {entity_text}')\n",
        "\n",
        "    # Naive splitting on commas for example output \n",
        "    entities = [e.strip() for e in entity_text.split(\",\")] \n",
        "    entities = [e for e in entities if e]\n",
        "    \n",
        "    return entities\n",
        " \n",
        "\n",
        "def build_sumary_request(article, prev_summary, missing_entities):\n",
        "\n",
        "    request = f\"Article: {article}\\n\\n\"\n",
        "    request += f\"Previous Summary: {prev_summary}\\n\\n\" \n",
        "    request += f\"Missing Entities: {missing_entities}\\n\\n\"\n",
        "    return request\n",
        "\n",
        "def split_to_sentences(text):\n",
        "    # logic to split text into sentences \n",
        "    return re.split(r\"[.!?]\\s\", text)\n",
        "\n",
        "   \n",
        "def get_article_chunks(article, chunk_size=800 ):\n",
        "    total_words = count_words(article) \n",
        "    if total_words <= chunk_size:\n",
        "        return [article]\n",
        "    \n",
        "    sentences = split_to_sentences(article)\n",
        "    \n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    curr_len = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_words = count_words(sentence)  \n",
        "        if curr_len + sentence_words < chunk_size:\n",
        "            # add sentence if under chunk size\n",
        "            current_chunk.append(sentence)\n",
        "            curr_len += sentence_words \n",
        "        else:\n",
        "            # otherwise save chunk and reset\n",
        "            chunks.append(\" \".join(current_chunk)) \n",
        "            current_chunk = [sentence]\n",
        "            curr_len = sentence_words\n",
        "            \n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "        \n",
        "    return chunks\n",
        "import re\n",
        "\n",
        "def extract_references(file_path):\n",
        "\n",
        "    with open(file_path) as f:\n",
        "        text = f.read() \n",
        "\n",
        "    start_idx = text.find(\"## References\")\n",
        "\n",
        "    if start_idx >= 0:\n",
        "        refs = text[start_idx:]\n",
        "        refs = refs.replace(\"## References\", \"\")\n",
        "        return refs\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_info(summary):\n",
        "    # NLP logic to extract topic and hypothetical questions \n",
        "    client = instructor.patch(OpenAI(api_key=api_key))\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            temperature=0.4,\n",
        "            max_retries=3,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Topic\")},\n",
        "                {\"role\": \"user\", \"content\": summary}\n",
        "            ],\n",
        "        )\n",
        "        topic = parse_response(response)\n",
        "        #print(f'Topic: {topic}')\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            temperature=0.4,\n",
        "            max_retries=3,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": build_system_prompt(prompt_type=\"Get Hypothetical Questions\")},\n",
        "                {\"role\": \"user\", \"content\": summary}\n",
        "            ],\n",
        "        )\n",
        "        questions = parse_response(response)\n",
        "        #print(f'Questions: {questions}')\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extracting questions: {e}\")\n",
        "        # Break out of the loop if there is an error\n",
        "        return None, None\n",
        "    return topic, questions\n",
        "\n",
        "def Incrementally_Refine_Article_Summary(article_info):\n",
        "\n",
        "    file_id = article_info[\"file_id\"]\n",
        "    file_path = article_info[\"file_path\"]\n",
        "    \n",
        "    store = SummaryStore() \n",
        "\n",
        "    references = extract_references(file_path)\n",
        "    #print(f\"References: {references}\")\n",
        "\n",
        "    article_text = open(file_path).read()\n",
        "    article_chunks = get_article_chunks(article_text)\n",
        "\n",
        "    try:\n",
        "        chunk_num = 0\n",
        "        for chunk in article_chunks:\n",
        "            # Generate an initial summary for each chunk\n",
        "            initial_summary = generate_summary(text=chunk, summary_type=\"SPR\")\n",
        "            \n",
        "            # Generate a refined summary for each chunk\n",
        "            refined_sumary, entities = get_entity_dense_sumary(chunk, initial_summary)\n",
        "\n",
        "            # Extract the topic and hypothetical questions from the refined summary\n",
        "            topic, questions = extract_info(refined_sumary)\n",
        "\n",
        "            # Store the summary, entities, and citation\n",
        "            chunk_name = f\"Chunk # {chunk_num}.\\n{chunk}\"\n",
        "            store.store(summary=refined_sumary, file_id=file_id, entities=entities, article=chunk_name, references=references, topic=topic, hypothetical_questions=questions)\n",
        "            chunk_num += 1\n",
        "        # return success\n",
        "        return True\n",
        "\n",
        "    except Exception as e: \n",
        "        print(f\"Error summarizing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import codecs\n",
        "\n",
        "def is_bibliography(file_path):\n",
        "\n",
        "    with codecs.open(file_path, 'rb') as f:\n",
        "        first_line = f.readline()\n",
        "        if b'# Bibliography Recommendation Report:' in first_line:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_article_list():\n",
        "\n",
        "    articles = []\n",
        "    \n",
        "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
        "        file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
        "        \n",
        "        # NEW CHECK \n",
        "        if is_bibliography(file_path):\n",
        "            continue\n",
        "            \n",
        "        if file_name.endswith(\".md\"):\n",
        "           \n",
        "            file_id = get_file_id(file_name)\n",
        "\n",
        "            info = {\n",
        "                \"file_id\": file_id, \n",
        "                \"file_path\": file_path\n",
        "            }\n",
        "\n",
        "            articles.append(info)\n",
        "\n",
        "    summarized_ids = set()\n",
        "    if os.path.exists(SUMMARY_JSON):\n",
        "        data = json.load(open(SUMMARY_JSON)) \n",
        "        summarized_ids = {item[\"file_id\"] for item in data}\n",
        "        \n",
        "    not_summarized = [a for a in articles if a[\"file_id\"] not in summarized_ids]\n",
        "   \n",
        "    return not_summarized\n",
        "\n",
        "def get_file_id(file_name):\n",
        "    # Extract base name without extension\n",
        "    return os.path.splitext(file_name)[0]\n",
        "\n",
        "article_list = get_article_list()\n",
        "print(article_list[0])\n",
        "\n",
        "\n",
        "# Loop through each article and generate a summary\n",
        "for article in article_list:\n",
        "    success = Incrementally_Refine_Article_Summary(article)\n",
        "    print(f\"Success: {success} for {article['file_id']}\")\n",
        "    \n",
        "# open summary.json to see the results \n",
        "\n",
        "with open(SUMMARY_JSON, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nougat '/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/Mitochondria Papers/izawa2017.pdf' -o \"/Users/tomriddle1/Documents/GitHub/ResearchAgentSwarm/swarm_files\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ecdPw0gF6Q0",
        "D30JwOfQWT_u",
        "zv0KV0sUFH4Z",
        "0yeeQ2K-GD10",
        "snYDZh2OG6ns",
        "1h43Eh0PSDHb",
        "F9zcFI2FHiIJ",
        "2g8Xb9JmIq3f",
        "KNp6_8aRJa-m",
        "D9qdcO7XKY35",
        "Kdis0QlPJ3-k",
        "KUbjOcq8LR0A",
        "x0zLIv1i75gJ",
        "tWpIbwqlLkKH",
        "mdwmgmy2MCxt",
        "DBbpIf8EOmCH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
